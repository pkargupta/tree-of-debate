import json


data = []

focus = """The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning ###### Abstract Large Language Models (LLMs) have shown enhanced capabilities of solving novel tasks by reasoning step-by-step known as Chain-of-Thought (CoT) reasoning; how can we still the same capability of reasoning step-by-step on unseen tasks into LMs that possess less than <100B parameters? To address this question, we first introduce the CoT Collection, a new instruction-tuning dataset that augments 1.88 million CoT rationales across 1,060 tasks. We show that continually fine-tuning Flan-T5 (3B & 11B) with the CoT Collection enables the 3B & 11B LMs to perform CoT better on unseen tasks, leading to an improvement in the average zero-shot accuracy on 27 datasets of the BIG-Bench-Hard benchmark by +4.34% and +2.44%, respectively. Furthermore, we show that instruction tuning with CoT allows LMs to possess stronger few-shot learning capabilities, resulting in an improvement of +2.97% and +2.37% on 4 domain-specific tasks over Flan-T5 (3B & 11B), respectively. We make our CoT Collection data and our trained models publicly available at [https://github.com/kaist-lklab/CoT-Collection](https://github.com/kaist-lklab/CoT-Collection). ## 1 Introduction Recent works show that Large Language Models (LLMs) pre-trained on extensive text corpora, can adapt to downstream tasks in both few-shot and zero-shot learning settings by incorporating task instructions and demonstrations (Raffel et al., 2020; Brown et al., 2020; Zhang et al., 2022; Chowdhery et al., 2022; Lampinen et al., 2022; Ye et al., 2023). One approach that has been particularly effective in enabling these LLMs to excel at a multitude of tasks is Chain-of-Thought (CoT) prompting, generating a CoT rationale to derive the answer (Wei et al., 2022; Kojima et al., 2022). However, these advancements present two main challenges: LLMs typically require more than 100 billion parameters, leading to significant computational cost, scalability, and accessibility issues (Kaplan et al., 2020; Liu et al., 2022; Min et al., 2022; Mhlanga, 2023; Li et al., 2023). Moreover, while CoT prompting proves to be powerful for LLMs, it does not necessarily confer the same benefits to smaller models as well (Tay et al., 2022; Suzgun et al., 2022; Wei et al., 2022; Chung et al., 2022). Addressing these challenges, subsequent research has focused on empowering relatively smaller LMs to effectively solve novel tasks as well, primarily through improving zero-shot generalization capabilities with instruction tuning (Wei et al., 2021; Sanh et al., 2021; Mishra et al., 2022; Wang et al., 2022; Chung et al., 2022; Longpre et al., 2023). Nevertheless, the community still lacks a comprehensive strategy to fully leverage CoT prompting to solve multiple unseen tasks in the context of smaller language models. While some attempts have been made towards this goal such as CoT fine-tuning for single target tasks (Shridhar et al., 2022; Ho et al., 2022; Fu et al., 2023), these approaches do not adequately address the issue of generalization to a broad range of unseen tasks. To bridge this gap, we present the CoT Collection, an instruction tuning dataset that augments 1.88 million CoT rationales across 1,060 tasks extracted from the FLAN Collection (Longpre et al., 2023). Each instance is composed of an instruction appended to the input instance, the ground truth output, and a supporting CoT rationale. By leveraging CoT fine-tuning with instruction tuning, we aim to endow smaller LMs with enhanced reasoning capabilities to effectively solve novel tasks. We first conduct a comprehensive examination of the quality, diversity, and reproducibility inherent in the CoT Collection. The findings suggest that our CoT rationale data stands out as being reliable, less redundant, logical, and notably informative when contrasted with nine instances of human-authored CoT rationale data used in the training of FLAN-T5 (Chung et al., 2022). This is achieved while maintaining a varied grammatical syntax. Moreover, while we deploy in-context learning (ICL) through LLMs for our dataset generation, we show that the assembly of our dataset is not anchored solely to a single API service. Our model, **C2F2**, is obtained by continually fine-tuning FLAN-T5 with CoT Collection, exhibiting significant improvements in their zero-shot ability to perform CoT prompting on unseen tasks, thereby leading to a +4.34% and +2.44% improvement in average across 3B and 11B models on the Big Bench Hard (BBH) benchmark (Suzgun et al., 2022). Moreover, we also show that our approach of continually fine-tuning instruction-tuned models with CoT Collection, holds effective on smaller data scale (P3 evaluation benchmark Sanh et al., 2021)) and on multilingual settings (MGSM benchmark (Shi et al., 2022)) as well. Further to the enhancements observed in zero-shot performance, we also find that our **C2F2** model also serves as a better base model in a few-shot learning setting, where LMs must quickly adapt to new tasks based on a minimal number of instances (Liu et al., 2022). To explore this, we apply Lora (Hu et al., 2021), a prominent parameter-efficient fine-tuning (PEFT) method, to **C2F2** for CoT fine-tuning on the target task. This process involves the initial generation CoT rationale data for the target task, followed by CoT fine-tuning of **C2F2** using the generated CoT rationale data. We assess the efficacy of our approach on 4 domain-specific datasets, two each from legal and medical fields, namely including LEDGAR (Tuggener et al., 2020), Case Hold (Zheng et al., 2021), MedNLI (Romanov and Shivade, 2018), and PubMedQA (Jin et al., 2019). Each dataset is represented by 64 randomly chosen instances. Interestingly, **C2F2** exhibits a +2.37% performance improvement compared to full direct fine-tuning using FLAN-T5, while concurrently updating 2,352x fewer parameters. Moreover, it demonstrates a +13.98% improvement over ChatGPT (OpenAI, 2022) leveraging ICL with demonstrations up to the maximum input length. The empirical results of **C2F2** suggest that a synergy between CoT fine-tuning and instruction tuning could potentially result in improvements of smaller models. Whereas previous work demonstrated that CoT prompting and CoT fine-tuning work effectively on reasoning tasks (Wei et al., 2022; Chuang et al., 2022), our results indicate that it could also benefit smaller models in the context of zero-shot and few-shot learning. Especially in the context of zero-shot and few-shot learning, our analysis indicates that a simple recipe of extracting rich supervision from existing datasets (CoT rationale data) and inducing CoT abilities with CoT fine-tuning could further enhance existing LMs. Through CoT Collection, we hope to illustrate the potential of utilizing CoT rationales in improving task generalization and provide benefits to future research. ## 2 Related Works ### Chain-of-Thought (CoT) Prompting Wei et al. (2022) proposed Chain of Thought (CoT) Prompting, a technique that triggers the model to generate a rationale before the answer. By generating a rationale, LLMs could improve their reasoning abilities to solve challenging tasks such as arithmetic reasoning, commonsense reasoning, and symbolic reasoning. Wang et al. (2022) show that taking a majority vote with multiple rationales makes up the weakness of an LLM generating a single incomplete rationale. Kojima et al. (2022) shows that by appending the phrase 'Let's think step by step', LLMs could perform CoT in a zero-shot setting. While LLMs could solve novel tasks with CoT Prompting, the effectiveness does not hold for smaller LMs as well (Chung et al., 2022). ### Instruction Tuning & CoT Fine-tuning Previous work show that instruction tuning enables generalization to unseen tasks (Wei et al., 2021; Sanh et al., 2021; Aribandi et al., 2021; Ouyang et al., 2022; Wang et al., 2022; Xu et al., 2022). Different works tried to improve Instruction Tuning by enabling cross-lingual generalization (Muennighoff et al., 2022), improving label generalization capability (Ye et al., 2022), showing models could be lifelong learners via continual learning (Scialom et al., 2022), and training modular expert LMs (Jang et al., 2023). Meanwhile, a line of works shows that CoT fine-tuning could improve the reasoning abilities of LMs on a single-seen task (Zelikman et al., 2022; Shridhar et al., 2022; Ho et al., 2022; Huang et al., 2022; Fu et al., 2023). As a follow-up study, we continually CoT fine-tune 1,060 instruction tasks from our CoT Collection and observe a significant improvement in terms of zero-shot generalization on multiple tasks. ## 3 The CoT Collection While previous work has demonstrated that CoT fine-tuning is effective for improving both smaller LMs and LLMs (Zelikman et al., 2022; Shridhar et al., 2022; Chung et al., 2022; Ho et al., 2022; Wang et al., 2022; Huang et al., 2022; Fu et al., 2023), CoT rationale data still remains scarce up to this day. This is mainly due to the difficulty in gathering human-authored explanations (Kim et al., 2023). Specifically, to the best of our knowledge, most of the work investigating CoT fine-tuning use the 9 publicly available NLP datasets as CoT rationales, namely QASC (Khot et al., 2020), AQuA (Amini et al., 2019), GSM8K (Cobbe et al., 2021), QED (Lamm et al., 2021), StrategyQA (Geva et al., 2021), SenseMaking (Wang et al., 2019), CREAK (Onoe et al., 2021), e-SNLI (Camburu et al., 2018), ECQA (Aggarwal et al., 2021). CoT Collection is an instruction-tuning dataset including 1.88 million CoT rationales obtained across 1,060 NLP tasks1. Figure 1 illustrates how CoT Collection consists. Throughout the section, we first explain our process of augmenting CoT rationales (Section 3.1). Then, we perform an analysis regarding the quality, diversity, and reproducibility of CoT Collection (Section 3.2). Footnote 1: Following Sanh et al. (2021), we use the notion of ‘task’ referring to each prompt applied to a dataset. ### CoT Rationale Augmentation Given an input \(X=[I,z]\) composed of an instruction \(I\), and an instance \(z\) along with the answer \(y\), we obtain a CoT rationale \(r\) by applying ICL with LLMs. Note that this differs from previous works which mostly focused on generating new instances using LLMs (West et al., 2022; Liu et al., 2022; Honovich et al., 2022; Wang et al., 2022). Source Dataset SelectionAs a source dataset to extract CoT Rationales, we choose the FLAN Collection (Longpre et al., 2023), consisting of 1,836 tasks from P3 (Sanh et al., 2021), SuperNaturalInstructions (Wang et al., 2022), FLAN (Wei et al., 2021), some additional dialogue and code datasets. Then we choose 1,060 tasks, narrowing our focus following the criteria as follows: * We exclude multilingual datasets including translation tasks as our base model, T5 (Raffel et al., 2020) has a lower coverage for some tokens in under-represented languages. Therefore, we focus on English datasets only. Figure 1: An illustration of the overall task group and dataset source of where we obtained the instance to augment CoT rationales in CoT Collection. Compared to the 9 datasets that provide publicly available CoT rationales (namely ‘FLAN-T5 MCQA’, ‘FLAN-T5 ExQA’, ‘FLAN-T5 NLI’, ‘FLAN-T5 Arithmetic’), we generate x52.36 more rationales (1.88 million rationales) and x177.78 more task variants (1,060 tasks). * We exclude a subset of generation tasks with long-form outputs since the total sum of generated CoT rationales and the ground truth answer exceeds the output token length during our experiments. * We exclude some datasets that are not publicly currently available, but plan to update as soon as they become available. * We exclude some datasets where the input and output do not correspond to each other, but plan to update as soon as it is resolved. * When there is an dataset overlap across P3, SuperNaturalInstructions (SNI), and FLAN, we choose to use the tasks from one of them. * During preliminary experiments, we find that the CoT rationale generated by LLMs remains either uninformative and very short (e.g., 'It's obvious') for some tasks such as sentiment analysis, sentence completion, coreference resolution, word disambiguation. In order to prevent negative task transfer abundant during multitask learning (Aribandi et al., 2021; Jang et al., 2023), we exclude these datasets. Prompt CreationTo create a prompt to apply ICL with LLMs, preparing demonstrations \(\mathcal{D}^{t}\) for each task \(t\) is most straightforward, but it becomes unfeasible to prepare demonstrations for every task as the number of tasks gets larger. Instead, we use a more efficient approach, by assigning each task \(t\) to \(T_{k}\), a family of tasks using a similar task format. Each family of tasks share \(\mathcal{D}^{T_{k}}\), a 6 \(\sim 8\) shot demonstration. To create \(\mathcal{D}^{T_{k}}\), 3 of the authors manually go through multiple rounds of creation and revision. Specifically, given multiple instances sampled from FLAN Collection, two people are assigned to generate a CoT rationale explaining how the answer could be derived given the instruction and instance. Then, the other person gets to conduct an A/B testing, choosing the better CoT rationale. With this process, we manually create 135 CoT rationales across 26 task groups. We include our prompts for different task groups in Appendix C. CoT Rationale AugmentationThe main consideration of the augmentation process is to generate consistent CoT rationales. To do this, we use the OpenAI Codex model. Formally, given (\(x_{i}^{t}\), \(y_{i}^{t}\)), the \(i^{th}\) instance and label of a task \(t\), the goal is to generate corresponding CoT rationale \(r_{i}^{t}\). Note that during preliminary experiments, we found that ordering the label \(y^{k_{i}}\) in front of the rationale \(r^{k_{i}}\) within the demonstration \(\mathcal{D}^{T_{k}}\) was crucial to generate good quality rationales. We conjecture this is because providing the label \(y^{k_{i}}\) in front of the rationale \(r^{k_{i}}\) loosens the need for the LLM to solve the underlying task. However, we also found that in some tasks such as arithmetic reasoning, LLMs fail to generate good quality CoT rationales, which we provide in Appendix A. FilteringAfter generating multiple CoT rationales, we filter to ensure high quality. We apply the following criteria: * We exclude CoT rationales that do not include the ground truth answer at least once. While a CoT rationale that doesn't include the answer isn't necessarily a bad rationale, we found it is effective to exclude inconsistent CoT rationales. * We exclude CoT rationales that have too long token length, where we constrain with a number of 256 tokens. * We exclude CoT rationales that are identical to previously augmented ones. ### Analysis of CoT Collections Quality of RationalesTo ensure the quality of CoT Collections, we use ROSCOE (Golovneva et al., 2022), a suite of metrics designed to evaluate rationales under different criteria within semantic alignment, semantic similarity, logical inference, language coherence. Since we do not have a \begin{table} \begin{tabular}{l c|c c} \hline \hline & Metrics & Human & CoT Collection \\ \hline \hline \multirow{4}{*}{Semantic} & faithfulness & 0.8836 & **0.8914** \\ & faithfulness\_ww & 0.8756 & **0.8793** \\ Alignment & repetition\_word & 0.9376 & **0.9419** \\ & informativeness\_step & 0.9519 & **0.9521** \\ \hline Semantic & informativeness\_chain & 0.2295 & **0.2797** \\ Similarity & repetition\_sent & 0.2453 & **0.2910** \\ \hline Logical & discourse\_representation & **0.4855** & 0.4687 \\ Inference & coherence\_step\_vs\_step & 0.7763 & **0.7813** \\ \hline \multirow{4}{*}{Language Coherence} & perplexity\_step & **0.0198** & 0.0122 \\ & perplexity\_chain & **0.0475** & 0.0255 \\ \cline{1-1} & perplexity\_step\_max & **0.0144** & 0.0088 \\ \cline{1-1} & grammar\_step\_max & **0.8883** & 0.8721 \\ \cline{1-1} & grammar\_step\_max & **0.8013** & 0.7724 \\ \hline \hline \end{tabular} \end{table} Table 1: Comparison of the quality between human-authored rationales and machine-generated rationales. 13 label-free metrics from ROSCOE (Golovneva et al., 2022) is used. ground truth CoT rationale label, we exclude metrics that require a label. We compare with the 165 CoT rationales manually handcrafted for prompting LLMs. The 13 ROSCOE scores are shown in Table 1. The results show that CoT Collection include CoT rationales that are faithful, less repetitive, informative and logical even when compared to human-authored CoT rationales. Diversity of RationalesTo take a look into the diversity of our CoT rationale data, we use Berkeley Neural Parser Kitaev and Klein (2018); Kitaev et al. (2019) to parse the CoT rationales. More specifically, the verb which is closest to the root of the parse tree along the noun object is extracted. We compare with the rationales from the 9 CoT datasets used in Chung et al. (2022). As shown in Figure 2, the 9 CoT data has a high proportion assigned to 'answer question' and 'consider following' whereas the CoT rationales in our data have diverse textual formats included. Is CoT Rationale Bootstrapping Over-reliant on OpenAI API?One could doubt reproducibility of CoTCollection due to usage of a OpenAI model in the process of CoT rationale augmentation. Nevertheless, we open-source a _fixed_ rationale dataset we generated. Also, we emphasize that our choice of OpenAI's Codex was due to limited academic budget. Moreover, we also provide an analysis of the rationales generated by other LLMs besides OpenAI's Codex similar quality at Appendix B, ensuring that our overall pipeline and model are not over-reliant on OpenAI. ## 4 Zero-shot Generalization In this section, we show how CoT fine-tuning on CoT Collection could effectively improve the LM's ability to solve unseen tasks. Experiment #1: FLAN-T5 SettingWe continually fine-tune FLAN-T5 Chung et al. (2022) with CoT Collection, obtaining our proposing model, **C2F2**, which we evaluate on the BBH benchmark Suzgun et al. (2022). In addition to FLAN-T5, we compare the performance with different baselines such as (1) T5-LM Raffel et al. (2020): the original base model of FLAN-T5, (2) T0 Sanh et al. (2021): an instruction-tuned LM trained with P3 instruction dataset, (3) Tk-Instruct Wang et al. (2022), an instruction-tuned LM trained with SNI dataset, and (4) GPT-3 Brown et al. (2020): a pre-trained LLM with 175 billion parameters. Moreover, to ablate if using CoT fine-tuning is more data efficient compared to conventional instruction tuning, we train T5-LM with CoT Collection (denoted as 'T5 + CoT Fine-tuning'). Note that FLAN Collection includes 15M instances, hence x7.98 larger compared to our CoT Collection. The results are shown in Table 2. When continu Figure 2: The top 20 common root verbs (inner circle) and their top 4 noun objects (outer circle) within the rationales of the 9 CoT tasks used in Chung et al. (2022) (left side) and our rationale training data (right side). ally training FLAN-T5-3B and FLAN-T5-11B with CoT Collection, **C2F2**(3B & 11B) achieves a +4.34% and +2.44% improvement over FLAN-T5 in terms of CoT evaluation performance. Surprisingly, even if CoT Collection doesn't include any direct instruction data, **C2F2-11B**'s direct performance slightly improves, resulting in a +1.98% total average improvement. This further supports that continually training an instruction-tuned model with additional CoT instruction data could LMs to adapt to unseen tasks. In terms of data efficiency, T5-11B + CoT Fine-tuning and T5-3B + CoT Fine-tuning outperforms FLAN-T5-11B and FLAN-T5-3B by a +3.89% and +1.45% margin, respectively, when evaluated with CoT prompting. Also, T5-3B + CoT Fine-tuning outperforms 4x larger models such as T0-11B and Tk-Instruct-11B in both direct and CoT evaluation. We conclude that CoT fine-tuning enables to achieve superior performance even when utilizing a small amount of data compared to direct instruction tuning. Experiment #2: T0 SettingTo test if CoT Fine-tuning using CoT Collection holds effective with less amount of tasks, we choose use the P3 \begin{table} \begin{tabular}{l|c c c} \hline \hline \multicolumn{1}{c}{**Method**} & **CoT** & **Direct** & **Total Avg** \\ \hline TS-LM-3B (Ruffie et al., 2020) & 26.68 & 26.96 & 26.82 \\ To-3B (Sanif et al., 2021) & 26.64 & 27.45 & 27.05 \\ Tk-Instruct-3B (Wang et al., 2022a) & 29.86 & 29.90 & 29.88 \\ Tk-Instruct-11B (Wang et al., 2022b) & 33.60 & 30.71 & 32.16 \\ To-11B (Sanif et al., 2021) & 31.83 & 33.57 & 32.70 \\ FLAN-T5-3B (Cheng et al., 2022) & 34.06 & 37.14 & 35.60 \\ GP1-1 (Yieng et al., 2020) & 38.30 & 33.60 & 38.30 \\ FLAN-T5-11B (Cheng et al., 2022) & 38.57 & 40.90 & 39.78 \\ \hline T5-3B + CoT Fine-tuning & 37.95 & 35.52 & 36.74 \\ **C2F2-3B** & 38.40 & 36.18 & 37.29 \\ T5-11B + CoT Fine-tuning & 40.02 & 38.76 & 39.54 \\ **C2F2-11B** & **41.01** & **42.90** & **41.76** \\ \hline \hline \end{tabular} \end{table} Table 2: Evaluation performance on 27 unseen datasets from BBH benchmark. All evaluations are held in a zero-shot setting. The best comparable performances are **bolded** and second best underlined. Our proposing model **C2F2** is equivalent to FLAN-T5 + CoT Instruction Tuning. Figure 3: The overall illustration of the zero-shot and few-shot experiments held out with our model **C2F2.****C2F2** is trained with the CoT rationales from CoT Collection, hence obtaining zero-shot generalization capabilities to solve unseen tasks with CoT prompting (Section 4). Then, **C2F2** serves as a better base model when adapting to novel tasks with few instances via CoT fine-tuning (Section 5. subset of the CoT Collection and apply CoT fine-tuning to T5 and T0 models. In addition to T0, we include different baselines such as (1) T5-LM (Raffel et al., 2020): the original base model of T0, (2) RoE (Jang et al., 2023): a modular expert LM that retrieves different expert models depending on the unseen task, (3) KiC (Pan et al., 2022): an instruction-tuned retrieval-augmented model that is trained to retrieve knowledge from a KB memory, and (4) Flipped (Ye et al., 2022): an instruction-tuned model that is trained to generate the instruction in order to resolve the LM over-fitting to the output label. As an upper-bound oracle, we also use T0-11B and GPT-3 (175B). The results are shown in Table 3. T0-3B + CoT Fine-tuning further improves T0-3B by a +8.65% margin. This shows that continually training an instruction-tuned model with additional CoT instruction data further unlocks the generalization capabilities of LMs on unseen tasks. More surprisingly, T5-3B + CoT Fine-tuning outperforms T0-3B by a +8.24% margin by generating a rationale during evaluation (denoted as Eval w/ CoT), while using only 3.22% of training data compared to T0. Even compared to T0-11B, it achieves better performance at sentence completion, and word sense disambiguation (WSD), and obtains similar performance at natural language inference and coreference resolution. Compared to T0-3B, which is a direct instruction-tuned model, these results indicate that training to generate rationales enables 3B-sized LMs to efficiently generalize more efficiently. Experiment #3: Multilingual SettingTo test if CoT Fine-tuning could hold effective in a multilingual setting as well, we use the experimental setting from Shi et al. (2022) with MGSM benchmark as a test bed. Instead of performing an apple-to-apple comparison with baselines as in the previous experiments, the purpose of this experiment is to show how CoT Fine-tuning could effectively enable LMs to adapt to different languages and solve unseen tasks. Specifically, we use about 0.001% amount of CoT instruction data on a single target language compared to the direct instruction data used to train the mT0 model (Muennighoff et al., 2022). To obtain CoT instruction data on the target language, we use ChatGPT API (OpenAI, 2022) for translation. The results are shown in Table 4. Across all the 5 different languages, MT5-3.7B + CoT Fine-tuning outperforms MT0-3.7B. Especially in resource-poor languages such as Korean, Japanese, and Chinese, training on a single target language with CoT instruction data holds an advantage over training with multiple languages which results in forgetting. This could be seen as a trade-off between training an expert LM on a single language versus training one generalist model that shows overall lower performance (Jang et al., 2023). Moreover, MT0-3.7B + CoT Fine-tuning successfully further improves over the base model mT0, even surpassing GPT-3 in every language. \begin{table} \begin{tabular}{l|c c c c c} \hline \hline **Model** & **ho** & **m** & **br** & **oh** & **ju** \\ \hline FLAN-T5-3B (Guenn et al., 2022) & 0.20 & 2.80 & 7.20 & 0.00 & 0.00 \\ FLAN-T5-11B (Guenn et al., 2022) & 0.00 & 5.20 & 1.32 & 0.00 & 0.00 \\ MTS-3.7B (Guenn et al., 2021) & 0.00 & 1.19 & 1.05 & 0.39 & 0.79 \\ MT0-3.7B (Guenn et al., 2021) & 0.00 & 4.54 & 7.14 & 1.59 & 2.37 \\ GPT-3.7B (Guenn et al., 2020) & 0.00 & 4.00 & 5.00 & 0.80 \\ \hline MT5-3.7B + CoT (Desitron) & 1.38 & 0.64 & 9.52 & 5.60 & 2.54 \\ MT0-3.7B + CoT (Desitron) & **7.84** & **90.32** & **15.40** & **11.11** & **10.84** \\ \hline \hline \end{tabular} \end{table} Table 4: Evaluation performance on MGSM benchmark (Shi et al., 2022) across 5 languages (Korean, Russian, French, Chinese, Japanese, respectively). All evaluations are held in a zero-shot setting except GPT-3 using a 6-Shot prompt for ICL. The best comparable performances are **bolded** and second best underlined. Note that our models were trained on a single target language instead of training with all the 46 languages used in mT0. Nonetheless, we use 0.001% of data compared to mT0 (Muennighoff et al., 2022). \begin{table} \begin{tabular}{l c c c c c c c c c c|c} \hline \hline \multicolumn{1}{c}{**Method**} & \multicolumn{3}{c}{Natural Language Inference} & \multicolumn{3}{c}{Sentence Completion} & \multicolumn{3}{c}{Conference Recok} & \multicolumn{1}{c}{WSD} & \multicolumn{1}{c}{**Total Aug**} \\ \cline{2-13} \cline{6-13} \multicolumn{1}{c}{} & **RTE** & **C** & **A** & **R** & **R** & **R** & **CoT** & **Heung** & **S**ove** & **C** & **W** & **W** & **W** \\ \hline T5-3B (Raffel et al., 2020) & 53.03 & 34.34 & 32.89 & 33.76 & 33.82 & 45.88 & 27.00 & 48.16 & 50.64 & 54.09 & 53.00 & 42.99 \\ To-3B (Sanif et al., 2021) & 60.61 & 48.81 & 35.10 & 32.37 & 35.25 & 71.31 & 27.18 & 84.91 & 50.91 & 65.00 & 51.27 & 51.43 \\ RoE-3B (Liang et al., 2022) & 64.01 & 45.7 & 35.89 & 34.64 & 31.22 & 79.25 & 34.60 & 86.33 & **64.60** & 22.12 & 52.92 & 53.48 \\ KCF-70M (Masro et al., 2022) & 74.00 & 67.09 & 36.30 & 53.00 & 37.60 & 85.30 & 29.60 & 94.04 & 55.30 & 65.40 & 52.40 & 57.56 \\ Flipped-3B (Ye et al., 2022) & 71.05 & 57.44 & 39.99 & 32.05 & 37.73 & 89.88 & **41.64** & **95.88** & 58.56 & 58.7 & 50.42 & 58.03 \\ \hline T5-3B + CoT (Instructions Tuning - Eval w/ defect) & 69.96 & 56.96 & 37.85 & 36.00 & 37.44 & 84.59 & 40.92 & 50.47 & 54.00 & 64.33 & 51.53 & 56.99 \\ T5-3B + CoT (Instructions Tuning - Eval w/ defect) & **83.99** & 65.00 & 39.49 & 35.13 & 35.88 & 88.27 & 41.04 & 92.13 & 56.40 & **69.36** & 53.00 & 56.97 \\ T5-3B + CoT (Instructions Tuning - Eval w/ CoT & 80.61 & 47.02 & 40.21 & 40.67 & 40.13 & 20.10 & **41.68** & 93.00 & 56.47 & 55.10 & **56.73** & 59.94 \\ T5-3B + CoT (Instructions Tuning - Eval w/ CoT & 80.25 & **72.62** & **47.17** & **37.22** & **48.98** & **90.85** & 90.44 & 57.47 & 57.05 & 58.42 & **69.08** \\ \hline T0-11B (Sanif et al., 2021) & 80.83 & 70.12 & 43.56 & 38.68 & 41.26 & 90.02 & 33.58 & 92.40 & 59.94 & 61.45 & 56.58 & 60.76 \\ GPT-3 (175B) (Brown et al., 2020) & 63.50 & 46.30 & 34.60 & 35.40 & 34.50 & 91.00 & 78.90 & 83.20 & 70.20 & 65.40 & 45.92 & 59.00 \\ \hline \hline \end{tabular} \end{table} Table 3: Evaluation performance on 11 different unseen P3 dataset (Sanif et al., 2021) categorized into 4 task categories. We report the direct performance of the baselines since they were only trained with direct instruction tuning. The best comparable performances are **bolded** and second best underlined among 770M \(\sim\) 3B models. We exclude FLAN-T5 and **C2F2** since they were trained on the unseen tasks, breaking unseen task assumption. ## 5 Few-shot Generalization Dataset SetupIn this section, we show how applying CoT fine-tuning over **C2F2** could effectively enable LMs to adapt in a few-shot setting. We choose 4 domain-specific datasets from legal and medical domains including LEDGAR (Tuggener et al., 2020), Case Hold (Zheng et al., 2021), MedNLI (Romanov and Shivade, 2018), and PubMedQA (Jin et al., 2019), each with 64 randomly sampled instances. Since the choice of training instances might highly affect the performance on each task, we conduct 3 runs with different random seeds for every setting and report the average score. Note that we prepare rationale data for the 64 instances using the same procedure as in **CoT Instruction Tuning**, utilizing the MCQA prompt from P3 dataset. In an applied setting, practitioners could obtain 64 rationales written by human experts, which could provide rich supervision when adapting to new tasks. Training SetupFor training LMs, we use 2 different baselines models (FLAN-T5, **C2F2**) across 3B and 11B scale and apply either (1) full-Finetuning, (2) CoT Fine-tuning, (3) LoRA Fine-tuning, and (4) LoRA CoT Fine-tuning. For applying Lora, we use a rank of 4 and train for 1K steps following Liu et al. (2022b). This results in training 2.35M parameters for 3B scale models and 4.72M parameters for 11B scale models. Also, we include ICL baselines using Claude API (Anthropic, 2023) and ChatGPT API (OpenAI, 2022) and append demonstrations up to the full context length. For CoT prompting with LLMs, we use the CoT demonstrations obtained from the augmented CoT rationale data used for fine-tuning. Experimental ResultsThe results are shown in Table 5. Overall, CoT fine-tuning **C2F2** integrated with LoRA obtains the best results overall for all 4 datasets. Surprisingly, when training FLAN-T5, applying full fine-tuning obtains better performance compared to its counterpart using LoRA fine-tuning. Nonetheless, in **C2F2**, LoRA achieves higher performance compared to full fine-tuning. One possible way to interpret this is that introducing only a few parameters enables the LM to maintain the CoT ability acquired from CoT Collection. Examining when LoRA is effective over full-finetuning in few-shot settings is relatively unexplored in literature, and we leave the additional analysis to future work. While CoT fine-tuning obtains similar or lower performance compared to direct fine-tuning in FLAN-T5, **C2F2** achieves higher performance with CoT fine-tuning compared to FLAN-T5 direct fine-tuning. This also supports the idea that **CoT Finetuning** in combination with instruction tuning empowers the few-shot adaptation of LMs. Lastly, fine-tuning methods obtain overall better results compared to ICL methods with LLMs. We conjecture this due to the long input length of legal and medical datasets, hindering to append all possible demonstrations. While increasing the context length could serve as a temporary solution, it becomes infeasible as the inference time increases quadratically in proportion to the input length. \begin{table} \begin{tabular}{l c|c c c c c} \hline **Method** & **\#Train Param** & **Ledgar** & **Case Hold** & **MedNLI** & **PubmedQA** & **Total Avg** \\ \hline Claude (Anthropic, 2023) + ICL & 0 & 55.70 & 57.20 & 75.94 & 54.58 & 60.85 \\ Claude (Anthropic, 2023) + CoT Prom. & 0 & 34.80 & 43.60 & 76.51 & 52.06 & 51.74 \\ ChatGPT (OpenAI, 2022) + ICL & 0 & 51.70 & 32.10 & 70.53 & 65.59 & 54.98 \\ ChatGPT (OpenAI, 2022) + CoT Prom. & 0 & 51.00 & 18.90 & 63.71 & 25.22 & 39.70 \\ \hline FLAN-T5-3B + Full FT. & 2.8B & 52.60 & 61.40 & 66.82 & 66.28 & 61.78 \\ FLAN-T5-3B + Full CoT FT. & 2.8B & 53.60 & 58.80 & 65.89 & 65.89 & 61.05 \\ **C2F2**-3B + Full CoT FT. (Ours) & 2.8B & 51.90 & 60.60 & 67.16 & 68.12 & 61.95 \\ FLAN-T5-3B + LoRA & 2.35M & 53.20 & 58.80 & 61.60 & 67.18 & 60.19 \\ FLAN-T5-3B + LoRA CoT FT. & 2.35M & 51.20 & 61.60 & 62.59 & 66.06 & 60.36 \\ **C2F2**-3B + LoRA CoT FT. (Ours) & 2.35M & 54.80 & 63.60 & 68.00 & 69.66 & 64.02 \\ \hline FLAN-T5-11B + LoRA & 4.72M & 55.30 & 64.90 & 75.91 & 70.25 & 66.59 \\ FLAN-T5-11B + LoRA CoT FT. & 4.72M & 52.10 & 65.50 & 71.63 & 71.60 & 65.21 \\ **C2F2**-11B + LoRA CoT FT. (Ours) & 4.72M & **56.10** & **68.30** & **78.02** & **73.42** & **68.96** \\ \hline \end{tabular} \end{table} Table 5: Evaluation performance on 4 domain-specific datasets. CoT Prom. denotes CoT Prompting and FT. denotes Fine-tuning. The best comparable performances are **bolded** and second best underlined. For a few-shot adaptation, we use 64 randomly sampled instances from each dataset. Conclusion In this work, we construct a large-scale instruction-tuning dataset consisting of 1.88M CoT rationales extracted across 1,060 NLP tasks. With our dataset, we demonstrated that CoT fine-tuning can enhance the zero-shot and few-shot learning capabilities of smaller LMs to solve novel tasks. We hope our data and model could further facilitate future work towards general-purpose LMs."""


cited = """# Tree of Clarifications: Answering Ambiguous Questions ###### Abstract Questions in open-domain question answering are often ambiguous, allowing multiple interpretations. One approach to handling them is to identify all possible interpretations of the ambiguous question (AQ) and to generate a long-form answer addressing them all, as suggested by Stelmakh et al. (2022). While it provides a comprehensive response without bothering the user for clarification, considering multiple dimensions of ambiguity and gathering corresponding knowledge remains a challenge. To cope with the challenge, we propose a novel framework, Tree of Clarifications (ToC): It recursively constructs a tree of disambiguations for the AQ--via few-shot prompting leveraging external knowledge--and uses it to generate a long-form answer. ToC outperforms existing baselines on ASQA in a few-shot setup across all metrics, while surpassing fully-supervised baselines trained on the whole training set in terms of Disambig-F1 and Disambig-ROUGE. Code is available at github.com/gankim/tree-of-clarifications. + Footnote †: \({}^{\dagger}\) Corresponding author ## 1 Introduction In open-domain question answering (ODQA), users often ask ambiguous questions (AQs), which can be interpreted in multiple ways. To handle AQs, several approaches have been proposed, such as providing individual answers to disambiguated questions (DQs) for all plausible interpretations of the given AQ (Min et al., 2020) or asking a clarification question (Guo et al., 2021). Among them, we adopt that of Stelmakh et al. (2022), which provides a comprehensive response without bothering the user for clarification: The task is to identify all DQs of the given AQ and generate a long-form answer addressing all the DQs (See Figure 1). There are two main challenges to this task: (1) the AQ may need to be clarified by considering multiple dimensions of ambiguity. For example, the AQ "_what country has the most medals in Olympic history_" in Figure 1 can be clarified with respect to the type of medals--gold, silver, or bronze--or Olympics--summer or winter; and (2) substantial knowledge is required to identify DQs and respective answers. For example, it requires knowledge to be aware of the existence of different types of medals and the exact counts for each country. To address the challenges and provide a long-form answer to AQ, we propose a novel framework, Tree of Clarifications (ToC): It recursively constructs a tree of DQs for the AQ--via few-shot Figure 1: Overview of Tree of Clarifications. (1) relevant passages for the ambiguous question (AQ) are retrieved. (2) leveraging the passages, disambiguated questions (DQs) for the AQ are recursively generated via few-shot prompting and pruned as necessary. (3) a long-form answer addressing all DQs is generated. prompting leveraging external knowledge--and uses it to generate a long-form answer. More specifically, first, relevant passages for the AQ are retrieved. Then, leveraging the passages, DQs for the AQ are recursively generated via few-shot prompting and pruned as necessary. Lastly, a long-form answer addressing all DQs is generated. The tree structure promotes exploring DQs in targeting particular dimensions of clarification, addressing the first challenge, and the external sources offer additional knowledge to cope with the second challenge. Experiments demonstrate that our proposed use of LLMs with retrieval-augmentation and guidance to pursue diverse paths of clarification results in the new state-of-the-art on ASQA (Stelmakh et al., 2022)--a long-form QA benchmark for AQs. ToC outperforms existing baselines on ASQA in a few-shot setup across all metrics. In addition, this 5-shot performance surpasses that of the fully-supervised baselines trained on the whole training set by 7.3 and 2.9 in terms of Disambig-F1 and Disambig-ROUGE, respectively. The main contribution of this work is proposing a novel framework, Tree of Clarifications (ToC), for generating long-form answers to AQs in ODQA, advancing the state-of-the-art on the ASQA benchmark. ToC introduces two main innovations: * It guides LLMs to explore diverse paths of clarification of the given AQ in a tree structure with the ability to prune unhelpful DQs. * To the best of our knowledge, it is the first to combine retrieval systems with LLM for generating long-form answers to AQs. ## 2 Related Work A line of studies (Min et al., 2020, 2021; Gao et al., 2021; Shao and Huang, 2022) extends retrieve-and-read frameworks dominant in ODQA task (Chen et al., 2017; Karpukhin et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021) to clarify AQ and generate DQs with corresponding answers to them. However, their approaches require fine-tuning models on the large-scale train set. On the other hand, our framework enables LLM to generate a comprehensive response addressing all DQs via few-shot prompting. Recent studies introduce LLM-based methods to generate a long-form answer to the AQ. Amplayo et al. (2023) suggest optimal prompts specifically engineered for the task. Kuhn et al. (2022) prompt LLMs to clarify ambiguous questions selectively. However, the studies do not utilize external information to ensure the factual correctness of the disambiguations, thereby potentially increasing the risk of hallucinations from LLMs. Moreover, the results could be bounded by inherent parametric knowledge of LLM. Concurrently, Lee et al. (2023) automatically generate clarifying questions to resolve ambiguity. Our framework involves the recursive tree architecture, inspired by several prior studies. Min et al. (2021) propose the tree-decoding algorithm to autoregressively rerank passages in ambiguous QA. Gao et al. (2021) iteratively explore additional interpretations and verify them in a round-trip manner. Concurrently, extending chain of thoughts (Wei et al., 2022) prompting, Yao et al. (2023) apply the tree architecture to reasoning tasks for deductive or mathematical problems. On the contrary, ToC recursively clarifies questions and introduces a self-verification method to prune unhelpful DQs. ## 3 Tree of Clarifications We introduce a novel framework, Tree of Clarifications (ToC), as illustrated in Figure 1. We first devise retrieval-augmented clarification (RAC; Sec. 3.1), a basic component that clarifies AQ and generates DQs based on relevant passages. ToC explores various fine-grained interpretations, represented as a tree structure (TS; Sec. 3.2) by recursively performing RAC and pruning unhelpful DQs. Lastly, it aggregates the tree and generates a long-form answer addressing all valid interpretations. ### Retrieval-Augmented Clarification (RAC) We first retrieve relevant Wikipedia documents for the AQ by using two retrieval systems, ColBERT (Khattab and Zaharia, 2020) and Bing search engine1. ColBERT is a recent dense retriever that has effective and efficient zero-shot search quality. Following Khattab et al. (2022), we use the off-the-shelf model pre-trained on MS-Marco (Bajaj et al., 2016). We additionally include the Bing search engine to promote the diversity of retrieved Wikipedia passages. Finally, we obtain over 200 passages by combining passages retrieved by each system. Footnote 1: [https://www.microsoft.com/bing](https://www.microsoft.com/bing) After collecting a passage set for the AQ, we rerank and choose top-\(k\) passages and augment them to a prompt. We use SentenceBERT Reimers and Gurevych (2019) pre-trained on MS-Marco as the reranker backbone. For in-context learning setup, we dynamically choose \(k\)-shot examples with the nearest neighbor search2 and add them to the prompt. We initiate with the instruction of Amplayo et al. (2023) and revise it for our setup. Given the prompt with relevant passages and AQs, LLM generates all possible DQs and their corresponding answers3. Footnote 2: See Appendix A.3 for detailed implementation Footnote 3: See Appendix C.2 for example prompts Footnote 4: It is suboptimal to adopt the depth-first search since it would encounter unambiguous questions more frequently. See Appendix 7 for failure cases. ### Tree Structure (TS) To effectively explore the diverse dimensions of ambiguity, we introduce a recursive tree structure of clarifications. Starting from the root node with AQ, it progressively inserts child nodes by recursively performing RAC, each of which contains a disambiguated question-answer pair. In each expansion step, passages are reranked again regarding the current query. It allows each step to focus on its own DQ, encouraging ToC to comprehend a wider range of knowledge. Exploration of a tree ends when it satisfies termination conditions; it reaches the maximum number of valid nodes or the maximum depth. We choose the breadth-first search (BFS) by default, hence the resulting tree could cover the broader interpretations5. Footnote 5: See Appendix C.3 for more detailed case studies Pruning with Self-VerificationTo remove unhelpful nodes, we design a pruning method, inspired by current studies for self-verification Kadavath et al. (2022); Cole et al. (2023). Specifically, we check the factual coherency between the answers in a target node and the AQ in the root node. By doing so, we discard the generated DQs that ask different or irrelevant facts from the original one. For example, given an AQ _"Who will host the next world cup 2022?"_, a generated disambiguation _"DQ: Who hosted the world cup 2018? A: Russia"_ is a factually consistent question-answer pair but it changes the original scope of the AQ6. We perform self-verification by prompting LLMs to determine whether the current node would be pruned or not. Prompted with AQ, the answer to the target DQ, and the answer-containing passage, LLM identifies if the given answer could be a correct answer to AQ. Footnote 6: See Appendix C.4 for an example prompt Answer GenerationOnce constructing the tree of clarifications, ToC aggregates all valid nodes and generates a comprehensive long-form answer to AQ. It selects the disambiguations in retained nodes of the resulting tree with the relevant passages. If the number of nodes is insufficient, we undo the pruning steps from closer nodes to the root node in BFS order. Passages that contain the answers of valid nodes are prioritized. It finally generates a long-form answer, encoding AQ, selected disambiguations, and relevant passages7. Footnote 7: See Appendix C.4 for more detailed case studies ## 4 Experiment ### Experimental Setup DatasetsAll baselines and our framework are evaluated on ASQA Stelmakh et al. (2022). It is a long-form QA dataset built upon the 6K ambiguous questions identified from AmbigNQ Min et al. (2020). More details are in Appendix A.1 Evaluation MetricsWe use three evaluation metrics, following Stelmakh et al. (2022). (1) **Disambig-F1 (D-F1)** measures the factual correctness of generated predictions. It extracts short answers to each DQ and computes their F1 accuracy. (2) **ROUGE-L (R-L)** measures the lexical overlap between long-form answers from references and predictions. (3) **DR** score is the geometric mean of \begin{table} \begin{tabular}{l r r r} \hline \hline **Model** & **D-F1** & **R-L** & **DR** \\ \hline \hline \multicolumn{4}{c}{_Fully-supervised_} \\ \hline T5-Large Closed-Book & 7.4 & 33.5 & 15.7 \\ T5-Large w/ JPR & 26.4 & 43.0 & 33.7 \\ PaLM w/ Soft Prompt Tuning\({}^{*}\) & 27.8 & 37.4 & 32.1 \\ \hline \multicolumn{4}{c}{_Few-shot Prompting (5-shot)_} \\ \hline PaLM\({}^{*}\) & 25.3 & 34.5 & 29.6 \\ GPT-3\({}^{*}\) & 25.0 & 31.8 & 28.2 \\ \hline \multicolumn{4}{l}{_Tree of Clarifications (ToC; Ours)_} \\ \hline GPT-3 + RAC & 31.1 & 39.6 & 35.1 \\ GPT-3 + RAC + TS & 32.4 & **40.0** & 36.0 \\ GPT-3 + RAC + TS w/ Pruning & **33.7** & 39.7 & **36.6** \\ \hline \hline \end{tabular} \({}^{*}\) from Amplayo et al. (2023) \end{table} Table 1: Evaluation results for long-form QA on ambiguous questions from the development set of ASQA Stelmakh et al. (2022). Baselines are either fully-supervised or 5-shot prompted. Note, ToC framework consists of retrieval-augmented clarification (RAC) and tree structure (TS). two scores, which assesses the overall performance. For validating intermediate nodes, we additionally use **Answer-F1** that measures the accuracy of generated short answers in disambiguation. Further details are in Appendix A.2. BaselinesStelmakh et al. (2022) propose fine-tuned baselines. They fine-tune T5-large (Raffel et al., 2020) to generate long-form answers on the whole train set. Models are evaluated in the closed-book setup or combined with JPR (Min et al., 2021), task-specific dense retriever for ambiguous QA by enhancing DPR (Karpukhin et al., 2020). On the other hand, Amplayo et al. (2023) propose a prompt engineering method to adapt LLMs to the ASQA benchmark. They employ PaLM (Chowdhery et al., 2022) and Instruct-GPT (Ouyang et al., 2022) that learn the soft prompts or adopt in-context learning with few-shot examples. They conduct experiments in the closed-book setup. Note that they share the same backbone with our models, GPT-3 with 175B parameters (text-davinci-002). ### Experimental Results ToC outperforms fully-supervised and few-shot prompting baselines.Table 1 shows the long-form QA performance of baselines and ToC on the development set of ASQA. Among baselines, using the whole training set (_Fully-supervised_) achieves greater performances than _Few-shot Prompting_ in all metrics. It implies that long-form QA task is challenging in the few-shot setup. In the closed-book setup, GPT-3 shows competitive performances with T5-large with JPR in D-F1 score, showing LLM's strong reasoning ability over its inherent knowledge. Among our models, LLM with RAC outperforms all other baselines in D-F1 and DR scores. It indicates the importance of leveraging external knowledge in clarifying AQs. Employing the tree structure (TS) helps the model to explore diverse interpretations, improving D-F1 and DR scores by 1.3 and 0.9. When pruning the tree with our proposed self-verification (TS w/ Pruning), the model achieves state-of-the-art performance in D-F1 and DR score, surpassing the previous few-shot baseline by 8.4 and 7.0. Notably, it outperforms the best model in a fully-supervised setup (T5-large with JPR) by 7.3 and 2.9. In the experiment, T5-Large in a closed-book setup achieves comparable performance with LLM baselines in ROUGE-L score despite its poor D-F1 scores. It reconfirms the observation from Krishna et al. (2021) that shows the limitations of the ROUGE-L metric. Discussion Ambiguity DetectionToC is designed to clarify AQs without bothering users; hence does not explicitly identify whether the given question is ambiguous or not. It tries to perform clarification even if the question cannot be disambiguated anymore, often resulting in generating duplicate or irrelevant DQs7. However, we could presume a question to be unambiguous if it can no longer be disambiguated8. In ToC, when it fails to disambiguate the given question or all generated disambiguations are pruned, the question could be regarded as unambiguous. Footnote 7: See Appendix 7 for failure cases Footnote 8: The idea is aligned with the annotation process of AmbigQA (Min et al., 2020), in which the target question is classified as ambiguous if multiple distinct answers to it were observed. Computational ComplexityAlthough ToC requires multiple LLM calls, its maximum number is less than 20 times per question. Exploration of the tree ends when it obtains the pre-defined number of valid nodes (10 in our experiments). Since the clarification process generates from two to five disambiguations for each question, it satisfies the termination condition in a few steps without the pruning method. Failing to expand three times in a row also terminates the exploration. Pruning steps consume a smaller amount of tokens since they encode a single passage without few-shot exemplars. Compared to the existing ensemble methods such as self-consistency (Wei et al., 2022) which cannot be directly adopted to the generative task, ToC achieves a state-of-the-art performance with a comparable number of LLM calls. GeneralizabilityThe key idea of ToC could be potentially generalized to other tasks and model architectures. It has a model-agnostic structure that could effectively explore diverse paths of recursive reasoning, which would be helpful for tasks that require multi-step reasoning, such as multi-hop QA. Future work might investigate the generalizability of ToC to diverse tasks, datasets, and LM architectures. ## 6 Conclusion In this work, we propose a novel framework, Tree of Clarifications. It recursively builds a tree of disambiguations for the AQ via few-shot prompting with external knowledge and utilizes it to generate a long-form answer. Our framework explores diverse dimensions of interpretations of ambiguity. Experimental results demonstrate ToC successfully guide LLMs to traverse diverse paths of clarification for a given AQ within tree structure and generate comprehensive answers. We hope this work could shed light on building robust clarification models, which can be generalized toward real-world scenarios. ### Limitations Although ToC is a model-agnostic framework that could be combined with other components, our study is limited in demonstrating the generalizability of different kinds or sizes of LLMs. In addition, the experiments are only conducted on a benchmark, ASQA (Stelmakh et al., 2022). Although ToC enables LLM to explore diverse reasoning paths by iteratively prompting LLM, the cost of multiple prompting is not negligible. We tried the recent prompting method, chain of thoughts (Wei et al., 2022), but failed to enhance the performance in our pilot experiments. It might indicate the disambiguation process requires external knowledge, which shows the importance of document-grounded or retrieval-augmented systems. Future work could suggest other pruning methods that identify unhelpful DQs more effectively. The performance could be further enhanced by using the state-of-the-art reranker in the answer sentence selection task, as proposed by recent works (Garg et al., 2020; Lauriola and Moschitti, 2021). ## Acknowledgements The first author, Gangwoo Kim, has been supported by the Hyundai Motor Chung Mong-Koo Foundation. This research was supported by the National Research Foundation of Korea (NRF-2023R1A2C3004176, RS-2023-00262002), the MSIT (Ministry of Science and ICT), Korea, under the ICT Creative Consilience program (IITP-2022-2020-0-01819) supervised by the IITP (Institute for Information & communications Technology Planning & Evaluation), and the Electronics and Telecommunications Research Institute (RS-2023-00220195).
"""

data.append({'focus':focus,'cited':cited})

with open('data.json', 'w') as file:
    json.dump(data, file)