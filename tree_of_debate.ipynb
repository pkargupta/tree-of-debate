{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/shared/data3/pk36/.cache'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=/shared/data3/pk36/.cache\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from debate import DebateNode\n",
    "from paper_details import Paper\n",
    "from persona import PaperAuthor\n",
    "from moderator import Moderator\n",
    "import argparse\n",
    "from typing import List\n",
    "from vllm import LLM\n",
    "import os\n",
    "import json\n",
    "from data_pairer import parse_papers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.log_dir = \"logs\"\n",
    "        self.focus_paper = \"https://arxiv.org/pdf/2406.11709\" # treeinstruct\n",
    "        self.cited_paper = \"https://arxiv.org/pdf/2310.10648\" # bridge\n",
    "        self.topic = {'argument_title': \"helping students fix their errors\", 'description': \"\"}\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args.log_dir):\n",
    "    os.makedirs(args.log_dir)\n",
    "\n",
    "# if not os.path.exists(\"data.json\"):\n",
    "parse_papers(args.focus_paper, args.cited_paper)\n",
    "with open('data.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-29 12:05:23 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-29 12:05:23 arg_utils.py:957] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 11-29 12:05:23 config.py:1021] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 11-29 12:05:23 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 11-29 12:05:24 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-29 12:05:24 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3743417)\u001b[0;0m INFO 11-29 12:05:24 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-29 12:05:25 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-29 12:05:25 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3743417)\u001b[0;0m INFO 11-29 12:05:25 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3743417)\u001b[0;0m INFO 11-29 12:05:25 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-29 12:05:25 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/pk36/.cache/vllm/gpu_p2p_access_cache_for_6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3743417)\u001b[0;0m INFO 11-29 12:05:25 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/pk36/.cache/vllm/gpu_p2p_access_cache_for_6,7.json\n",
      "INFO 11-29 12:05:25 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f7bdfddc370>, local_subscribe_port=58463, remote_subscribe_port=None)\n",
      "INFO 11-29 12:05:25 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3743417)\u001b[0;0m INFO 11-29 12:05:25 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 11-29 12:05:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3743417)\u001b[0;0m INFO 11-29 12:05:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b1b80bbf344590a2a26734492b59bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3743417)\u001b[0;0m INFO 11-29 12:05:31 model_runner.py:1067] Loading model weights took 7.5122 GB\n",
      "INFO 11-29 12:05:31 model_runner.py:1067] Loading model weights took 7.5122 GB\n",
      "INFO 11-29 12:05:32 distributed_gpu_executor.py:57] # GPU blocks: 15590, # CPU blocks: 4096\n",
      "INFO 11-29 12:05:32 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 1.90x\n",
      "INFO 11-29 12:05:34 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-29 12:05:34 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3743417)\u001b[0;0m INFO 11-29 12:05:34 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3743417)\u001b[0;0m INFO 11-29 12:05:34 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-29 12:05:44 custom_all_reduce.py:233] Registering 1040 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3743417)\u001b[0;0m INFO 11-29 12:05:44 model_runner.py:1523] Graph capturing finished in 9 secs.\n",
      "INFO 11-29 12:05:44 custom_all_reduce.py:233] Registering 1040 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3743417)\u001b[0;0m INFO 11-29 12:05:44 model_runner.py:1523] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "model_server = LLM(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",tensor_parallel_size=2, gpu_memory_utilization=0.5, max_num_seqs=100) #,enable_prefix_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.33it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.84it/s]\n"
     ]
    }
   ],
   "source": [
    "item = data[0]\n",
    "f_pap = item['focus']\n",
    "c_pap = item['cited']\n",
    "\n",
    "focus_paper = PaperAuthor(\n",
    "    model = model_server,\n",
    "    paper = Paper(f_pap),\n",
    "    focus=True,\n",
    "    id=0\n",
    ")\n",
    "\n",
    "cited_paper = PaperAuthor(\n",
    "    model = model_server,\n",
    "    paper = Paper(c_pap),\n",
    "    focus=False,\n",
    "    id=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 29.58it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 605/605 [00:20<00:00, 28.90it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.87s/it, est. speed input: 165.58 toks/s, output: 62.40 toks/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# each node has a topic\u001b[39;00m\n\u001b[1;32m     11\u001b[0m root_node \u001b[38;5;241m=\u001b[39m DebateNode(leaf_node_label)\n\u001b[0;32m---> 12\u001b[0m subtrees \u001b[38;5;241m=\u001b[39m \u001b[43mroot_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconduct_self_deliberation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleaf_node_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaper_authors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# k new, finer topics to discuss\u001b[39;00m\n",
      "File \u001b[0;32m~/tree-of-debate/debate.py:48\u001b[0m, in \u001b[0;36mDebateNode.conduct_self_deliberation\u001b[0;34m(self, topic, paper_authors, log, num_evidence, num_arg)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# develop k arguments\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m paper_author\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_delib\u001b[38;5;241m.\u001b[39mkeys(): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_delib[paper_author\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 48\u001b[0m author_args \u001b[38;5;241m=\u001b[39m \u001b[43mpaper_author\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_arguments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevidence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_arg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_delib[paper_author\u001b[38;5;241m.\u001b[39mid]\u001b[38;5;241m.\u001b[39mextend(author_args)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# check if paper is the focus\u001b[39;00m\n",
      "File \u001b[0;32m~/tree-of-debate/persona.py:96\u001b[0m, in \u001b[0;36mPaperAuthor.generate_arguments\u001b[0;34m(self, topic, evidence, temperature, top_p, k)\u001b[0m\n\u001b[1;32m     85\u001b[0m         prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mBelow is a list of relevant evidence retrieved from your paper:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mformatted_evidence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBased on the evidence, output a list of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m diverse, specific arguments on your paper\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms major unique contributions towards \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, that are all supported by the evidence. Each argument should make a unique point. Output your list of arguments in the following JSON format:\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument_list\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124m    [\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124m    ]\u001b[39m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     94\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m unidecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(prompt,\n\u001b[1;32m     95\u001b[0m                     sampling_params\u001b[38;5;241m=\u001b[39msampling_params)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m---> 96\u001b[0m         \u001b[43mlog_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(outputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margument_list\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/tree-of-debate/persona.py:49\u001b[0m, in \u001b[0;36mlog_llm\u001b[0;34m(prompt, output)\u001b[0m\n\u001b[1;32m     47\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPROMPT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     48\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOUTPUT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--------------------------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "moderator = Moderator(model_server)\n",
    "\n",
    "paper_authors = [focus_paper, cited_paper]\n",
    "leaf_node_label = args.topic\n",
    "\n",
    "if args.log_dir != \"\":\n",
    "    with open(os.path.join(args.log_dir, 'self_deliberation.txt'), 'w') as f:\n",
    "        f.write(f'Topic: {args.topic}\\n\\n')\n",
    "\n",
    "# each node has a topic\n",
    "root_node = DebateNode(leaf_node_label)\n",
    "subtrees = root_node.conduct_self_deliberation(leaf_node_label, paper_authors, log=args.log_dir) # k new, finer topics to discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: []}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node.self_delib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'Does my paper also address the claim, \"bidirectional language model architecture\"?': ['We used a beam size of 21 and a = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]',\n",
       "   'Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]'],\n",
       "  'Does my paper also address the claim, \"advancements in pre-training objectives\"?': ['During inference, we Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative semi-supervised semi-supervised semi-supervised semi-supervised semi-supervised multi-task generative Petrov et al',\n",
       "   'We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences']},\n",
       " 1: {'Does my paper also address the claim, \"transformer architecture offers superior performance\"?': ['BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model',\n",
       "   '(2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)'],\n",
       "  'Does my paper also address the claim, \"transformer architecture overcomes recurrent model limitations\"?': ['The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017)',\n",
       "   '(2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)']}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node.preemption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Transformer Architecture Offers Superior Performance,\n",
       " Transformer Architecture Overcomes Recurrent Model Limitations]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRESENT ARGUMENT FOR AUTHOR 0:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling FSM index for all state transitions: 100%|██████████| 57/57 [00:02<00:00, 25.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an author of a paper that is debating another author on your claim topic:\n",
      "\t- Topic: Transformer Architecture Offers Superior Performance\n",
      "\t- Topic Description: Our paper proposes a novel Transformer architecture that outperforms existing models in machine translation tasks. This is supported by evidence that our model achieves superior quality while requiring significantly less training time, as shown in Retrieved Evidence #1. Furthermore, our model generalizes well to other tasks such as English constituency parsing, as demonstrated in Retrieved Evidence #3.\n",
      "\n",
      "Your debate claim is that your paper's contributions towards the topic are all novel relative to the other paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.\n",
      "\n",
      "You used the following evidence to support your arguments:\n",
      "\t- Evidence #1. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train\n",
      "\t- Evidence #2. Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]\n",
      "\t- Evidence #3. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. *Equal contribution\n",
      "\t- Evidence #4. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]\n",
      "\t- Evidence #5. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature\n",
      "\n",
      "You also have preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "\t- Opposition Claim #1: Does my paper also address the opposition's claim, \"bidirectional language model architecture\"?\n",
      "\t\t- Your Counter Evidence #1: We used a beam size of 21 and a = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]\n",
      "\t\t- Your Counter Evidence #2: Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]\n",
      "\n",
      "\t- Opposition Claim #2: Does my paper also address the opposition's claim, \"advancements in pre-training objectives\"?\n",
      "\t\t- Your Counter Evidence #1: During inference, we Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative semi-supervised semi-supervised semi-supervised semi-supervised semi-supervised multi-task generative Petrov et al\n",
      "\t\t- Your Counter Evidence #2: We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences\n",
      "\n",
      "Given the above (your initial argument, your evidence, the opposition paper's claimed points of novelty, and your counter evidence), make an argument for a specific reason why your paper's contributions towards the topic are all novel relative to the other paper, with respect to the topic, Transformer Architecture Offers Superior Performance. \n",
      "\n",
      "Output your argument in the following JSON format:\n",
      "\n",
      "{\n",
      "    \"argument_title\": <should be a brief, 10-15 word string where the value is the argument argument_title>,\n",
      "    \"description\": <2-3 sentence string explaining the argument>\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PRESENT ARGUMENT FOR AUTHOR 1:\n",
      "\n",
      "You are an author of a paper that is debating another author about their claimed novelty:\n",
      "\t- Novelty Claim: Transformer Architecture Offers Superior Performance\n",
      "\t- Novelty Claim Description: Our paper proposes a novel Transformer architecture that outperforms existing models in machine translation tasks. This is supported by evidence that our model achieves superior quality while requiring significantly less training time, as shown in Retrieved Evidence #1. Furthermore, our model generalizes well to other tasks such as English constituency parsing, as demonstrated in Retrieved Evidence #3.\n",
      "\n",
      "Your debate claim is that the other paper's contributions towards the topic are not novel relative to your own paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.\n",
      "\n",
      "You initially argued that your own paper has the following novelties:\n",
      "\t- Argument #1. Bidirectional Language Model Architecture: Our paper introduces a novel bidirectional language model architecture that extracts context-sensitive features from both left-to-right and right-to-left language models. This is supported by evidence that standard language models are unidirectional, and our architecture allows for the fusion of left and right context, as shown in evidence #4 and #5. Furthermore, our architecture is able to unambiguously represent both a single sentence and a pair of sentences in one token sequence, as demonstrated in evidence #3.\n",
      "\t- Argument #2. Advancements in Pre-training Objectives: Our paper introduces a novel pre-training objective that enables the representation to fuse the left and the right context, allowing for the pre-training of a deep bidirectional Transformer. This is supported by evidence #4 and #5, which show that our pre-training objective outperforms left-to-right language model pre-training and achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks.\n",
      "\n",
      "\n",
      "You used the following evidence to support your arguments:\n",
      "\t- Evidence #1. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations\n",
      "\t- Evidence #2. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017)\n",
      "\t- Evidence #3. 4We note that in the literature the bidirectional TransBERTBERTE[CLS]E1 E[SEP]...ENE1'...EM'CT1T[SEP]...TNT1'...TM'[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1'...EM'CT1T[SEP]...TNT1'...TM'[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., (cid:104) Question, Answer (cid:105)) in one token sequence. Throughout this work, a \"sentence\" can be an arbitrary span of contiguous text, rather than an actual linguistic sentence\n",
      "\t- Evidence #4. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. * BERT advances the state of the art for eleven NLP tasks\n",
      "\t- Evidence #5. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to preIn additrain a deep bidirectional Transformer. tion to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations\n",
      "\n",
      "You also have preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "\t- Opposition Claim #1: Does my paper also address the opposition's claim, \"transformer architecture offers superior performance\"?\n",
      "\t\t- Your Counter Evidence #1: BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model\n",
      "\t\t- Your Counter Evidence #2: (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)\n",
      "\n",
      "\t- Opposition Claim #2: Does my paper also address the opposition's claim, \"transformer architecture overcomes recurrent model limitations\"?\n",
      "\t\t- Your Counter Evidence #1: The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017)\n",
      "\t\t- Your Counter Evidence #2: (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)\n",
      "\n",
      "Given the above (your initial argument, your evidence, the opposition paper's claimed points of novelty, and your counter evidence), make an argument for a specific reason why the other paper's contributions towards the topic are not novel relative to your own paper, with respect to the topic, Transformer Architecture Offers Superior Performance. \n",
      "\n",
      "Output your argument in the following JSON format:\n",
      "\n",
      "{\n",
      "    \"argument_title\": <should be a brief, 10-15 word string where the value is the argument argument_title>,\n",
      "    \"description\": <2-3 sentence string explaining the argument>\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RESPOND ARGUMENT FOR AUTHOR 0:\n",
      "\n",
      "You are an author of a paper that is debating another author.\n",
      "\n",
      "Your debate claim is that your paper's contributions towards the \"topic\" are all novel relative to the other paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.\n",
      "\n",
      "You used the following evidence to support your arguments:\n",
      "\t- Evidence #1. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train\n",
      "\t- Evidence #2. Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]\n",
      "\t- Evidence #3. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. *Equal contribution\n",
      "\t- Evidence #4. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]\n",
      "\t- Evidence #5. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature\n",
      "\n",
      "You also have preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "\t- Opposition Claim #1: Does my paper also address the opposition's claim, \"bidirectional language model architecture\"?\n",
      "\t\t- Your Counter Evidence #1: We used a beam size of 21 and a = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]\n",
      "\t\t- Your Counter Evidence #2: Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]\n",
      "\n",
      "\t- Opposition Claim #2: Does my paper also address the opposition's claim, \"advancements in pre-training objectives\"?\n",
      "\t\t- Your Counter Evidence #1: During inference, we Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative semi-supervised semi-supervised semi-supervised semi-supervised semi-supervised multi-task generative Petrov et al\n",
      "\t\t- Your Counter Evidence #2: We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences\n",
      "\n",
      "Based on the debate history and your/your opposition's arguments and evidence, you must respond to the last argument presented by your opposition in debate.\n",
      "\n",
      "Debate Topic Information:\n",
      "\t- Topic: Transformer Architecture Offers Superior Performance\n",
      "\t- Topic Description: Our paper proposes a novel Transformer architecture that outperforms existing models in machine translation tasks. This is supported by evidence that our model achieves superior quality while requiring significantly less training time, as shown in Retrieved Evidence #1. Furthermore, our model generalizes well to other tasks such as English constituency parsing, as demonstrated in Retrieved Evidence #3.\n",
      "\n",
      "Debate History:\n",
      "\n",
      "\t-You: I argue that novel architecture and training methods outperform existing models. Our paper introduces a novel Transformer architecture that outperforms existing models in machine translation tasks. This is supported by evidence that our model achieves superior quality while requiring significantly less training time, as shown in Retrieved Evidence #1. Furthermore, our model generalizes well to other tasks such as English constituency parsing, as demonstrated in Retrieved Evidence #3. The opposition's claims of novelty are addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our pre-training objectives are not advancements, but rather a natural extension of existing methods.\n",
      "\t-Opposition: I argue that opposition's contributions lack novelty in transformer architecture. The opposition's contributions lack novelty in Transformer architecture as they build upon existing work, specifically BERT, which has already demonstrated superior performance. Their proposed architecture is an extension of BERT, and their results are comparable to BERT's performance. Furthermore, their pre-training objectives are also similar to BERT's, indicating a lack of novelty in their contributions.\n",
      "\n",
      "\n",
      "Output your new response in the following JSON format:\n",
      "\n",
      "{\n",
      "    \"argument_title\": <should be a brief, 10-15 word string where the value is the main argument of your response to the opposition>,\n",
      "    \"description\": <2-3 sentence string explaining your response to the opposition's last turn, based on the provided pool of evidence>\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RESPOND ARGUMENT FOR AUTHOR 1:\n",
      "\n",
      "You are an author of a paper that is debating another author.\n",
      "\n",
      "Your debate claim is that the other paper's contributions towards the \"topic\" are not novel relative to your own paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.You used the following evidence to support your arguments:\n",
      "\t- Evidence #1. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations\n",
      "\t- Evidence #2. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017)\n",
      "\t- Evidence #3. 4We note that in the literature the bidirectional TransBERTBERTE[CLS]E1 E[SEP]...ENE1'...EM'CT1T[SEP]...TNT1'...TM'[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1'...EM'CT1T[SEP]...TNT1'...TM'[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., (cid:104) Question, Answer (cid:105)) in one token sequence. Throughout this work, a \"sentence\" can be an arbitrary span of contiguous text, rather than an actual linguistic sentence\n",
      "\t- Evidence #4. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. * BERT advances the state of the art for eleven NLP tasks\n",
      "\t- Evidence #5. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to preIn additrain a deep bidirectional Transformer. tion to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations\n",
      "\n",
      "You also have preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "\t- Opposition Claim #1: Does my paper also address the opposition's claim, \"transformer architecture offers superior performance\"?\n",
      "\t\t- Your Counter Evidence #1: BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model\n",
      "\t\t- Your Counter Evidence #2: (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)\n",
      "\n",
      "\t- Opposition Claim #2: Does my paper also address the opposition's claim, \"transformer architecture overcomes recurrent model limitations\"?\n",
      "\t\t- Your Counter Evidence #1: The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017)\n",
      "\t\t- Your Counter Evidence #2: (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)\n",
      "\n",
      "Based on the debate history and your/your opposition's arguments and evidence, you must respond to the last argument presented by your opposition in debate.\n",
      "\n",
      "Debate Topic Information:\n",
      "\t- Topic: Transformer Architecture Offers Superior Performance\n",
      "\t- Topic Description: Our paper proposes a novel Transformer architecture that outperforms existing models in machine translation tasks. This is supported by evidence that our model achieves superior quality while requiring significantly less training time, as shown in Retrieved Evidence #1. Furthermore, our model generalizes well to other tasks such as English constituency parsing, as demonstrated in Retrieved Evidence #3.\n",
      "\n",
      "Debate History:\n",
      "\n",
      "\t-Opposition: I argue that novel architecture and training methods outperform existing models. Our paper introduces a novel Transformer architecture that outperforms existing models in machine translation tasks. This is supported by evidence that our model achieves superior quality while requiring significantly less training time, as shown in Retrieved Evidence #1. Furthermore, our model generalizes well to other tasks such as English constituency parsing, as demonstrated in Retrieved Evidence #3. The opposition's claims of novelty are addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our pre-training objectives are not advancements, but rather a natural extension of existing methods.\n",
      "\t-You: I argue that opposition's contributions lack novelty in transformer architecture. The opposition's contributions lack novelty in Transformer architecture as they build upon existing work, specifically BERT, which has already demonstrated superior performance. Their proposed architecture is an extension of BERT, and their results are comparable to BERT's performance. Furthermore, their pre-training objectives are also similar to BERT's, indicating a lack of novelty in their contributions.\n",
      "\n",
      "\t-Opposition: I believe that novelty in transformer architecture is demonstrated. The opposition's claims of lack of novelty in our Transformer architecture are addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our pre-training objectives are not advancements, but rather a natural extension of existing methods. Furthermore, our model generalizes well to other tasks such as English constituency parsing, as demonstrated in Retrieved Evidence #3. This demonstrates that our contributions are novel and superior to existing models.\n",
      "\n",
      "Output your new response in the following JSON format:\n",
      "\n",
      "{\n",
      "    \"argument_title\": <should be a brief, 10-15 word string where the value is the main argument of your response to the opposition>,\n",
      "    \"description\": <2-3 sentence string explaining your response to the opposition's last turn, based on the provided pool of evidence>\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "REVISE ARGUMENT FOR AUTHOR 0:\n",
      "\n",
      "You are an author of a paper that is debating another author.\n",
      "\n",
      "Your debate claim is that your paper's contributions towards the \"topic\" are all novel relative to the other paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.\n",
      "\n",
      "Use the following evidence to support your arguments:\n",
      "\t- Evidence #1. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train\n",
      "\t- Evidence #2. Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]\n",
      "\t- Evidence #3. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. *Equal contribution\n",
      "\t- Evidence #4. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]\n",
      "\t- Evidence #5. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature\n",
      "\n",
      "You also had preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "\t- Opposition Claim #1: Does my paper also address the opposition's claim, \"bidirectional language model architecture\"?\n",
      "\t\t- Your Counter Evidence #1: We used a beam size of 21 and a = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]\n",
      "\t\t- Your Counter Evidence #2: Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]\n",
      "\n",
      "\t- Opposition Claim #2: Does my paper also address the opposition's claim, \"advancements in pre-training objectives\"?\n",
      "\t\t- Your Counter Evidence #1: During inference, we Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative semi-supervised semi-supervised semi-supervised semi-supervised semi-supervised multi-task generative Petrov et al\n",
      "\t\t- Your Counter Evidence #2: We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences\n",
      "\n",
      "Based on the debate history and your/your opposition's arguments and evidence, you must construct a stronger argument given the responses of the opposition.\n",
      "\n",
      "Debate Topic Information:\n",
      "\t- Topic: Transformer Architecture Offers Superior Performance\n",
      "\t- Topic Description: Our paper proposes a novel Transformer architecture that outperforms existing models in machine translation tasks. This is supported by evidence that our model achieves superior quality while requiring significantly less training time, as shown in Retrieved Evidence #1. Furthermore, our model generalizes well to other tasks such as English constituency parsing, as demonstrated in Retrieved Evidence #3.\n",
      "\n",
      "Debate History:\n",
      "\n",
      "\t-You: I argue that novel architecture and training methods outperform existing models. Our paper introduces a novel Transformer architecture that outperforms existing models in machine translation tasks. This is supported by evidence that our model achieves superior quality while requiring significantly less training time, as shown in Retrieved Evidence #1. Furthermore, our model generalizes well to other tasks such as English constituency parsing, as demonstrated in Retrieved Evidence #3. The opposition's claims of novelty are addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our pre-training objectives are not advancements, but rather a natural extension of existing methods.\n",
      "\t-Opposition: I argue that opposition's contributions lack novelty in transformer architecture. The opposition's contributions lack novelty in Transformer architecture as they build upon existing work, specifically BERT, which has already demonstrated superior performance. Their proposed architecture is an extension of BERT, and their results are comparable to BERT's performance. Furthermore, their pre-training objectives are also similar to BERT's, indicating a lack of novelty in their contributions.\n",
      "\n",
      "\t-You: I believe that novelty in transformer architecture is demonstrated. The opposition's claims of lack of novelty in our Transformer architecture are addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our pre-training objectives are not advancements, but rather a natural extension of existing methods. Furthermore, our model generalizes well to other tasks such as English constituency parsing, as demonstrated in Retrieved Evidence #3. This demonstrates that our contributions are novel and superior to existing models.\n",
      "\t-Opposition: I believe that opposition's contributions lack novelty in transformer architecture. The opposition's claims of novelty in their Transformer architecture are addressed by my counter evidence, which shows that their model performs well even without task-specific tuning and that their pre-training objectives are not advancements, but rather a natural extension of existing methods. Furthermore, their model's performance is comparable to BERT's performance, indicating a lack of novelty in their contributions. This demonstrates that their contributions are not novel and do not offer superior performance compared to existing models.\n",
      "\n",
      "\n",
      "Output your new, stronger argument in the following JSON format:\n",
      "{\n",
      "    \"argument_title\": <should be a brief, 10-15 word string where the value is your updated, strong response to the opposition's arguments>,\n",
      "    \"description\": <2-3 sentence string explaining your new argument as a response to the opposition's last turn, based on the provided pool of evidence>\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "REVISE ARGUMENT FOR AUTHOR 1:\n",
      "\n",
      "You are an author of a paper that is debating another author.\n",
      "\n",
      "Your debate claim is that the other paper's contributions towards the \"topic\" are not novel relative to your own paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.Use the following evidence to support your arguments:\n",
      "\t- Evidence #1. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations\n",
      "\t- Evidence #2. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017)\n",
      "\t- Evidence #3. 4We note that in the literature the bidirectional TransBERTBERTE[CLS]E1 E[SEP]...ENE1'...EM'CT1T[SEP]...TNT1'...TM'[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1'...EM'CT1T[SEP]...TNT1'...TM'[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., (cid:104) Question, Answer (cid:105)) in one token sequence. Throughout this work, a \"sentence\" can be an arbitrary span of contiguous text, rather than an actual linguistic sentence\n",
      "\t- Evidence #4. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. * BERT advances the state of the art for eleven NLP tasks\n",
      "\t- Evidence #5. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to preIn additrain a deep bidirectional Transformer. tion to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations\n",
      "\n",
      "You also had preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "\t- Opposition Claim #1: Does my paper also address the opposition's claim, \"transformer architecture offers superior performance\"?\n",
      "\t\t- Your Counter Evidence #1: BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model\n",
      "\t\t- Your Counter Evidence #2: (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)\n",
      "\n",
      "\t- Opposition Claim #2: Does my paper also address the opposition's claim, \"transformer architecture overcomes recurrent model limitations\"?\n",
      "\t\t- Your Counter Evidence #1: The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017)\n",
      "\t\t- Your Counter Evidence #2: (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)\n",
      "\n",
      "Based on the debate history and your/your opposition's arguments and evidence, you must construct a stronger argument given the responses of the opposition.\n",
      "\n",
      "Debate Topic Information:\n",
      "\t- Topic: Transformer Architecture Offers Superior Performance\n",
      "\t- Topic Description: Our paper proposes a novel Transformer architecture that outperforms existing models in machine translation tasks. This is supported by evidence that our model achieves superior quality while requiring significantly less training time, as shown in Retrieved Evidence #1. Furthermore, our model generalizes well to other tasks such as English constituency parsing, as demonstrated in Retrieved Evidence #3.\n",
      "\n",
      "Debate History:\n",
      "\n",
      "\t-Opposition: I argue that novel architecture and training methods outperform existing models. Our paper introduces a novel Transformer architecture that outperforms existing models in machine translation tasks. This is supported by evidence that our model achieves superior quality while requiring significantly less training time, as shown in Retrieved Evidence #1. Furthermore, our model generalizes well to other tasks such as English constituency parsing, as demonstrated in Retrieved Evidence #3. The opposition's claims of novelty are addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our pre-training objectives are not advancements, but rather a natural extension of existing methods.\n",
      "\t-You: I argue that opposition's contributions lack novelty in transformer architecture. The opposition's contributions lack novelty in Transformer architecture as they build upon existing work, specifically BERT, which has already demonstrated superior performance. Their proposed architecture is an extension of BERT, and their results are comparable to BERT's performance. Furthermore, their pre-training objectives are also similar to BERT's, indicating a lack of novelty in their contributions.\n",
      "\n",
      "\t-Opposition: I believe that novelty in transformer architecture is demonstrated. The opposition's claims of lack of novelty in our Transformer architecture are addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our pre-training objectives are not advancements, but rather a natural extension of existing methods. Furthermore, our model generalizes well to other tasks such as English constituency parsing, as demonstrated in Retrieved Evidence #3. This demonstrates that our contributions are novel and superior to existing models.\n",
      "\t-You: I believe that opposition's contributions lack novelty in transformer architecture. The opposition's claims of novelty in their Transformer architecture are addressed by my counter evidence, which shows that their model performs well even without task-specific tuning and that their pre-training objectives are not advancements, but rather a natural extension of existing methods. Furthermore, their model's performance is comparable to BERT's performance, indicating a lack of novelty in their contributions. This demonstrates that their contributions are not novel and do not offer superior performance compared to existing models.\n",
      "\n",
      "\t-Opposition: I argue that novel transformer architecture offers superior performance and generalizability. Our paper's contributions are novel and superior to existing models, as demonstrated by our model's performance in machine translation tasks and its generalizability to other tasks such as English constituency parsing. The opposition's claims of lack of novelty are addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our pre-training objectives are not advancements, but rather a natural extension of existing methods. Furthermore, our model's performance is not comparable to BERT's performance, but rather outperforms existing models in machine translation tasks.\n",
      "\n",
      "Output your new, stronger argument in the following JSON format:\n",
      "{\n",
      "    \"argument_title\": <should be a brief, 10-15 word string where the value is your updated, strong response to the opposition's arguments>,\n",
      "    \"description\": <2-3 sentence string explaining your new argument as a response to the opposition's last turn, based on the provided pool of evidence>\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convo_history = root_node.children[0].conduct_debate(paper_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate Topic Information:\n",
      "\t- Topic: Transformer Architecture Offers Superior Performance\n",
      "\t- Topic Description: Our paper proposes a novel Transformer architecture that outperforms existing models in machine translation tasks. This is supported by evidence that our model achieves superior quality while requiring significantly less training time, as shown in Retrieved Evidence #1. Furthermore, our model generalizes well to other tasks such as English constituency parsing, as demonstrated in Retrieved Evidence #3.\n",
      "\n",
      "Debate History:\n",
      "\n",
      "\t-Author 0: I argue that novel architecture and training methods outperform existing models. Our paper introduces a novel Transformer architecture that outperforms existing models in machine translation tasks. This is supported by evidence that our model achieves superior quality while requiring significantly less training time, as shown in Retrieved Evidence #1. Furthermore, our model generalizes well to other tasks such as English constituency parsing, as demonstrated in Retrieved Evidence #3. The opposition's claims of novelty are addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our pre-training objectives are not advancements, but rather a natural extension of existing methods.\n",
      "\t-Author 1: I argue that opposition's contributions lack novelty in transformer architecture. The opposition's contributions lack novelty in Transformer architecture as they build upon existing work, specifically BERT, which has already demonstrated superior performance. Their proposed architecture is an extension of BERT, and their results are comparable to BERT's performance. Furthermore, their pre-training objectives are also similar to BERT's, indicating a lack of novelty in their contributions.\n",
      "\n",
      "\t-Author 0: I believe that novelty in transformer architecture is demonstrated. The opposition's claims of lack of novelty in our Transformer architecture are addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our pre-training objectives are not advancements, but rather a natural extension of existing methods. Furthermore, our model generalizes well to other tasks such as English constituency parsing, as demonstrated in Retrieved Evidence #3. This demonstrates that our contributions are novel and superior to existing models.\n",
      "\t-Author 1: I believe that opposition's contributions lack novelty in transformer architecture. The opposition's claims of novelty in their Transformer architecture are addressed by my counter evidence, which shows that their model performs well even without task-specific tuning and that their pre-training objectives are not advancements, but rather a natural extension of existing methods. Furthermore, their model's performance is comparable to BERT's performance, indicating a lack of novelty in their contributions. This demonstrates that their contributions are not novel and do not offer superior performance compared to existing models.\n",
      "\n",
      "\t-Author 0: I argue that novel transformer architecture offers superior performance and generalizability. Our paper's contributions are novel and superior to existing models, as demonstrated by our model's performance in machine translation tasks and its generalizability to other tasks such as English constituency parsing. The opposition's claims of lack of novelty are addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our pre-training objectives are not advancements, but rather a natural extension of existing methods. Furthermore, our model's performance is not comparable to BERT's performance, but rather outperforms existing models in machine translation tasks.\n",
      "\t-Author 1: I argue that opposition's contributions lack novelty in transformer architecture. The opposition's claims of novelty in their Transformer architecture are addressed by my counter evidence, which shows that their model performs well even without task-specific tuning and that their pre-training objectives are not advancements, but rather a natural extension of existing methods. Furthermore, their model's performance is comparable to BERT's performance, indicating a lack of novelty in their contributions. This demonstrates that their contributions are not novel and do not offer superior performance compared to existing models.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(convo_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'argument_list': [{'title': 'Argument 1',\n",
       "   'description': 'Argument 1 description'},\n",
       "  {'title': 'Argument 2', 'description': 'Argument 2 description'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node.children[0].response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "queue_of_rounds: List[DebateNode] = []\n",
    "queue_of_rounds.extend(subtrees)\n",
    "\n",
    "\n",
    "while len(queue_of_rounds) > 0:\n",
    "    round = queue_of_rounds.pop(0)\n",
    "    conversation, new_focus_arg, new_cited_arg = round.conduct_debate(focus_paper, cited_paper)\n",
    "    conversation_history.extend(conversation)\n",
    "    if moderator.is_expand(round.self_delib, [new_focus_arg, new_cited_arg]):\n",
    "        new_subtrees = round.conduct_self_deliberation(round.round_topic, paper_authors)\n",
    "        queue_of_rounds.extend(new_subtrees)\n",
    "\n",
    "with open('conversation_history.txt', 'w+') as f:\n",
    "    f.write('\\n'.join(conversation_history))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
