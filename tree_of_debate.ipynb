{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishikaa2/miniconda3/envs/tod/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-07 14:58:39,086\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2,3\"\n",
    "from debate import DebateNode\n",
    "from paper_details import Paper\n",
    "from persona import PaperAuthor\n",
    "from moderator import Moderator\n",
    "import argparse\n",
    "from typing import List\n",
    "from vllm import LLM\n",
    "import os\n",
    "import json\n",
    "from data_pairer import parse_papers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.log_dir = \"logs\"\n",
    "        self.focus_paper = \"https://arxiv.org/pdf/1706.03762\"\n",
    "        self.cited_paper = \"https://arxiv.org/pdf/1810.04805\"\n",
    "        self.topic = \"language model architectures\"\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args.log_dir):\n",
    "    os.makedirs(args.log_dir)\n",
    "\n",
    "# if not os.path.exists(\"data.json\"):\n",
    "parse_papers(args.focus_paper, args.cited_paper)\n",
    "with open('data.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-07 14:58:51 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-07 14:58:51 arg_utils.py:957] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 11-07 14:58:51 config.py:1021] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 11-07 14:58:51 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 11-07 14:58:52 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-07 14:58:52 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2755133)\u001b[0;0m INFO 11-07 14:58:56 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-07 14:58:56 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2755133)\u001b[0;0m INFO 11-07 14:58:56 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2755133)\u001b[0;0m INFO 11-07 14:58:56 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-07 14:58:56 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-07 14:58:56 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ishikaa2/.cache/vllm/gpu_p2p_access_cache_for_2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2755133)\u001b[0;0m INFO 11-07 14:58:56 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ishikaa2/.cache/vllm/gpu_p2p_access_cache_for_2,3.json\n",
      "INFO 11-07 14:58:56 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fd4800dd600>, local_subscribe_port=36601, remote_subscribe_port=None)\n",
      "INFO 11-07 14:58:56 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2755133)\u001b[0;0m INFO 11-07 14:58:56 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 11-07 14:58:57 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2755133)\u001b[0;0m INFO 11-07 14:58:57 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.57it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.62it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.93it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.67it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.78it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-07 14:58:59 model_runner.py:1067] Loading model weights took 7.5122 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2755133)\u001b[0;0m INFO 11-07 14:59:00 model_runner.py:1067] Loading model weights took 7.5122 GB\n",
      "INFO 11-07 14:59:00 distributed_gpu_executor.py:57] # GPU blocks: 13892, # CPU blocks: 4096\n",
      "INFO 11-07 14:59:00 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 1.70x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2755133)\u001b[0;0m INFO 11-07 14:59:02 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2755133)\u001b[0;0m INFO 11-07 14:59:02 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-07 14:59:02 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-07 14:59:02 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-07 14:59:11 custom_all_reduce.py:233] Registering 1040 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2755133)\u001b[0;0m INFO 11-07 14:59:11 custom_all_reduce.py:233] Registering 1040 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2755133)\u001b[0;0m INFO 11-07 14:59:11 model_runner.py:1523] Graph capturing finished in 9 secs.\n",
      "INFO 11-07 14:59:11 model_runner.py:1523] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "model_server = LLM(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",tensor_parallel_size=2, gpu_memory_utilization=0.5, max_num_seqs=100) #,enable_prefix_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = data[0]\n",
    "f_pap = item['focus']\n",
    "c_pap = item['cited']\n",
    "\n",
    "focus_paper = PaperAuthor(\n",
    "    model = model_server,\n",
    "    paper = Paper(f_pap),\n",
    "    focus=True,\n",
    "    id=0\n",
    ")\n",
    "\n",
    "cited_paper = PaperAuthor(\n",
    "    model = model_server,\n",
    "    paper = Paper(c_pap),\n",
    "    focus=False,\n",
    "    id=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1024,) and (768,) not aligned: 1024 (dim 0) != 768 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfocus_paper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpaper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_top_k\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tree-of-debate/paper_details.py:18\u001b[0m, in \u001b[0;36mPaper.retrieve_top_k\u001b[0;34m(self, query, k)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_top_k\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_top_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tree-of-debate/retrieval/retrieval.py:34\u001b[0m, in \u001b[0;36mfind_top_k\u001b[0;34m(query, corpus_emb, k)\u001b[0m\n\u001b[1;32m     32\u001b[0m similarities \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk, emb \u001b[38;5;129;01min\u001b[39;00m corpus_emb\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 34\u001b[0m     sim \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     similarities\u001b[38;5;241m.\u001b[39mappend((chunk, sim))\n\u001b[1;32m     36\u001b[0m similarities\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/tree-of-debate/retrieval/retrieval.py:27\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(vec1, vec2)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m norm1 \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m norm2 \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvec1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvec2\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m (norm1 \u001b[38;5;241m*\u001b[39m norm2)\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1024,) and (768,) not aligned: 1024 (dim 0) != 768 (dim 0)"
     ]
    }
   ],
   "source": [
    "focus_paper.paper.retrieve_top_k(\"transformers\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderator = Moderator(model_server)\n",
    "\n",
    "paper_authors = [focus_paper, cited_paper]\n",
    "leaf_node_label = args.topic\n",
    "\n",
    "if args.log_dir != \"\":\n",
    "    with open(os.path.join(args.log_dir, 'self_deliberation.txt'), 'w') as f:\n",
    "        f.write(f'Topic: {args.topic}\\n\\n')\n",
    "\n",
    "# each node has a topic\n",
    "root_node = DebateNode(leaf_node_label)\n",
    "# subtrees = root_node.conduct_self_deliberation(leaf_node_label, paper_authors, log=args.log_dir) # k new, finer topics to discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m paper_author \u001b[38;5;129;01min\u001b[39;00m paper_authors:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# gather evidence\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     evidence, scores \u001b[38;5;241m=\u001b[39m paper_author\u001b[38;5;241m.\u001b[39mgather_evidence(topic, return_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m paper_author\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m evidence\u001b[38;5;241m.\u001b[39mkeys(): evidence[paper_author\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m     evidence[paper_author\u001b[38;5;241m.\u001b[39mid]\u001b[38;5;241m.\u001b[39mappend(evidence)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# develop k arguments\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "topic = args.topic\n",
    "log = None\n",
    "focus_paper = None\n",
    "arguments = []\n",
    "children = []\n",
    "evidence = {}\n",
    "\n",
    "for paper_author in paper_authors:\n",
    "    # gather evidence\n",
    "    evidence, scores = paper_author.gather_evidence(topic, return_scores=True)\n",
    "\n",
    "    if paper_author.id not in evidence.keys(): evidence[paper_author.id] = []\n",
    "    evidence[paper_author.id].append(evidence)\n",
    "\n",
    "    # develop k arguments\n",
    "    if paper_author.id not in arguments.keys(): arguments[paper_author.id] = []\n",
    "    author_args = paper_author.generate_arguments(topic, evidence,k=2)\n",
    "    arguments[paper_author.id].append(author_args)\n",
    "\n",
    "    # check if paper is the focus\n",
    "    if paper_author.focus:\n",
    "        focus_paper = paper_author\n",
    "\n",
    "    # logging\n",
    "    if log is not None:\n",
    "        with open(os.path.join(log, 'self_deliberation.txt'), 'a') as f:\n",
    "            f.write(f'Topic: {topic}\\n\\n')\n",
    "            f.write(f'Gather Evidence:\\n\\n')\n",
    "            temp = \"\\n\".join([f'{s} - {e}' for s, e in zip(scores, evidence)])\n",
    "            f.write(f'{paper_author.focus} paper:\\n{temp}\\n\\n')\n",
    "\n",
    "            f.write(f'Develop Arguments:\\n\\n')\n",
    "            f.write(f'{paper_author.focus} paper:\\n{author_args}\\n\\n')\n",
    "\n",
    "# preemption\n",
    "for i in range(len(paper_authors)):\n",
    "    other_arguments = [arguments[paper_authors[j].id] for j in range(len(paper_authors)) if j != i]\n",
    "    other_evidence = [evidence[paper_authors[j].id] for j in range(len(paper_authors)) if j != i]\n",
    "\n",
    "    preemption = paper_authors[i].preempt_arguments(other_arguments, other_evidence)\n",
    "    \n",
    "    # logging\n",
    "    if log is not None:\n",
    "        with open(os.path.join(log, 'self_deliberation.txt'), 'a') as f:\n",
    "            f.write(f'Preemption:\\n\\n')\n",
    "            f.write(f'{paper_author.focus} paper:\\n{preemption}\\n\\n')\n",
    "\n",
    "    evidence[paper_authors[i].id].append(preemption)         \n",
    "\n",
    "# for child_topic in arguments[focus_paper.id]:\n",
    "#     children.append(DebateNode(child_topic, parent=self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "queue_of_rounds: List[DebateNode] = []\n",
    "queue_of_rounds.extend(subtrees)\n",
    "\n",
    "\n",
    "while len(queue_of_rounds) > 0:\n",
    "    round = queue_of_rounds.pop(0)\n",
    "    conversation, new_focus_arg, new_cited_arg = round.conduct_debate(focus_paper, cited_paper)\n",
    "    conversation_history.extend(conversation)\n",
    "    if moderator.is_expand(round.arguments, [new_focus_arg, new_cited_arg]):\n",
    "        new_subtrees = round.conduct_self_deliberation(round.round_topic, paper_authors)\n",
    "        queue_of_rounds.extend(new_subtrees)\n",
    "\n",
    "with open('conversation_history.txt', 'w+') as f:\n",
    "    f.write('\\n'.join(conversation_history))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
