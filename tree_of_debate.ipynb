{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishikaa2/miniconda3/envs/tod/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-08 13:11:17,815\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2,3\"\n",
    "from debate import DebateNode\n",
    "from paper_details import Paper\n",
    "from persona import PaperAuthor\n",
    "from moderator import Moderator\n",
    "import argparse\n",
    "from typing import List\n",
    "from vllm import LLM\n",
    "import os\n",
    "import json\n",
    "from data_pairer import parse_papers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        log_dir = \"logs\"\n",
    "        focus_paper = \"https://arxiv.org/pdf/1706.03762\"\n",
    "        cited_paper = \"https://arxiv.org/pdf/1810.04805\"\n",
    "        topic = \"language model architectures\"\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args.log_dir):\n",
    "    os.makedirs(args.log_dir)\n",
    "\n",
    "# if not os.path.exists(\"data.json\"):\n",
    "parse_papers(args.focus_paper, args.cited_paper)\n",
    "with open('data.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-08 13:12:02 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-08 13:12:02 arg_utils.py:957] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 11-08 13:12:02 config.py:1021] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 11-08 13:12:02 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 11-08 13:12:02 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-08 13:12:02 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3169513)\u001b[0;0m INFO 11-08 13:12:06 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-08 13:12:06 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3169513)\u001b[0;0m INFO 11-08 13:12:06 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-08 13:12:06 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3169513)\u001b[0;0m INFO 11-08 13:12:06 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-08 13:12:07 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ishikaa2/.cache/vllm/gpu_p2p_access_cache_for_2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3169513)\u001b[0;0m INFO 11-08 13:12:07 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ishikaa2/.cache/vllm/gpu_p2p_access_cache_for_2,3.json\n",
      "INFO 11-08 13:12:07 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f202995c400>, local_subscribe_port=43185, remote_subscribe_port=None)\n",
      "INFO 11-08 13:12:07 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3169513)\u001b[0;0m INFO 11-08 13:12:07 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 11-08 13:12:07 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3169513)\u001b[0;0m INFO 11-08 13:12:07 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.52it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.66it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.89it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.62it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.74it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-08 13:12:10 model_runner.py:1067] Loading model weights took 7.5122 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3169513)\u001b[0;0m INFO 11-08 13:12:10 model_runner.py:1067] Loading model weights took 7.5122 GB\n",
      "INFO 11-08 13:12:11 distributed_gpu_executor.py:57] # GPU blocks: 13892, # CPU blocks: 4096\n",
      "INFO 11-08 13:12:11 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 1.70x\n",
      "INFO 11-08 13:12:12 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-08 13:12:12 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3169513)\u001b[0;0m INFO 11-08 13:12:12 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3169513)\u001b[0;0m INFO 11-08 13:12:12 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-08 13:12:21 custom_all_reduce.py:233] Registering 1040 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3169513)\u001b[0;0m INFO 11-08 13:12:21 custom_all_reduce.py:233] Registering 1040 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3169513)\u001b[0;0m INFO 11-08 13:12:21 model_runner.py:1523] Graph capturing finished in 9 secs.\n",
      "INFO 11-08 13:12:21 model_runner.py:1523] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "model_server = LLM(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",tensor_parallel_size=2, gpu_memory_utilization=0.5, max_num_seqs=100) #,enable_prefix_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "item = data[0]\n",
    "f_pap = item['focus']\n",
    "c_pap = item['cited']\n",
    "\n",
    "focus_paper = PaperAuthor(\n",
    "    model = model_server,\n",
    "    paper = Paper(f_pap),\n",
    "    focus=True,\n",
    "    id=0\n",
    ")\n",
    "\n",
    "cited_paper = PaperAuthor(\n",
    "    model = model_server,\n",
    "    paper = Paper(c_pap),\n",
    "    focus=False,\n",
    "    id=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor',\n",
       "  0.8149391),\n",
       " ('      Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht-1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples',\n",
       "  0.80097723),\n",
       " ('On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours',\n",
       "  0.80001384),\n",
       " ('Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training This section describes the training regime for our models. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]',\n",
       "  0.7986704),\n",
       " ('Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output',\n",
       "  0.79826057)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_paper.paper.retrieve_top_k(\"language model architectures\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor',\n",
       "  0.8341009),\n",
       " ('Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]',\n",
       "  0.82007766),\n",
       " ('i  Rdmodelxdk , W K i  Rdmodelxdk , W V i  Rdmodelxdv In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: * In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence',\n",
       "  0.8193624),\n",
       " ('On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours',\n",
       "  0.8136689),\n",
       " ('The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as: Attention(Q, K, V ) = softmax( QK Tdk)V(1) The two most commonly used attention functions are additive attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor . Additive attention computes the compatibility function using a feed-forward network withof a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.dk While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]',\n",
       "  0.81150734)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_paper.paper.retrieve_top_k(\"multi headed attention\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderator = Moderator(model_server)\n",
    "\n",
    "paper_authors = [focus_paper, cited_paper]\n",
    "leaf_node_label = args.topic\n",
    "\n",
    "if args.log_dir != \"\":\n",
    "    with open(os.path.join(args.log_dir, 'self_deliberation.txt'), 'w') as f:\n",
    "        f.write(f'Topic: {args.topic}\\n\\n')\n",
    "\n",
    "# each node has a topic\n",
    "root_node = DebateNode(leaf_node_label)\n",
    "# subtrees = root_node.conduct_self_deliberation(leaf_node_label, paper_authors, log=args.log_dir) # k new, finer topics to discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence, scores = paper_authors[0].gather_evidence(args.topic, return_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor',\n",
       " '      Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht-1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples',\n",
       " 'On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours',\n",
       " 'Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training This section describes the training regime for our models. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]',\n",
       " 'Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.11s/it, est. speed input: 99.07 toks/s, output: 56.63 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"argument_list\": [{\"title\": \"RNNs are outperform by newer language model architectures\", \"description\": \"Newer architectures such as attention-based models, have outperformed traditional RNN-based models in sequence modeling and transduction problems such as machine translation and language modeling.\", \"evidence\": [\"Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5].\", \"Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].\", \"Recurrent models typically factor computation along the symbol positions of the input and output sequences.\", \"On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.\", \"In the former task our best model outperforms even all previously reported ensembles.\"]}, {\"title\": \"Newer architectures, such as self-attention, are necessary for sequence modeling due to the inherent sequential nature of RNNs\", \"description\": \"The sequential nature of RNNs precludes parallelization within training examples, which can become a limiting factor as sequence lengths increase, suggesting a need for newer architectures that can handle parallelization, such as self-attention.\" , \"evidence\": [\"Recurrent models typically factor computation along the symbol positions of the input and output sequences.\", \"Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht-1 and the input for position t.\", \"This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples\", \"Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences\", \"Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences\"]}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.08s/it, est. speed input: 106.74 toks/s, output: 55.48 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"argument_list\": [{\"title\": \"Bidirectional Transformer architectures outperform unidirectional architectures on a variety of linguistic tasks.\", \"description\": \"Extensive studies have shown that the pre-training objectives, Masked Language Modeling (MLM) and Next Sentence Prediction (NSP), lead to the development of powerful bidirectional transformer architectures, such as BERT, which achieve state-of-the-art results on a variety of NLP tasks, including question answering, natural language inference, and sentiment analysis. In addition, the ability of BERT to handle a variety of downstream tasks is a result of its input representation, which can unambiguously represent both a single sentence and a pair of sentences in one token sequence. Furthermore, studies have shown that fine-tuning pre-trained Bidirectional architectures with downstream specific objectives can unlock even state-of-the-art results, demonstrating the superiority of Bidirectional transformer architectures. \\\"Evidence\\\": [\" ,\"evidence\": [\"The same pre-trained model parameters are used to initialize models for different downstream tasks.\", \"these results enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks.\", \"-fedus WUA }.stem (\"]},{\"title\": \"Unidirectional language model architectures provide a more efficient alternative for linguistic tasks requiring less computational resources.\", \"description\": \"A key advantage of unidirectional architectures is their ability to handle the need for less computational resources. Studies have shown that deep unidirectional architectures can benefit even low-resource tasks, suggesting that a subset of tasks may not require the computational prowess of bidirectional models. Furthermore, fine-tuning is directly discussed about their potential in this direction to add efficiently. Also for designing type of Sentence embeddings.S-A;if sentence esome research especial hoànite additional variant with with entities mention going atoms behaviors such other several ways incorporatedWSF-onlyhap indicatesFullFor some pre-training objectives such as the cloze task, can improve robustness of text task generation models correspondingwhile-related Natural drawing raceprobablyNot to shall Tasks formab)).\",\"evidence\":[\"it can be used for text generation\"]}] }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    }
   ],
   "source": [
    "topic = args.topic\n",
    "log = None\n",
    "focus_paper = None\n",
    "arguments = {}\n",
    "children = []\n",
    "evidences = {}\n",
    "\n",
    "focus_paper = None\n",
    "for paper_author in paper_authors:\n",
    "    # gather evidence\n",
    "    evidence, scores = paper_author.gather_evidence(topic, return_scores=True)\n",
    "\n",
    "    if paper_author.id not in evidences.keys(): evidences[paper_author.id] = []\n",
    "    evidences[paper_author.id].append(evidence)\n",
    "\n",
    "    # develop k arguments\n",
    "    if paper_author.id not in arguments.keys(): arguments[paper_author.id] = []\n",
    "    author_args = paper_author.generate_arguments(topic, evidence,k=2)\n",
    "    arguments[paper_author.id].append(author_args)\n",
    "\n",
    "    # check if paper is the focus\n",
    "    if paper_author.focus:\n",
    "        focus_paper = paper_author\n",
    "\n",
    "    # logging\n",
    "    if log is not None:\n",
    "        with open(os.path.join(log, 'self_deliberation.txt'), 'a') as f:\n",
    "            f.write(f'Topic: {topic}\\n\\n')\n",
    "            f.write(f'Gather Evidence:\\n\\n')\n",
    "            temp = \"\\n\".join([f'{s} - {e}' for s, e in zip(scores, evidence)])\n",
    "            f.write(f'{paper_author.focus} paper:\\n{temp}\\n\\n')\n",
    "\n",
    "            f.write(f'Develop Arguments:\\n\\n')\n",
    "            f.write(f'{paper_author.focus} paper:\\n{author_args}\\n\\n')\n",
    "\n",
    "# preemption\n",
    "for i in range(len(paper_authors)):\n",
    "    other_arguments = [arguments[paper_authors[j].id] for j in range(len(paper_authors)) if j != i]\n",
    "    other_evidence = [evidences[paper_authors[j].id] for j in range(len(paper_authors)) if j != i]\n",
    "\n",
    "    preemption = paper_authors[i].preempt_arguments(other_arguments, other_evidence)\n",
    "    \n",
    "    # logging\n",
    "    if log is not None:\n",
    "        with open(os.path.join(log, 'self_deliberation.txt'), 'a') as f:\n",
    "            f.write(f'Preemption:\\n\\n')\n",
    "            f.write(f'{paper_author.focus} paper:\\n{preemption}\\n\\n')\n",
    "\n",
    "    evidences[paper_authors[i].id].append(preemption)         \n",
    "\n",
    "# for child_topic in arguments[focus_paper.id]:\n",
    "#     children.append(DebateNode(child_topic, parent=self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Does my paper also include or address [{\\'argument_list\\': [{\\'title\\': \\'Bidirectional Transformer architectures outperform unidirectional architectures on a variety of linguistic tasks.\\', \\'description\\': \\'Extensive studies have shown that the pre-training objectives, Masked Language Modeling (MLM) and Next Sentence Prediction (NSP), lead to the development of powerful bidirectional transformer architectures, such as BERT, which achieve state-of-the-art results on a variety of NLP tasks, including question answering, natural language inference, and sentiment analysis. In addition, the ability of BERT to handle a variety of downstream tasks is a result of its input representation, which can unambiguously represent both a single sentence and a pair of sentences in one token sequence. Furthermore, studies have shown that fine-tuning pre-trained Bidirectional architectures with downstream specific objectives can unlock even state-of-the-art results, demonstrating the superiority of Bidirectional transformer architectures. \"Evidence\": [\\', \\'evidence\\': [\\'The same pre-trained model parameters are used to initialize models for different downstream tasks.\\', \\'these results enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks.\\', \\'-fedus WUA }.stem (\\']}, {\\'title\\': \\'Unidirectional language model architectures provide a more efficient alternative for linguistic tasks requiring less computational resources.\\', \\'description\\': \\'A key advantage of unidirectional architectures is their ability to handle the need for less computational resources. Studies have shown that deep unidirectional architectures can benefit even low-resource tasks, suggesting that a subset of tasks may not require the computational prowess of bidirectional models. Furthermore, fine-tuning is directly discussed about their potential in this direction to add efficiently. Also for designing type of Sentence embeddings.S-A;if sentence esome research especial hoanite additional variant with with entities mention going atoms behaviors such other several ways incorporatedWSF-onlyhap indicatesFullFor some pre-training objectives such as the cloze task, can improve robustness of text task generation models correspondingwhile-related Natural drawing raceprobablyNot to shall Tasks formab)).\\', \\'evidence\\': [\\'it can be used for text generation\\']}]}]?': (['We used a beam size of 21 and a = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K sentences. 7 Conclusion In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers',\n",
       "   'This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input',\n",
       "   'Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training This section describes the training regime for our models. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]',\n",
       "   'Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output',\n",
       "   'Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model'],\n",
       "  [0.93159544, 0.9313577, 0.9292154, 0.92902064, 0.9282186]),\n",
       " 'Does my paper propose a better claim/idea than \"[{\\'argument_list\\': [{\\'title\\': \\'Bidirectional Transformer architectures outperform unidirectional architectures on a variety of linguistic tasks.\\', \\'description\\': \\'Extensive studies have shown that the pre-training objectives, Masked Language Modeling (MLM) and Next Sentence Prediction (NSP), lead to the development of powerful bidirectional transformer architectures, such as BERT, which achieve state-of-the-art results on a variety of NLP tasks, including question answering, natural language inference, and sentiment analysis. In addition, the ability of BERT to handle a variety of downstream tasks is a result of its input representation, which can unambiguously represent both a single sentence and a pair of sentences in one token sequence. Furthermore, studies have shown that fine-tuning pre-trained Bidirectional architectures with downstream specific objectives can unlock even state-of-the-art results, demonstrating the superiority of Bidirectional transformer architectures. \"Evidence\": [\\', \\'evidence\\': [\\'The same pre-trained model parameters are used to initialize models for different downstream tasks.\\', \\'these results enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks.\\', \\'-fedus WUA }.stem (\\']}, {\\'title\\': \\'Unidirectional language model architectures provide a more efficient alternative for linguistic tasks requiring less computational resources.\\', \\'description\\': \\'A key advantage of unidirectional architectures is their ability to handle the need for less computational resources. Studies have shown that deep unidirectional architectures can benefit even low-resource tasks, suggesting that a subset of tasks may not require the computational prowess of bidirectional models. Furthermore, fine-tuning is directly discussed about their potential in this direction to add efficiently. Also for designing type of Sentence embeddings.S-A;if sentence esome research especial hoanite additional variant with with entities mention going atoms behaviors such other several ways incorporatedWSF-onlyhap indicatesFullFor some pre-training objectives such as the cloze task, can improve robustness of text task generation models correspondingwhile-related Natural drawing raceprobablyNot to shall Tasks formab)).\\', \\'evidence\\': [\\'it can be used for text generation\\']}]}]\"?': (['This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input',\n",
       "   'We used a beam size of 21 and a = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K sentences. 7 Conclusion In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers',\n",
       "   'Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output',\n",
       "   'Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training This section describes the training regime for our models. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]',\n",
       "   'Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model'],\n",
       "  [0.9316725, 0.93082386, 0.92839825, 0.92777026, 0.92711246])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidences[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "queue_of_rounds: List[DebateNode] = []\n",
    "queue_of_rounds.extend(subtrees)\n",
    "\n",
    "\n",
    "while len(queue_of_rounds) > 0:\n",
    "    round = queue_of_rounds.pop(0)\n",
    "    conversation, new_focus_arg, new_cited_arg = round.conduct_debate(focus_paper, cited_paper)\n",
    "    conversation_history.extend(conversation)\n",
    "    if moderator.is_expand(round.arguments, [new_focus_arg, new_cited_arg]):\n",
    "        new_subtrees = round.conduct_self_deliberation(round.round_topic, paper_authors)\n",
    "        queue_of_rounds.extend(new_subtrees)\n",
    "\n",
    "with open('conversation_history.txt', 'w+') as f:\n",
    "    f.write('\\n'.join(conversation_history))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
