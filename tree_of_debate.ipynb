{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/shared/data3/pk36/.cache'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=/shared/data3/pk36/.cache\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from debate import DebateNode\n",
    "from paper_details import Paper\n",
    "from persona import PaperAuthor\n",
    "from moderator import Moderator\n",
    "import argparse\n",
    "from typing import List\n",
    "from vllm import LLM\n",
    "import os\n",
    "import json\n",
    "from data_pairer import parse_papers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.log_dir = \"logs\"\n",
    "        self.focus_paper = \"https://arxiv.org/pdf/1706.03762\"\n",
    "        self.cited_paper = \"https://arxiv.org/pdf/1810.04805\"\n",
    "        self.topic = \"language model architectures\"\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args.log_dir):\n",
    "    os.makedirs(args.log_dir)\n",
    "\n",
    "# if not os.path.exists(\"data.json\"):\n",
    "parse_papers(args.focus_paper, args.cited_paper)\n",
    "with open('data.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-23 07:02:12 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-23 07:02:12 arg_utils.py:957] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 11-23 07:02:12 config.py:1021] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 11-23 07:02:12 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 11-23 07:02:12 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-23 07:02:12 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=132748)\u001b[0;0m INFO 11-23 07:02:13 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-23 07:02:13 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-23 07:02:13 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=132748)\u001b[0;0m INFO 11-23 07:02:13 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=132748)\u001b[0;0m INFO 11-23 07:02:13 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-23 07:02:14 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/pk36/.cache/vllm/gpu_p2p_access_cache_for_6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=132748)\u001b[0;0m INFO 11-23 07:02:14 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/pk36/.cache/vllm/gpu_p2p_access_cache_for_6,7.json\n",
      "INFO 11-23 07:02:14 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f89eeff9370>, local_subscribe_port=39909, remote_subscribe_port=None)\n",
      "INFO 11-23 07:02:14 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=132748)\u001b[0;0m INFO 11-23 07:02:14 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=132748)\u001b[0;0m INFO 11-23 07:02:14 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 11-23 07:02:14 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c52ab9b6d344448b18174b9f28452b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=132748)\u001b[0;0m INFO 11-23 07:02:18 model_runner.py:1067] Loading model weights took 7.5122 GB\n",
      "INFO 11-23 07:02:18 model_runner.py:1067] Loading model weights took 7.5122 GB\n",
      "INFO 11-23 07:02:19 distributed_gpu_executor.py:57] # GPU blocks: 15590, # CPU blocks: 4096\n",
      "INFO 11-23 07:02:19 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 1.90x\n",
      "INFO 11-23 07:02:21 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=132748)\u001b[0;0m INFO 11-23 07:02:21 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-23 07:02:21 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=132748)\u001b[0;0m INFO 11-23 07:02:21 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-23 07:02:29 custom_all_reduce.py:233] Registering 1040 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=132748)\u001b[0;0m INFO 11-23 07:02:29 custom_all_reduce.py:233] Registering 1040 cuda graph addresses\n",
      "INFO 11-23 07:02:29 model_runner.py:1523] Graph capturing finished in 8 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=132748)\u001b[0;0m INFO 11-23 07:02:29 model_runner.py:1523] Graph capturing finished in 8 secs.\n"
     ]
    }
   ],
   "source": [
    "model_server = LLM(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",tensor_parallel_size=2, gpu_memory_utilization=0.5, max_num_seqs=100) #,enable_prefix_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.90it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "item = data[0]\n",
    "f_pap = item['focus']\n",
    "c_pap = item['cited']\n",
    "\n",
    "focus_paper = PaperAuthor(\n",
    "    model = model_server,\n",
    "    paper = Paper(f_pap),\n",
    "    focus=True,\n",
    "    id=0\n",
    ")\n",
    "\n",
    "cited_paper = PaperAuthor(\n",
    "    model = model_server,\n",
    "    paper = Paper(c_pap),\n",
    "    focus=False,\n",
    "    id=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train',\n",
       "  0.73787886),\n",
       " ('Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]',\n",
       "  0.73269325),\n",
       " ('We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. *Equal contribution',\n",
       "  0.7009047),\n",
       " ('This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]',\n",
       "  0.7007855),\n",
       " ('We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature',\n",
       "  0.69941336)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_paper.paper.retrieve_top_k(\"language model architectures\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('With a single attention head, averaging inhibits this. MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O where headi = Attention(QW Q i , KW Ki , V W Vi ) Where the projections are parameter matrices W Q and W O Rhdvxdmodel',\n",
       "  0.8372327),\n",
       " ('We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences',\n",
       "  0.7794119),\n",
       " ('These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions',\n",
       "  0.77801174),\n",
       " ('The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]',\n",
       "  0.76742196),\n",
       " ('While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality',\n",
       "  0.7652785)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_paper.paper.retrieve_top_k(\"multi headed attention\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 39.37it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.47s/it, est. speed input: 179.10 toks/s, output: 63.62 toks/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 38.56it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.51s/it, est. speed input: 185.76 toks/s, output: 64.10 toks/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 44.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 39.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 35.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 41.32it/s]\n"
     ]
    }
   ],
   "source": [
    "moderator = Moderator(model_server)\n",
    "\n",
    "paper_authors = [focus_paper, cited_paper]\n",
    "leaf_node_label = args.topic\n",
    "\n",
    "if args.log_dir != \"\":\n",
    "    with open(os.path.join(args.log_dir, 'self_deliberation.txt'), 'w') as f:\n",
    "        f.write(f'Topic: {args.topic}\\n\\n')\n",
    "\n",
    "# each node has a topic\n",
    "root_node = DebateNode(leaf_node_label)\n",
    "subtrees = root_node.conduct_self_deliberation(leaf_node_label, paper_authors, log=args.log_dir) # k new, finer topics to discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [{'title': 'New Architecture Significantly Outperforms Traditional RNN Architectures',\n",
       "   'description': 'Our Transformer architecture is more effective than traditional RNN models in two key translation tasks, demonstrating superiority in quality. We based our conclusion on experiments with two machine translation tasks showing that models are superior in quality while being more parallelizable and requiring significantly less time to train.'},\n",
       "  {'title': 'The Transformer Helps Solve Major Challenges in Sequence Modelling Tasks',\n",
       "   'description': 'The proposed Transformer architecture has generalizability skills helping the model complete tasks such as English constituency parsing even with limited data successfuly,We outshine with state-of-the-art results in such hard data regimes that other RNN sequence-to-sequence models are unable to reach.'}],\n",
       " 1: [{'title': 'Advancements in Bidirectionality for Pre-training Language Models',\n",
       "   'description': 'Our work offers a novel bidirectional pre-training paradigm which can handle sequence dependencies in both directions, representing context-sensitive features by concatenating left-to-right and right-to-left representations. Evidence like 1 and 2 illustrate the deficiency of traditional unidirectional models, highlighting the need for a more comprehensive approach. A deep bidirectional Transformer like the one proposed in evidence 5 effectively exploits the power of bidirectionality, leading to gains in pre-training capabilities.'},\n",
       "  {'title': 'Enriched Pre-training Objectives Promote Deeper Representation Learning',\n",
       "   'description': 'The incorporation of a next sentence prediction task, along with the masked language model, is a critical contribution that allows for a more nuanced understanding of context and enables pre-training a deeper bidirectional model. Evidence 4 shows the groundbreaking impact of this approach, achieving state-of-the-art performance across various NLP tasks. This indicates that a more complex pre-training objective is crucial for unlocking rich representations in language models.'}]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node.self_delib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'Does my paper also address the claim, \"advancements in bidirectionality for pre-training language models\"?': ['We used a beam size of 21 and a = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]',\n",
       "   'In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing'],\n",
       "  'Does my paper also address the claim, \"enriched pre-training objectives promote deeper representation learning\"?': ['We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training',\n",
       "   'The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9]']},\n",
       " 1: {'Does my paper also address the claim, \"new architecture significantly outperforms traditional rnn architectures\"?': ['The results compared to prior leaderboard entries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, excluding systems that use BERT as one of their components. We observe a +5.1 F1 improvement over the previous best system',\n",
       "   'The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark (Wang language modelLeft-to-right et al., 2018a)'],\n",
       "  'Does my paper also address the claim, \"the transformer helps solve major challenges in sequence modelling tasks\"?': ['As we show in Figure 1, C is used for next sentence prediction (NSP).5 Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI. 6 former is often referred to as a \"Transformer encoder\" while the left-context-only version is referred to as a \"Transformer decoder\" since it can be used for text generation',\n",
       "   'It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences. 3.2 Fine-tuning BERT Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream tasks-whether they involve single text or text pairs--by swapping out the appropriate inputs and outputs']}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node.preemption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an author of a paper that is debating another author about the following topic:\n",
      "\t- Topic: New Architecture Significantly Outperforms Traditional RNN Architectures\n",
      "\t- Topic Description: Our Transformer architecture is more effective than traditional RNN models in two key translation tasks, demonstrating superiority in quality. We based our conclusion on experiments with two machine translation tasks showing that models are superior in quality while being more parallelizable and requiring significantly less time to train.\n",
      "\n",
      "Your debate claim is that your paper's contributions towards the topic are all novel relative to the other paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.\n",
      "\n",
      "You used the following evidence to support your arguments:\n",
      "\t- Evidence #1. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train\n",
      "\t- Evidence #2. Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]\n",
      "\t- Evidence #3. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. *Equal contribution\n",
      "\t- Evidence #4. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]\n",
      "\t- Evidence #5. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature\n",
      "\n",
      "You also have preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "Does my paper also address the opposition's claim, \"advancements in bidirectionality for pre-training language models\"?:\n",
      "\t- Counter Evidence #1: We used a beam size of 21 and a = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]\n",
      "\t- Counter Evidence #2: In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing\n",
      "\n",
      "Does my paper also address the opposition's claim, \"enriched pre-training objectives promote deeper representation learning\"?:\n",
      "\t- Counter Evidence #1: We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training\n",
      "\t- Counter Evidence #2: The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9]\n",
      "\n",
      "\n",
      "Given the above (your initial argument, your evidence, the opposition paper's claimed points of novelty, and your counter evidence), make an argument on why your paper's contributions towards the topic are all novel relative to the other paper, with respect to the topic, New Architecture Significantly Outperforms Traditional RNN Architectures. \n",
      "\n",
      "\n",
      "Output your argument in the following JSON format:\n",
      "\n",
      "{\n",
      "    \"title\": <should be a brief, 10-15 word string where the value is the argument title>,\n",
      "    \"description\": <2-3 sentence string explaining the argument>\n",
      "}\n",
      "\n",
      "PRESENTING ARGUMENTS FOR True PAPER:\n",
      "\n",
      "{ \"title\": \"Consistent Superiority in Sequence Modeling Tasks for the Proposed Transformer Architecture.\", \"description\": \"Our paper's contributions are novel and significant in sequence modeling and transduction, consistently surpassing traditional RNN architectures in accuracy and efficiency. Through a rigorous critique of the opposing author's points of novelty and the evidence presented in our own paper, we demonstrate the superiority of the Transformer architecture over traditional RNN models in multiple machine translation tasks.\" }\n"
     ]
    }
   ],
   "source": [
    "out = focus_paper.present_argument(root_node.children[0], root_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an author of a paper that is debating another author about their claimed novelty:\n",
      "\t- Novelty Claim: New Architecture Significantly Outperforms Traditional RNN Architectures\n",
      "\t- Novelty Claim Description: Our Transformer architecture is more effective than traditional RNN models in two key translation tasks, demonstrating superiority in quality. We based our conclusion on experiments with two machine translation tasks showing that models are superior in quality while being more parallelizable and requiring significantly less time to train.\n",
      "\n",
      "Your debate claim is that the other paper's contributions towards the topic are not novel relative to your own paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.\n",
      "\n",
      "You initially argued that your own paper has the following novelties:\n",
      "\t- Argument #1. Advancements in Bidirectionality for Pre-training Language Models: Our work offers a novel bidirectional pre-training paradigm which can handle sequence dependencies in both directions, representing context-sensitive features by concatenating left-to-right and right-to-left representations. Evidence like 1 and 2 illustrate the deficiency of traditional unidirectional models, highlighting the need for a more comprehensive approach. A deep bidirectional Transformer like the one proposed in evidence 5 effectively exploits the power of bidirectionality, leading to gains in pre-training capabilities.\n",
      "\t- Argument #2. Enriched Pre-training Objectives Promote Deeper Representation Learning: The incorporation of a next sentence prediction task, along with the masked language model, is a critical contribution that allows for a more nuanced understanding of context and enables pre-training a deeper bidirectional model. Evidence 4 shows the groundbreaking impact of this approach, achieving state-of-the-art performance across various NLP tasks. This indicates that a more complex pre-training objective is crucial for unlocking rich representations in language models.\n",
      "\n",
      "\n",
      "You used the following evidence to support your arguments:\n",
      "\t- Evidence #1. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations\n",
      "\t- Evidence #2. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017)\n",
      "\t- Evidence #3. 4We note that in the literature the bidirectional TransBERTBERTE[CLS]E1 E[SEP]...ENE1'...EM'CT1T[SEP]...TNT1'...TM'[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1'...EM'CT1T[SEP]...TNT1'...TM'[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., (cid:104) Question, Answer (cid:105)) in one token sequence. Throughout this work, a \"sentence\" can be an arbitrary span of contiguous text, rather than an actual linguistic sentence\n",
      "\t- Evidence #4. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. * BERT advances the state of the art for eleven NLP tasks\n",
      "\t- Evidence #5. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to preIn additrain a deep bidirectional Transformer. tion to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations\n",
      "\n",
      "You also have preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "\t- Opposition Claim #1: Does my paper also address the opposition's claim, \"new architecture significantly outperforms traditional rnn architectures\"?\n",
      "\t- Your Counter Evidence #1: The results compared to prior leaderboard entries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, excluding systems that use BERT as one of their components. We observe a +5.1 F1 improvement over the previous best system\n",
      "\t- Your Counter Evidence #2: The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark (Wang language modelLeft-to-right et al., 2018a)\n",
      "\n",
      "\t- Opposition Claim #2: Does my paper also address the opposition's claim, \"the transformer helps solve major challenges in sequence modelling tasks\"?\n",
      "\t- Your Counter Evidence #1: As we show in Figure 1, C is used for next sentence prediction (NSP).5 Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI. 6 former is often referred to as a \"Transformer encoder\" while the left-context-only version is referred to as a \"Transformer decoder\" since it can be used for text generation\n",
      "\t- Your Counter Evidence #2: It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences. 3.2 Fine-tuning BERT Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream tasks-whether they involve single text or text pairs--by swapping out the appropriate inputs and outputs\n",
      "\n",
      "Given the above (your initial argument, your evidence, the opposition paper's claimed points of novelty, and your counter evidence), make an argument for a specific reason why the other paper's contributions towards the topic are not novel relative to your own paper, with respect to the topic, New Architecture Significantly Outperforms Traditional RNN Architectures. \n",
      "\n",
      "Output your argument in the following JSON format:\n",
      "\n",
      "{\n",
      "    \"title\": <should be a brief, 10-15 word string where the value is the argument title>,\n",
      "    \"description\": <2-3 sentence string explaining the argument>\n",
      "}\n",
      "\n",
      "PRESENTING ARGUMENTS FOR False PAPER:\n",
      "\n",
      "{ \"title\": \"Lack of Novelty in Transformer Architecture\", \"description\": \"The opposition's claimed novelty in their Transformer architecture is not novel relative to our own paper's contributions, specifically in the areas of bidirectionality and pre-training objectives. Our work has already demonstrated the effectiveness of a deep bidirectional Transformer and enriched pre-training objectives, which are critical components of the opposition's architecture. Furthermore, our results show that our approach outperforms traditional RNN architectures, rendering the opposition's claimed novelty as non-innovative.\" }\n"
     ]
    }
   ],
   "source": [
    "cited_out = cited_paper.present_argument(root_node.children[0], root_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Consistent Superiority in Sequence Modeling Tasks for the Proposed Transformer Architecture.',\n",
       " 'description': \"Our paper's contributions are novel and significant in sequence modeling and transduction, consistently surpassing traditional RNN architectures in accuracy and efficiency. Through a rigorous critique of the opposing author's points of novelty and the evidence presented in our own paper, we demonstrate the superiority of the Transformer architecture over traditional RNN models in multiple machine translation tasks.\"}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "queue_of_rounds: List[DebateNode] = []\n",
    "queue_of_rounds.extend(subtrees)\n",
    "\n",
    "\n",
    "while len(queue_of_rounds) > 0:\n",
    "    round = queue_of_rounds.pop(0)\n",
    "    conversation, new_focus_arg, new_cited_arg = round.conduct_debate(focus_paper, cited_paper)\n",
    "    conversation_history.extend(conversation)\n",
    "    if moderator.is_expand(round.self_delib, [new_focus_arg, new_cited_arg]):\n",
    "        new_subtrees = round.conduct_self_deliberation(round.round_topic, paper_authors)\n",
    "        queue_of_rounds.extend(new_subtrees)\n",
    "\n",
    "with open('conversation_history.txt', 'w+') as f:\n",
    "    f.write('\\n'.join(conversation_history))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
