{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "# os.environ['HF_HOME'] = '/shared/data3/pk36/.cache'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export HF_HOME=/shared/data3/pk36/.cache\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishikaa2/miniconda3/envs/tod/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-29 12:15:45,730\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from debate import DebateNode\n",
    "from paper_details import Paper\n",
    "from persona import PaperAuthor\n",
    "from moderator import Moderator\n",
    "import argparse\n",
    "from typing import List\n",
    "from vllm import LLM\n",
    "import os\n",
    "import json\n",
    "from data_pairer import parse_papers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.log_dir = \"logs\"\n",
    "        self.focus_paper = \"https://arxiv.org/pdf/2406.11709\" # treeinstruct\n",
    "        self.cited_paper = \"https://arxiv.org/pdf/2310.10648\" # bridge\n",
    "        self.topic = {'argument_title': \"helping students fix their errors\", 'description': \"\"}\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args.log_dir):\n",
    "    os.makedirs(args.log_dir)\n",
    "\n",
    "# if not os.path.exists(\"data.json\"):\n",
    "parse_papers(args.focus_paper, args.cited_paper)\n",
    "with open('data.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-29 12:15:59 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-29 12:15:59 arg_utils.py:957] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 11-29 12:15:59 config.py:1021] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 11-29 12:15:59 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 11-29 12:16:00 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-29 12:16:00 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1653491)\u001b[0;0m INFO 11-29 12:16:04 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-29 12:16:04 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1653491)\u001b[0;0m INFO 11-29 12:16:04 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-29 12:16:04 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1653491)\u001b[0;0m INFO 11-29 12:16:04 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-29 12:16:05 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ishikaa2/.cache/vllm/gpu_p2p_access_cache_for_2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1653491)\u001b[0;0m INFO 11-29 12:16:05 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ishikaa2/.cache/vllm/gpu_p2p_access_cache_for_2,3.json\n",
      "INFO 11-29 12:16:05 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f869920fa90>, local_subscribe_port=48973, remote_subscribe_port=None)\n",
      "INFO 11-29 12:16:05 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1653491)\u001b[0;0m INFO 11-29 12:16:05 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 11-29 12:16:05 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1653491)\u001b[0;0m INFO 11-29 12:16:05 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.79it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.84it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.08it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.75it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.89it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-29 12:16:07 model_runner.py:1067] Loading model weights took 7.5122 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1653491)\u001b[0;0m INFO 11-29 12:16:08 model_runner.py:1067] Loading model weights took 7.5122 GB\n",
      "INFO 11-29 12:16:09 distributed_gpu_executor.py:57] # GPU blocks: 13892, # CPU blocks: 4096\n",
      "INFO 11-29 12:16:09 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 1.70x\n",
      "INFO 11-29 12:16:10 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1653491)\u001b[0;0m INFO 11-29 12:16:10 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1653491)\u001b[0;0m INFO 11-29 12:16:10 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-29 12:16:10 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-29 12:16:17 custom_all_reduce.py:233] Registering 1040 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1653491)\u001b[0;0m INFO 11-29 12:16:17 custom_all_reduce.py:233] Registering 1040 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1653491)\u001b[0;0m INFO 11-29 12:16:17 model_runner.py:1523] Graph capturing finished in 7 secs.\n",
      "INFO 11-29 12:16:17 model_runner.py:1523] Graph capturing finished in 7 secs.\n"
     ]
    }
   ],
   "source": [
    "model_server = LLM(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",tensor_parallel_size=2, gpu_memory_utilization=0.5, max_num_seqs=100) #,enable_prefix_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.02it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.76it/s]\n"
     ]
    }
   ],
   "source": [
    "item = data[0]\n",
    "f_pap = item['focus']\n",
    "c_pap = item['cited']\n",
    "\n",
    "focus_paper = PaperAuthor(\n",
    "    model = model_server,\n",
    "    paper = Paper(f_pap),\n",
    "    focus=True,\n",
    "    id=0\n",
    ")\n",
    "\n",
    "cited_paper = PaperAuthor(\n",
    "    model = model_server,\n",
    "    paper = Paper(c_pap),\n",
    "    focus=False,\n",
    "    id=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 78.27it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.87s/it, est. speed input: 165.28 toks/s, output: 58.11 toks/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 83.43it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.36s/it, est. speed input: 258.28 toks/s, output: 56.74 toks/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 75.68it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 123/123 [00:04<00:00, 25.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 76.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 78.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 82.68it/s]\n"
     ]
    }
   ],
   "source": [
    "moderator = Moderator(model_server)\n",
    "\n",
    "paper_authors = [focus_paper, cited_paper]\n",
    "leaf_node_label = args.topic\n",
    "\n",
    "if args.log_dir != \"\":\n",
    "    with open(os.path.join(args.log_dir, 'self_deliberation.txt'), 'w') as f:\n",
    "        f.write(f'Topic: {args.topic}\\n\\n')\n",
    "\n",
    "# each node has a topic\n",
    "root_node = DebateNode(leaf_node_label)\n",
    "subtrees = root_node.conduct_self_deliberation(leaf_node_label, paper_authors, log=args.log_dir) # k new, finer topics to discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [{'argument_title': 'Our approach helps students learn from their mistakes through self-discovery',\n",
       "   'description': \"Our method guides students to identify and fix their own errors, promoting self-discovery and learning. This is supported by the Instructor's single-turn responses in the 'Help Fix Code' and 'Question from Code' modules, which direct students towards their mistakes and use natural language to describe bug fixes, as seen in Retrieved Evidence #1. Furthermore, the Instructor guides students to generate a list of all bug fixes based on their interactions, allowing students to resolve their own conceptual and syntactical errors in a Socratic fashion, as mentioned in Retrieved Evidence #2.\"},\n",
       "  {'argument_title': 'Our approach helps students develop problem-solving skills through Socratic questioning',\n",
       "   'description': 'Our method uses Socratic questions to guide students through the problem-solving process, helping them develop critical thinking and problem-solving skills. This is supported by the fact that our approach traverses the space using Socratic questions and traces which variables have been resolved, as mentioned in Retrieved Evidence #5. Additionally, the Socratic Debugging Benchmark dataset used in our approach consists of problems that require students to understand and apply concepts, rather than simply memorizing code, as seen in Retrieved Evidence #3.'}],\n",
       " 1: [{'argument_title': 'Novice tutors can be aided by LLMs to address student mistakes',\n",
       "   'description': 'Our work explores the potential of LLMs to close the novice-expert knowledge gap in remediating math mistakes. This is particularly relevant given the growing demand for novice tutors who struggle to address student mistakes.'},\n",
       "  {'argument_title': 'Expert-guided decision-making can be scaled with our error categories and strategies',\n",
       "   'description': 'Our error categories and strategies can be used to engage with student error traces and provide high-quality tutoring, even with novice tutors or LLMs.'}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node.self_delib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'Does my paper also address the claim, \"bidirectional language model architecture\"?': ['We used a beam size of 21 and a = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]',\n",
       "   'Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]'],\n",
       "  'Does my paper also address the claim, \"advancements in pre-training objectives\"?': ['During inference, we Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative semi-supervised semi-supervised semi-supervised semi-supervised semi-supervised multi-task generative Petrov et al',\n",
       "   'We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences']},\n",
       " 1: {'Does my paper also address the claim, \"transformer architecture offers superior performance\"?': ['BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model',\n",
       "   '(2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)'],\n",
       "  'Does my paper also address the claim, \"transformer architecture overcomes recurrent model limitations\"?': ['The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017)',\n",
       "   '(2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)']}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node.preemption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Transformer Architecture Offers Superior Performance,\n",
       " Transformer Architecture Overcomes Recurrent Model Limitations]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRESENT ARGUMENT FOR AUTHOR 0:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling FSM index for all state transitions: 100%|██████████| 57/57 [00:02<00:00, 23.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an author of a paper that is debating another author on your claim topic:\n",
      "\t- Topic: Our approach guides students to resolve errors through Socratic questioning\n",
      "\t- Topic Description: Our method encourages students to think critically and understand the underlying concepts, rather than simply providing direct code answers. This is supported by Evidence #1, which shows that our Instructor provides single-turn responses that direct the student towards where their mistake is and uses natural language to describe the bug fixes, allowing students to learn from their errors.\n",
      "\n",
      "Your debate claim is that your paper's contributions towards the topic are all novel relative to the other paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.\n",
      "\n",
      "You used the following evidence to support your arguments:\n",
      "\t- Evidence #1. In their \"Help Fix Code\" and \"Question from Code\" modules, the Instructor provides single-turn responses to the Student for answering questions, explaining concepts, and helping to write code. However, these modules direct the Student towards where their mistake is and uses natural language to describe the bug fixes\n",
      "\t- Evidence #2. The Instructor guides the Student to generate a list of all bug fixes based on their interactions with the Instructor. The overall goal is for the Student to resolve their own conceptual and syntactical errors in a Socratic fashion to reach the correct code\n",
      "\t- Evidence #3. First, we use the Socratic Debugging Benchmark dataset from (Al-Hossami et al., 2023b), which consists of 149 problemseach with a problem statement, student buggy code, bug fixes and descriptions in English, and correct code. However, these problems lack sufficient difficulty, often requiring small fixes and minimal problem comprehension\n",
      "\t- Evidence #4. Suppose the student is missing a base case and incorrectly calling the recursive function. Solving one bug requires adequate understanding of the other, thereby making it easier to solve\n",
      "\t- Evidence #5. We traverse the space using Socratic questions and trace which variables have been resolved, grounded based on the Student's responses. While existing LLM-based tutors are effective in fixing the Student's code with high success, they are either prone to directly revealing code answers or cannot be adapted to new Student responses\n",
      "\n",
      "You also have preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "\t- Opposition Claim #1: Does my paper also address the opposition's claim, \"novice tutors can be aided by llms to address student mistakes\"?\n",
      "\t\t- Your Counter Evidence #1: Note that we assume bug fixes are provided, a common scenario in educational settings (e.g., assignments, exams) where ground-truth solutions are available from human instructors or platforms like LeetCode. As our focus is steering an LLM towards Socratic guidance, generating these solutions for real-world tutoring applications is left for future work\n",
      "\n",
      "\t- Opposition Claim #2: Does my paper also address the opposition's claim, \"expert-guided decision-making can be scaled with our error categorization system\"?\n",
      "\t\t- Your Counter Evidence #1: Additionally, our method is dependent on the base model's reasoning capabilities, specifically for the Verifier agent. In our results, with a stronger model, we see higher scores for Logic and Success\n",
      "\t\t- Your Counter Evidence #2: We claim that the optimal state space can be represented by a series S of k tasks which leads the Student from their buggy code B to (1) understanding their conceptual and syntactical mistakes and (2) correcting their code. Each of these tasks is a state variable ti which either has a value of True or False based on whether the Student has completed it\n",
      "\n",
      "Given the above (your initial argument, your evidence, the opposition paper's claimed points of novelty, and your counter evidence), make an argument for a specific reason why your paper's contributions towards the topic are all novel relative to the other paper, with respect to the topic, Our approach guides students to resolve errors through Socratic questioning. \n",
      "\n",
      "Output your argument in the following JSON format:\n",
      "\n",
      "{\n",
      "    \"argument_title\": <should be a brief, 10-15 word string where the value is the argument argument_title>,\n",
      "    \"description\": <2-3 sentence string explaining the argument>\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PRESENT ARGUMENT FOR AUTHOR 1:\n",
      "\n",
      "You are an author of a paper that is debating another author about their claimed novelty:\n",
      "\t- Novelty Claim: Our approach guides students to resolve errors through Socratic questioning\n",
      "\t- Novelty Claim Description: Our method encourages students to think critically and understand the underlying concepts, rather than simply providing direct code answers. This is supported by Evidence #1, which shows that our Instructor provides single-turn responses that direct the student towards where their mistake is and uses natural language to describe the bug fixes, allowing students to learn from their errors.\n",
      "\n",
      "Your debate claim is that the other paper's contributions towards the topic are not novel relative to your own paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.\n",
      "\n",
      "You initially argued that your own paper has the following novelties:\n",
      "\t- Argument #1. Novice tutors can be aided by LLMs to address student mistakes: Our work explores the potential of LLMs to close the novice-expert knowledge gap in remediating math mistakes. This is particularly relevant given the growing demand for novice tutors who struggle to address student mistakes.\n",
      "\t- Argument #2. Expert-guided decision-making can be scaled with our error categorization system: Our error categories are topic-agnostic descriptions of a student's understanding, complementing expert-guided decision-making strategies. This allows for high-quality tutoring to be scaled with our system.\n",
      "\n",
      "\n",
      "You used the following evidence to support your arguments:\n",
      "\t- Evidence #1. Due to growing demand, many platforms employ novice tutors who, unlike experienced educators, struggle to address student mistakes and thus fail to seize prime learning opportunities. Our work explores the potential of large language models (LLMs) to close the novice-expert knowledge gap in remediating math mistakes\n",
      "\t- Evidence #2. Our approach intends to support novices who are not necessarily content experts. Therefore we define \"error\" as a student's degree of understanding, which aligns with literature on math curriculum design and psychometrics that maintain continuous scales of student understanding (Gagne, 1962, 1968; White, 1973; Resnick et al., 1973; Glaser and Nitko, 1970; Vygotsky and Cole, 1978; Wertsch, 1985; Embretson and Reise, 2013)\n",
      "\t- Evidence #3. Novices and LLMs alone use passive remediation language and do not engage with the student's error traces. Our findings indicate promising avenues for scaling high-quality tutoring with expert-guided decision-making\n",
      "\t- Evidence #4. This resulted in Step A: Infer the student's error to answer the first question. Experts used several techniques to engage with the student's error, such as asking questions and simplifying the problem to meet the student's level of understanding\n",
      "\t- Evidence #5. As such, our error categories are topic-agnostic descriptions of a student's understanding, and complement the topic-agnostic strategies in Step B. The categories are: guess: The student does not seem to understand or guessed the answer; misinterpret: The student misinterpreted the question; careless: The student made a careless mistake; right-idea: The student has the right idea, but is not quite there1; imprecise: The student's answer is not precise enough or the tutor is being too picky about the form of the student's answer; not-sure: Not sure, but I'm going to try to diagnose the student (used sparingly); N/A: None of the above (used sparingly)\n",
      "\n",
      "You also have preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "\t- Opposition Claim #1: Does my paper also address the opposition's claim, \"our approach guides students to resolve errors through socratic questioning\"?\n",
      "\t\t- Your Counter Evidence #1: Effective remediation coincides with educators engaging with the mathematical details in student responses, which in turn fosters strong teacher-student relationships and student motivation (Wentzel, 1997; Pianta et al., 2003; Robinson, 2022; Wentzel, 2022; Easley and Zwoyer, 1975; Brown and Burton, 1978; Carpenter et al., 1999, 2003; Lester, 2007; Loewenberg Ball and Forzani, 2009). Prior education research discusses multiple good practices in remediating student mistakes, ranging from visual aids (CAST, 2018) to the Socratic method (Lepper and Woolverton, 2002)\n",
      "\t\t- Your Counter Evidence #2: As such, our error categories are topic-agnostic descriptions of a student's understanding, and complement the topic-agnostic strategies in Step B. The categories are: guess: The student does not seem to understand or guessed the answer; misinterpret: The student misinterpreted the question; careless: The student made a careless mistake; right-idea: The student has the right idea, but is not quite there1; imprecise: The student's answer is not precise enough or the tutor is being too picky about the form of the student's answer; not-sure: Not sure, but I'm going to try to diagnose the student (used sparingly); N/A: None of the above (used sparingly)\n",
      "\n",
      "\t- Opposition Claim #2: Does my paper also address the opposition's claim, \"our approach adapts to new student responses and provides personalized feedback\"?\n",
      "\t\t- Your Counter Evidence #1: We do not address the opposition's claim: our approach adapts to new student responses and provides personalized feedback\n",
      "\n",
      "Given the above (your initial argument, your evidence, the opposition paper's claimed points of novelty, and your counter evidence), make an argument for a specific reason why the other paper's contributions towards the topic are not novel relative to your own paper, with respect to the topic, Our approach guides students to resolve errors through Socratic questioning. \n",
      "\n",
      "Output your argument in the following JSON format:\n",
      "\n",
      "{\n",
      "    \"argument_title\": <should be a brief, 10-15 word string where the value is the argument argument_title>,\n",
      "    \"description\": <2-3 sentence string explaining the argument>\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RESPOND ARGUMENT FOR AUTHOR 0:\n",
      "\n",
      "You are an author of a paper that is debating another author.\n",
      "\n",
      "Your debate claim is that your paper's contributions towards the \"topic\" are all novel relative to the other paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.\n",
      "\n",
      "You used the following evidence to support your arguments:\n",
      "\t- Evidence #1. In their \"Help Fix Code\" and \"Question from Code\" modules, the Instructor provides single-turn responses to the Student for answering questions, explaining concepts, and helping to write code. However, these modules direct the Student towards where their mistake is and uses natural language to describe the bug fixes\n",
      "\t- Evidence #2. The Instructor guides the Student to generate a list of all bug fixes based on their interactions with the Instructor. The overall goal is for the Student to resolve their own conceptual and syntactical errors in a Socratic fashion to reach the correct code\n",
      "\t- Evidence #3. First, we use the Socratic Debugging Benchmark dataset from (Al-Hossami et al., 2023b), which consists of 149 problemseach with a problem statement, student buggy code, bug fixes and descriptions in English, and correct code. However, these problems lack sufficient difficulty, often requiring small fixes and minimal problem comprehension\n",
      "\t- Evidence #4. Suppose the student is missing a base case and incorrectly calling the recursive function. Solving one bug requires adequate understanding of the other, thereby making it easier to solve\n",
      "\t- Evidence #5. We traverse the space using Socratic questions and trace which variables have been resolved, grounded based on the Student's responses. While existing LLM-based tutors are effective in fixing the Student's code with high success, they are either prone to directly revealing code answers or cannot be adapted to new Student responses\n",
      "\n",
      "You also have preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "\t- Opposition Claim #1: Does my paper also address the opposition's claim, \"novice tutors can be aided by llms to address student mistakes\"?\n",
      "\t\t- Your Counter Evidence #1: Note that we assume bug fixes are provided, a common scenario in educational settings (e.g., assignments, exams) where ground-truth solutions are available from human instructors or platforms like LeetCode. As our focus is steering an LLM towards Socratic guidance, generating these solutions for real-world tutoring applications is left for future work\n",
      "\n",
      "\t- Opposition Claim #2: Does my paper also address the opposition's claim, \"expert-guided decision-making can be scaled with our error categorization system\"?\n",
      "\t\t- Your Counter Evidence #1: Additionally, our method is dependent on the base model's reasoning capabilities, specifically for the Verifier agent. In our results, with a stronger model, we see higher scores for Logic and Success\n",
      "\t\t- Your Counter Evidence #2: We claim that the optimal state space can be represented by a series S of k tasks which leads the Student from their buggy code B to (1) understanding their conceptual and syntactical mistakes and (2) correcting their code. Each of these tasks is a state variable ti which either has a value of True or False based on whether the Student has completed it\n",
      "\n",
      "Based on the debate history and your/your opposition's arguments and evidence, you must respond to the last argument presented by your opposition in debate.\n",
      "\n",
      "Debate Topic Information:\n",
      "\t- Topic: Our approach guides students to resolve errors through Socratic questioning\n",
      "\t- Topic Description: Our method encourages students to think critically and understand the underlying concepts, rather than simply providing direct code answers. This is supported by Evidence #1, which shows that our Instructor provides single-turn responses that direct the student towards where their mistake is and uses natural language to describe the bug fixes, allowing students to learn from their errors.\n",
      "\n",
      "Debate History:\n",
      "\n",
      "\t-You: I argue that novelty in socratic questioning approach. Our paper's contributions are novel due to the unique combination of Socratic questioning and LLM-based tutoring, which steers students towards resolving errors through critical thinking. Unlike the opposing paper, our method does not rely on direct code answers or error categorization systems, but rather focuses on guiding students to understand their conceptual and syntactical mistakes. This approach is supported by our evidence, which shows that our Instructor provides single-turn responses that direct the student towards where their mistake is and uses natural language to describe the bug fixes.\n",
      "\t-Opposition: I argue that the opposition's socratic questioning approach is not novel relative to our work. The opposition's approach is not novel because it is a common practice in education research, and our work explores the potential of LLMs to close the novice-expert knowledge gap in remediating math mistakes. Furthermore, our error categories are topic-agnostic descriptions of a student's understanding, complementing expert-guided decision-making strategies. This allows for high-quality tutoring to be scaled with our system, which is not addressed by the opposition's approach.\n",
      "\n",
      "\n",
      "Output your new response in the following JSON format:\n",
      "\n",
      "{\n",
      "    \"argument_title\": <should be a brief, 10-15 word string where the value is the main argument of your response to the opposition>,\n",
      "    \"description\": <2-3 sentence string explaining your response to the opposition's last turn, based on the provided pool of evidence>\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RESPOND ARGUMENT FOR AUTHOR 1:\n",
      "\n",
      "You are an author of a paper that is debating another author.\n",
      "\n",
      "Your debate claim is that the other paper's contributions towards the \"topic\" are not novel relative to your own paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.You used the following evidence to support your arguments:\n",
      "\t- Evidence #1. Due to growing demand, many platforms employ novice tutors who, unlike experienced educators, struggle to address student mistakes and thus fail to seize prime learning opportunities. Our work explores the potential of large language models (LLMs) to close the novice-expert knowledge gap in remediating math mistakes\n",
      "\t- Evidence #2. Our approach intends to support novices who are not necessarily content experts. Therefore we define \"error\" as a student's degree of understanding, which aligns with literature on math curriculum design and psychometrics that maintain continuous scales of student understanding (Gagne, 1962, 1968; White, 1973; Resnick et al., 1973; Glaser and Nitko, 1970; Vygotsky and Cole, 1978; Wertsch, 1985; Embretson and Reise, 2013)\n",
      "\t- Evidence #3. Novices and LLMs alone use passive remediation language and do not engage with the student's error traces. Our findings indicate promising avenues for scaling high-quality tutoring with expert-guided decision-making\n",
      "\t- Evidence #4. This resulted in Step A: Infer the student's error to answer the first question. Experts used several techniques to engage with the student's error, such as asking questions and simplifying the problem to meet the student's level of understanding\n",
      "\t- Evidence #5. As such, our error categories are topic-agnostic descriptions of a student's understanding, and complement the topic-agnostic strategies in Step B. The categories are: guess: The student does not seem to understand or guessed the answer; misinterpret: The student misinterpreted the question; careless: The student made a careless mistake; right-idea: The student has the right idea, but is not quite there1; imprecise: The student's answer is not precise enough or the tutor is being too picky about the form of the student's answer; not-sure: Not sure, but I'm going to try to diagnose the student (used sparingly); N/A: None of the above (used sparingly)\n",
      "\n",
      "You also have preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "\t- Opposition Claim #1: Does my paper also address the opposition's claim, \"our approach guides students to resolve errors through socratic questioning\"?\n",
      "\t\t- Your Counter Evidence #1: Effective remediation coincides with educators engaging with the mathematical details in student responses, which in turn fosters strong teacher-student relationships and student motivation (Wentzel, 1997; Pianta et al., 2003; Robinson, 2022; Wentzel, 2022; Easley and Zwoyer, 1975; Brown and Burton, 1978; Carpenter et al., 1999, 2003; Lester, 2007; Loewenberg Ball and Forzani, 2009). Prior education research discusses multiple good practices in remediating student mistakes, ranging from visual aids (CAST, 2018) to the Socratic method (Lepper and Woolverton, 2002)\n",
      "\t\t- Your Counter Evidence #2: As such, our error categories are topic-agnostic descriptions of a student's understanding, and complement the topic-agnostic strategies in Step B. The categories are: guess: The student does not seem to understand or guessed the answer; misinterpret: The student misinterpreted the question; careless: The student made a careless mistake; right-idea: The student has the right idea, but is not quite there1; imprecise: The student's answer is not precise enough or the tutor is being too picky about the form of the student's answer; not-sure: Not sure, but I'm going to try to diagnose the student (used sparingly); N/A: None of the above (used sparingly)\n",
      "\n",
      "\t- Opposition Claim #2: Does my paper also address the opposition's claim, \"our approach adapts to new student responses and provides personalized feedback\"?\n",
      "\t\t- Your Counter Evidence #1: We do not address the opposition's claim: our approach adapts to new student responses and provides personalized feedback\n",
      "\n",
      "Based on the debate history and your/your opposition's arguments and evidence, you must respond to the last argument presented by your opposition in debate.\n",
      "\n",
      "Debate Topic Information:\n",
      "\t- Topic: Our approach guides students to resolve errors through Socratic questioning\n",
      "\t- Topic Description: Our method encourages students to think critically and understand the underlying concepts, rather than simply providing direct code answers. This is supported by Evidence #1, which shows that our Instructor provides single-turn responses that direct the student towards where their mistake is and uses natural language to describe the bug fixes, allowing students to learn from their errors.\n",
      "\n",
      "Debate History:\n",
      "\n",
      "\t-Opposition: I argue that novelty in socratic questioning approach. Our paper's contributions are novel due to the unique combination of Socratic questioning and LLM-based tutoring, which steers students towards resolving errors through critical thinking. Unlike the opposing paper, our method does not rely on direct code answers or error categorization systems, but rather focuses on guiding students to understand their conceptual and syntactical mistakes. This approach is supported by our evidence, which shows that our Instructor provides single-turn responses that direct the student towards where their mistake is and uses natural language to describe the bug fixes.\n",
      "\t-You: I argue that the opposition's socratic questioning approach is not novel relative to our work. The opposition's approach is not novel because it is a common practice in education research, and our work explores the potential of LLMs to close the novice-expert knowledge gap in remediating math mistakes. Furthermore, our error categories are topic-agnostic descriptions of a student's understanding, complementing expert-guided decision-making strategies. This allows for high-quality tutoring to be scaled with our system, which is not addressed by the opposition's approach.\n",
      "\n",
      "\t-Opposition: I believe that novelty in socratic questioning approach. The opposition's claim that our approach is not novel is unfounded, as our method uniquely combines Socratic questioning with LLM-based tutoring to steer students towards resolving errors through critical thinking. Our evidence shows that our Instructor provides single-turn responses that direct the student towards where their mistake is and uses natural language to describe the bug fixes, unlike the opposing paper's direct code answers. Furthermore, our approach is not reliant on error categorization systems, but rather focuses on guiding students to understand their conceptual and syntactical mistakes.\n",
      "\n",
      "Output your new response in the following JSON format:\n",
      "\n",
      "{\n",
      "    \"argument_title\": <should be a brief, 10-15 word string where the value is the main argument of your response to the opposition>,\n",
      "    \"description\": <2-3 sentence string explaining your response to the opposition's last turn, based on the provided pool of evidence>\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "REVISE ARGUMENT FOR AUTHOR 0:\n",
      "\n",
      "You are an author of a paper that is debating another author.\n",
      "\n",
      "Your debate claim is that your paper's contributions towards the \"topic\" are all novel relative to the other paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.\n",
      "\n",
      "Use the following evidence to support your arguments:\n",
      "\t- Evidence #1. In their \"Help Fix Code\" and \"Question from Code\" modules, the Instructor provides single-turn responses to the Student for answering questions, explaining concepts, and helping to write code. However, these modules direct the Student towards where their mistake is and uses natural language to describe the bug fixes\n",
      "\t- Evidence #2. The Instructor guides the Student to generate a list of all bug fixes based on their interactions with the Instructor. The overall goal is for the Student to resolve their own conceptual and syntactical errors in a Socratic fashion to reach the correct code\n",
      "\t- Evidence #3. First, we use the Socratic Debugging Benchmark dataset from (Al-Hossami et al., 2023b), which consists of 149 problemseach with a problem statement, student buggy code, bug fixes and descriptions in English, and correct code. However, these problems lack sufficient difficulty, often requiring small fixes and minimal problem comprehension\n",
      "\t- Evidence #4. Suppose the student is missing a base case and incorrectly calling the recursive function. Solving one bug requires adequate understanding of the other, thereby making it easier to solve\n",
      "\t- Evidence #5. We traverse the space using Socratic questions and trace which variables have been resolved, grounded based on the Student's responses. While existing LLM-based tutors are effective in fixing the Student's code with high success, they are either prone to directly revealing code answers or cannot be adapted to new Student responses\n",
      "\n",
      "You also had preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "\t- Opposition Claim #1: Does my paper also address the opposition's claim, \"novice tutors can be aided by llms to address student mistakes\"?\n",
      "\t\t- Your Counter Evidence #1: Note that we assume bug fixes are provided, a common scenario in educational settings (e.g., assignments, exams) where ground-truth solutions are available from human instructors or platforms like LeetCode. As our focus is steering an LLM towards Socratic guidance, generating these solutions for real-world tutoring applications is left for future work\n",
      "\n",
      "\t- Opposition Claim #2: Does my paper also address the opposition's claim, \"expert-guided decision-making can be scaled with our error categorization system\"?\n",
      "\t\t- Your Counter Evidence #1: Additionally, our method is dependent on the base model's reasoning capabilities, specifically for the Verifier agent. In our results, with a stronger model, we see higher scores for Logic and Success\n",
      "\t\t- Your Counter Evidence #2: We claim that the optimal state space can be represented by a series S of k tasks which leads the Student from their buggy code B to (1) understanding their conceptual and syntactical mistakes and (2) correcting their code. Each of these tasks is a state variable ti which either has a value of True or False based on whether the Student has completed it\n",
      "\n",
      "Based on the debate history and your/your opposition's arguments and evidence, you must construct a stronger argument given the responses of the opposition.\n",
      "\n",
      "Debate Topic Information:\n",
      "\t- Topic: Our approach guides students to resolve errors through Socratic questioning\n",
      "\t- Topic Description: Our method encourages students to think critically and understand the underlying concepts, rather than simply providing direct code answers. This is supported by Evidence #1, which shows that our Instructor provides single-turn responses that direct the student towards where their mistake is and uses natural language to describe the bug fixes, allowing students to learn from their errors.\n",
      "\n",
      "Debate History:\n",
      "\n",
      "\t-You: I argue that novelty in socratic questioning approach. Our paper's contributions are novel due to the unique combination of Socratic questioning and LLM-based tutoring, which steers students towards resolving errors through critical thinking. Unlike the opposing paper, our method does not rely on direct code answers or error categorization systems, but rather focuses on guiding students to understand their conceptual and syntactical mistakes. This approach is supported by our evidence, which shows that our Instructor provides single-turn responses that direct the student towards where their mistake is and uses natural language to describe the bug fixes.\n",
      "\t-Opposition: I argue that the opposition's socratic questioning approach is not novel relative to our work. The opposition's approach is not novel because it is a common practice in education research, and our work explores the potential of LLMs to close the novice-expert knowledge gap in remediating math mistakes. Furthermore, our error categories are topic-agnostic descriptions of a student's understanding, complementing expert-guided decision-making strategies. This allows for high-quality tutoring to be scaled with our system, which is not addressed by the opposition's approach.\n",
      "\n",
      "\t-You: I believe that novelty in socratic questioning approach. The opposition's claim that our approach is not novel is unfounded, as our method uniquely combines Socratic questioning with LLM-based tutoring to steer students towards resolving errors through critical thinking. Our evidence shows that our Instructor provides single-turn responses that direct the student towards where their mistake is and uses natural language to describe the bug fixes, unlike the opposing paper's direct code answers. Furthermore, our approach is not reliant on error categorization systems, but rather focuses on guiding students to understand their conceptual and syntactical mistakes.\n",
      "\t-Opposition: I believe that novelty of socratic questioning approach is unfounded. The opposition's claim of novelty in socratic questioning is unfounded as it is a common practice in education research. Our work explores the potential of LLMs to close the novice-expert knowledge gap in remediating math mistakes, which is not addressed by the opposition's approach. Furthermore, our error categories are topic-agnostic descriptions of a student's understanding, complementing expert-guided decision-making strategies.\n",
      "\n",
      "\n",
      "Output your new, stronger argument in the following JSON format:\n",
      "{\n",
      "    \"argument_title\": <should be a brief, 10-15 word string where the value is your updated, strong response to the opposition's arguments>,\n",
      "    \"description\": <2-3 sentence string explaining your new argument as a response to the opposition's last turn, based on the provided pool of evidence>\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "REVISE ARGUMENT FOR AUTHOR 1:\n",
      "\n",
      "You are an author of a paper that is debating another author.\n",
      "\n",
      "Your debate claim is that the other paper's contributions towards the \"topic\" are not novel relative to your own paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.Use the following evidence to support your arguments:\n",
      "\t- Evidence #1. Due to growing demand, many platforms employ novice tutors who, unlike experienced educators, struggle to address student mistakes and thus fail to seize prime learning opportunities. Our work explores the potential of large language models (LLMs) to close the novice-expert knowledge gap in remediating math mistakes\n",
      "\t- Evidence #2. Our approach intends to support novices who are not necessarily content experts. Therefore we define \"error\" as a student's degree of understanding, which aligns with literature on math curriculum design and psychometrics that maintain continuous scales of student understanding (Gagne, 1962, 1968; White, 1973; Resnick et al., 1973; Glaser and Nitko, 1970; Vygotsky and Cole, 1978; Wertsch, 1985; Embretson and Reise, 2013)\n",
      "\t- Evidence #3. Novices and LLMs alone use passive remediation language and do not engage with the student's error traces. Our findings indicate promising avenues for scaling high-quality tutoring with expert-guided decision-making\n",
      "\t- Evidence #4. This resulted in Step A: Infer the student's error to answer the first question. Experts used several techniques to engage with the student's error, such as asking questions and simplifying the problem to meet the student's level of understanding\n",
      "\t- Evidence #5. As such, our error categories are topic-agnostic descriptions of a student's understanding, and complement the topic-agnostic strategies in Step B. The categories are: guess: The student does not seem to understand or guessed the answer; misinterpret: The student misinterpreted the question; careless: The student made a careless mistake; right-idea: The student has the right idea, but is not quite there1; imprecise: The student's answer is not precise enough or the tutor is being too picky about the form of the student's answer; not-sure: Not sure, but I'm going to try to diagnose the student (used sparingly); N/A: None of the above (used sparingly)\n",
      "\n",
      "You also had preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "\t- Opposition Claim #1: Does my paper also address the opposition's claim, \"our approach guides students to resolve errors through socratic questioning\"?\n",
      "\t\t- Your Counter Evidence #1: Effective remediation coincides with educators engaging with the mathematical details in student responses, which in turn fosters strong teacher-student relationships and student motivation (Wentzel, 1997; Pianta et al., 2003; Robinson, 2022; Wentzel, 2022; Easley and Zwoyer, 1975; Brown and Burton, 1978; Carpenter et al., 1999, 2003; Lester, 2007; Loewenberg Ball and Forzani, 2009). Prior education research discusses multiple good practices in remediating student mistakes, ranging from visual aids (CAST, 2018) to the Socratic method (Lepper and Woolverton, 2002)\n",
      "\t\t- Your Counter Evidence #2: As such, our error categories are topic-agnostic descriptions of a student's understanding, and complement the topic-agnostic strategies in Step B. The categories are: guess: The student does not seem to understand or guessed the answer; misinterpret: The student misinterpreted the question; careless: The student made a careless mistake; right-idea: The student has the right idea, but is not quite there1; imprecise: The student's answer is not precise enough or the tutor is being too picky about the form of the student's answer; not-sure: Not sure, but I'm going to try to diagnose the student (used sparingly); N/A: None of the above (used sparingly)\n",
      "\n",
      "\t- Opposition Claim #2: Does my paper also address the opposition's claim, \"our approach adapts to new student responses and provides personalized feedback\"?\n",
      "\t\t- Your Counter Evidence #1: We do not address the opposition's claim: our approach adapts to new student responses and provides personalized feedback\n",
      "\n",
      "Based on the debate history and your/your opposition's arguments and evidence, you must construct a stronger argument given the responses of the opposition.\n",
      "\n",
      "Debate Topic Information:\n",
      "\t- Topic: Our approach guides students to resolve errors through Socratic questioning\n",
      "\t- Topic Description: Our method encourages students to think critically and understand the underlying concepts, rather than simply providing direct code answers. This is supported by Evidence #1, which shows that our Instructor provides single-turn responses that direct the student towards where their mistake is and uses natural language to describe the bug fixes, allowing students to learn from their errors.\n",
      "\n",
      "Debate History:\n",
      "\n",
      "\t-Opposition: I argue that novelty in socratic questioning approach. Our paper's contributions are novel due to the unique combination of Socratic questioning and LLM-based tutoring, which steers students towards resolving errors through critical thinking. Unlike the opposing paper, our method does not rely on direct code answers or error categorization systems, but rather focuses on guiding students to understand their conceptual and syntactical mistakes. This approach is supported by our evidence, which shows that our Instructor provides single-turn responses that direct the student towards where their mistake is and uses natural language to describe the bug fixes.\n",
      "\t-You: I argue that the opposition's socratic questioning approach is not novel relative to our work. The opposition's approach is not novel because it is a common practice in education research, and our work explores the potential of LLMs to close the novice-expert knowledge gap in remediating math mistakes. Furthermore, our error categories are topic-agnostic descriptions of a student's understanding, complementing expert-guided decision-making strategies. This allows for high-quality tutoring to be scaled with our system, which is not addressed by the opposition's approach.\n",
      "\n",
      "\t-Opposition: I believe that novelty in socratic questioning approach. The opposition's claim that our approach is not novel is unfounded, as our method uniquely combines Socratic questioning with LLM-based tutoring to steer students towards resolving errors through critical thinking. Our evidence shows that our Instructor provides single-turn responses that direct the student towards where their mistake is and uses natural language to describe the bug fixes, unlike the opposing paper's direct code answers. Furthermore, our approach is not reliant on error categorization systems, but rather focuses on guiding students to understand their conceptual and syntactical mistakes.\n",
      "\t-You: I believe that novelty of socratic questioning approach is unfounded. The opposition's claim of novelty in socratic questioning is unfounded as it is a common practice in education research. Our work explores the potential of LLMs to close the novice-expert knowledge gap in remediating math mistakes, which is not addressed by the opposition's approach. Furthermore, our error categories are topic-agnostic descriptions of a student's understanding, complementing expert-guided decision-making strategies.\n",
      "\n",
      "\t-Opposition: I argue that novelty in socratic questioning approach is supported by unique llm-based tutoring. Our method uniquely combines Socratic questioning with LLM-based tutoring to steer students towards resolving errors through critical thinking. Unlike the opposing paper, our approach does not rely on direct code answers or error categorization systems, but rather focuses on guiding students to understand their conceptual and syntactical mistakes. This approach is supported by our evidence, which shows that our Instructor provides single-turn responses that direct the student towards where their mistake is and uses natural language to describe the bug fixes.\n",
      "\n",
      "Output your new, stronger argument in the following JSON format:\n",
      "{\n",
      "    \"argument_title\": <should be a brief, 10-15 word string where the value is your updated, strong response to the opposition's arguments>,\n",
      "    \"description\": <2-3 sentence string explaining your new argument as a response to the opposition's last turn, based on the provided pool of evidence>\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convo_history = root_node.children[0].conduct_debate(paper_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate Topic Information:\n",
      "\t- Topic: Our approach guides students to resolve errors through Socratic questioning\n",
      "\t- Topic Description: Our method encourages students to think critically and understand the underlying concepts, rather than simply providing direct code answers. This is supported by Evidence #1, which shows that our Instructor provides single-turn responses that direct the student towards where their mistake is and uses natural language to describe the bug fixes, allowing students to learn from their errors.\n",
      "\n",
      "Debate History:\n",
      "\n",
      "\t-Author 0: I argue that novelty in socratic questioning approach. Our paper's contributions are novel due to the unique combination of Socratic questioning and LLM-based tutoring, which steers students towards resolving errors through critical thinking. Unlike the opposing paper, our method does not rely on direct code answers or error categorization systems, but rather focuses on guiding students to understand their conceptual and syntactical mistakes. This approach is supported by our evidence, which shows that our Instructor provides single-turn responses that direct the student towards where their mistake is and uses natural language to describe the bug fixes.\n",
      "\t-Author 1: I argue that the opposition's socratic questioning approach is not novel relative to our work. The opposition's approach is not novel because it is a common practice in education research, and our work explores the potential of LLMs to close the novice-expert knowledge gap in remediating math mistakes. Furthermore, our error categories are topic-agnostic descriptions of a student's understanding, complementing expert-guided decision-making strategies. This allows for high-quality tutoring to be scaled with our system, which is not addressed by the opposition's approach.\n",
      "\n",
      "\t-Author 0: I believe that novelty in socratic questioning approach. The opposition's claim that our approach is not novel is unfounded, as our method uniquely combines Socratic questioning with LLM-based tutoring to steer students towards resolving errors through critical thinking. Our evidence shows that our Instructor provides single-turn responses that direct the student towards where their mistake is and uses natural language to describe the bug fixes, unlike the opposing paper's direct code answers. Furthermore, our approach is not reliant on error categorization systems, but rather focuses on guiding students to understand their conceptual and syntactical mistakes.\n",
      "\t-Author 1: I believe that novelty of socratic questioning approach is unfounded. The opposition's claim of novelty in socratic questioning is unfounded as it is a common practice in education research. Our work explores the potential of LLMs to close the novice-expert knowledge gap in remediating math mistakes, which is not addressed by the opposition's approach. Furthermore, our error categories are topic-agnostic descriptions of a student's understanding, complementing expert-guided decision-making strategies.\n",
      "\n",
      "\t-Author 0: I argue that novelty in socratic questioning approach is supported by unique llm-based tutoring. Our method uniquely combines Socratic questioning with LLM-based tutoring to steer students towards resolving errors through critical thinking. Unlike the opposing paper, our approach does not rely on direct code answers or error categorization systems, but rather focuses on guiding students to understand their conceptual and syntactical mistakes. This approach is supported by our evidence, which shows that our Instructor provides single-turn responses that direct the student towards where their mistake is and uses natural language to describe the bug fixes.\n",
      "\t-Author 1: I argue that novelty of socratic questioning approach is unfounded. The opposition's claim of novelty in socratic questioning is unfounded as it is a common practice in education research. Our work explores the potential of LLMs to close the novice-expert knowledge gap in remediating math mistakes, which is not addressed by the opposition's approach. Furthermore, our error categories are topic-agnostic descriptions of a student's understanding, complementing expert-guided decision-making strategies.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(convo_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'argument_list': [{'title': 'Argument 1',\n",
       "   'description': 'Argument 1 description'},\n",
       "  {'title': 'Argument 2', 'description': 'Argument 2 description'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node.children[0].response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "queue_of_rounds: List[DebateNode] = []\n",
    "queue_of_rounds.extend(subtrees)\n",
    "\n",
    "\n",
    "while len(queue_of_rounds) > 0:\n",
    "    round = queue_of_rounds.pop(0)\n",
    "    conversation, new_focus_arg, new_cited_arg = round.conduct_debate(focus_paper, cited_paper)\n",
    "    conversation_history.extend(conversation)\n",
    "    if moderator.is_expand(round.self_delib, [new_focus_arg, new_cited_arg]):\n",
    "        new_subtrees = round.conduct_self_deliberation(round.round_topic, paper_authors)\n",
    "        queue_of_rounds.extend(new_subtrees)\n",
    "\n",
    "with open('conversation_history.txt', 'w+') as f:\n",
    "    f.write('\\n'.join(conversation_history))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
