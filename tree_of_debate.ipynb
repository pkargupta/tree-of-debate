{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/shared/data3/pk36/.cache'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=/shared/data3/pk36/.cache\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"4,5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from debate import DebateNode\n",
    "from paper_details import Paper\n",
    "from persona import PaperAuthor\n",
    "from moderator import Moderator\n",
    "import argparse\n",
    "from typing import List\n",
    "from vllm import LLM\n",
    "import os\n",
    "import json\n",
    "from data_pairer import parse_papers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.log_dir = \"logs\"\n",
    "        self.focus_paper = \"https://arxiv.org/pdf/1706.03762\"\n",
    "        self.cited_paper = \"https://arxiv.org/pdf/1810.04805\"\n",
    "        self.topic = \"language model architectures\"\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args.log_dir):\n",
    "    os.makedirs(args.log_dir)\n",
    "\n",
    "# if not os.path.exists(\"data.json\"):\n",
    "parse_papers(args.focus_paper, args.cited_paper)\n",
    "with open('data.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 18:03:06 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-20 18:03:06 arg_utils.py:957] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 11-20 18:03:06 config.py:1021] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 11-20 18:03:06 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 11-20 18:03:07 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-20 18:03:07 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3521382)\u001b[0;0m INFO 11-20 18:03:08 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-20 18:03:09 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-20 18:03:09 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3521382)\u001b[0;0m INFO 11-20 18:03:09 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3521382)\u001b[0;0m INFO 11-20 18:03:09 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-20 18:03:11 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/pk36/.cache/vllm/gpu_p2p_access_cache_for_4,5.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3521382)\u001b[0;0m INFO 11-20 18:03:11 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/pk36/.cache/vllm/gpu_p2p_access_cache_for_4,5.json\n",
      "INFO 11-20 18:03:11 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f16efee4160>, local_subscribe_port=49609, remote_subscribe_port=None)\n",
      "INFO 11-20 18:03:11 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3521382)\u001b[0;0m INFO 11-20 18:03:11 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 11-20 18:03:13 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3521382)\u001b[0;0m INFO 11-20 18:03:14 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44187cec2c40431ab0f05c31cf192941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 18:03:16 model_runner.py:1067] Loading model weights took 7.5122 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3521382)\u001b[0;0m INFO 11-20 18:03:17 model_runner.py:1067] Loading model weights took 7.5122 GB\n",
      "INFO 11-20 18:03:22 distributed_gpu_executor.py:57] # GPU blocks: 15590, # CPU blocks: 4096\n",
      "INFO 11-20 18:03:22 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 1.90x\n",
      "INFO 11-20 18:03:24 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-20 18:03:24 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3521382)\u001b[0;0m INFO 11-20 18:03:24 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3521382)\u001b[0;0m INFO 11-20 18:03:24 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-20 18:03:35 custom_all_reduce.py:233] Registering 1040 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3521382)\u001b[0;0m INFO 11-20 18:03:35 custom_all_reduce.py:233] Registering 1040 cuda graph addresses\n",
      "INFO 11-20 18:03:35 model_runner.py:1523] Graph capturing finished in 12 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3521382)\u001b[0;0m INFO 11-20 18:03:35 model_runner.py:1523] Graph capturing finished in 12 secs.\n"
     ]
    }
   ],
   "source": [
    "model_server = LLM(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",tensor_parallel_size=2, gpu_memory_utilization=0.5, max_num_seqs=100) #,enable_prefix_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f3c9d61be747678ea47c88763484ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef2bf070ac342419f17241fa78c96be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2994c8f4bc4714b5a2aaa5132bfede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3660867b0a48f7b06943fad6f0a4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43acd71042349249426fc16432837c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/779 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ba84f635494b9f9c5aa9e050fd8eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [03:43<00:00, 111.77s/it]\n",
      "100%|██████████| 2/2 [04:48<00:00, 144.29s/it]\n"
     ]
    }
   ],
   "source": [
    "item = data[0]\n",
    "f_pap = item['focus']\n",
    "c_pap = item['cited']\n",
    "\n",
    "focus_paper = PaperAuthor(\n",
    "    model = model_server,\n",
    "    paper = Paper(f_pap),\n",
    "    focus=True,\n",
    "    id=0\n",
    ")\n",
    "\n",
    "cited_paper = PaperAuthor(\n",
    "    model = model_server,\n",
    "    paper = Paper(c_pap),\n",
    "    focus=False,\n",
    "    id=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train',\n",
       "  0.73787904),\n",
       " ('Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]',\n",
       "  0.7326936),\n",
       " ('We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. *Equal contribution',\n",
       "  0.7009049),\n",
       " ('This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]',\n",
       "  0.70078576),\n",
       " ('We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature',\n",
       "  0.6994137)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_paper.paper.retrieve_top_k(\"language model architectures\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('With a single attention head, averaging inhibits this. MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O where headi = Attention(QW Q i , KW Ki , V W Vi ) Where the projections are parameter matrices W Q and W O Rhdvxdmodel',\n",
       "  0.8372327),\n",
       " ('We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences',\n",
       "  0.7794119),\n",
       " ('These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions',\n",
       "  0.77801174),\n",
       " ('The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]',\n",
       "  0.76742196),\n",
       " ('While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality',\n",
       "  0.7652785)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_paper.paper.retrieve_top_k(\"multi headed attention\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderator = Moderator(model_server)\n",
    "\n",
    "paper_authors = [focus_paper, cited_paper]\n",
    "leaf_node_label = args.topic\n",
    "\n",
    "if args.log_dir != \"\":\n",
    "    with open(os.path.join(args.log_dir, 'self_deliberation.txt'), 'w') as f:\n",
    "        f.write(f'Topic: {args.topic}\\n\\n')\n",
    "\n",
    "# each node has a topic\n",
    "root_node = DebateNode(leaf_node_label)\n",
    "subtrees = root_node.conduct_self_deliberation(leaf_node_label, paper_authors, log=args.log_dir) # k new, finer topics to discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.00it/s]\n"
     ]
    }
   ],
   "source": [
    "evidence, scores = paper_authors[1].gather_evidence(args.topic, k=5, return_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations',\n",
       " 'The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017)',\n",
       " '4We note that in the literature the bidirectional TransBERTBERTE[CLS]E1 E[SEP]...ENE1\\'...EM\\'CT1T[SEP]...TNT1\\'...TM\\'[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1\\'...EM\\'CT1T[SEP]...TNT1\\'...TM\\'[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., (cid:104) Question, Answer (cid:105)) in one token sequence. Throughout this work, a \"sentence\" can be an arbitrary span of contiguous text, rather than an actual linguistic sentence',\n",
       " 'BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. * BERT advances the state of the art for eleven NLP tasks',\n",
       " 'Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to preIn additrain a deep bidirectional Transformer. tion to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.11s/it, est. speed input: 99.07 toks/s, output: 56.63 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"argument_list\": [{\"title\": \"RNNs are outperform by newer language model architectures\", \"description\": \"Newer architectures such as attention-based models, have outperformed traditional RNN-based models in sequence modeling and transduction problems such as machine translation and language modeling.\", \"evidence\": [\"Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5].\", \"Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].\", \"Recurrent models typically factor computation along the symbol positions of the input and output sequences.\", \"On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.\", \"In the former task our best model outperforms even all previously reported ensembles.\"]}, {\"title\": \"Newer architectures, such as self-attention, are necessary for sequence modeling due to the inherent sequential nature of RNNs\", \"description\": \"The sequential nature of RNNs precludes parallelization within training examples, which can become a limiting factor as sequence lengths increase, suggesting a need for newer architectures that can handle parallelization, such as self-attention.\" , \"evidence\": [\"Recurrent models typically factor computation along the symbol positions of the input and output sequences.\", \"Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht-1 and the input for position t.\", \"This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples\", \"Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences\", \"Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences\"]}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.08s/it, est. speed input: 106.74 toks/s, output: 55.48 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"argument_list\": [{\"title\": \"Bidirectional Transformer architectures outperform unidirectional architectures on a variety of linguistic tasks.\", \"description\": \"Extensive studies have shown that the pre-training objectives, Masked Language Modeling (MLM) and Next Sentence Prediction (NSP), lead to the development of powerful bidirectional transformer architectures, such as BERT, which achieve state-of-the-art results on a variety of NLP tasks, including question answering, natural language inference, and sentiment analysis. In addition, the ability of BERT to handle a variety of downstream tasks is a result of its input representation, which can unambiguously represent both a single sentence and a pair of sentences in one token sequence. Furthermore, studies have shown that fine-tuning pre-trained Bidirectional architectures with downstream specific objectives can unlock even state-of-the-art results, demonstrating the superiority of Bidirectional transformer architectures. \\\"Evidence\\\": [\" ,\"evidence\": [\"The same pre-trained model parameters are used to initialize models for different downstream tasks.\", \"these results enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks.\", \"-fedus WUA }.stem (\"]},{\"title\": \"Unidirectional language model architectures provide a more efficient alternative for linguistic tasks requiring less computational resources.\", \"description\": \"A key advantage of unidirectional architectures is their ability to handle the need for less computational resources. Studies have shown that deep unidirectional architectures can benefit even low-resource tasks, suggesting that a subset of tasks may not require the computational prowess of bidirectional models. Furthermore, fine-tuning is directly discussed about their potential in this direction to add efficiently. Also for designing type of Sentence embeddings.S-A;if sentence esome research especial hoànite additional variant with with entities mention going atoms behaviors such other several ways incorporatedWSF-onlyhap indicatesFullFor some pre-training objectives such as the cloze task, can improve robustness of text task generation models correspondingwhile-related Natural drawing raceprobablyNot to shall Tasks formab)).\",\"evidence\":[\"it can be used for text generation\"]}] }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    }
   ],
   "source": [
    "topic = args.topic\n",
    "log = None\n",
    "focus_paper = None\n",
    "arguments = {}\n",
    "children = []\n",
    "evidences = {}\n",
    "\n",
    "focus_paper = None\n",
    "for paper_author in paper_authors:\n",
    "    # gather evidence\n",
    "    evidence, scores = paper_author.gather_evidence(topic, return_scores=True)\n",
    "\n",
    "    if paper_author.id not in evidences.keys(): evidences[paper_author.id] = []\n",
    "    evidences[paper_author.id].append(evidence)\n",
    "\n",
    "    # develop k arguments\n",
    "    if paper_author.id not in arguments.keys(): arguments[paper_author.id] = []\n",
    "    author_args = paper_author.generate_arguments(topic, evidence,k=2)\n",
    "    arguments[paper_author.id].append(author_args)\n",
    "\n",
    "    # check if paper is the focus\n",
    "    if paper_author.focus:\n",
    "        focus_paper = paper_author\n",
    "\n",
    "    # logging\n",
    "    if log is not None:\n",
    "        with open(os.path.join(log, 'self_deliberation.txt'), 'a') as f:\n",
    "            f.write(f'Topic: {topic}\\n\\n')\n",
    "            f.write(f'Gather Evidence:\\n\\n')\n",
    "            temp = \"\\n\".join([f'{s} - {e}' for s, e in zip(scores, evidence)])\n",
    "            f.write(f'{paper_author.focus} paper:\\n{temp}\\n\\n')\n",
    "\n",
    "            f.write(f'Develop Arguments:\\n\\n')\n",
    "            f.write(f'{paper_author.focus} paper:\\n{author_args}\\n\\n')\n",
    "\n",
    "# preemption\n",
    "for i in range(len(paper_authors)):\n",
    "    other_arguments = [arguments[paper_authors[j].id] for j in range(len(paper_authors)) if j != i]\n",
    "    other_evidence = [evidences[paper_authors[j].id] for j in range(len(paper_authors)) if j != i]\n",
    "\n",
    "    preemption = paper_authors[i].preempt_arguments(other_arguments, other_evidence)\n",
    "    \n",
    "    # logging\n",
    "    if log is not None:\n",
    "        with open(os.path.join(log, 'self_deliberation.txt'), 'a') as f:\n",
    "            f.write(f'Preemption:\\n\\n')\n",
    "            f.write(f'{paper_author.focus} paper:\\n{preemption}\\n\\n')\n",
    "\n",
    "    evidences[paper_authors[i].id].append(preemption)         \n",
    "\n",
    "# for child_topic in arguments[focus_paper.id]:\n",
    "#     children.append(DebateNode(child_topic, parent=self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Does my paper also include or address [{\\'argument_list\\': [{\\'title\\': \\'Bidirectional Transformer architectures outperform unidirectional architectures on a variety of linguistic tasks.\\', \\'description\\': \\'Extensive studies have shown that the pre-training objectives, Masked Language Modeling (MLM) and Next Sentence Prediction (NSP), lead to the development of powerful bidirectional transformer architectures, such as BERT, which achieve state-of-the-art results on a variety of NLP tasks, including question answering, natural language inference, and sentiment analysis. In addition, the ability of BERT to handle a variety of downstream tasks is a result of its input representation, which can unambiguously represent both a single sentence and a pair of sentences in one token sequence. Furthermore, studies have shown that fine-tuning pre-trained Bidirectional architectures with downstream specific objectives can unlock even state-of-the-art results, demonstrating the superiority of Bidirectional transformer architectures. \"Evidence\": [\\', \\'evidence\\': [\\'The same pre-trained model parameters are used to initialize models for different downstream tasks.\\', \\'these results enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks.\\', \\'-fedus WUA }.stem (\\']}, {\\'title\\': \\'Unidirectional language model architectures provide a more efficient alternative for linguistic tasks requiring less computational resources.\\', \\'description\\': \\'A key advantage of unidirectional architectures is their ability to handle the need for less computational resources. Studies have shown that deep unidirectional architectures can benefit even low-resource tasks, suggesting that a subset of tasks may not require the computational prowess of bidirectional models. Furthermore, fine-tuning is directly discussed about their potential in this direction to add efficiently. Also for designing type of Sentence embeddings.S-A;if sentence esome research especial hoanite additional variant with with entities mention going atoms behaviors such other several ways incorporatedWSF-onlyhap indicatesFullFor some pre-training objectives such as the cloze task, can improve robustness of text task generation models correspondingwhile-related Natural drawing raceprobablyNot to shall Tasks formab)).\\', \\'evidence\\': [\\'it can be used for text generation\\']}]}]?': (['We used a beam size of 21 and a = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K sentences. 7 Conclusion In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers',\n",
       "   'This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input',\n",
       "   'Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training This section describes the training regime for our models. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]',\n",
       "   'Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output',\n",
       "   'Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model'],\n",
       "  [0.93159544, 0.9313577, 0.9292154, 0.92902064, 0.9282186]),\n",
       " 'Does my paper propose a better claim/idea than \"[{\\'argument_list\\': [{\\'title\\': \\'Bidirectional Transformer architectures outperform unidirectional architectures on a variety of linguistic tasks.\\', \\'description\\': \\'Extensive studies have shown that the pre-training objectives, Masked Language Modeling (MLM) and Next Sentence Prediction (NSP), lead to the development of powerful bidirectional transformer architectures, such as BERT, which achieve state-of-the-art results on a variety of NLP tasks, including question answering, natural language inference, and sentiment analysis. In addition, the ability of BERT to handle a variety of downstream tasks is a result of its input representation, which can unambiguously represent both a single sentence and a pair of sentences in one token sequence. Furthermore, studies have shown that fine-tuning pre-trained Bidirectional architectures with downstream specific objectives can unlock even state-of-the-art results, demonstrating the superiority of Bidirectional transformer architectures. \"Evidence\": [\\', \\'evidence\\': [\\'The same pre-trained model parameters are used to initialize models for different downstream tasks.\\', \\'these results enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks.\\', \\'-fedus WUA }.stem (\\']}, {\\'title\\': \\'Unidirectional language model architectures provide a more efficient alternative for linguistic tasks requiring less computational resources.\\', \\'description\\': \\'A key advantage of unidirectional architectures is their ability to handle the need for less computational resources. Studies have shown that deep unidirectional architectures can benefit even low-resource tasks, suggesting that a subset of tasks may not require the computational prowess of bidirectional models. Furthermore, fine-tuning is directly discussed about their potential in this direction to add efficiently. Also for designing type of Sentence embeddings.S-A;if sentence esome research especial hoanite additional variant with with entities mention going atoms behaviors such other several ways incorporatedWSF-onlyhap indicatesFullFor some pre-training objectives such as the cloze task, can improve robustness of text task generation models correspondingwhile-related Natural drawing raceprobablyNot to shall Tasks formab)).\\', \\'evidence\\': [\\'it can be used for text generation\\']}]}]\"?': (['This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input',\n",
       "   'We used a beam size of 21 and a = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur- prisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley- Parser [29] even when training only on the WSJ training set of 40K sentences. 7 Conclusion In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers',\n",
       "   'Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output',\n",
       "   'Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training This section describes the training regime for our models. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]',\n",
       "   'Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model'],\n",
       "  [0.9316725, 0.93082386, 0.92839825, 0.92777026, 0.92711246])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "evidences[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "queue_of_rounds: List[DebateNode] = []\n",
    "queue_of_rounds.extend(subtrees)\n",
    "\n",
    "\n",
    "while len(queue_of_rounds) > 0:\n",
    "    round = queue_of_rounds.pop(0)\n",
    "    conversation, new_focus_arg, new_cited_arg = round.conduct_debate(focus_paper, cited_paper)\n",
    "    conversation_history.extend(conversation)\n",
    "    if moderator.is_expand(round.arguments, [new_focus_arg, new_cited_arg]):\n",
    "        new_subtrees = round.conduct_self_deliberation(round.round_topic, paper_authors)\n",
    "        queue_of_rounds.extend(new_subtrees)\n",
    "\n",
    "with open('conversation_history.txt', 'w+') as f:\n",
    "    f.write('\\n'.join(conversation_history))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
