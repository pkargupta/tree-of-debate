{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/shared/data3/pk36/.cache'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=/shared/data3/pk36/.cache\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from debate import DebateNode\n",
    "from paper_details import Paper\n",
    "from persona import PaperAuthor\n",
    "from moderator import Moderator\n",
    "import argparse\n",
    "from typing import List\n",
    "from vllm import LLM\n",
    "import os\n",
    "import json\n",
    "from data_pairer import parse_papers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.log_dir = \"logs\"\n",
    "        self.focus_paper = \"https://arxiv.org/pdf/1706.03762\" # transformer\n",
    "        self.cited_paper = \"https://arxiv.org/pdf/1810.04805\" # bert\n",
    "        self.topic = \"language model architectures\"\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args.log_dir):\n",
    "    os.makedirs(args.log_dir)\n",
    "\n",
    "# if not os.path.exists(\"data.json\"):\n",
    "parse_papers(args.focus_paper, args.cited_paper)\n",
    "with open('data.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-23 13:19:58 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-23 13:19:58 arg_utils.py:957] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 11-23 13:19:58 config.py:1021] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 11-23 13:19:58 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 11-23 13:19:58 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-23 13:19:58 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=535423)\u001b[0;0m INFO 11-23 13:19:59 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-23 13:19:59 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-23 13:19:59 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=535423)\u001b[0;0m INFO 11-23 13:19:59 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=535423)\u001b[0;0m INFO 11-23 13:19:59 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-23 13:20:00 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/pk36/.cache/vllm/gpu_p2p_access_cache_for_6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=535423)\u001b[0;0m INFO 11-23 13:20:00 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/pk36/.cache/vllm/gpu_p2p_access_cache_for_6,7.json\n",
      "INFO 11-23 13:20:00 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f038d3ac5e0>, local_subscribe_port=56847, remote_subscribe_port=None)\n",
      "INFO 11-23 13:20:00 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=535423)\u001b[0;0m INFO 11-23 13:20:00 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 11-23 13:20:00 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=535423)\u001b[0;0m INFO 11-23 13:20:00 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23a470b88e74841bb145736bee20489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-23 13:20:03 model_runner.py:1067] Loading model weights took 7.5122 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=535423)\u001b[0;0m INFO 11-23 13:20:03 model_runner.py:1067] Loading model weights took 7.5122 GB\n",
      "INFO 11-23 13:20:05 distributed_gpu_executor.py:57] # GPU blocks: 15590, # CPU blocks: 4096\n",
      "INFO 11-23 13:20:05 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 1.90x\n",
      "INFO 11-23 13:20:07 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-23 13:20:07 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=535423)\u001b[0;0m INFO 11-23 13:20:07 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=535423)\u001b[0;0m INFO 11-23 13:20:07 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-23 13:20:16 custom_all_reduce.py:233] Registering 1040 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=535423)\u001b[0;0m INFO 11-23 13:20:16 custom_all_reduce.py:233] Registering 1040 cuda graph addresses\n",
      "INFO 11-23 13:20:16 model_runner.py:1523] Graph capturing finished in 9 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=535423)\u001b[0;0m INFO 11-23 13:20:16 model_runner.py:1523] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "model_server = LLM(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",tensor_parallel_size=2, gpu_memory_utilization=0.5, max_num_seqs=100) #,enable_prefix_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.91it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "item = data[0]\n",
    "f_pap = item['focus']\n",
    "c_pap = item['cited']\n",
    "\n",
    "focus_paper = PaperAuthor(\n",
    "    model = model_server,\n",
    "    paper = Paper(f_pap),\n",
    "    focus=True,\n",
    "    id=0\n",
    ")\n",
    "\n",
    "cited_paper = PaperAuthor(\n",
    "    model = model_server,\n",
    "    paper = Paper(c_pap),\n",
    "    focus=False,\n",
    "    id=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train',\n",
       "  0.73787886),\n",
       " ('Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]',\n",
       "  0.73269325),\n",
       " ('We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. *Equal contribution',\n",
       "  0.7009047),\n",
       " ('This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]',\n",
       "  0.7007855),\n",
       " ('We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature',\n",
       "  0.69941336)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_paper.paper.retrieve_top_k(\"language model architectures\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('With a single attention head, averaging inhibits this. MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O where headi = Attention(QW Q i , KW Ki , V W Vi ) Where the projections are parameter matrices W Q and W O Rhdvxdmodel',\n",
       "  0.8372327),\n",
       " ('We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences',\n",
       "  0.7794119),\n",
       " ('These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions',\n",
       "  0.77801174),\n",
       " ('The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]',\n",
       "  0.76742196),\n",
       " ('While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality',\n",
       "  0.7652785)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_paper.paper.retrieve_top_k(\"multi headed attention\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 38.01it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it, est. speed input: 229.25 toks/s, output: 63.56 toks/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 41.15it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.00s/it, est. speed input: 216.41 toks/s, output: 62.26 toks/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 36.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 38.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 42.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 38.70it/s]\n"
     ]
    }
   ],
   "source": [
    "moderator = Moderator(model_server)\n",
    "\n",
    "paper_authors = [focus_paper, cited_paper]\n",
    "leaf_node_label = args.topic\n",
    "\n",
    "if args.log_dir != \"\":\n",
    "    with open(os.path.join(args.log_dir, 'self_deliberation.txt'), 'w') as f:\n",
    "        f.write(f'Topic: {args.topic}\\n\\n')\n",
    "\n",
    "# each node has a topic\n",
    "root_node = DebateNode(leaf_node_label)\n",
    "subtrees = root_node.conduct_self_deliberation(leaf_node_label, paper_authors, log=args.log_dir) # k new, finer topics to discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [{'title': 'Transformer Architecture Offers Superior Performance',\n",
       "   'description': 'Our proposed Transformer architecture outperforms existing recurrent neural networks in machine translation tasks, as shown in experiments on two tasks. This is a significant contribution to language model architectures as it demonstrates the potential of attention mechanisms in sequence modeling.'},\n",
       "  {'title': 'Transformer Generalizes Well to Other Tasks',\n",
       "   'description': 'The Transformer architecture is not limited to machine translation tasks, as it successfully applies to English constituency parsing with both large and limited training data, showcasing its generalizability and versatility.'}],\n",
       " 1: [{'title': 'Bidirectional Language Model Architecture',\n",
       "   'description': 'Our paper introduces a novel bidirectional language model architecture that extracts context-sensitive features from both left-to-right and right-to-left language models. This is supported by evidence that shows the concatenation of left-to-right and right-to-left representations to form contextual representations of each token. Furthermore, our architecture allows for the pre-training of a deep bidirectional Transformer, which is a major improvement over traditional left-to-right language models.'},\n",
       "  {'title': 'Advancements in Pre-training Objectives',\n",
       "   'description': 'Our paper introduces a novel pre-training objective that enables the representation to fuse the left and the right context, allowing for the pre-training of a deep bidirectional Transformer. This is supported by evidence that shows the use of a masked language model and a next sentence prediction task to jointly pretrain text-pair representations.'}]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node.self_delib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'Does my paper also address the claim, \"bidirectional language model architecture\"?': ['We used a beam size of 21 and a = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]',\n",
       "   'Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]'],\n",
       "  'Does my paper also address the claim, \"advancements in pre-training objectives\"?': ['During inference, we Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative semi-supervised semi-supervised semi-supervised semi-supervised semi-supervised multi-task generative Petrov et al',\n",
       "   'We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences']},\n",
       " 1: {'Does my paper also address the claim, \"transformer architecture offers superior performance\"?': ['BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model',\n",
       "   '(2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)'],\n",
       "  'Does my paper also address the claim, \"transformer generalizes well to other tasks\"?': ['BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model',\n",
       "   '(2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)']}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node.preemption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Transformer Architecture Offers Superior Performance,\n",
       " Transformer Generalizes Well to Other Tasks]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PRESENT ARGUMENT FOR AUTHOR 0:\n",
      "\n",
      "You are an author of a paper that is debating another author on your claim topic:\n",
      "\t- Topic: Transformer Architecture Offers Superior Performance\n",
      "\t- Topic Description: Our proposed Transformer architecture outperforms existing recurrent neural networks in machine translation tasks, as shown in experiments on two tasks. This is a significant contribution to language model architectures as it demonstrates the potential of attention mechanisms in sequence modeling.\n",
      "\n",
      "Your debate claim is that your paper's contributions towards the topic are all novel relative to the other paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.\n",
      "\n",
      "You used the following evidence to support your arguments:\n",
      "\t- Evidence #1. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train\n",
      "\t- Evidence #2. Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]\n",
      "\t- Evidence #3. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. *Equal contribution\n",
      "\t- Evidence #4. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]\n",
      "\t- Evidence #5. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature\n",
      "\n",
      "You also have preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "\t- Opposition Claim #1: Does my paper also address the opposition's claim, \"bidirectional language model architecture\"?\n",
      "\t\t- Your Counter Evidence #1: We used a beam size of 21 and a = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]\n",
      "\t\t- Your Counter Evidence #2: Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]\n",
      "\n",
      "\t- Opposition Claim #2: Does my paper also address the opposition's claim, \"advancements in pre-training objectives\"?\n",
      "\t\t- Your Counter Evidence #1: During inference, we Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative semi-supervised semi-supervised semi-supervised semi-supervised semi-supervised multi-task generative Petrov et al\n",
      "\t\t- Your Counter Evidence #2: We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences\n",
      "\n",
      "Given the above (your initial argument, your evidence, the opposition paper's claimed points of novelty, and your counter evidence), make an argument for a specific reason why your paper's contributions towards the topic are all novel relative to the other paper, with respect to the topic, Transformer Architecture Offers Superior Performance. \n",
      "\n",
      "Output your argument in the following JSON format:\n",
      "\n",
      "{\n",
      "    \"title\": <should be a brief, 10-15 word string where the value is the argument title>,\n",
      "    \"description\": <2-3 sentence string explaining the argument>\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PRESENT ARGUMENT FOR AUTHOR 1:\n",
      "\n",
      "You are an author of a paper that is debating another author about their claimed novelty:\n",
      "\t- Novelty Claim: Transformer Architecture Offers Superior Performance\n",
      "\t- Novelty Claim Description: Our proposed Transformer architecture outperforms existing recurrent neural networks in machine translation tasks, as shown in experiments on two tasks. This is a significant contribution to language model architectures as it demonstrates the potential of attention mechanisms in sequence modeling.\n",
      "\n",
      "Your debate claim is that the other paper's contributions towards the topic are not novel relative to your own paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.\n",
      "\n",
      "You initially argued that your own paper has the following novelties:\n",
      "\t- Argument #1. Bidirectional Language Model Architecture: Our paper introduces a novel bidirectional language model architecture that extracts context-sensitive features from both left-to-right and right-to-left language models. This is supported by evidence that shows the concatenation of left-to-right and right-to-left representations to form contextual representations of each token. Furthermore, our architecture allows for the pre-training of a deep bidirectional Transformer, which is a major improvement over traditional left-to-right language models.\n",
      "\t- Argument #2. Advancements in Pre-training Objectives: Our paper introduces a novel pre-training objective that enables the representation to fuse the left and the right context, allowing for the pre-training of a deep bidirectional Transformer. This is supported by evidence that shows the use of a masked language model and a next sentence prediction task to jointly pretrain text-pair representations.\n",
      "\n",
      "\n",
      "You used the following evidence to support your arguments:\n",
      "\t- Evidence #1. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations\n",
      "\t- Evidence #2. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017)\n",
      "\t- Evidence #3. 4We note that in the literature the bidirectional TransBERTBERTE[CLS]E1 E[SEP]...ENE1'...EM'CT1T[SEP]...TNT1'...TM'[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1'...EM'CT1T[SEP]...TNT1'...TM'[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., (cid:104) Question, Answer (cid:105)) in one token sequence. Throughout this work, a \"sentence\" can be an arbitrary span of contiguous text, rather than an actual linguistic sentence\n",
      "\t- Evidence #4. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. * BERT advances the state of the art for eleven NLP tasks\n",
      "\t- Evidence #5. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to preIn additrain a deep bidirectional Transformer. tion to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations\n",
      "\n",
      "You also have preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "\t- Opposition Claim #1: Does my paper also address the opposition's claim, \"transformer architecture offers superior performance\"?\n",
      "\t\t- Your Counter Evidence #1: BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model\n",
      "\t\t- Your Counter Evidence #2: (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)\n",
      "\n",
      "\t- Opposition Claim #2: Does my paper also address the opposition's claim, \"transformer generalizes well to other tasks\"?\n",
      "\t\t- Your Counter Evidence #1: BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model\n",
      "\t\t- Your Counter Evidence #2: (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)\n",
      "\n",
      "Given the above (your initial argument, your evidence, the opposition paper's claimed points of novelty, and your counter evidence), make an argument for a specific reason why the other paper's contributions towards the topic are not novel relative to your own paper, with respect to the topic, Transformer Architecture Offers Superior Performance. \n",
      "\n",
      "Output your argument in the following JSON format:\n",
      "\n",
      "{\n",
      "    \"title\": <should be a brief, 10-15 word string where the value is the argument title>,\n",
      "    \"description\": <2-3 sentence string explaining the argument>\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RESPOND ARGUMENT FOR AUTHOR 0:\n",
      "\n",
      "You are an author of a paper that is debating another author.\n",
      "\n",
      "Your debate claim is that your paper's contributions towards the \"topic\" are all novel relative to the other paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.\n",
      "\n",
      "You used the following evidence to support your arguments:\n",
      "\t- Evidence #1. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train\n",
      "\t- Evidence #2. Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]\n",
      "\t- Evidence #3. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. *Equal contribution\n",
      "\t- Evidence #4. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]\n",
      "\t- Evidence #5. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature\n",
      "\n",
      "You also have preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "\t- Opposition Claim #1: Does my paper also address the opposition's claim, \"bidirectional language model architecture\"?\n",
      "\t\t- Your Counter Evidence #1: We used a beam size of 21 and a = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]\n",
      "\t\t- Your Counter Evidence #2: Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]\n",
      "\n",
      "\t- Opposition Claim #2: Does my paper also address the opposition's claim, \"advancements in pre-training objectives\"?\n",
      "\t\t- Your Counter Evidence #1: During inference, we Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative semi-supervised semi-supervised semi-supervised semi-supervised semi-supervised multi-task generative Petrov et al\n",
      "\t\t- Your Counter Evidence #2: We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences\n",
      "\n",
      "Based on the debate history and your/your opposition's arguments and evidence, you must respond to the last argument presented by your opposition in debate.\n",
      "\n",
      "Debate Topic Information:\n",
      "\t- Topic: Transformer Architecture Offers Superior Performance\n",
      "\t- Topic Description: Our proposed Transformer architecture outperforms existing recurrent neural networks in machine translation tasks, as shown in experiments on two tasks. This is a significant contribution to language model architectures as it demonstrates the potential of attention mechanisms in sequence modeling.\n",
      "\n",
      "Debate History:\n",
      "\n",
      "\t-You: I argue that novel contributions in transformer architecture. Our paper's contributions are novel due to the Transformer's superior performance and generalizability, as well as its ability to learn complex tasks without relying on recurrence or convolutions. The opposition's claims of novelty are addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our attention mechanisms learn to perform different tasks and exhibit behavior related to sentence structure. Furthermore, our paper's results demonstrate that the Transformer generalizes well to other tasks, such as English constituency parsing, and presents specific challenges that RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes.\n",
      "\t-Opposition: I argue that lack of novelty in transformer architecture contributions. The other paper's contributions are not novel relative to our own paper, as they do not address the limitations of unidirectional language models and do not provide a novel pre-training objective. Our paper introduces a novel bidirectional language model architecture and a novel pre-training objective that enables the representation to fuse the left and the right context, allowing for the pre-training of a deep bidirectional Transformer. Furthermore, our paper shows that BERTLARGE performs competitively with state-of-the-art methods, indicating that the Transformer architecture is not superior in all cases.\n",
      "\n",
      "\n",
      "Output your new response in the following JSON format:\n",
      "\n",
      "{\n",
      "    \"title\": <should be a brief, 10-15 word string where the value is the main argument of your response to the opposition>,\n",
      "    \"description\": <2-3 sentence string explaining your response to the opposition's last turn, based on the provided pool of evidence>\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RESPOND ARGUMENT FOR AUTHOR 1:\n",
      "\n",
      "You are an author of a paper that is debating another author.\n",
      "\n",
      "Your debate claim is that the other paper's contributions towards the \"topic\" are not novel relative to your own paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.You used the following evidence to support your arguments:\n",
      "\t- Evidence #1. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations\n",
      "\t- Evidence #2. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017)\n",
      "\t- Evidence #3. 4We note that in the literature the bidirectional TransBERTBERTE[CLS]E1 E[SEP]...ENE1'...EM'CT1T[SEP]...TNT1'...TM'[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1'...EM'CT1T[SEP]...TNT1'...TM'[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., (cid:104) Question, Answer (cid:105)) in one token sequence. Throughout this work, a \"sentence\" can be an arbitrary span of contiguous text, rather than an actual linguistic sentence\n",
      "\t- Evidence #4. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. * BERT advances the state of the art for eleven NLP tasks\n",
      "\t- Evidence #5. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to preIn additrain a deep bidirectional Transformer. tion to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations\n",
      "\n",
      "You also have preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "\t- Opposition Claim #1: Does my paper also address the opposition's claim, \"transformer architecture offers superior performance\"?\n",
      "\t\t- Your Counter Evidence #1: BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model\n",
      "\t\t- Your Counter Evidence #2: (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)\n",
      "\n",
      "\t- Opposition Claim #2: Does my paper also address the opposition's claim, \"transformer generalizes well to other tasks\"?\n",
      "\t\t- Your Counter Evidence #1: BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model\n",
      "\t\t- Your Counter Evidence #2: (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)\n",
      "\n",
      "Based on the debate history and your/your opposition's arguments and evidence, you must respond to the last argument presented by your opposition in debate.\n",
      "\n",
      "Debate Topic Information:\n",
      "\t- Topic: Transformer Architecture Offers Superior Performance\n",
      "\t- Topic Description: Our proposed Transformer architecture outperforms existing recurrent neural networks in machine translation tasks, as shown in experiments on two tasks. This is a significant contribution to language model architectures as it demonstrates the potential of attention mechanisms in sequence modeling.\n",
      "\n",
      "Debate History:\n",
      "\n",
      "\t-Opposition: I argue that novel contributions in transformer architecture. Our paper's contributions are novel due to the Transformer's superior performance and generalizability, as well as its ability to learn complex tasks without relying on recurrence or convolutions. The opposition's claims of novelty are addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our attention mechanisms learn to perform different tasks and exhibit behavior related to sentence structure. Furthermore, our paper's results demonstrate that the Transformer generalizes well to other tasks, such as English constituency parsing, and presents specific challenges that RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes.\n",
      "\t-You: I argue that lack of novelty in transformer architecture contributions. The other paper's contributions are not novel relative to our own paper, as they do not address the limitations of unidirectional language models and do not provide a novel pre-training objective. Our paper introduces a novel bidirectional language model architecture and a novel pre-training objective that enables the representation to fuse the left and the right context, allowing for the pre-training of a deep bidirectional Transformer. Furthermore, our paper shows that BERTLARGE performs competitively with state-of-the-art methods, indicating that the Transformer architecture is not superior in all cases.\n",
      "\n",
      "\t-Opposition: I believe that addressing novelty in bidirectional language model architecture. The opposition's claim of novelty in bidirectional language model architecture is addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our attention mechanisms learn to perform different tasks and exhibit behavior related to sentence structure. Furthermore, our paper's results demonstrate that the Transformer generalizes well to other tasks, such as English constituency parsing, and presents specific challenges that RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes. Our model's ability to learn complex tasks without relying on recurrence or convolutions is a significant contribution to language model architectures.\n",
      "\n",
      "Output your new response in the following JSON format:\n",
      "\n",
      "{\n",
      "    \"title\": <should be a brief, 10-15 word string where the value is the main argument of your response to the opposition>,\n",
      "    \"description\": <2-3 sentence string explaining your response to the opposition's last turn, based on the provided pool of evidence>\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "REVISE ARGUMENT FOR AUTHOR 0:\n",
      "\n",
      "You are an author of a paper that is debating another author.\n",
      "\n",
      "Your debate claim is that your paper's contributions towards the \"topic\" are all novel relative to the other paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.\n",
      "\n",
      "Use the following evidence to support your arguments:\n",
      "\t- Evidence #1. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train\n",
      "\t- Evidence #2. Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]\n",
      "\t- Evidence #3. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. *Equal contribution\n",
      "\t- Evidence #4. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]\n",
      "\t- Evidence #5. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature\n",
      "\n",
      "You also had preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "\t- Opposition Claim #1: Does my paper also address the opposition's claim, \"bidirectional language model architecture\"?\n",
      "\t\t- Your Counter Evidence #1: We used a beam size of 21 and a = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]\n",
      "\t\t- Your Counter Evidence #2: Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]\n",
      "\n",
      "\t- Opposition Claim #2: Does my paper also address the opposition's claim, \"advancements in pre-training objectives\"?\n",
      "\t\t- Your Counter Evidence #1: During inference, we Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative WSJ only, discriminative semi-supervised semi-supervised semi-supervised semi-supervised semi-supervised multi-task generative Petrov et al\n",
      "\t\t- Your Counter Evidence #2: We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences\n",
      "\n",
      "Based on the debate history and your/your opposition's arguments and evidence, you must strengthen your claims given the reponses of the opposition.\n",
      "\n",
      "Debate Topic Information:\n",
      "\t- Topic: Transformer Architecture Offers Superior Performance\n",
      "\t- Topic Description: Our proposed Transformer architecture outperforms existing recurrent neural networks in machine translation tasks, as shown in experiments on two tasks. This is a significant contribution to language model architectures as it demonstrates the potential of attention mechanisms in sequence modeling.\n",
      "\n",
      "Debate History:\n",
      "\n",
      "\t-You: I argue that novel contributions in transformer architecture. Our paper's contributions are novel due to the Transformer's superior performance and generalizability, as well as its ability to learn complex tasks without relying on recurrence or convolutions. The opposition's claims of novelty are addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our attention mechanisms learn to perform different tasks and exhibit behavior related to sentence structure. Furthermore, our paper's results demonstrate that the Transformer generalizes well to other tasks, such as English constituency parsing, and presents specific challenges that RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes.\n",
      "\t-Opposition: I argue that lack of novelty in transformer architecture contributions. The other paper's contributions are not novel relative to our own paper, as they do not address the limitations of unidirectional language models and do not provide a novel pre-training objective. Our paper introduces a novel bidirectional language model architecture and a novel pre-training objective that enables the representation to fuse the left and the right context, allowing for the pre-training of a deep bidirectional Transformer. Furthermore, our paper shows that BERTLARGE performs competitively with state-of-the-art methods, indicating that the Transformer architecture is not superior in all cases.\n",
      "\n",
      "\t-You: I believe that addressing novelty in bidirectional language model architecture. The opposition's claim of novelty in bidirectional language model architecture is addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our attention mechanisms learn to perform different tasks and exhibit behavior related to sentence structure. Furthermore, our paper's results demonstrate that the Transformer generalizes well to other tasks, such as English constituency parsing, and presents specific challenges that RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes. Our model's ability to learn complex tasks without relying on recurrence or convolutions is a significant contribution to language model architectures.\n",
      "\t-Opposition: I believe that lack of novelty in bidirectional language model architecture. The opposition's claim of novelty in bidirectional language model architecture is not supported by their evidence, as their model's performance is comparable to our own BERTLARGE model. Furthermore, their model's ability to learn complex tasks without recurrence or convolutions is not a novel contribution, as our own paper has shown that BERTLARGE performs competitively with state-of-the-art methods. The opposition's results on English constituency parsing do not demonstrate a significant contribution to language model architectures.\n",
      "\n",
      "\n",
      "Output your new, revised argument in the following JSON format:\n",
      "{\n",
      "    \"title\": <should be a brief, 10-15 word string where the value is the high-level response to the opposition>,\n",
      "    \"description\": <2-3 sentence string explaining your response to the opposition's last turn, based on the provided pool of evidence>\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "REVISE ARGUMENT FOR AUTHOR 1:\n",
      "\n",
      "You are an author of a paper that is debating another author.\n",
      "\n",
      "Your debate claim is that the other paper's contributions towards the \"topic\" are not novel relative to your own paper. Refer to their arguments and presented evidence, as well as your own paper's segments as evidence when refining your arguments.Use the following evidence to support your arguments:\n",
      "\t- Evidence #1. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations\n",
      "\t- Evidence #2. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017)\n",
      "\t- Evidence #3. 4We note that in the literature the bidirectional TransBERTBERTE[CLS]E1 E[SEP]...ENE1'...EM'CT1T[SEP]...TNT1'...TM'[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1'...EM'CT1T[SEP]...TNT1'...TM'[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., (cid:104) Question, Answer (cid:105)) in one token sequence. Throughout this work, a \"sentence\" can be an arbitrary span of contiguous text, rather than an actual linguistic sentence\n",
      "\t- Evidence #4. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. * BERT advances the state of the art for eleven NLP tasks\n",
      "\t- Evidence #5. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to preIn additrain a deep bidirectional Transformer. tion to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations\n",
      "\n",
      "You also had preemptively collected some counter evidence from your own paper based on the opposing author's claimed points of novelty:\n",
      "\t- Opposition Claim #1: Does my paper also address the opposition's claim, \"transformer architecture offers superior performance\"?\n",
      "\t\t- Your Counter Evidence #1: BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model\n",
      "\t\t- Your Counter Evidence #2: (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)\n",
      "\n",
      "\t- Opposition Claim #2: Does my paper also address the opposition's claim, \"transformer generalizes well to other tasks\"?\n",
      "\t\t- Your Counter Evidence #1: BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model\n",
      "\t\t- Your Counter Evidence #2: (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)\n",
      "\n",
      "Based on the debate history and your/your opposition's arguments and evidence, you must strengthen your claims given the reponses of the opposition.\n",
      "\n",
      "Debate Topic Information:\n",
      "\t- Topic: Transformer Architecture Offers Superior Performance\n",
      "\t- Topic Description: Our proposed Transformer architecture outperforms existing recurrent neural networks in machine translation tasks, as shown in experiments on two tasks. This is a significant contribution to language model architectures as it demonstrates the potential of attention mechanisms in sequence modeling.\n",
      "\n",
      "Debate History:\n",
      "\n",
      "\t-Opposition: I argue that novel contributions in transformer architecture. Our paper's contributions are novel due to the Transformer's superior performance and generalizability, as well as its ability to learn complex tasks without relying on recurrence or convolutions. The opposition's claims of novelty are addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our attention mechanisms learn to perform different tasks and exhibit behavior related to sentence structure. Furthermore, our paper's results demonstrate that the Transformer generalizes well to other tasks, such as English constituency parsing, and presents specific challenges that RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes.\n",
      "\t-You: I argue that lack of novelty in transformer architecture contributions. The other paper's contributions are not novel relative to our own paper, as they do not address the limitations of unidirectional language models and do not provide a novel pre-training objective. Our paper introduces a novel bidirectional language model architecture and a novel pre-training objective that enables the representation to fuse the left and the right context, allowing for the pre-training of a deep bidirectional Transformer. Furthermore, our paper shows that BERTLARGE performs competitively with state-of-the-art methods, indicating that the Transformer architecture is not superior in all cases.\n",
      "\n",
      "\t-Opposition: I believe that addressing novelty in bidirectional language model architecture. The opposition's claim of novelty in bidirectional language model architecture is addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our attention mechanisms learn to perform different tasks and exhibit behavior related to sentence structure. Furthermore, our paper's results demonstrate that the Transformer generalizes well to other tasks, such as English constituency parsing, and presents specific challenges that RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes. Our model's ability to learn complex tasks without relying on recurrence or convolutions is a significant contribution to language model architectures.\n",
      "\t-You: I believe that lack of novelty in bidirectional language model architecture. The opposition's claim of novelty in bidirectional language model architecture is not supported by their evidence, as their model's performance is comparable to our own BERTLARGE model. Furthermore, their model's ability to learn complex tasks without recurrence or convolutions is not a novel contribution, as our own paper has shown that BERTLARGE performs competitively with state-of-the-art methods. The opposition's results on English constituency parsing do not demonstrate a significant contribution to language model architectures.\n",
      "\n",
      "\t-Opposition: I argue that addressing novelty in bidirectional language model architecture. The opposition's claim of novelty in bidirectional language model architecture is addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our attention mechanisms learn to perform different tasks and exhibit behavior related to sentence structure. Furthermore, our paper's results demonstrate that the Transformer generalizes well to other tasks, such as English constituency parsing, and presents specific challenges that RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes. Our model's ability to learn complex tasks without relying on recurrence or convolutions is a significant contribution to language model architectures.\n",
      "\n",
      "Output your new, revised argument in the following JSON format:\n",
      "{\n",
      "    \"title\": <should be a brief, 10-15 word string where the value is the high-level response to the opposition>,\n",
      "    \"description\": <2-3 sentence string explaining your response to the opposition's last turn, based on the provided pool of evidence>\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "convo_history = root_node.children[0].conduct_debate(paper_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate Topic Information:\n",
      "\t- Topic: Transformer Architecture Offers Superior Performance\n",
      "\t- Topic Description: Our proposed Transformer architecture outperforms existing recurrent neural networks in machine translation tasks, as shown in experiments on two tasks. This is a significant contribution to language model architectures as it demonstrates the potential of attention mechanisms in sequence modeling.\n",
      "\n",
      "Debate History:\n",
      "\n",
      "\t-Author 0: I argue that novel contributions in transformer architecture. Our paper's contributions are novel due to the Transformer's superior performance and generalizability, as well as its ability to learn complex tasks without relying on recurrence or convolutions. The opposition's claims of novelty are addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our attention mechanisms learn to perform different tasks and exhibit behavior related to sentence structure. Furthermore, our paper's results demonstrate that the Transformer generalizes well to other tasks, such as English constituency parsing, and presents specific challenges that RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes.\n",
      "\t-Author 1: I argue that lack of novelty in transformer architecture contributions. The other paper's contributions are not novel relative to our own paper, as they do not address the limitations of unidirectional language models and do not provide a novel pre-training objective. Our paper introduces a novel bidirectional language model architecture and a novel pre-training objective that enables the representation to fuse the left and the right context, allowing for the pre-training of a deep bidirectional Transformer. Furthermore, our paper shows that BERTLARGE performs competitively with state-of-the-art methods, indicating that the Transformer architecture is not superior in all cases.\n",
      "\n",
      "\t-Author 0: I believe that addressing novelty in bidirectional language model architecture. The opposition's claim of novelty in bidirectional language model architecture is addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our attention mechanisms learn to perform different tasks and exhibit behavior related to sentence structure. Furthermore, our paper's results demonstrate that the Transformer generalizes well to other tasks, such as English constituency parsing, and presents specific challenges that RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes. Our model's ability to learn complex tasks without relying on recurrence or convolutions is a significant contribution to language model architectures.\n",
      "\t-Author 1: I believe that lack of novelty in bidirectional language model architecture. The opposition's claim of novelty in bidirectional language model architecture is not supported by their evidence, as their model's performance is comparable to our own BERTLARGE model. Furthermore, their model's ability to learn complex tasks without recurrence or convolutions is not a novel contribution, as our own paper has shown that BERTLARGE performs competitively with state-of-the-art methods. The opposition's results on English constituency parsing do not demonstrate a significant contribution to language model architectures.\n",
      "\n",
      "\t-Author 0: I argue that addressing novelty in bidirectional language model architecture. The opposition's claim of novelty in bidirectional language model architecture is addressed by our counter evidence, which shows that our model performs well even without task-specific tuning and that our attention mechanisms learn to perform different tasks and exhibit behavior related to sentence structure. Furthermore, our paper's results demonstrate that the Transformer generalizes well to other tasks, such as English constituency parsing, and presents specific challenges that RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes. Our model's ability to learn complex tasks without relying on recurrence or convolutions is a significant contribution to language model architectures.\n",
      "\t-Author 1: I argue that lack of novelty in transformer architecture contributions. The opposition's claims of novelty in transformer architecture are not supported by their evidence, as their model's performance is comparable to our own BERTLARGE model. Furthermore, their model's ability to learn complex tasks without recurrence or convolutions is not a novel contribution, as our own paper has shown that BERTLARGE performs competitively with state-of-the-art methods. The opposition's results on English constituency parsing do not demonstrate a significant contribution to language model architectures. Our paper's novel bidirectional language model architecture and pre-training objective are still the most significant contributions to language model architectures.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(convo_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'argument_list': [{'title': 'Argument 1',\n",
       "   'description': 'Argument 1 description'},\n",
       "  {'title': 'Argument 2', 'description': 'Argument 2 description'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node.children[0].response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "queue_of_rounds: List[DebateNode] = []\n",
    "queue_of_rounds.extend(subtrees)\n",
    "\n",
    "\n",
    "while len(queue_of_rounds) > 0:\n",
    "    round = queue_of_rounds.pop(0)\n",
    "    conversation, new_focus_arg, new_cited_arg = round.conduct_debate(focus_paper, cited_paper)\n",
    "    conversation_history.extend(conversation)\n",
    "    if moderator.is_expand(round.self_delib, [new_focus_arg, new_cited_arg]):\n",
    "        new_subtrees = round.conduct_self_deliberation(round.round_topic, paper_authors)\n",
    "        queue_of_rounds.extend(new_subtrees)\n",
    "\n",
    "with open('conversation_history.txt', 'w+') as f:\n",
    "    f.write('\\n'.join(conversation_history))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
