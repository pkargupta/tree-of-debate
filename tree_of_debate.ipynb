{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/shared/data3/pk36/.cache'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HOME=/shared/data3/pk36/.cache\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"5,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from debate import DebateNode\n",
    "from paper_details import Paper\n",
    "from persona import PaperAuthor\n",
    "from moderator import Moderator\n",
    "import argparse\n",
    "from typing import List\n",
    "from vllm import LLM\n",
    "import os\n",
    "import json\n",
    "from data_pairer import parse_papers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.log_dir = \"logs\"\n",
    "        self.focus_paper = \"https://arxiv.org/pdf/1706.03762\"\n",
    "        self.cited_paper = \"https://arxiv.org/pdf/1810.04805\"\n",
    "        self.topic = \"language model architectures\"\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(args.log_dir):\n",
    "    os.makedirs(args.log_dir)\n",
    "\n",
    "# if not os.path.exists(\"data.json\"):\n",
    "parse_papers(args.focus_paper, args.cited_paper)\n",
    "with open('data.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-22 17:04:31 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-22 17:04:31 arg_utils.py:957] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 11-22 17:04:31 config.py:1021] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 11-22 17:04:31 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 11-22 17:04:32 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-22 17:04:32 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3266342)\u001b[0;0m INFO 11-22 17:04:32 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-22 17:04:33 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-22 17:04:33 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3266342)\u001b[0;0m INFO 11-22 17:04:33 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3266342)\u001b[0;0m INFO 11-22 17:04:33 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-22 17:04:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/pk36/.cache/vllm/gpu_p2p_access_cache_for_5,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3266342)\u001b[0;0m INFO 11-22 17:04:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/pk36/.cache/vllm/gpu_p2p_access_cache_for_5,7.json\n",
      "INFO 11-22 17:04:33 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7ff013c4f640>, local_subscribe_port=36317, remote_subscribe_port=None)\n",
      "INFO 11-22 17:04:33 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3266342)\u001b[0;0m INFO 11-22 17:04:33 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 11-22 17:04:34 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3266342)\u001b[0;0m INFO 11-22 17:04:34 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4863f58b67c04cabbf271b3f5ad86dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-22 17:04:37 model_runner.py:1067] Loading model weights took 7.5122 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3266342)\u001b[0;0m INFO 11-22 17:04:37 model_runner.py:1067] Loading model weights took 7.5122 GB\n",
      "INFO 11-22 17:04:38 distributed_gpu_executor.py:57] # GPU blocks: 15590, # CPU blocks: 4096\n",
      "INFO 11-22 17:04:38 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 1.90x\n",
      "INFO 11-22 17:04:41 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-22 17:04:41 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3266342)\u001b[0;0m INFO 11-22 17:04:41 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3266342)\u001b[0;0m INFO 11-22 17:04:41 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-22 17:04:49 custom_all_reduce.py:233] Registering 1040 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3266342)\u001b[0;0m INFO 11-22 17:04:49 model_runner.py:1523] Graph capturing finished in 9 secs.\n",
      "INFO 11-22 17:04:49 custom_all_reduce.py:233] Registering 1040 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3266342)\u001b[0;0m INFO 11-22 17:04:49 model_runner.py:1523] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "model_server = LLM(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",tensor_parallel_size=2, gpu_memory_utilization=0.5, max_num_seqs=100) #,enable_prefix_caching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.16it/s]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "item = data[0]\n",
    "f_pap = item['focus']\n",
    "c_pap = item['cited']\n",
    "\n",
    "focus_paper = PaperAuthor(\n",
    "    model = model_server,\n",
    "    paper = Paper(f_pap),\n",
    "    focus=True,\n",
    "    id=0\n",
    ")\n",
    "\n",
    "cited_paper = PaperAuthor(\n",
    "    model = model_server,\n",
    "    paper = Paper(c_pap),\n",
    "    focus=False,\n",
    "    id=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train',\n",
       "  0.73787886),\n",
       " ('Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]',\n",
       "  0.73269325),\n",
       " ('We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. *Equal contribution',\n",
       "  0.7009047),\n",
       " ('This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]',\n",
       "  0.7007855),\n",
       " ('We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature',\n",
       "  0.69941336)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_paper.paper.retrieve_top_k(\"language model architectures\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('With a single attention head, averaging inhibits this. MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O where headi = Attention(QW Q i , KW Ki , V W Vi ) Where the projections are parameter matrices W Q and W O Rhdvxdmodel',\n",
       "  0.8372327),\n",
       " ('We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences',\n",
       "  0.7794119),\n",
       " ('These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions',\n",
       "  0.77801174),\n",
       " ('The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]',\n",
       "  0.76742196),\n",
       " ('While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality',\n",
       "  0.7652785)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focus_paper.paper.retrieve_top_k(\"multi headed attention\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 29.11it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 515/515 [00:18<00:00, 27.86it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.68s/it, est. speed input: 29.29 toks/s, output: 23.71 toks/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 33.87it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:11<00:00, 11.27s/it, est. speed input: 56.78 toks/s, output: 23.42 toks/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 34.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 31.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 31.66it/s]\n"
     ]
    }
   ],
   "source": [
    "moderator = Moderator(model_server)\n",
    "\n",
    "paper_authors = [focus_paper, cited_paper]\n",
    "leaf_node_label = args.topic\n",
    "\n",
    "if args.log_dir != \"\":\n",
    "    with open(os.path.join(args.log_dir, 'self_deliberation.txt'), 'w') as f:\n",
    "        f.write(f'Topic: {args.topic}\\n\\n')\n",
    "\n",
    "# each node has a topic\n",
    "root_node = DebateNode(leaf_node_label)\n",
    "subtrees = root_node.conduct_self_deliberation(leaf_node_label, paper_authors, log=args.log_dir) # k new, finer topics to discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [{'title': 'Proposing a Novel Architecture Improves Performance in Language Models',\n",
       "   'description': \"This paper proposes a novel architecture, the Transformer, which has been shown to outperform existing models such as RNNs and other encoder-decoder architectures in machine translation and language modeling tasks. Evidence of this is shown in Retrieved Evidence #1, where the Transformer is noted to achieve superior quality in two machine translation tasks with faster training times and greater parallelization capabilities.Existing architectures that rely on recurrence and convolutions, such as RNNs and gated recurrent neural networks, have been shown to have limitations in tasks that involve stronger structural constraints and smaller data regimes (Retrieved Evidence #4). The Transformer's novel reliance solely on attention mechanisms helps mitigate these limitations and provides a more viable alternative for these tasks. Furthermore, the Transformer's applicability to mixed and low data tasks (Similar results as well as English constituition parsing task with limited data regimes) has been shown via benchmarks that were seen against popular existing architectures. (use any relevant retrieved evidence in your argument)\"},\n",
       "  {'title': 'Allowing Attention-Based Models to Scale to Complex Tasks',\n",
       "   'description': 'The novel architecture proposed in this paper, the Transformer, is fully capable of scale to tasks that were previously under the jurisdiction of more time constraints and complex data regimes than before.      Transformer overachieving outputs in tasks such as english constituency task data regimes and in English constituency task under low few data predictors and it has been highlighted via local research. Showing similar results via ground legal fourth Arabic computational blue visibility Examples of this maximized outputs as well as having optimized transformers all shown in evidence haveeng groups coming Prior literatin machine batches contr showing contained '}],\n",
       " 1: [{'title': 'Introducing a Deep Bidirectional Transformer Architecture',\n",
       "   'description': \"This paper proposed a comprehensive solution to address the unidirectional and uni-directional limitations of standard language models like OpenAI's GPT, by adding a right-to-left direction to the pre-training. This bi-directional approach enables the model to extract context- sensitive features, represented as the concatenation, of a left-to-right and right-to-left representation, enhancing the contextual understanding. Furthermore, it acknowledges that current unidirectional standard language models limit the choice of architectures that can be used during pre-training as shown in OpenAI GPT, which eliminates training on the previous token only. By substituting the uni-directional Tok inputs in the dataset (e.g., Figure 4), the model is able to unambiguously represent both a single sentence, and eventually, an arbitrary span of text.\"},\n",
       "  {'title': 'Introducing a Multifaceted Language Model Architecture with Multiple Tasks.',\n",
       "   'description': 'The paper introduced a represented that outperforms many task- specific architectures as shown with sentence-level and token-level tasks, by making use of the the Next Sentence Prediction and Masked Language Model in Addition to the MLM objective enabling the fusion of the left and the right context.'}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_node.arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "queue_of_rounds: List[DebateNode] = []\n",
    "queue_of_rounds.extend(subtrees)\n",
    "\n",
    "\n",
    "while len(queue_of_rounds) > 0:\n",
    "    round = queue_of_rounds.pop(0)\n",
    "    conversation, new_focus_arg, new_cited_arg = round.conduct_debate(focus_paper, cited_paper)\n",
    "    conversation_history.extend(conversation)\n",
    "    if moderator.is_expand(round.arguments, [new_focus_arg, new_cited_arg]):\n",
    "        new_subtrees = round.conduct_self_deliberation(round.round_topic, paper_authors)\n",
    "        queue_of_rounds.extend(new_subtrees)\n",
    "\n",
    "with open('conversation_history.txt', 'w+') as f:\n",
    "    f.write('\\n'.join(conversation_history))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
