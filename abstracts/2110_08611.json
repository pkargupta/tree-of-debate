{
    "title": "Deep Active Learning by Leveraging Training Dynamics",
    "abstract": "Active learning theories and methods have been extensively studied in classical statistical learning settings. However, deep active learning, i.e., active learning with deep learning models, is usually based on empirical criteria without solid theoretical justification, thus suffering from heavy doubts when some of those fail to provide benefits in real applications. In this paper, by exploring the connection between the generalization performance and the training dynamics, we propose a theory-driven deep active learning method (dynamicAL) which selects samples to maximize training dynamics. In particular, we prove that the convergence speed of training and the generalization performance are positively correlated under the ultra-wide condition and show that maximizing the training dynamics leads to better generalization performance. Furthermore, to scale up to large deep neural networks and data sets, we introduce two relaxations for the subset selection problem and reduce the time complexity from polynomial to constant. Empirical results show that dynamicAL not only outperforms the other baselines consistently but also scales well on large deep learning models. We hope our work would inspire more attempts on bridging the theoretical findings of deep networks and practical impacts of deep active learning in real applications.",
    "introduction": "Training deep learning (DL) models usually requires large amount of high-quality labeled data [1] to optimize a model with a massive number of parameters. The acquisition of such annotated data is usually time-consuming and expensive, making it unaffordable in the fields that require high domain expertise. A promising approach for minimizing the labeling effort is active learning (AL), which aims to identify and label the maximally informative samples, so that a high-performing classifier can be trained with minimal labeling effort [2]. Under classical statistical learning settings, theories of active learning have been extensively studied from the perspective of VC dimension [3]. As a result, a variety of methods have been proposed, such as (i) the version-space-based approaches, which require maintaining a set of models [4, 5], and (ii) the clustering-based approaches, which assume that the data within the same cluster have pure labels [6]. However, the theoretical analyses for these classical settings may not hold for over-parameterized deep neural networks where the traditional wisdom is ineffective [1]. For example, margin-based methods select the labeling examples in the vicinity of the learned decision boundary [ 7, 8 ]. However, in the over-parameterized regime, every labeled example could potentially be near the learned decision boundary [ 9]. As a result, theoretically, such analysis can hardly guide us to design practical active arXiv:2110.08611v2 [cs.LG] 20 Nov 2022learning methods. Besides, empirically, multiple deep active learning works, borrowing observations and insights from the classical theories and methods, have been observed unable to outperform their passive learning counterparts in a few application scenarios [10, 11]. On the other hand, the analysis of neural network’s optimization and generalization performance has witnessed several exciting developments in recent years in terms of the deep learning theory [ 12– 14]. It is shown that the training dynamics of deep neural networks using gradient descent can be characterized by the Neural Tangent Kernel (NTK) of infinite [ 12] or finite [ 15] width networks. This is further leveraged to characterize the generalization of over-parameterized networks through Rademacher complexity analysis [ 13 , 16 ]. We are therefore inspired to ask: How can we design a practical and generic active learning method for deep neural networks with theoretical justifications? To answer this question, we firstly explore the connection between the model performance on testing data and the convergence speed on training data for the over-parameterized deep neural networks. Based on the NTK framework [12 , 13], we theoretically show that if a deep neural network converges faster (“Train Faster”), then it tends to have better generalization performance (“Generalize Better”), which matches the existing observations [17 –21 ]. Motivated by the aforementioned connection, we first introduce Training Dynamics, the derivative of training loss with respect to iteration, as a proxy to quantitatively describe the training process. On top of it, we formally propose our generic and theoretically-motivated deep active learning method, dynamicAL, which will query labels for a subset of unlabeled samples that maximally increase the training dynamics. In order to compute the training dynamics by merely using the unlabeled samples, we leverage two relaxations Pseudo-labeling and Subset Approximation to solve this non-trivial subset selection problem. Our relaxed approaches are capable of effectively estimating the training dynamics as well as efficiently solving the subset selection problem by reducing the complexity from O(N b) to O(b). In theory, we coin a new term Alignment to measure the length of the label vector’s projection on the neural tangent kernel space. Then, we demonstrate that higher alignment usually comes with a faster convergence speed and a lower generalization bound. Furthermore, with the help of the maximum mean discrepancy [ 22], we extend the previous analysis to an active learning setting where the i.i.d. assumption may not hold. Finally, we show that alignment is positively correlated with our active learning goal, training dynamics, which implies that maximizing training dynamics will lead to better generalization performance. Regarding experiments, we have empirically verified our theory by conducting extensive experiments on three datasets, CIFAR10 [ 23 ], SVHN [24], and Caltech101 [25 ] using three types of network structures: vanilla CNN, ResNet [ 26], and VGG [ 27 ]. We first show that the result of the subset selection problem delivered by the subset approximation is close to the global optimal solution. Furthermore, under the active learning setting, our method not only outperforms other baselines but also scales well on large deep learning models. The main contributions of our paper can be summarized as follows: • We propose a theory-driven deep active learning method, dynamicAL, inspired by the observation of “train faster, generalize better”. To this end, we introduce the Training Dynamics, as a proxy to describe the training process. • We demonstrate that the convergence speed of training and the generalization performance is strongly (positively) correlated under the ultra-wide condition; we also show that maximizing the training dynamics will lead to a lower generalization error in the scenario of active learning. • Our method is easy to implement. We conduct extensive experiments to evaluate the effectiveness of dynamicAL and empirically show tha",
    "arxiv_key": "2110_08611"
}