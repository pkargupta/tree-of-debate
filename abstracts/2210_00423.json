{
    "title": "Improved Algorithms for Neural Active Learning",
    "abstract": "We improve the theoretical and empirical performance of neural-network(NN)based active learning algorithms for the non-parametric streaming setting. In particular, we introduce two regret metrics by minimizing the population loss that are more suitable in active learning than the one used in state-of-the-art (SOTA) related work. Then, the proposed algorithm leverages the powerful representation of NNs for both exploitation and exploration, has the query decision-maker tailored for k-class classification problems with the performance guarantee, utilizes the full feedback, and updates parameters in a more practical and efficient manner. These careful designs lead to an instance-dependent regret upper bound, roughly improving by a multiplicative factor O(log T ) and removing the curse of input dimensionality. Furthermore, we show that the algorithm can achieve the same performance as the Bayes-optimal classifier in the long run under the hard-margin setting in classification problems. In the end, we use extensive experiments to evaluate the proposed algorithm and SOTA baselines, to show the improved empirical performance.",
    "introduction": "The Neural Network (NN) is one of the indispensable paradigms in machine learning and is widely used in multifarious supervised-learning tasks [ 23 ]. As more and more complicated NNs are developed, the requirement of the training procedure on the labeled data grows, incurring significant cost of label annotation. Active learning investigates effective techniques on a much smaller labeled data set while attaining the comparable generalization performance to passive learning [19 ]. In this paper, we focus on the classification problem in the streaming setting of active learning with NN models. At every round, the learner receives an instance and is compelled to decide on-the-fly whether or not to observe the label associated with this instance. This problem seeks to maximize the generalization capability of learned NNs in a sequence of rounds, such that the model has robust performance on the unseen data from the same distribution [40]. In active learning, given access to the i.i.d. generated instances from a distribution D, suppose there exist a class of functions F that formulate the mapping from instances to theirs labels. In the parametric setting, i.e., F has finite VC-dimension [ 25 ], existing works [24, 14 , 7] have shown that the active learning algorithms can achieve the convergence rate of  ̃O(1/√N ) to the best population loss in F, where N is the number of label queries. In the non-parametric setting, recent works [ 34 , 35] provide the similar convergence results while suffering from the curse of input dimensionality. Unfortunately, most of NN-based approaches to active learning do not come with the performance guarantee, despite having powerful empirical results. The first performance guarantee for neural active learning has been established in a recent work by [ 48 ], and the analysis is for over-parameterized neural networks with the assistance of Neural Tangent Kernel (NTK). We carefully investigate the limitations of [ 48], which turn into the main motivations of our paper. First, [48 ] transforms the classification problem into a multi-armed bandit problem [ 55 ], to minimize a pseudo regret metric. Yet, on the grounds that they seek to minimize the conditional population loss on a sequence of given data, it is dubious that the pseudo regret used in [ 48 ] can explicitly measure the generalization capability of given algorithms (see Remark 2.1). Second, the training process for NN models is not efficient, as [48] uses vanilla gradient descent and starts from randomly initialized parameters in every round. Third, although [48 ] removes the curse of input dimensionality d, the performance guarantee strongly suffers from another introduced term, the effective dimensionality  ̃d, which can be thought of as the non-linear dimensionalities of Hilbert space spanned by NTK. In the worse case, the magnitude of  ̃d can be an unacceptably large number and thus the performance guarantee collapses. 1.1 Main contributions In this paper, we propose a novel algorithm, I-NeurAL (Improved Algorithms for Neural Active Learning), to tackle the above limitations. Our contributions can be summarized as follows: (1) We consider the k-class classification problem, and we introduce two new regret metrics to minimize the population loss, which can directly reflect the generalization capability of NN-based algorithms. (2) I-NeurAL has a neural exploration strategy with a novel component to decide whether or not to query the label, coming with the performance guarantee. I-NeurAL exploits the full feedback in active learning which is a subtle but effective idea. (3) I-NeurAL is designed to support minibatch Stochastic Gradient Descent (SGD). In particular, at every round, I-NeurAL does mini-batch SGD starting with the parameters of the last round, i.e., with warm start, which is more efficient and practical compared to [48]. (4) Without any noise assumption on the data distribution, we provide an instance-dependent performance guarantee of I-NeurAL for over-parameterized neural networks. Compared to [ 48 ], we remove the curse of both the input dimensionality d and the effective dimensionality  ̃d; Moreover, we roughly improve the regret by a multiplicative factor log(T ), where T is the number of rounds. (5) under a hard-margin assumption on the data distribution, we provide that NN models can achieve the same generalization capability as Bayes-optimal classifier after O(log T ) number of label queries; (6) we conduct extensive experiments on real-world data sets to demonstrate the improved performance of I-NeurAL over state-of-the-art baselines including the closest work [48] which has not provided empirical validation of their proposed algorithms.",
    "arxiv_key": "2210_00423"
}