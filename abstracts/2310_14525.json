{
    "title": "Graph Ranking Contrastive Learning: A Extremely Simple yet Efficient Method",
    "abstract": "Graph contrastive learning (GCL) has emerged as a representative graph self-supervised method, achieving significant success. The currently prevalent optimization objective for GCL is InfoNCE. Typically, it employs augmentation techniques to obtain two views, where a node in one view acts as the anchor, the corresponding node in the other view serves as the positive sample, and all other nodes are regarded as negative samples. The goal is to minimize the distance between the anchor node and positive samples and maximize the distance to negative samples. However, due to the lack of label information during training, InfoNCE inevitably treats samples from the same class as negative samples, leading to the issue of false negative samples. This can impair the learned node representations and subsequently hinder performance in downstream tasks. While numerous methods have been proposed to mitigate the impact of false negatives, they still face various challenges. For instance, while increasing the number of negative samples can dilute the impact of false negatives, it concurrently increases computational burden. Thus, we propose GraphRank, a simple yet efficient graph contrastive learning method that addresses the problem of false negative samples by redefining the concept of negative samples to a certain extent, thereby avoiding the issue of false negative samples. The effectiveness of GraphRank is empirically validated through experiments on the node, edge, and graph level tasks.",
    "introduction": "Graph Neural Networks (GNNs) have become the standard approach for handling graph data, given their ability to leverage the underlying structure and features of graphs for effective analysis. Albeit the immense success achieved by supervised or semisupervised GNNs [ 16 , 31 ] across numerous application domains, their effectiveness is tied to the availability of labeled data for learning robust and impactful node representations. However, obtaining labeled data in real-world scenarios is a costly and timeconsuming endeavor, often constraining the availability of such data in many applications. Consequently, to mitigate this reliance on label data, graph self-supervised learning is attracting increasing attention, with graph contrastive learning emerging as the predominant method. Graph Contrastive Learning (GCL) typically starts with generating several views of a given graph through augmentation techniques. From these different views, one view serves as the anchor, with corresponding nodes in other views as positive examples and all other nodes as negative samples. The goal of GCL is then to bring the positive samples closer to the anchor in the representation space while pushing the negative samples further apart. Among various GCL approaches, InfoNCE [ 25 , 45, 46 ] has been recognized as the most commonly used optimization goal. It upholds the principle of minimizing distances between positive pairs and maximizing those between negative pairs, leveraging the contrastive nature of learning based on their representations. GRACE [ 45] relies on hybrid feature augmentations including node feature masking and edge dropping. Based on this data augmentation strategy, GCA [46 ] further introduces an adaptive augmentation for graph-structured data and make a competitive performance. GraphCL [ 39 ] further extends to graph-level representation to pull two views closer. However, InfoNCE experiences the issue of false negative samples [5, 23 ]. Its optimization objective is to minimize the distance with the positive samples and maximize the distance with the negative samples. Given that InfoNCE treats all nodes except the anchor node as negative samples, it unavoidably treats nodes of the same class as the anchor node as negative samples, which are referred to as false negative samples. Such false negative samples can impair the learned node representation and hinder downstream tasks. As shown in Figure 1, GRACE, a representative graph contrastive learning method using InfoNCE loss, is used to validate the issue of false negative samples across three academic citation network datasets. During the training of the GRACE method, we artificially removed the false negatives, which led to substantial performance improvements across all three datasets. However, in practical scenarios, there is a lack of data label information at the time of training, preventing manual removal of false negative samples based on label information. Therefore, in order to alleviate the issue of false negative samples and improve the performance of graph contrastive learning methods, many works have been explored, which are mainly in three ways. Firstly, increasing the number of negative samples helps dilute the impact of false negative samples. In cases where the quantities of various types of nodes are relatively balanced, the proportion of nodes in the same class is less, and increasing the number of negative samples can alleviate the issue of false negative samples. However, an increase in the number of negative samples bears computational and storage burdens, and as shown in Figure 1, GRACE, even when using all available negative samples, still experiences a notable false negative sample issue. Secondly, some works have proposed mechanisms for screening negative samples to attempt to remove false negative samples, thus improving the quality of negative samples. AUGCL [4] establishes a discriminative model based on collective affinity information to assess the uncertainty of negative samples, thereby facilitating the filtration of negative samples. Additionally, [ 23 ] designes a mechanism based on node similarity to sample high-quality positive and negative samples. Although a negative sample screening mechanism can effectively reduce the sampling of false negative samples, it necessitates the design of a complex and intricate screening mechanism to ensure the selection of high-quality negative samples. This, in turn, would introduce additional computational overhead. Lastly, some works have decided to forego the use of negative samples by employing contrastive learning methods that do not use negative samples, thereby avoiding the issue of false negative samples altogether. BGRL [ 30 ] is a contrastive learning method that does not require negative samples. It obtains two views through augmentation techniques; one view is used for learning the online representation, and the other view is used for learning the target representation. Updates are conducted by maximizing the similarity between these two views. However, its success relies on a relatively complex training strategy, specifically requiring a dual-encoder scheme with momentum update and exponential moving average to stabilize the training process. In light of the shortcomings of these graph contrastive methods above, we propose a new framework for graph self-supervised learning called GraphRank. The GraphRank framework involves generating two augmented graph views by applying random masks to nodes and edges. Subsequently,we utilize a GNN as the encoder and employ rank loss as the objective function for training. Specifically, we select a node ùë£ùëñ as the target node in view 1, the node ùë£+ ùëñ corresponding to it in view 2 as a positive sample, and then randomly pick a node ùë£ ùëó from view 2 as a negative sample. The representations of these nodes are derived by the encoder, and then the similarity between the target node and the positive and negative samples are calculated accordingly. By employing rank loss as the objective function, our aim is to ensure that the similarity between the target node and the positive samples is greater than the similarity between the target node and the negative samples. GraphRank can effectively address the problems mentioned above. Firstly, a simple random mask approach is applied to GraphRank to obtain augmented graph data, which does not require sophisticatedly designed graph augmentation techniques to obtain highquality augmented graph data, nor does it require a complicated training strategy to stabilize the training. Secondly, we use rank loss as the objective function. Similar to the contrastive loss, e.g. InfoNCE, rank loss also endeavors to maximize the agreement between the target node and the positive sample. Different from contrastive loss, the purpose of rank loss is to make the similarity between the target node and the positive samples greater than the similarity between the target node and the negative samples, rather than separating the target node from the negative samples as much as possible, as in InfoNCE. Therefore, the rank loss would not separate the negative samples as far apart as possible, even if the negative samples selected were false negative samples. Finally, the calculation of rank loss involves only one positive and one negative sample resulting in a smaller computational overload compared to contrastive losses like InfoNCE. As a result, rank loss exhibits better scalability, making it more feasible for large-scale applications compared to typical contrastive losses.",
    "arxiv_key": "2310_14525"
}