{
    "title": "Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling",
    "abstract": "Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to and corrects errors in the LLM, successfully blending noisy internet-scale information from LLMs with knowledge grounded in environment dynamics.",
    "introduction": "Despite evidence that practical sequential decision making systems require efficient exploitation of prior knowledge regarding a task, the current prevailing paradigm in reinforcement learning (RL) is to train tabula rasa, without any pretraining or external knowledge (Agarwal et al., 2022). In an effort to shift away from this paradigm, we focus on the task of creating embodied RL agents that can effectively exploit large-scale external knowledge sources presented in the form of pretrained large language models (LLMs). LLMs contain potentially useful knowledge for completing tasks and compiling knowledge sources (Petroni et al., 2019). Previous work has attempted to apply knowledge from LLMs to decision-making by generating action plans for executing in an embodied environment (Ichter et al., 2022; Huang et al., 2022b; Song et al., 2022b; Singh et al., 2022; Liang et al., 2022b; Huang et al., 2022a). However, LLMs still often fail when generating plans due to a lack of grounding (Valmeekam et al., 2022). Additionally, many of these agents that rely on LLM knowledge at execution time are limited in performance by the accuracy of LLM output. We hypothesize that if LLMs are instead applied to improving exploration during training, resulting policies will not be constrained by the accuracy of an LLM. Exploration in environments with sparse rewards becomes increasingly difficult as the size of the explorable state space increases. For example, the popular 3D embodied environment Minecraft has a large technology tree of craftable items with complex dependencies and a high branching factor. Before crafting a stone pickaxe in Minecraft an agent must: collect logs, craft logs into planks and then sticks, craft a crafting table from planks, use the crafting table to craft a wooden pickaxe from sticks and planks, use the wooden pickaxe to collect cobblestone, and finally use the crafting table to craft a stone pickaxe from sticks and cobblestone. Reaching a goal item is difficult without expert knowledge of Minecraft via dense rewards (Baker et al., 2022; Hafner et al., 2023) or expert demonstrations (Skrynnik et al., 2021; Patil et al., 2020), making item crafting in Minecraft a longstanding AI challenge (Guss et al., 2019; Fan et al., 2022). We propose DECKARD* (DECision-making for Knowledgable Autonomous Reinforcement-learning Dreamers), an agent that hypothesizes an Abstract World Model (AWM) over subgoals by few-shot prompting an LLM, then exploits the AWM for exploration and verifies the AWM with grounded experience. As seen in Figure 1, DECKARD operates in two phases: (1) the Dream phase where it uses the hypothesized AWM to suggest the next node to explore from the directed acyclic graph (DAG) of subgoals; and (2) the Wake phase where it learns a modular policy of subgoals, each trained on RL objectives, and verifies the hypothesized AWM with grounded environment dynamics. Figure 1 shows two iterations of the DECKARD agent learning the “craft a stone pickaxe” task in Minecraft. During the first Dream phase, the agent has already verified the nodes log and plank, and DECKARD suggests exploring towards the stick subgoal, ignoring nodes such as door that are not predicted to complete the task. Then, during the following Wake phase, DECKARD executes each subgoal in the branch ending in the stick node and then explores until it successfully crafts a stick. If successful, the agent marks the newly discovered node as verified in its AWM before proceeding to the next iteration. We evaluate DECKARD on learning to craft items in the Minecraft technology tree. We show that LLM-guidance is essential to exploration in DECKARD, with a version of our agent without LLM-guidance taking over twice as long to craft most items during open-ended exploration. Whereas, when exploring towards a specific task, DECKARD improves sample-efficiency by an order of magnitude versus comparable agents, (12x the ablated DECKARD without LLM-guidance). Our method is also robust to task decomposition errors in the LLM, consistently outperforming baselines as we introduce errors in the LLM output. DECKARD demonstrates the potential for robustly applying LLMs to RL, thus enabling RL agents to effectively use large-scale, noisy prior knowledge sources for exploration.",
    "arxiv_key": "2301_12050"
}