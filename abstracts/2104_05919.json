{
    "title": "Document-Level Event Argument Extraction by Conditional Generation",
    "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human information seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional  following event templates. We also compile a new document-level event extraction benchmark dataset WIKIEVENTS which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WIKIEVENTS datasets respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
    "introduction": "By converting a large amount of unstructured text into trigger-argument structures, event extraction models provide unique value in assisting us process volumes of documents to form insights. While real-world events are often described throughout a news document (or even span multiple documents), the scope of operation for existing event extraction models have long been limited to the sentence level. Early work on event extraction originally posed the task as document level role filling (Grishman and Sundheim, 1996) on a set of narrow scenarios and evaluated on small datasets. The release of ACE2, a large scale dataset with complete event annotation, opened the possibility of applying powerful machine learning models which led to substantial improvement in event extraction. The success of such models and the widespread adoption of ACE as the training dataset established sentencelevel event extraction as the mainstream task defintion. This formulation signifies a misalignment between the information seeking behavior in real life and the exhaustive annotation process in creating the datasets. An information seeking session (Mai, 2016) can be divided into 6 stages: task initiation, topic selection, pre-focus exploration, focus information, information collection and search closure (Kuhlthau, 1991). Given a target event ontology, we can safely assume that topic selection is complete and users start from skimming the documents before they discover events of interest, focus on such events and then aggregate all relevant information for the events. In both the “pre-focus exploration” and “information collection” stages, users naturally cross sentence boundaries. Empirically, using sentence boundaries as event scopes conveniently simplifies the problem, but also introduces fundamental flaws: the resulting extractions are incomplete and uninformative. We show two examples of this phenomenon in Figure 1. The first example exemplifies the case of implicit arguments across sentences. The sentence that contains the PaymentBarter argument \"$280.32\" is not the sentence that contains the trigger \"reserve\" for the ExchangeBuySell event. Without a documentlevel model, such arguments would be missed and result in incomplete extraction. In the second example, the arguments are present in the same sentence, but written as pronouns. Such extraction would be uninformative to the reader without cross-sentence coreference resolution. We propose a new end-to-end document-level event argument extraction model by framing the problem as conditional generation given a template. Conditioned on the unfilled template and a given context, the model is asked to generate a filledin template with arguments as shown in Figure 2. Our model does not require entity recognition nor coreference resolution as a preprocessing step and can work with long contexts beyond single sentences. Since templates are usually provided as part of the event ontology definition, this requires no additional human effort. Compared to recent efforts (Du and Cardie, 2020; Feng et al., 2020; Chen et al., 2020) that retarget question answering (QA) models for event extraction, our generationbased model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning thresholds and can extract all arguments in a single pass. In order to evaluate the performance of document-level event extraction, we collect and annotate a new benchmark dataset WIKIEVENTS. This document-level evaluation also allows us to move beyond the nearest mention of the argument and instead seek the most informative mention3 in the entire document context. In particular, only 34.5% of the arguments detected in the same sentence as the trigger can be considered informative. We present this new task of document-level informative argument extraction and show that while this task requires much more cross-sentence infer- 3We prefer name mentions over nominal mentions and only use pronoun mentions when no other mentions exist. ence, our model can still perform reliably well. Since we provide the ontology information (which roles are needed for the event) through the template as an external condition, our model has excellent portability to unseen event types. By pairing up our argument extraction model with a keyword-based zero-shot trigger extraction model, we enable zero-shot transfer for new event types. The major contributions of this paper can be summarized as follows: 1. We address the document-level argument extraction task with an end-to-end neural event argument extraction model by conditional text generation. Our model does not rely on entity extraction nor entity/event coreference resolution. Compared to QA-based approaches, it can easily handle missing arguments and multiple arguments in the same role. 2. We present the first document-level event extraction benchmark dataset with complete event and coreference annotation. We also introduce the new document-level informative argument extraction task, which evaluates the ability of models to learn entity-event relations over long ranges. 3. We release the first end-to-end zero-shot event extraction framework by combining our argument extraction model with a zero-shot event trigger classification model.",
    "arxiv_key": "2104_05919"
}