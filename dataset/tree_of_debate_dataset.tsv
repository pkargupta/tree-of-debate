Topic	Paper #1 arXiv Link	Paper #1 Title	Paper #1 Abstract	Paper #1 Introduction	Paper #2 arXiv Link	Paper #2 Title	Paper #2 Abstract	Paper #2 Introduction	Method (0) or Task (1)	No Cite (0) or Cite (1)
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2411.04425	DELIFT: Data Efficient Language Model Instruction Fine-Tuning	Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model’s responses to other samples, effectively measuring the informational value relative to the model’s current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.	Fine-tuning large language models (LLMs) is pivotal for adapting these powerful architectures (Devlin et al., 2019; Brown et al., 2020a; Touvron et al., 2023) to specialized tasks such as intricate reasoning, precise question-answering, and the seamless integration of new information (Ouyang et al., 2022). This transformation—from a general-purpose model to a task-specific agent—heavily relies on the quality and nature of the data employed during fine-tuning, which critically determines the model’s subsequent performance (Wei et al., 2022; Zhou et al., 2023; Hoffmann et al., 2024). The effectiveness of fine-tuning hinges on the quality, diversity, and relevance of the selected data (Gururangan et al., 2020; Wei et al., 2022; Zhou et al., 2023). High-quality data ensures accurate learning, diverse data enhances generalization, and relevant data aligns the model’s capabilities with specific application needs. However, optimizing data selection across different fine-tuning phases remains a significant challenge, leading to our central research question: How can we create a unified framework for efficient data selection across all fine-tuning stages of LLMs, while optimizing performance and maximizing data efficiency? To address this challenge, we present DELIFT (Data Efficient Language model Instruction Fine- Tuning), a novel, unified, and computationally efficient algorithm engineered to optimize data selection across all stages of the fine-tuning process. The key innovation of DELIFT lies in its pairwise utility metric, which assesses the informational value of data samples relative to both the model’s current capabilities and other samples within the dataset. This metric, combined with submodular optimization techniques, allows DELIFT to efficiently select optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. The typical fine-tuning process comprises three key stages: 1. Instruction Tuning: Enhances the model’s ability to follow general instructions (Mishra et al., 2022; Wei et al., 2022; Longpre et al., 2023); 2. Task-Specific Fine-Tuning: Refines the model’s expertise in specific domains (Gururangan et al., 2020; Cobbe et al., 2021); 3. Continual Fine-tuning: Enables the model to integrate new information while mitigating catastrophic forgetting (Madotto et al., 2021; Wu et al., 2024). DELIFT is able to optimize data selection processes across all three stages. Additionally, DELIFT offers significant benefits for In-Context Learning (ICL) (Brown et al., 2020b; Xue et al., 2024). By utilizing the selected subsets as the ICL example pool, DELIFT achieves similar or better performance compared to using the entire dataset, thereby enhancing data efficiency in ICL scenarios. This dual functionality is empirically validated in our experimental results. Existing data selection methodologies often fail to address the nuanced requirements of the aforementioned distinct fine-tuning stages. Many approaches are tailored to a single stage, lacking the adaptability needed for comprehensive fine-tuning (Xia et al., 2024; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024). Others depend on computationally intensive procedures, such as exhaustive gradient computations, rendering them impractical for large-scale models and datasets (Killamsetty et al., 2021b;a; Xia et al., 2024; Zhang et al., 2024). Additionally, some methods utilize features obtained from an independent model that are not specifically aligned with the model undergoing fine-tuning, reducing their effectiveness (Killamsetty et al., 2023; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024; Du et al., 2023). DELIFT addresses these limitations by adapting to the unique requirements of each fine-tuning stage. 1. Instruction Tuning: Selects diverse data to enhance general instruction-following capabilities; 2. Task-Specific Fine-Tuning: Prioritizes data that is aligned with the target task, to refine specialized expertise; 3. Continual Fine-tuning: Identifies novel, complementary information to expand the model’s knowledge base while safeguarding against catastrophic forgetting. Figure 1 illustrates how DELIFT optimizes data selection across these stages, demonstrating the selection and pruning processes in each fine-tuning phase. By leveraging submodular optimization techniques (Fujishige, 2005; Bilmes, 2022) and submodular information measures (Iyer et al., 2021), DELIFT efficiently selects optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. This approach effectively balances data utility and computational efficiency. Our key contributions are as follows: 1) Versatile Pairwise Utility Metric: A novel, easy-to-compute metric for assessing data informativeness, incorporating model feedback applicable across all fine-tuning stages. 2) Unified Data Selection Algorithm: DELIFT systematically optimizes data selection for instruction tuning, task-specific fine-tuning, and continual fine-tuning within a single framework. 3) Computational Efficiency: Circumvents resource-intensive operations, ensuring scalability to large datasets and models. DELIFT achieves at least 70% reduction in computational time compared to gradient-based methods on benchmark tasks. 4) Enhanced Performance with Reduced Data: Demonstrates the ability to reduce fine-tuning data size by up to 70% without compromising performance, and achieves comparable efficacy as to utilizing the full dataset. 5) Improvement over Existing Methods: Outperforms current data selection techniques by up to 26% in effectiveness across diverse tasks and model scales (see Section 4). The remainder of this paper is organized as follows: Section 2 provides background on fine-tuning LLMs and reviews related work. Section 3 details the methodology behind DELIFT, including the development of our pairwise utility metric and the submodular optimization process. Section 4 presents experimental results that showcase the effectiveness and efficiency of our method. Section 5 discusses the implications of our findings and potential future directions. Finally, we release our code base for further research.	https://arxiv.org/pdf/2402.04333	LESS: Selecting Influential data for Targeted Instruction Tuning	Instruction tuning has unlocked powerful capabilities in large language models (LLMs), using combined datasets to develop general-purpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application. To facilitate future work, we release code and data at princetonnlp/LESS.	Instruction tuning has made large language models (LLMs) adept at following human instructions (Ouyang et al., 2022) as versatile chatbots (OpenAI, 2022; 2023; Anthropic, 2023; Google, 2023). Recent efforts curating highly diverse and wide-ranging instruction tuning datasets (Taori et al., 2023; Wang et al.; Mukherjee et al., 2023; Xu et al., 2023, inter alia) induce remarkably strong generalization even from a small number of examples (Zhou et al., 2023). Regardless, it remains an open problem to understand how to best utilize these various datasets. Many real-world applications call for cultivating a specific suite of capabilities in LLMs (e.g., reasoning skills). However, training LLMs with mixed instruction tuning datasets can hinder the development of these specific capabilities. For example, Wang et al. (2023b) demonstrates that LLMs trained on a mix of instruction tuning datasets exhibit worse performance than those trained on a subset of the data. Additionally, considering the broad spectrum of user queries and the multitude of skills required to respond to them, there may not always be enough in-domain data available. Therefore, we hope to be able to effectively use the general instruction tuning data to improve specific capabilities. We frame this setting as targeted instruction tuning: Given just a handful of examples embodying a specific capability, how can we effectively select relevant fine-tuning data from a large collection of instruction datasets? We approach this problem by prioritizing training on data that directly minimizes loss on a target task instead of relying on surface form features (Gururangan et al., 2020; Xie et al., 2023b). Inspired by past works estimating the influence of individual training datapoints with gradient information (Pruthi et al., 2020; Han et al., 2023), we design an optimizer-aware approach to select such data. However, straightforward application of this influence formulation faces several challenges unique to the instruction tuning setting: (1) LLMs are traditionally fine-tuned with the Adam optimizer (Kingma & Ba, 2015) instead of the canonical SGD optimizer; (2) using sequence-level gradients of variable-length instruction data can derail the influence estimation; and (3) the large number of trainable parameters in LLMs makes the computation and storage of gradient information extremely resource-intensive. We address these concerns in LESS, an algorithm that performs Low-rank gradiEnt Similarity Search to select relevant instruction tuning data for a target application, which exhibits the following properties: 1. Compatible with instruction tuning with Adam (§2 and §3): LESS adapts the gradient features from classical influence formulations (Pruthi et al., 2020) to work with the Adam optimizer and variable-length instruction data. The optimization insights and influence formulation may be of independent interest as well. 2. Efficient (§4.1): LESS uses LoRA (Hu et al., 2021) and random projections (Johnson & Lindenstrauss, 1984) to construct a gradient datastore with lowdimensional, easily manipulable gradient features that permit efficient and effective dataset selection. The gradient datastore can be reused for new target tasks. 3. Transferable (§5.3): Data selected using small models’ gradient features induce strong performance in large models and models from different families, adding to the efficiency of LESS (Table 2). 4. Interpretable (§6.2): Qualitative analysis shows that LESS selects data with similar reasoning and skill types as the target task, whereas existing approaches often select data based on surface form cues (e.g., language or topic). We evaluate our approach on three diverse downstream datasets—MMLU (Hendrycks et al., 2020), TY- DIQA (Clark et al., 2020), and BBH (Suzgun et al., 2023)—each containing distinct subtasks that effectively simulate targeted instruction tuning scenarios. Results show that LESS often selects a small subset of the data (5%) that outperforms training on the full dataset, and the selected subset remains universally effective across model scales and families (Table 2). Comparisons with other data selection methods show that LESS is the only consistently effective approach, justifying its relatively high computational cost.	0	1
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2411.04425	DELIFT: Data Efficient Language Model Instruction Fine-Tuning	Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model’s responses to other samples, effectively measuring the informational value relative to the model’s current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.	Fine-tuning large language models (LLMs) is pivotal for adapting these powerful architectures (Devlin et al., 2019; Brown et al., 2020a; Touvron et al., 2023) to specialized tasks such as intricate reasoning, precise question-answering, and the seamless integration of new information (Ouyang et al., 2022). This transformation—from a general-purpose model to a task-specific agent—heavily relies on the quality and nature of the data employed during fine-tuning, which critically determines the model’s subsequent performance (Wei et al., 2022; Zhou et al., 2023; Hoffmann et al., 2024). The effectiveness of fine-tuning hinges on the quality, diversity, and relevance of the selected data (Gururangan et al., 2020; Wei et al., 2022; Zhou et al., 2023). High-quality data ensures accurate learning, diverse data enhances generalization, and relevant data aligns the model’s capabilities with specific application needs. However, optimizing data selection across different fine-tuning phases remains a significant challenge, leading to our central research question: How can we create a unified framework for efficient data selection across all fine-tuning stages of LLMs, while optimizing performance and maximizing data efficiency? To address this challenge, we present DELIFT (Data Efficient Language model Instruction Fine- Tuning), a novel, unified, and computationally efficient algorithm engineered to optimize data selection across all stages of the fine-tuning process. The key innovation of DELIFT lies in its pairwise utility metric, which assesses the informational value of data samples relative to both the model’s current capabilities and other samples within the dataset. This metric, combined with submodular optimization techniques, allows DELIFT to efficiently select optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. The typical fine-tuning process comprises three key stages: 1. Instruction Tuning: Enhances the model’s ability to follow general instructions (Mishra et al., 2022; Wei et al., 2022; Longpre et al., 2023); 2. Task-Specific Fine-Tuning: Refines the model’s expertise in specific domains (Gururangan et al., 2020; Cobbe et al., 2021); 3. Continual Fine-tuning: Enables the model to integrate new information while mitigating catastrophic forgetting (Madotto et al., 2021; Wu et al., 2024). DELIFT is able to optimize data selection processes across all three stages. Additionally, DELIFT offers significant benefits for In-Context Learning (ICL) (Brown et al., 2020b; Xue et al., 2024). By utilizing the selected subsets as the ICL example pool, DELIFT achieves similar or better performance compared to using the entire dataset, thereby enhancing data efficiency in ICL scenarios. This dual functionality is empirically validated in our experimental results. Existing data selection methodologies often fail to address the nuanced requirements of the aforementioned distinct fine-tuning stages. Many approaches are tailored to a single stage, lacking the adaptability needed for comprehensive fine-tuning (Xia et al., 2024; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024). Others depend on computationally intensive procedures, such as exhaustive gradient computations, rendering them impractical for large-scale models and datasets (Killamsetty et al., 2021b;a; Xia et al., 2024; Zhang et al., 2024). Additionally, some methods utilize features obtained from an independent model that are not specifically aligned with the model undergoing fine-tuning, reducing their effectiveness (Killamsetty et al., 2023; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024; Du et al., 2023). DELIFT addresses these limitations by adapting to the unique requirements of each fine-tuning stage. 1. Instruction Tuning: Selects diverse data to enhance general instruction-following capabilities; 2. Task-Specific Fine-Tuning: Prioritizes data that is aligned with the target task, to refine specialized expertise; 3. Continual Fine-tuning: Identifies novel, complementary information to expand the model’s knowledge base while safeguarding against catastrophic forgetting. Figure 1 illustrates how DELIFT optimizes data selection across these stages, demonstrating the selection and pruning processes in each fine-tuning phase. By leveraging submodular optimization techniques (Fujishige, 2005; Bilmes, 2022) and submodular information measures (Iyer et al., 2021), DELIFT efficiently selects optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. This approach effectively balances data utility and computational efficiency. Our key contributions are as follows: 1) Versatile Pairwise Utility Metric: A novel, easy-to-compute metric for assessing data informativeness, incorporating model feedback applicable across all fine-tuning stages. 2) Unified Data Selection Algorithm: DELIFT systematically optimizes data selection for instruction tuning, task-specific fine-tuning, and continual fine-tuning within a single framework. 3) Computational Efficiency: Circumvents resource-intensive operations, ensuring scalability to large datasets and models. DELIFT achieves at least 70% reduction in computational time compared to gradient-based methods on benchmark tasks. 4) Enhanced Performance with Reduced Data: Demonstrates the ability to reduce fine-tuning data size by up to 70% without compromising performance, and achieves comparable efficacy as to utilizing the full dataset. 5) Improvement over Existing Methods: Outperforms current data selection techniques by up to 26% in effectiveness across diverse tasks and model scales (see Section 4). The remainder of this paper is organized as follows: Section 2 provides background on fine-tuning LLMs and reviews related work. Section 3 details the methodology behind DELIFT, including the development of our pairwise utility metric and the submodular optimization process. Section 4 presents experimental results that showcase the effectiveness and efficiency of our method. Section 5 discusses the implications of our findings and potential future directions. Finally, we release our code base for further research.	https://arxiv.org/pdf/1906.01827	Coresets for Data-Efficient Training of Machine Learning Models	Incremental gradient (IG) methods, such as stochastic gradient descent and its variants are commonly used for large scale optimization in machine learning. Despite the sustained effort to make IG methods more data-efficient, it remains an open question how to select a training data subset that can theoretically and practically perform on par with the full dataset. Here we develop CRAIG, a method to select a weighted subset (or coreset) of training data that closely estimates the full gradient by maximizing a submodular function. We prove that applying IG to this subset is guaranteed to converge to the (near)optimal solution with the same convergence rate as that of IG for convex optimization. As a result, CRAIG achieves a speedup that is inversely proportional to the size of the subset. To our knowledge, this is the first rigorous method for data-efficient training of general machine learning models. Our extensive set of experiments show that CRAIG, while achieving practically the same solution, speeds up various IG methods by up to 6x for logistic regression and 3x for training deep neural networks.	Mathematical optimization lies at the core of training largescale machine learning systems, and is now widely used over massive data sets with great practical success, assuming sufficient data resources are available. Achieving this success, however, also requires large amounts of (often GPU) computing, as well as concomitant financial expenditures and energy usage (Strubell et al., 2019). Significantly decreasing these costs without decreasing the learnt system’s resulting accuracy is one of the grand challenges of machine learning and artificial intelligence today (Asi & Duchi, 2019). Training machine learning models often reduces to optimizing a regularized empirical risk function. Given a convex loss l, and a μ-strongly convex regularizer r, one aims to find model parameter vector w∗ over the parameter space W that minimizes the loss f over the training data V : w∗ ∈ arg minw∈W f (w), f (w) := ∑ i∈V fi(w) + r(w), fi(w) = l(w, (xi, yi)), (1) where V = {1, . . . , n} is an index set of the training data, and functions fi : Rd �' R are associated with training examples (xi, yi), where xi ∈ Rd is the feature vector, and yi is the point i’s label. Standard Gradient Descent can find the minimizer of this problem, but requires repeated computations of the full gradient ∇f (w)—sum of the gradients over all training data points/functions i—and is therefore prohibitive for massive data sets. This issue is further exacerbated in case of deep neural networks where gradient computations (backpropagation) are expensive. Incremental Gradient (IG) methods, such as Stochastic Gradient Descent (SGD) and its accelerated variants, including SGD with momentum (Qian, 1999), Adagrad (Duchi et al., 2011), Adam (Kingma & Ba, 2014), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013) iteratively estimate the gradient on random subsets/batches of training data. While this provides an unbiased estimate of the full gradient, the randomized batches introduce variance in the gradient estimate (Hofmann et al., 2015), and therefore stochastic gradient methods are in general slow to converge (Johnson & Zhang, 2013; Defazio et al., 2014). The majority of the work speeding up IG methods has thus primarily focused on reducing the variance of the gradient estimate (SAGA (Defazio et al., 2014), SVRG (Johnson & Zhang, 2013), Katysha (Allen-Zhu, 2017)) or more carefully selecting the gradient stepsize (Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), Adam (Kingma & Ba, 2014)). However, the direction that remains largely unexplored is how to carefully select a small subset S ⊆ V of the fullCoresets for Data-efficient Training of Machine Learning Models PAGE 2 training data V , so that the model is trained only on the subset S while still (approximately) converging to the globally optimal solution (i.e., the model parameters that would be obtained if training/optimizing on the full V ). If such a subset S can be quickly found, then this would directly lead to a speedup of |V |/|S| (which can be very large if |S| ≪ |V |) per epoch of IG. There are four main challenges in finding such a subset S. First is that a guiding principle for selecting S is unclear. For example, selecting training points close to the decision boundary might allow the model to fine tune the decision boundary, while picking the most diverse set of data points would allow the model to get a better sense of the training data distribution. Second is that finding S must be fast, as otherwise identifying the set S may take longer than the actual optimization, and so no overall speed-up would be achieved. Third is that finding a subset S is not enough. One also has to decide on a gradient stepsize for each data point in S, as they affect the convergence. And last, while the method might work well empirically on some data sets, one also requires theoretical understanding and mathematical convergence guarantees. Here we develop Coresets for Accelerating Incremental Gradient descent (CRAIG), for selecting a subset of training data points to speed up training of large machine learning models. Our key idea is to select a weighted subset S of training data V that best approximates the full gradient of V . We prove that the subset S that minimizes an upper-bound on the error of estimating the full gradient maximizes a submodular facility location function. Hence, S can be efficiently found using a fast greedy algorithm. We also provide theoretical analysis of CRAIG and prove its convergence. Most importantly, we show that any incremental gradient method (IG) on S converges in the same number epochs as the same IG would on the full V , which means that we obtain a speed-up inversely proportional to the size of S. In particular, for a μ-strongly convex risk function and a subset S selected by CRAIG that estimates the full gradient by an error of at most ǫ, we prove that IG on S with diminishing stepsize αk = α/kτ at epoch k (with 0 < τ < 1 and 0 < α), converges to an 2Rǫ/μ2 neighborhood of the optimal solution at rate O(1/√k). Here, R = min{d0, (rγmaxC + ǫ)/μ} where d0 is the initial distance to the optimum, C is an upperbound on the norm of the gradients, r = |S|, and γmax is the largest weight for the elements in the subset obtained by CRAIG. Moreover, we prove that if in addition to the strong convexity, component functions have smooth gradients, IG with the same diminishing step size on subset S converges to a 2ǫ/μ neighborhood of the optimum solution at rate O(1/kτ ). The above implies that IG on S converges to the same solution and in the same number of epochs as IG on the full V . But because every epoch only uses a subset S of the data, it requires fewer gradient computations and thus leads to a |V |/|S| speedup over traditional IG methods, while still (approximately) converging to the optimal solution. We also note that CRAIG is complementary to various incremental gradient (IG) methods (SGD, SAGA, SVRG, Adam), and such methods can be used on the subset S found by CRAIG. We also demonstrate the effectiveness of CRAIG via an extensive set of experiments using logistic regression (a convex optimization problem) as well as training deep neural networks (non-convex optimization problems). We show that CRAIG speeds up incremental gradient methods, including SGD, SAGA, and SVRG. In particular, CRAIG while achieving practically the same loss and accuracy as the underlying incremental gradient descent methods, speeds up gradient methods by up to 6x for convex and 3x for non-convex loss functions.	0	1
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2411.04425	DELIFT: Data Efficient Language Model Instruction Fine-Tuning	Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model’s responses to other samples, effectively measuring the informational value relative to the model’s current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.	Fine-tuning large language models (LLMs) is pivotal for adapting these powerful architectures (Devlin et al., 2019; Brown et al., 2020a; Touvron et al., 2023) to specialized tasks such as intricate reasoning, precise question-answering, and the seamless integration of new information (Ouyang et al., 2022). This transformation—from a general-purpose model to a task-specific agent—heavily relies on the quality and nature of the data employed during fine-tuning, which critically determines the model’s subsequent performance (Wei et al., 2022; Zhou et al., 2023; Hoffmann et al., 2024). The effectiveness of fine-tuning hinges on the quality, diversity, and relevance of the selected data (Gururangan et al., 2020; Wei et al., 2022; Zhou et al., 2023). High-quality data ensures accurate learning, diverse data enhances generalization, and relevant data aligns the model’s capabilities with specific application needs. However, optimizing data selection across different fine-tuning phases remains a significant challenge, leading to our central research question: How can we create a unified framework for efficient data selection across all fine-tuning stages of LLMs, while optimizing performance and maximizing data efficiency? To address this challenge, we present DELIFT (Data Efficient Language model Instruction Fine- Tuning), a novel, unified, and computationally efficient algorithm engineered to optimize data selection across all stages of the fine-tuning process. The key innovation of DELIFT lies in its pairwise utility metric, which assesses the informational value of data samples relative to both the model’s current capabilities and other samples within the dataset. This metric, combined with submodular optimization techniques, allows DELIFT to efficiently select optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. The typical fine-tuning process comprises three key stages: 1. Instruction Tuning: Enhances the model’s ability to follow general instructions (Mishra et al., 2022; Wei et al., 2022; Longpre et al., 2023); 2. Task-Specific Fine-Tuning: Refines the model’s expertise in specific domains (Gururangan et al., 2020; Cobbe et al., 2021); 3. Continual Fine-tuning: Enables the model to integrate new information while mitigating catastrophic forgetting (Madotto et al., 2021; Wu et al., 2024). DELIFT is able to optimize data selection processes across all three stages. Additionally, DELIFT offers significant benefits for In-Context Learning (ICL) (Brown et al., 2020b; Xue et al., 2024). By utilizing the selected subsets as the ICL example pool, DELIFT achieves similar or better performance compared to using the entire dataset, thereby enhancing data efficiency in ICL scenarios. This dual functionality is empirically validated in our experimental results. Existing data selection methodologies often fail to address the nuanced requirements of the aforementioned distinct fine-tuning stages. Many approaches are tailored to a single stage, lacking the adaptability needed for comprehensive fine-tuning (Xia et al., 2024; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024). Others depend on computationally intensive procedures, such as exhaustive gradient computations, rendering them impractical for large-scale models and datasets (Killamsetty et al., 2021b;a; Xia et al., 2024; Zhang et al., 2024). Additionally, some methods utilize features obtained from an independent model that are not specifically aligned with the model undergoing fine-tuning, reducing their effectiveness (Killamsetty et al., 2023; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024; Du et al., 2023). DELIFT addresses these limitations by adapting to the unique requirements of each fine-tuning stage. 1. Instruction Tuning: Selects diverse data to enhance general instruction-following capabilities; 2. Task-Specific Fine-Tuning: Prioritizes data that is aligned with the target task, to refine specialized expertise; 3. Continual Fine-tuning: Identifies novel, complementary information to expand the model’s knowledge base while safeguarding against catastrophic forgetting. Figure 1 illustrates how DELIFT optimizes data selection across these stages, demonstrating the selection and pruning processes in each fine-tuning phase. By leveraging submodular optimization techniques (Fujishige, 2005; Bilmes, 2022) and submodular information measures (Iyer et al., 2021), DELIFT efficiently selects optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. This approach effectively balances data utility and computational efficiency. Our key contributions are as follows: 1) Versatile Pairwise Utility Metric: A novel, easy-to-compute metric for assessing data informativeness, incorporating model feedback applicable across all fine-tuning stages. 2) Unified Data Selection Algorithm: DELIFT systematically optimizes data selection for instruction tuning, task-specific fine-tuning, and continual fine-tuning within a single framework. 3) Computational Efficiency: Circumvents resource-intensive operations, ensuring scalability to large datasets and models. DELIFT achieves at least 70% reduction in computational time compared to gradient-based methods on benchmark tasks. 4) Enhanced Performance with Reduced Data: Demonstrates the ability to reduce fine-tuning data size by up to 70% without compromising performance, and achieves comparable efficacy as to utilizing the full dataset. 5) Improvement over Existing Methods: Outperforms current data selection techniques by up to 26% in effectiveness across diverse tasks and model scales (see Section 4). The remainder of this paper is organized as follows: Section 2 provides background on fine-tuning LLMs and reviews related work. Section 3 details the methodology behind DELIFT, including the development of our pairwise utility metric and the submodular optimization process. Section 4 presents experimental results that showcase the effectiveness and efficiency of our method. Section 5 discusses the implications of our findings and potential future directions. Finally, we release our code base for further research.	https://arxiv.org/pdf/2310.16776	DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection	Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT-UCS, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to identify a smaller, representative dataset that reduces the data needed to finetune PLMs for the text-generation task of textediting. We examine the efficacy of DEFT-UCS across multiple text-editing tasks, and compare to the state-of-the art text-editing model, CoEDIT. Our results demonstrate that DEFTUCS models are just as accurate as CoEDIT, across eight different datasets consisting of six different editing tasks, while finetuned on 70% less data.	How much data do we need to fine-tune a pretrained language model (PLM) for a specific downstream task? While successes in language modelling have led to numerous publicly available PLMs and ability to produce fine-tuned models for downstream tasks - the answer mostly remains, "as large as possible, and of good quality". For example, Alpaca, an instruction-following model, is trained with 52k data samples (Taori et al., 2023). Similarly, CoPoet, a collaborative poetry writing system is fine-tuned using 87k data samples (Chakrabarty et al., 2022). MetaMath, a mathreasoning LLM is fine-tuned with 395k data samples (Yu et al., 2023). Although fine-tuning PLMs on specific task results in performance gain, acquiring large amounts of data for fine-tuning is not easy for real-world applications which often require niche knowledge and domain expertise. Researchers have explored variety of methods primarily focused on improving the computational efficiency of fine-tuning, including parameterefficient fine-tuning approaches (PEFT) to reduce computational costs by optimizing parameter updates (Fu et al., 2023; Hu et al., 2021) as well as leveraging active-learning for iteratively selecting data samples during training (Su et al., 2022; Diao et al., 2023). Instead, our work focuses on improving the data efficiency of PLM fine-tuning without requiring iterative fine-tuning. Specifically, we explore how to fine-tune PLMs with significantly less data samples and without a cost to model performance. Related to language models, researchers have experimented with different core-set selection metrics (Paul et al., 2021; Sorscher et al., 2022) to improve the data efficiency during pre-training. Marion et al. (2023) demonstrated how perplexity, L2-Error Norm (EL2N) and memorization can be utilized to select smaller, good quality datasets for pre-training. Similarly, (Attendu and Corbeil, 2023) leverage EL2N to dynamically remove data samples with high EL2N between training epochs. However, these metrics assume access to task data and reference models to perform dataset pruning. In real world applications, utilizing such supervised, data-pruning metrics are less realistic since large amounts of annotated task-specific data may be costly to acquire. This leads us to our main research question: How can we leverage unsupervised data pruning to fine-tune PLMs for downstream tasks in a more data efficient manner? In this work, we introduce a new data-efficient fine-tuning framework, DEFT-UCS, that uses unsupervised core-set selection to minimize the amount of labelled data needed to fine-tune PLMs for the text-generation task of text-editing. Our framework is inspired by (Sorscher et al., 2022), who utilize clustering-based dataset pruning to reduce training samples for image-classification models, and to the best of our knowledge, our framework is the first to leverage unsupervised core-set selection for data-efficient fine-tuning of PLMs. We investigate the utility of DEFT-UCS in finetuning PLMs for text-generation across eight different datasets consisting of six different text-editing tasks, and compare DEFT-UCS models to the stateof-the-art text-editing model, CoEDIT(Raheja et al., 2023). Our contributions are as follows: - We introduce DEFT-UCS, a data-efficient-fine tuning framework that leverages unsupervised core-set selection via clustering to identify a smaller representative set of data needed to fine-tune PLMs. - We show that DEFT-UCS, utilizing only 32.5% of CoEDIT’s training data, is able to produce fine-tuned models with improved accuracy on four different text-editing tasks, and similar accuracy on two text-editing tasks compared to CoEDIT (Raheja et al., 2023). - We performed a human evaluation with 3 evaluators to assess the quality of text-edits from our DEFT-UCS model. Evaluators found edits generated by DEFT-UCS model as similar or preferred over CoEDIT (Raheja et al., 2023)	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2411.04425	DELIFT: Data Efficient Language Model Instruction Fine-Tuning	Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model’s responses to other samples, effectively measuring the informational value relative to the model’s current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.	Fine-tuning large language models (LLMs) is pivotal for adapting these powerful architectures (Devlin et al., 2019; Brown et al., 2020a; Touvron et al., 2023) to specialized tasks such as intricate reasoning, precise question-answering, and the seamless integration of new information (Ouyang et al., 2022). This transformation—from a general-purpose model to a task-specific agent—heavily relies on the quality and nature of the data employed during fine-tuning, which critically determines the model’s subsequent performance (Wei et al., 2022; Zhou et al., 2023; Hoffmann et al., 2024). The effectiveness of fine-tuning hinges on the quality, diversity, and relevance of the selected data (Gururangan et al., 2020; Wei et al., 2022; Zhou et al., 2023). High-quality data ensures accurate learning, diverse data enhances generalization, and relevant data aligns the model’s capabilities with specific application needs. However, optimizing data selection across different fine-tuning phases remains a significant challenge, leading to our central research question: How can we create a unified framework for efficient data selection across all fine-tuning stages of LLMs, while optimizing performance and maximizing data efficiency? To address this challenge, we present DELIFT (Data Efficient Language model Instruction Fine- Tuning), a novel, unified, and computationally efficient algorithm engineered to optimize data selection across all stages of the fine-tuning process. The key innovation of DELIFT lies in its pairwise utility metric, which assesses the informational value of data samples relative to both the model’s current capabilities and other samples within the dataset. This metric, combined with submodular optimization techniques, allows DELIFT to efficiently select optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. The typical fine-tuning process comprises three key stages: 1. Instruction Tuning: Enhances the model’s ability to follow general instructions (Mishra et al., 2022; Wei et al., 2022; Longpre et al., 2023); 2. Task-Specific Fine-Tuning: Refines the model’s expertise in specific domains (Gururangan et al., 2020; Cobbe et al., 2021); 3. Continual Fine-tuning: Enables the model to integrate new information while mitigating catastrophic forgetting (Madotto et al., 2021; Wu et al., 2024). DELIFT is able to optimize data selection processes across all three stages. Additionally, DELIFT offers significant benefits for In-Context Learning (ICL) (Brown et al., 2020b; Xue et al., 2024). By utilizing the selected subsets as the ICL example pool, DELIFT achieves similar or better performance compared to using the entire dataset, thereby enhancing data efficiency in ICL scenarios. This dual functionality is empirically validated in our experimental results. Existing data selection methodologies often fail to address the nuanced requirements of the aforementioned distinct fine-tuning stages. Many approaches are tailored to a single stage, lacking the adaptability needed for comprehensive fine-tuning (Xia et al., 2024; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024). Others depend on computationally intensive procedures, such as exhaustive gradient computations, rendering them impractical for large-scale models and datasets (Killamsetty et al., 2021b;a; Xia et al., 2024; Zhang et al., 2024). Additionally, some methods utilize features obtained from an independent model that are not specifically aligned with the model undergoing fine-tuning, reducing their effectiveness (Killamsetty et al., 2023; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024; Du et al., 2023). DELIFT addresses these limitations by adapting to the unique requirements of each fine-tuning stage. 1. Instruction Tuning: Selects diverse data to enhance general instruction-following capabilities; 2. Task-Specific Fine-Tuning: Prioritizes data that is aligned with the target task, to refine specialized expertise; 3. Continual Fine-tuning: Identifies novel, complementary information to expand the model’s knowledge base while safeguarding against catastrophic forgetting. Figure 1 illustrates how DELIFT optimizes data selection across these stages, demonstrating the selection and pruning processes in each fine-tuning phase. By leveraging submodular optimization techniques (Fujishige, 2005; Bilmes, 2022) and submodular information measures (Iyer et al., 2021), DELIFT efficiently selects optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. This approach effectively balances data utility and computational efficiency. Our key contributions are as follows: 1) Versatile Pairwise Utility Metric: A novel, easy-to-compute metric for assessing data informativeness, incorporating model feedback applicable across all fine-tuning stages. 2) Unified Data Selection Algorithm: DELIFT systematically optimizes data selection for instruction tuning, task-specific fine-tuning, and continual fine-tuning within a single framework. 3) Computational Efficiency: Circumvents resource-intensive operations, ensuring scalability to large datasets and models. DELIFT achieves at least 70% reduction in computational time compared to gradient-based methods on benchmark tasks. 4) Enhanced Performance with Reduced Data: Demonstrates the ability to reduce fine-tuning data size by up to 70% without compromising performance, and achieves comparable efficacy as to utilizing the full dataset. 5) Improvement over Existing Methods: Outperforms current data selection techniques by up to 26% in effectiveness across diverse tasks and model scales (see Section 4). The remainder of this paper is organized as follows: Section 2 provides background on fine-tuning LLMs and reviews related work. Section 3 details the methodology behind DELIFT, including the development of our pairwise utility metric and the submodular optimization process. Section 4 presents experimental results that showcase the effectiveness and efficiency of our method. Section 5 discusses the implications of our findings and potential future directions. Finally, we release our code base for further research.	https://arxiv.org/pdf/2403.08370	SMART: Submodular Data Mixture Strategy for Instruction Tuning	Instruction Tuning involves finetuning a language model on a collection of instructionformatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) — a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal mixing. Furthermore, SMART facilitates the creation of data mixtures based on a few representative subsets of tasks alone and through task pruning analysis, we reveal that in a limited budget setting, allocating budget among a subset of representative tasks yields superior performance compared to distributing the budget among all tasks. The code for reproducing our results is open-sourced at https://github.com/kowndinyarenduchintala/SMART.	One of the main goals of artificial intelligence (AI) research is to build machines that can communicate (Turing, 1950), and an essential part of communication is to understand and follow instructions. Large Language Models (LLMs), which are pre-trained over massive text corpora on next-tokenprediction objective, can perform a wide range of NLP tasks via "prompting" (Brown et al., 2020; Kojima et al., 2022; Almazrouei et al., 2023; Liu et al., 2023; Touvron et al., 2023). Instruction Tuning (Wei et al., 2021; Sanh et al., 2021; Chung et al., 2022) is an approach that further enhances the instruction-following ability and generalizability of pre-trained LLMs to unseen tasks. It involves fine-tuning an LLM on a collection of instruction-formatted instances (encompassing multiple tasks) - each consisting of an instruction (or task description), an optional input, the corresponding output (the ground truth) and optionally a few demonstrations/examples. It is a special case of multitask learning where the LLM is finetuned on a collection of instruction-formatted multitask datasets (Chung et al., 2022). Finetuning on multiple tasks simultaneously, allows the model to share and transfer information across tasks, resulting in a better common internal representation that is preferred by all tasks while suppressing taskdependent noise (Caruana, 1997). Consequently, the model learns to generalize to unseen tasks by discerning helpful cues from both implicitly and explicitly related tasks that it has previously seen. The performance enhancement from instruction tuning is heavily contingent on data quality, data quantity, and task composition (Wang et al., 2023b). Studies by Iyer et al. (2022) and Longpre et al. (2023) have shown that while scaling the number of tasks is important, the relative proportion of various tasks (mixture weighting) merits as much attention for optimal instruction tuning. Intuitively, we want the model to see enough data for a given task that it can perform well on it, but not to see so much data that it memorizes the training set (Raffel et al., 2020). Iyer et al. (2022) performed manual tuning of various benchmark proportions and decided on a final mixture, whereas Longpre et al. (2023) studied the impact of removing each benchmark from the finetuning mixture and relied on their practioners’ intuition from there on, to decide on the exact proportions of benchmarks. In this work, we would like to explore a more systematic approach to mixture weighting. Specifically, we are motivated by the fact that in a large multitask dataset like FLAN 2022 (Longpre et al., 2023), which has 1840 tasks, there will likely be many similar tasks leading to redundancies and not all of them may require sampling in equal proportions. For instance, there might be many tasks of the type Natural Language Inference (NLI), and it might be enough to sample relatively more instances from a few representative NLI tasks and less from the others. Furthermore, which samples we select from each task is also crucial because the samples should faithfully represent the task at hand. A random subset may fail to do this as it can miss out on essential corner cases. With this context, we focus on the following two fundamental research questions (RQs) that form the basis for our subsequent inquiry: - (RQ1) Given a huge multitask instructiontuning dataset and a limited fine-tuning budget which is defined by the total number of (prompt, response) instances that can be used for fine-tuning, how do we divide this budget among thousands of tasks present in the dataset? i.e., how many instances to sample from each task? and which instances to sample from each task? - (RQ2) Can we go a step further and strategically prune some tasks altogether and only fine-tune on a small subset of representative tasks without hurting the performance? If yes, what is the nature of this subset? To the best of our knowledge, there’s currently no principled approach to determining task compositions for instruction tuning, other than manual tuning and/or practioners’ intuition. As a first step towards addressing both of the above RQs, we first define a common subset selection problem (more formally stated in Section 3) as follows - Given a huge collection of M instructionformatted task datasets, a task budget M ′ ≤ M and a total budget (N ′) of (prompt, response) pairs, which M ′ tasks to select? and how many instances to select from each of these M ′ tasks and which instances to select? Note that RQ1 is an instance of this problem where M ′ = M . Constrained Submodular Maximization (Section 2) proves to be a good model for discovering representative subsets (or coresets) of a massive training dataset (or ground set) that acts as surrogate (i.e., achieves similar performance) and are much better than uniformly-at-random subsets. Intuitively, this is because submodular functions model information in subsets, and hence maximizing a submodular function subject to a constraint yields non-redundant subsets of the ground set. An essential feature of this model is that it returns weighted subsets, i.e., each sample in the coreset comes with an associated score, which indicates how important the sample is. Inspired by submodular functions, we propose our solution (Section 3) to the above subset selection problem for instruction tuning that works in two stages. In the first stage, we select a weighted subset of tasks from the full dataset where the weights will determine how many samples to select from each task. In the next stage, we select samples from each task based on the assigned task budgets. Note that the submodular functions used in each stage are not necessarily identical (Section 4.8). The main contributions of our work can be summarized as follows: - We introduce SMART — a novel data mixture strategy for instruction tuning that models the data mixture problem (Section 3) as a sequence of two cardinality-constrained submodular maximization problems and offer empirical evidence that it outperforms both examples proportional and equal mixing baselines (Section 4) as well as the mixture weights proposed by Longpre et al. (2023). - Existing works like Longpre et al. (2023) have reported a continuous increase in performance upon increasing the number of tasks (though the gains themselves may be diminishing). However, we posit that this depends on the order in which new tasks are incorporated and show empirically that in the case of SMART mixtures, a performance peak is observed with an initial addition of few representative tasks and upon adding more and more tasks, the performance is not sustained (Section 4.6). - We find that the nature of instances that should be selected in each task (i.e, whether a representative or diverse subset) also depends on the total task budget, M ′ (Section 4.8). For higher M ′s, each task on average gets the relatively low budget and selecting representative samples is more important; however for lower M ′s, when there is sufficient enough budget for each task, the need for diversity dominates	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2411.04425	DELIFT: Data Efficient Language Model Instruction Fine-Tuning	Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model’s responses to other samples, effectively measuring the informational value relative to the model’s current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.	Fine-tuning large language models (LLMs) is pivotal for adapting these powerful architectures (Devlin et al., 2019; Brown et al., 2020a; Touvron et al., 2023) to specialized tasks such as intricate reasoning, precise question-answering, and the seamless integration of new information (Ouyang et al., 2022). This transformation—from a general-purpose model to a task-specific agent—heavily relies on the quality and nature of the data employed during fine-tuning, which critically determines the model’s subsequent performance (Wei et al., 2022; Zhou et al., 2023; Hoffmann et al., 2024). The effectiveness of fine-tuning hinges on the quality, diversity, and relevance of the selected data (Gururangan et al., 2020; Wei et al., 2022; Zhou et al., 2023). High-quality data ensures accurate learning, diverse data enhances generalization, and relevant data aligns the model’s capabilities with specific application needs. However, optimizing data selection across different fine-tuning phases remains a significant challenge, leading to our central research question: How can we create a unified framework for efficient data selection across all fine-tuning stages of LLMs, while optimizing performance and maximizing data efficiency? To address this challenge, we present DELIFT (Data Efficient Language model Instruction Fine- Tuning), a novel, unified, and computationally efficient algorithm engineered to optimize data selection across all stages of the fine-tuning process. The key innovation of DELIFT lies in its pairwise utility metric, which assesses the informational value of data samples relative to both the model’s current capabilities and other samples within the dataset. This metric, combined with submodular optimization techniques, allows DELIFT to efficiently select optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. The typical fine-tuning process comprises three key stages: 1. Instruction Tuning: Enhances the model’s ability to follow general instructions (Mishra et al., 2022; Wei et al., 2022; Longpre et al., 2023); 2. Task-Specific Fine-Tuning: Refines the model’s expertise in specific domains (Gururangan et al., 2020; Cobbe et al., 2021); 3. Continual Fine-tuning: Enables the model to integrate new information while mitigating catastrophic forgetting (Madotto et al., 2021; Wu et al., 2024). DELIFT is able to optimize data selection processes across all three stages. Additionally, DELIFT offers significant benefits for In-Context Learning (ICL) (Brown et al., 2020b; Xue et al., 2024). By utilizing the selected subsets as the ICL example pool, DELIFT achieves similar or better performance compared to using the entire dataset, thereby enhancing data efficiency in ICL scenarios. This dual functionality is empirically validated in our experimental results. Existing data selection methodologies often fail to address the nuanced requirements of the aforementioned distinct fine-tuning stages. Many approaches are tailored to a single stage, lacking the adaptability needed for comprehensive fine-tuning (Xia et al., 2024; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024). Others depend on computationally intensive procedures, such as exhaustive gradient computations, rendering them impractical for large-scale models and datasets (Killamsetty et al., 2021b;a; Xia et al., 2024; Zhang et al., 2024). Additionally, some methods utilize features obtained from an independent model that are not specifically aligned with the model undergoing fine-tuning, reducing their effectiveness (Killamsetty et al., 2023; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024; Du et al., 2023). DELIFT addresses these limitations by adapting to the unique requirements of each fine-tuning stage. 1. Instruction Tuning: Selects diverse data to enhance general instruction-following capabilities; 2. Task-Specific Fine-Tuning: Prioritizes data that is aligned with the target task, to refine specialized expertise; 3. Continual Fine-tuning: Identifies novel, complementary information to expand the model’s knowledge base while safeguarding against catastrophic forgetting. Figure 1 illustrates how DELIFT optimizes data selection across these stages, demonstrating the selection and pruning processes in each fine-tuning phase. By leveraging submodular optimization techniques (Fujishige, 2005; Bilmes, 2022) and submodular information measures (Iyer et al., 2021), DELIFT efficiently selects optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. This approach effectively balances data utility and computational efficiency. Our key contributions are as follows: 1) Versatile Pairwise Utility Metric: A novel, easy-to-compute metric for assessing data informativeness, incorporating model feedback applicable across all fine-tuning stages. 2) Unified Data Selection Algorithm: DELIFT systematically optimizes data selection for instruction tuning, task-specific fine-tuning, and continual fine-tuning within a single framework. 3) Computational Efficiency: Circumvents resource-intensive operations, ensuring scalability to large datasets and models. DELIFT achieves at least 70% reduction in computational time compared to gradient-based methods on benchmark tasks. 4) Enhanced Performance with Reduced Data: Demonstrates the ability to reduce fine-tuning data size by up to 70% without compromising performance, and achieves comparable efficacy as to utilizing the full dataset. 5) Improvement over Existing Methods: Outperforms current data selection techniques by up to 26% in effectiveness across diverse tasks and model scales (see Section 4). The remainder of this paper is organized as follows: Section 2 provides background on fine-tuning LLMs and reviews related work. Section 3 details the methodology behind DELIFT, including the development of our pairwise utility metric and the submodular optimization process. Section 4 presents experimental results that showcase the effectiveness and efficiency of our method. Section 5 discusses the implications of our findings and potential future directions. Finally, we release our code base for further research.	https://arxiv.org/pdf/2012.10630	GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning	Large scale machine learning and deep models are extremely data-hungry. Unfortunately, obtaining large amounts of labeled data is expensive, and training state-of-the-art models (with hyperparameter tuning) requires significant computing resources and time. Secondly, real-world data is noisy and imbalanced. As a result, several recent papers try to make the training process more efficient and robust. However, most existing work either focuses on robustness or efficiency, but not both. In this work, we introduce GLISTER, a GeneraLIzation based data Subset selecTion for Efficient and Robust learning framework. We formulate GLISTER as a mixed discretecontinuous bi-level optimization problem to select a subset of the training data, which maximizes the log-likelihood on a held-out validation set. We then analyze GLISTER for simple classifiers such as gaussian and multinomial naive-bayes, k-nearest neighbor classifier, and linear regression and show connections to submodularity. Next, we propose an iterative online algorithm GLISTER-ONLINE, which performs data selection iteratively along with the parameter updates and can be applied to any loss-based learning algorithm. We then show that for a rich class of loss functions including cross-entropy, hinge-loss, squared-loss, and logistic-loss, the inner discrete data selection is an instance of (weakly) submodular optimization, and we analyze conditions for which GLISTER-ONLINE reduces the validation loss and converges. Finally, we propose GLISTER-ACTIVE, an extension to batch active learning, and we empirically demonstrate the performance of GLISTER on a wide range of tasks including, (a) data selection to reduce training time, (b) robust learning under label noise and imbalance settings, and (c) batch-active learning with several deep and shallow models. We show that our framework improves upon state of the art both in efficiency and accuracy (in cases (a) and (c)) and is more efficient compared to other state-ofthe-art robust learning algorithms in case (b). The code for GLISTERis at:https://github.com/dssresearch/GLISTER.	With the quest to achieve human-like performance for machine learning and deep learning systems, the cost of training and deploying machine learning models has been significantly increasing. The wasted computational and engineering energy becomes evident in deep learning algorithms, wherein extensive hyper-parameter tuning and network architecture search need to be done. This results in staggering compute costs and running times. As a result, efficient and robust machine learning is a very relevant and substantial research problem. In this paper, we shall focus on three goals: Goal 1: Train machine learning and deep learning models on effective subsets of data, thereby significantly reducing training time and compute while not sacrificing accuracy. Goal 2: To (iteratively) select effective subsets of labeled data to reduce the labeling cost. Goal 3: Select data subsets to remove noisy labels and class imbalance, which is increasingly common in operational machine learning settings. Most prior work discussed above, either study robustness or efficiency, but not both. For example, the data selection approaches such as (Wei, Iyer, and Bilmes 2015; Mirzasoleiman, Bilmes, and Leskovec 2020; Shinohara 2014) and others focus on approximating either gradients or performance on the training sets, and hence would not be suitable for scenarios such as label noise and imbalance. On the other hand, the approaches like (Ren et al. 2018; Jiang et al. 2018) and others, focus on robustness but are not necessarily efficient. For example, the approach of (Ren et al. 2018) requires 3x the standard (deep) training cost, to obtain a robust model. GLISTER is the first framework, to the best of our knowledge, which focuses on both efficiency and robustness. Our work is closely related to the approaches of (Wei, Iyer, and Bilmes 2015) and (Ren et al. 2018). We build upon the work of (Wei, Iyer, and Bilmes 2015), by first generalizing their framework beyond simple classifiers (like nearest neighbor and naive bayes), but with general loss functions. We do this by proposing an iterative algorithm GLISTER-ONLINE which does data selection via a meta-learning based approach along with parameter updates. Furthermore, we pose the problem as optimizing the validation set performance as opposed to training set performance, thereby encouraging generalization. Next, our approach also bears similarity to (Ren et al. 2018), except that we need to solve a discrete optimization problem instead of a meta-gradient update. Moreover, we do not run our data selection every iteration, thereby ensuring that we are significantly faster than a single training run. Finally, we extend our algorithm to the active learning scenario. We demonstrate that our framework is more efficient and accurate compared to existing data selection and active learning algorithms, and secondly, also generalizes well under noisy data, and class imbalance scenarios. In particular, we show that GLISTER achieves a 3x - 6x speedup on a wide range of models and datasets, with very small loss in accuracy.	0	1
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2411.04425	DELIFT: Data Efficient Language Model Instruction Fine-Tuning	Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model’s responses to other samples, effectively measuring the informational value relative to the model’s current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.	Fine-tuning large language models (LLMs) is pivotal for adapting these powerful architectures (Devlin et al., 2019; Brown et al., 2020a; Touvron et al., 2023) to specialized tasks such as intricate reasoning, precise question-answering, and the seamless integration of new information (Ouyang et al., 2022). This transformation—from a general-purpose model to a task-specific agent—heavily relies on the quality and nature of the data employed during fine-tuning, which critically determines the model’s subsequent performance (Wei et al., 2022; Zhou et al., 2023; Hoffmann et al., 2024). The effectiveness of fine-tuning hinges on the quality, diversity, and relevance of the selected data (Gururangan et al., 2020; Wei et al., 2022; Zhou et al., 2023). High-quality data ensures accurate learning, diverse data enhances generalization, and relevant data aligns the model’s capabilities with specific application needs. However, optimizing data selection across different fine-tuning phases remains a significant challenge, leading to our central research question: How can we create a unified framework for efficient data selection across all fine-tuning stages of LLMs, while optimizing performance and maximizing data efficiency? To address this challenge, we present DELIFT (Data Efficient Language model Instruction Fine- Tuning), a novel, unified, and computationally efficient algorithm engineered to optimize data selection across all stages of the fine-tuning process. The key innovation of DELIFT lies in its pairwise utility metric, which assesses the informational value of data samples relative to both the model’s current capabilities and other samples within the dataset. This metric, combined with submodular optimization techniques, allows DELIFT to efficiently select optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. The typical fine-tuning process comprises three key stages: 1. Instruction Tuning: Enhances the model’s ability to follow general instructions (Mishra et al., 2022; Wei et al., 2022; Longpre et al., 2023); 2. Task-Specific Fine-Tuning: Refines the model’s expertise in specific domains (Gururangan et al., 2020; Cobbe et al., 2021); 3. Continual Fine-tuning: Enables the model to integrate new information while mitigating catastrophic forgetting (Madotto et al., 2021; Wu et al., 2024). DELIFT is able to optimize data selection processes across all three stages. Additionally, DELIFT offers significant benefits for In-Context Learning (ICL) (Brown et al., 2020b; Xue et al., 2024). By utilizing the selected subsets as the ICL example pool, DELIFT achieves similar or better performance compared to using the entire dataset, thereby enhancing data efficiency in ICL scenarios. This dual functionality is empirically validated in our experimental results. Existing data selection methodologies often fail to address the nuanced requirements of the aforementioned distinct fine-tuning stages. Many approaches are tailored to a single stage, lacking the adaptability needed for comprehensive fine-tuning (Xia et al., 2024; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024). Others depend on computationally intensive procedures, such as exhaustive gradient computations, rendering them impractical for large-scale models and datasets (Killamsetty et al., 2021b;a; Xia et al., 2024; Zhang et al., 2024). Additionally, some methods utilize features obtained from an independent model that are not specifically aligned with the model undergoing fine-tuning, reducing their effectiveness (Killamsetty et al., 2023; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024; Du et al., 2023). DELIFT addresses these limitations by adapting to the unique requirements of each fine-tuning stage. 1. Instruction Tuning: Selects diverse data to enhance general instruction-following capabilities; 2. Task-Specific Fine-Tuning: Prioritizes data that is aligned with the target task, to refine specialized expertise; 3. Continual Fine-tuning: Identifies novel, complementary information to expand the model’s knowledge base while safeguarding against catastrophic forgetting. Figure 1 illustrates how DELIFT optimizes data selection across these stages, demonstrating the selection and pruning processes in each fine-tuning phase. By leveraging submodular optimization techniques (Fujishige, 2005; Bilmes, 2022) and submodular information measures (Iyer et al., 2021), DELIFT efficiently selects optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. This approach effectively balances data utility and computational efficiency. Our key contributions are as follows: 1) Versatile Pairwise Utility Metric: A novel, easy-to-compute metric for assessing data informativeness, incorporating model feedback applicable across all fine-tuning stages. 2) Unified Data Selection Algorithm: DELIFT systematically optimizes data selection for instruction tuning, task-specific fine-tuning, and continual fine-tuning within a single framework. 3) Computational Efficiency: Circumvents resource-intensive operations, ensuring scalability to large datasets and models. DELIFT achieves at least 70% reduction in computational time compared to gradient-based methods on benchmark tasks. 4) Enhanced Performance with Reduced Data: Demonstrates the ability to reduce fine-tuning data size by up to 70% without compromising performance, and achieves comparable efficacy as to utilizing the full dataset. 5) Improvement over Existing Methods: Outperforms current data selection techniques by up to 26% in effectiveness across diverse tasks and model scales (see Section 4). The remainder of this paper is organized as follows: Section 2 provides background on fine-tuning LLMs and reviews related work. Section 3 details the methodology behind DELIFT, including the development of our pairwise utility metric and the submodular optimization process. Section 4 presents experimental results that showcase the effectiveness and efficiency of our method. Section 5 discusses the implications of our findings and potential future directions. Finally, we release our code base for further research.	https://arxiv.org/pdf/2410.11303	TSDS: Data Selection for Task-Specific Model Finetuning	Finetuning foundation models for specific tasks is an emerging paradigm in modern machine learning. The efficacy of task-specific finetuning largely depends on the selection of appropriate training data. We present TSDS (Task-Specific Data Selection), a framework to select data for task-specific model finetuning, guided by a small but representative set of examples from the target task. To do so, we formulate data selection for task-specific finetuning as an optimization problem with a distribution alignment loss based on optimal transport to capture the discrepancy between the selected data and the target distribution. In addition, we add a regularizer to encourage the diversity of the selected data and incorporate kernel density estimation into the regularizer to reduce the negative effects of near-duplicates among the candidate data. We connect our optimization problem to nearest neighbor search and design efficient algorithms to compute the optimal solution based on approximate nearest neighbor search techniques. We evaluate our method on data selection for both continued pretraining and instruction tuning of language models. We show that instruction tuning using data selected by our method with a 1% selection ratio often outperforms using the full dataset and beats the baseline selection methods by 1.5 points in F1 score on average. Our code is available at https://github.com/ZifanL/TSDS.	inetuning foundation models [3 ] is the de-facto paradigm for building machine learning applications that focus on specific tasks. Models such as BERT [10] and LLaMA [ 43] are large-scale models pretrained on massive unlabeled data across a wide range of domains. Those models can be specialized to downstream tasks through finetuning. Finetuning can take a variety of forms depending on the target task. For instance, continued pretraining [ 17 ] extends the pretraining stage of a model on a dataset that is more closely related to a target domain. As another setting, instruction tuning [51 ] trains a generative foundation model on instruction-response pairs to improve its performance in responding to task-specific instructions. Finetuning foundation models can lead to significant improvement in downstream tasks, but the effectiveness heavily relies on the right choice of training data [ 17 , 30 , 48 , 47 ]. However, the data repositories that one considers during training of generative models tend to be large—consider for example the use of Common Crawl, which contains 250 billion web pages, or The Pile [14 ]—and hence, it is impractical to manually select the data that are distributed like the use cases in the target task. Therefore, automated task-specific data selection becomes critical. In this paper, we propose TSDS (Task-Specific Data Selection), a framework to select data for taskspecific model finetuning. We consider the scenario of finetuning a foundation model to customize it for a specific task characterized by a few representative examples. The input to our framework is the representative examples and a massive repository of candidate data. Guided by the representative examples, we select training data from the repository for task-specific finetuning. We identify the following requirements for our framework. (Distribution Alignment) First, the distribution of the selected data should match the distribution of the representative examples from the target task. Distribution alignment is essential for a model to learn the target distribution and enable data-efficient finetuning for the target task [ 40 ]. Many works [38 , 17, 2, 50 , 47 ] retrieve candidate examples that are most similar to the representative examples. Such heuristics do not ensure distribution alignment between the selected data and the representative examples. A recent work [ 48 ] selects data by importance resampling to match the target distribution but is limited to an n-gram feature space, which cannot capture high-level semantics. (Diversity) Second, the selected data should be diverse so that the model can learn a wide range of related knowledge rather than overfitting to specific examples. In practice, data repositories created by web crawling may contain a large portion of near-duplicates [13 , 28 ] that can compromise diversity and negatively impact model performance [ 28, 19 ]. For example, a study [ 13 ] on several snapshots of ClueWeb and Common Crawl shows that 14% to 52% of the documents are near-duplicates. Previous works [38, 17 , 2 , 50 , 48 , 47 ] on task-specific data selection overlook near-duplicates, leading to the over-representation of such examples in the selected data. We require our framework to ensure diversity in selection even when a large fraction of the candidate examples are near-duplicates. (Scalability) Finally, the selection algorithm should be efficient, considering the increasing scale of modern data repositories. The high volume of candidate data (e.g., 250 billion pages in Common Crawl) poses a great challenge to efficient selection. Our framework formulates task-specific data selection as an optimization problem that allows a smooth trade-off between two crucial objectives: distribution alignment and diversity. The solution to the optimization problem is a categorical distribution assigned to the candidates which we will sample from. In the optimization objective, we use optimal transport to measure the discrepancy between the distribution assigned to the candidates and the target distribution, encouraging the alignment between them. We show that the optimization problem admits efficient algorithms to compute the optimal solution. In addition, our framework supports distribution alignment in any metric space that supports efficient nearest-neighbor search, including model-agnostic semantic embedding and model-specific features such as gradients. Our contributions: 1) We formulate data selection for task-specific finetuning as an optimization problem based on optimal transport for distribution alignment, with a regularization term that encourages diversity. 2) We make our framework robust to near-duplicates by incorporating kernel density estimation [36 ] into the regularization term. 3) We show the connection between the optimal solution to the optimization problem and nearest neighbor search, which allows us to develop efficient algorithms employing approximate nearest-neighbor search techniques [23]. We conduct extensive experiments to validate the effectiveness of our framework. We focus on natural language processing tasks where foundation models have shown great advancements. We show that our framework beats the state-of-the-art baseline [47 ] by 1.5 points in F1 score on average with a selection ratio of 1% on instruction tuning for two modern large language models on three tasks. In addition, continued pretraining using domain-specific data selected by our framework outperforms the other selection methods by up to 3 F1 points on four classification tasks from various domains. We also demonstrate that our framework is robust to near-duplicates in the data repository, maintaining consistent performance when 1% of the candidate examples are duplicate for up to 1,000 times. Our method is efficient, taking 28 hours to preprocess a corpus of 150M examples and less than 1 hour for each task-specific selection.	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2411.04425	DELIFT: Data Efficient Language Model Instruction Fine-Tuning	Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model’s responses to other samples, effectively measuring the informational value relative to the model’s current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.	Fine-tuning large language models (LLMs) is pivotal for adapting these powerful architectures (Devlin et al., 2019; Brown et al., 2020a; Touvron et al., 2023) to specialized tasks such as intricate reasoning, precise question-answering, and the seamless integration of new information (Ouyang et al., 2022). This transformation—from a general-purpose model to a task-specific agent—heavily relies on the quality and nature of the data employed during fine-tuning, which critically determines the model’s subsequent performance (Wei et al., 2022; Zhou et al., 2023; Hoffmann et al., 2024). The effectiveness of fine-tuning hinges on the quality, diversity, and relevance of the selected data (Gururangan et al., 2020; Wei et al., 2022; Zhou et al., 2023). High-quality data ensures accurate learning, diverse data enhances generalization, and relevant data aligns the model’s capabilities with specific application needs. However, optimizing data selection across different fine-tuning phases remains a significant challenge, leading to our central research question: How can we create a unified framework for efficient data selection across all fine-tuning stages of LLMs, while optimizing performance and maximizing data efficiency? To address this challenge, we present DELIFT (Data Efficient Language model Instruction Fine- Tuning), a novel, unified, and computationally efficient algorithm engineered to optimize data selection across all stages of the fine-tuning process. The key innovation of DELIFT lies in its pairwise utility metric, which assesses the informational value of data samples relative to both the model’s current capabilities and other samples within the dataset. This metric, combined with submodular optimization techniques, allows DELIFT to efficiently select optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. The typical fine-tuning process comprises three key stages: 1. Instruction Tuning: Enhances the model’s ability to follow general instructions (Mishra et al., 2022; Wei et al., 2022; Longpre et al., 2023); 2. Task-Specific Fine-Tuning: Refines the model’s expertise in specific domains (Gururangan et al., 2020; Cobbe et al., 2021); 3. Continual Fine-tuning: Enables the model to integrate new information while mitigating catastrophic forgetting (Madotto et al., 2021; Wu et al., 2024). DELIFT is able to optimize data selection processes across all three stages. Additionally, DELIFT offers significant benefits for In-Context Learning (ICL) (Brown et al., 2020b; Xue et al., 2024). By utilizing the selected subsets as the ICL example pool, DELIFT achieves similar or better performance compared to using the entire dataset, thereby enhancing data efficiency in ICL scenarios. This dual functionality is empirically validated in our experimental results. Existing data selection methodologies often fail to address the nuanced requirements of the aforementioned distinct fine-tuning stages. Many approaches are tailored to a single stage, lacking the adaptability needed for comprehensive fine-tuning (Xia et al., 2024; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024). Others depend on computationally intensive procedures, such as exhaustive gradient computations, rendering them impractical for large-scale models and datasets (Killamsetty et al., 2021b;a; Xia et al., 2024; Zhang et al., 2024). Additionally, some methods utilize features obtained from an independent model that are not specifically aligned with the model undergoing fine-tuning, reducing their effectiveness (Killamsetty et al., 2023; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024; Du et al., 2023). DELIFT addresses these limitations by adapting to the unique requirements of each fine-tuning stage. 1. Instruction Tuning: Selects diverse data to enhance general instruction-following capabilities; 2. Task-Specific Fine-Tuning: Prioritizes data that is aligned with the target task, to refine specialized expertise; 3. Continual Fine-tuning: Identifies novel, complementary information to expand the model’s knowledge base while safeguarding against catastrophic forgetting. Figure 1 illustrates how DELIFT optimizes data selection across these stages, demonstrating the selection and pruning processes in each fine-tuning phase. By leveraging submodular optimization techniques (Fujishige, 2005; Bilmes, 2022) and submodular information measures (Iyer et al., 2021), DELIFT efficiently selects optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. This approach effectively balances data utility and computational efficiency. Our key contributions are as follows: 1) Versatile Pairwise Utility Metric: A novel, easy-to-compute metric for assessing data informativeness, incorporating model feedback applicable across all fine-tuning stages. 2) Unified Data Selection Algorithm: DELIFT systematically optimizes data selection for instruction tuning, task-specific fine-tuning, and continual fine-tuning within a single framework. 3) Computational Efficiency: Circumvents resource-intensive operations, ensuring scalability to large datasets and models. DELIFT achieves at least 70% reduction in computational time compared to gradient-based methods on benchmark tasks. 4) Enhanced Performance with Reduced Data: Demonstrates the ability to reduce fine-tuning data size by up to 70% without compromising performance, and achieves comparable efficacy as to utilizing the full dataset. 5) Improvement over Existing Methods: Outperforms current data selection techniques by up to 26% in effectiveness across diverse tasks and model scales (see Section 4). The remainder of this paper is organized as follows: Section 2 provides background on fine-tuning LLMs and reviews related work. Section 3 details the methodology behind DELIFT, including the development of our pairwise utility metric and the submodular optimization process. Section 4 presents experimental results that showcase the effectiveness and efficiency of our method. Section 5 discusses the implications of our findings and potential future directions. Finally, we release our code base for further research.	https://arxiv.org/pdf/2501.04155	MM-GEN: Enhancing Task Performance Through Targeted Multimodal Data Curation	Vision-language models (VLMs) are highly effective but often underperform on specialized tasks, for example Llava1.5 struggles on chart and diagram understanding, due to scarce task-specific training data. Existing training data, sourced from general-purpose datasets, fails to capture the nuanced details needed for these tasks. We introduce MMGEN, a scalable method that generates task-specific, highquality synthetic text for candidate images by leveraging stronger models. MM-GEN employs a three-stage targeted process: partitioning data into subgroups, generating targeted text based on task descriptions, and filtering out redundant and outlier data. Fine-tuning VLMs with data generated by MM-GEN leads to significant performance gains, including 29% on spatial reasoning and 15% on diagram understanding for Llava-1.5 (7B). Compared to human-curated caption data, MM-GEN achieves up to 1.6× better improvements for the original models, proving its effectiveness in enhancing task-specific VLM performance and bridging the gap between general-purpose datasets and specialized requirements. Code available at https://github.com/sjoshi804/MM-Gen.	While vision-language models (VLMs) demonstrate stateof-the-art performance on several multi-modal tasks [29], they are often on tasks that are simpler in nature [3]. These models still struggle with more complex tasks, e.g., those that require fine-grained understanding of details in images [3, 15, 21, 49]. We posit that the primary limitation for these VLMs is the quality and nature of the training data. VLMs are typically trained on large-scale image-text data scraped from the internet; while the images are rich and informative, the associated text descriptions can (i) have limited relevance to the image [39], or (ii) omit references to several specific details captured in the image [25]. Fig. 1 shows examples of such images and web-scraped captions. While the images are relevant for the tasks of chart understanding, spatial reasoning, and diagram understanding, respectively, the text fails to capture details essential for these tasks. While synthetic caption generation strategies proposed in prior work [25, 39, 57] can create more descriptive text annotations (by referring to more visual details), they are agnostic of the downstream target task. Consequently, they cannot ensure that relevant details are captured in the text annotations. Recently, Shi et al. [48] manually curated a task-specific dataset aimed at the task of multimodal mathematical question-answering by augmenting existing imagetext data with detailed textual annotations, based on their domain expertise, using strong VLMs. While effective, such a curation pipeline involves significant human effort and is not scalable [33, 59]. To address these limitations, we present MM-GEN, a highly general framework for automatically synthesizing taskrelevant text annotations for images by leveraging stronger VLMs (i.e., VLMs that perform well on the specific task) and requiring minimal human effort. MM-GEN takes as input a small set of examples from the target task (henceforth referred to as "reference samples"), a list of image types associated with the task, and a pool of task-relevant candidate images for training. In practice, these inputs can be easily obtained: a small number of reference samples and associated image types can be directly collected from the target task, and a task-relevant image pool can be found via image search with search engines or retrieved from largescale image-caption datasets [5, 46, 47]. Using the reference samples to specify the details of the task to the stronger VLM, MM-GEN generates text-annotations that are taskrelevant for the candidate images. Fig. 1 shows how the text generated by MM-GEN captures task-relevant details. This simple approach is highly effective, resulting in significant improvements across a variety of target tasks. Moreover, human-effort in this process is limited to providing (i) a small set of reference samples for the task, and (ii) a pool of candidate images. To further improve the quality and efficacy of the generated data, MM-GEN introduces a perplexity [4] based data-filtering approach to improve performance on target task using a high-value subset of the synthesized data. The components in MM-GEN are general and applicable to any image-text based target task enabling it to easily generalize across tasks and scale. We evaluate MM-GEN on improving VLMs' (e.g., Llava1.5 7B and 13B parameter versions) performance on finegrained image understanding tasks — chart understanding and reasoning, diagram understanding, and spatial reasoning on maps. The data curated by MM-GEN enables an absolute improvement over Llava-1.5 (7B) of 15%, 14% and 29%, respectively. We also see improvements over the much larger Llava-1.5 (13B) of 13%, , respectively. Moreover, MM-GEN’s filtering strategy helps in shrinking data volumes by up to 50% with no / minimal loss in performance. Empirical results show that models trained via MM-GEN data have a better performance than those trained via generated generic captions. MM-GEN data is also more effective than text annotations generated without task-specific reference examples, showing the importance of a targeted, data-centric approach for describing tasks. Finally, we analyze the effects of key design choices in MMGEN through ablation studies on e.g. size of the reference sample set, generating with / without partitioning, scaling number of in-context samples.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2411.04425	DELIFT: Data Efficient Language Model Instruction Fine-Tuning	Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model’s responses to other samples, effectively measuring the informational value relative to the model’s current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.	Fine-tuning large language models (LLMs) is pivotal for adapting these powerful architectures (Devlin et al., 2019; Brown et al., 2020a; Touvron et al., 2023) to specialized tasks such as intricate reasoning, precise question-answering, and the seamless integration of new information (Ouyang et al., 2022). This transformation—from a general-purpose model to a task-specific agent—heavily relies on the quality and nature of the data employed during fine-tuning, which critically determines the model’s subsequent performance (Wei et al., 2022; Zhou et al., 2023; Hoffmann et al., 2024). The effectiveness of fine-tuning hinges on the quality, diversity, and relevance of the selected data (Gururangan et al., 2020; Wei et al., 2022; Zhou et al., 2023). High-quality data ensures accurate learning, diverse data enhances generalization, and relevant data aligns the model’s capabilities with specific application needs. However, optimizing data selection across different fine-tuning phases remains a significant challenge, leading to our central research question: How can we create a unified framework for efficient data selection across all fine-tuning stages of LLMs, while optimizing performance and maximizing data efficiency? To address this challenge, we present DELIFT (Data Efficient Language model Instruction Fine- Tuning), a novel, unified, and computationally efficient algorithm engineered to optimize data selection across all stages of the fine-tuning process. The key innovation of DELIFT lies in its pairwise utility metric, which assesses the informational value of data samples relative to both the model’s current capabilities and other samples within the dataset. This metric, combined with submodular optimization techniques, allows DELIFT to efficiently select optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. The typical fine-tuning process comprises three key stages: 1. Instruction Tuning: Enhances the model’s ability to follow general instructions (Mishra et al., 2022; Wei et al., 2022; Longpre et al., 2023); 2. Task-Specific Fine-Tuning: Refines the model’s expertise in specific domains (Gururangan et al., 2020; Cobbe et al., 2021); 3. Continual Fine-tuning: Enables the model to integrate new information while mitigating catastrophic forgetting (Madotto et al., 2021; Wu et al., 2024). DELIFT is able to optimize data selection processes across all three stages. Additionally, DELIFT offers significant benefits for In-Context Learning (ICL) (Brown et al., 2020b; Xue et al., 2024). By utilizing the selected subsets as the ICL example pool, DELIFT achieves similar or better performance compared to using the entire dataset, thereby enhancing data efficiency in ICL scenarios. This dual functionality is empirically validated in our experimental results. Existing data selection methodologies often fail to address the nuanced requirements of the aforementioned distinct fine-tuning stages. Many approaches are tailored to a single stage, lacking the adaptability needed for comprehensive fine-tuning (Xia et al., 2024; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024). Others depend on computationally intensive procedures, such as exhaustive gradient computations, rendering them impractical for large-scale models and datasets (Killamsetty et al., 2021b;a; Xia et al., 2024; Zhang et al., 2024). Additionally, some methods utilize features obtained from an independent model that are not specifically aligned with the model undergoing fine-tuning, reducing their effectiveness (Killamsetty et al., 2023; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024; Du et al., 2023). DELIFT addresses these limitations by adapting to the unique requirements of each fine-tuning stage. 1. Instruction Tuning: Selects diverse data to enhance general instruction-following capabilities; 2. Task-Specific Fine-Tuning: Prioritizes data that is aligned with the target task, to refine specialized expertise; 3. Continual Fine-tuning: Identifies novel, complementary information to expand the model’s knowledge base while safeguarding against catastrophic forgetting. Figure 1 illustrates how DELIFT optimizes data selection across these stages, demonstrating the selection and pruning processes in each fine-tuning phase. By leveraging submodular optimization techniques (Fujishige, 2005; Bilmes, 2022) and submodular information measures (Iyer et al., 2021), DELIFT efficiently selects optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. This approach effectively balances data utility and computational efficiency. Our key contributions are as follows: 1) Versatile Pairwise Utility Metric: A novel, easy-to-compute metric for assessing data informativeness, incorporating model feedback applicable across all fine-tuning stages. 2) Unified Data Selection Algorithm: DELIFT systematically optimizes data selection for instruction tuning, task-specific fine-tuning, and continual fine-tuning within a single framework. 3) Computational Efficiency: Circumvents resource-intensive operations, ensuring scalability to large datasets and models. DELIFT achieves at least 70% reduction in computational time compared to gradient-based methods on benchmark tasks. 4) Enhanced Performance with Reduced Data: Demonstrates the ability to reduce fine-tuning data size by up to 70% without compromising performance, and achieves comparable efficacy as to utilizing the full dataset. 5) Improvement over Existing Methods: Outperforms current data selection techniques by up to 26% in effectiveness across diverse tasks and model scales (see Section 4). The remainder of this paper is organized as follows: Section 2 provides background on fine-tuning LLMs and reviews related work. Section 3 details the methodology behind DELIFT, including the development of our pairwise utility metric and the submodular optimization process. Section 4 presents experimental results that showcase the effectiveness and efficiency of our method. Section 5 discusses the implications of our findings and potential future directions. Finally, we release our code base for further research.	https://arxiv.org/pdf/1708.00489	Active Learning for Convolutional Neural Networks: A Core-Set Approach	Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (i.e. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.	Deep convolutional neural networks (CNNs) have shown unprecedented success in many areas of research in computer vision and pattern recognition, such as image classification, object detection, and scene segmentation. Although CNNs are universally successful in many tasks, they have a major drawback; they need a very large amount of labeled data to be able to learn their large number of parameters. More importantly, it is almost always better to have more data since the accuracy of CNNs is often not saturated with increasing dataset size. Hence, there is a constant desire to collect more and more data. Although this a desired behavior from an algorithmic perspective (higher representative power is typically better), labeling a dataset is a time consuming and an expensive task. These practical considerations raise a critical question: "what is the optimal way to choose data points to label such that the highest accuracy can be obtained given a fixed labeling budget." Active learning is one of the common paradigms to address this question. The goal of active learning is to find effective ways to choose data points to label, from a pool of unlabeled data points, in order to maximize the accuracy. Although it is not possible to obtain a universally good active learning strategy (Dasgupta, 2004), there exist many heuristics (Settles, 2010) which have been proven to be effective in practice. Active learning is typically an iterative process in which a model is learned at each iteration and a set of points is chosen to be labelled from a pool of unlabelled points using these aforementioned heuristics. We experiment with many of these heuristics in this paper and find them not effective when applied to CNNs. We argue that the main factor behind this ineffectiveness is the correlation caused via batch acquisition/sampling. In the classical setting, the active learning algorithms typically choose a single point at each iteration; however, this is not feasible for CNNs since i) a single point is likely to have no statistically significant impact on the accuracy due to the local optimization methods, and ii) each iteration requires a full training until convergence which makes it intractable to query labels one-by-one. Hence, it is necessary to query labels for a large subset at each iteration and it results in correlated samples even for moderately small subset sizes. In order to tailor an active learning method for the batch sampling case, we decided to define the active learning as core-set selection problem. Core-set selection problem aims to find a small subset given a large labeled dataset such that a model learned over the small subset is competitive over the whole dataset. Since we have no labels available, we perform the core-set selection without using the labels. In order to attack the unlabeled core-set problem for CNNs, we provide a rigorous bound between an average loss over any given subset of the dataset and the remaining data points via the geometry of the data points. As an active learning algorithm, we try to choose a subset such that this bound is minimized. Moreover, minimization of this bound turns out to be equivalent to the k-Center problem (Wolf, 2011) and we adopt an efficient approximate solution to this combinatorial optimization problem. We further study the behavior of our proposed algorithm empirically for the problem of image classification using three different datasets. Our empirical analysis demonstrates state-of-the-art performance by a large margin.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2411.04425	DELIFT: Data Efficient Language Model Instruction Fine-Tuning	Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model’s responses to other samples, effectively measuring the informational value relative to the model’s current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.	Fine-tuning large language models (LLMs) is pivotal for adapting these powerful architectures (Devlin et al., 2019; Brown et al., 2020a; Touvron et al., 2023) to specialized tasks such as intricate reasoning, precise question-answering, and the seamless integration of new information (Ouyang et al., 2022). This transformation—from a general-purpose model to a task-specific agent—heavily relies on the quality and nature of the data employed during fine-tuning, which critically determines the model’s subsequent performance (Wei et al., 2022; Zhou et al., 2023; Hoffmann et al., 2024). The effectiveness of fine-tuning hinges on the quality, diversity, and relevance of the selected data (Gururangan et al., 2020; Wei et al., 2022; Zhou et al., 2023). High-quality data ensures accurate learning, diverse data enhances generalization, and relevant data aligns the model’s capabilities with specific application needs. However, optimizing data selection across different fine-tuning phases remains a significant challenge, leading to our central research question: How can we create a unified framework for efficient data selection across all fine-tuning stages of LLMs, while optimizing performance and maximizing data efficiency? To address this challenge, we present DELIFT (Data Efficient Language model Instruction Fine- Tuning), a novel, unified, and computationally efficient algorithm engineered to optimize data selection across all stages of the fine-tuning process. The key innovation of DELIFT lies in its pairwise utility metric, which assesses the informational value of data samples relative to both the model’s current capabilities and other samples within the dataset. This metric, combined with submodular optimization techniques, allows DELIFT to efficiently select optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. The typical fine-tuning process comprises three key stages: 1. Instruction Tuning: Enhances the model’s ability to follow general instructions (Mishra et al., 2022; Wei et al., 2022; Longpre et al., 2023); 2. Task-Specific Fine-Tuning: Refines the model’s expertise in specific domains (Gururangan et al., 2020; Cobbe et al., 2021); 3. Continual Fine-tuning: Enables the model to integrate new information while mitigating catastrophic forgetting (Madotto et al., 2021; Wu et al., 2024). DELIFT is able to optimize data selection processes across all three stages. Additionally, DELIFT offers significant benefits for In-Context Learning (ICL) (Brown et al., 2020b; Xue et al., 2024). By utilizing the selected subsets as the ICL example pool, DELIFT achieves similar or better performance compared to using the entire dataset, thereby enhancing data efficiency in ICL scenarios. This dual functionality is empirically validated in our experimental results. Existing data selection methodologies often fail to address the nuanced requirements of the aforementioned distinct fine-tuning stages. Many approaches are tailored to a single stage, lacking the adaptability needed for comprehensive fine-tuning (Xia et al., 2024; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024). Others depend on computationally intensive procedures, such as exhaustive gradient computations, rendering them impractical for large-scale models and datasets (Killamsetty et al., 2021b;a; Xia et al., 2024; Zhang et al., 2024). Additionally, some methods utilize features obtained from an independent model that are not specifically aligned with the model undergoing fine-tuning, reducing their effectiveness (Killamsetty et al., 2023; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024; Du et al., 2023). DELIFT addresses these limitations by adapting to the unique requirements of each fine-tuning stage. 1. Instruction Tuning: Selects diverse data to enhance general instruction-following capabilities; 2. Task-Specific Fine-Tuning: Prioritizes data that is aligned with the target task, to refine specialized expertise; 3. Continual Fine-tuning: Identifies novel, complementary information to expand the model’s knowledge base while safeguarding against catastrophic forgetting. Figure 1 illustrates how DELIFT optimizes data selection across these stages, demonstrating the selection and pruning processes in each fine-tuning phase. By leveraging submodular optimization techniques (Fujishige, 2005; Bilmes, 2022) and submodular information measures (Iyer et al., 2021), DELIFT efficiently selects optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. This approach effectively balances data utility and computational efficiency. Our key contributions are as follows: 1) Versatile Pairwise Utility Metric: A novel, easy-to-compute metric for assessing data informativeness, incorporating model feedback applicable across all fine-tuning stages. 2) Unified Data Selection Algorithm: DELIFT systematically optimizes data selection for instruction tuning, task-specific fine-tuning, and continual fine-tuning within a single framework. 3) Computational Efficiency: Circumvents resource-intensive operations, ensuring scalability to large datasets and models. DELIFT achieves at least 70% reduction in computational time compared to gradient-based methods on benchmark tasks. 4) Enhanced Performance with Reduced Data: Demonstrates the ability to reduce fine-tuning data size by up to 70% without compromising performance, and achieves comparable efficacy as to utilizing the full dataset. 5) Improvement over Existing Methods: Outperforms current data selection techniques by up to 26% in effectiveness across diverse tasks and model scales (see Section 4). The remainder of this paper is organized as follows: Section 2 provides background on fine-tuning LLMs and reviews related work. Section 3 details the methodology behind DELIFT, including the development of our pairwise utility metric and the submodular optimization process. Section 4 presents experimental results that showcase the effectiveness and efficiency of our method. Section 5 discusses the implications of our findings and potential future directions. Finally, we release our code base for further research.	https://arxiv.org/pdf/2301.13287	MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning	Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results indicate that MILO can train models 3 x -10x faster and tune hyperparameters 20 x -75x faster than full-dataset training or tuning without compromising performance.	Deep learning has achieved remarkable success in a multitude of machine learning tasks, including natural language processing, computer vision, and speech recognition in recent years. This success is partially due to the availability of massive training datasets and the capacity to train large-scale neural networks. However, training deep models on extensive datasets is computationally demanding, incurring significant financial costs and generating substantial CO2 emissions [63, 59 ]. Bhavya et al. [ 2] overview several research trajectories aimed at enhancing model convergence and reducing training time and costs, including data subset selection, curriculum learning, model architecture improvements, and optimization algorithm enhancements. Our work specifically focuses on selecting useful, generalizable data subsets for efficient deep neural network training. Recent research [66 , 5 , 21 ] suggests that many current training datasets are redundant, indicating that a non-redundant, informative subset could achieve comparable performance to training on the full dataset. To identify such informative samples, metrics such as prediction uncertainty [ 7], prediction flips [ 66 , 72 ], loss [42 ], or gradient/gradient-norm [ 21, 47 , 28, 27 , 29 ] have been applied. These metrics, calculated using either the downstream machine learning model or a lightweight surrogate [ 7], require a fully converged model, a requirement that runs counter to the goal of efficient training. To address this, existing subset selection algorithms [ 47 , 28 , 27, 29, 30 , 54 ] for efficient learning utilize the downstream model during training for heuristic computation and periodically update the subset as the model-dependent metrics for each sample evolve. Drawbacks of Model-Dependent Subset Selection: Despite the theoretical advantages of existing subset selection strategies[ 28 , 27, 47 ], they often fall short in computational efficiency compared to adaptive random subset selection, which selects random subsets periodically. This is primarily because traditional approaches rely on downstream models and typically require the calculation of sample metrics, such as gradients, before each subset selection step. Furthermore, these computationally demanding subset selection steps occur during model training. For instance, Figure 1 compares the convergence rate of the ResNet18 model on the CIFAR100 dataset, in terms of both time and epochs. Here, the model uses 10% subsets chosen every epoch by GradMatchPB [ 27 ], a state-of-the-art data subset selection strategy for efficient training, CraigPB [ 47 ], and Adaptive-Random (where a new 10% subset is randomly selected periodically). The selection of a new subset every epoch is done to demonstrate the maximum performance attainable by GradMatchPB and CraigPB. The results indicate that GradMatchPB achieves faster epoch convergence than both Adaptive-Random and CraigPB when selecting a new subset each epoch. However, due to the necessity for a computationally intensive subset selection step every epoch, both GradMatchPB and CraigPB demonstrate considerable inefficiency in terms of training time. In their respective studies, Killamsetty et al. [27 ] and Mirzasoleiman et al. [47] suggested selecting a new subset every R epochs to enhance training efficiency. However, this comes at the cost of the model’s convergence rate. Lastly, model-dependent subset selection requires a computationally intensive subset selection steps each time a new model is trained. Model-Agnostic Selection: Our primary insight is that a model-agnostic subset selection framework can circumvent the computationally intensive subset selection steps during model training by conducting subset selection in the preprocessing phase. By pre-selecting subsets and storing them as metadata with each dataset, we can train numerous models without incurring additional costs, effectively spreading out the expense of subset selection. In this work, we endeavor to answer the following question: Is it possible to develop a model-agnostic subset selection method that selects new subsets in a minimal amount of time, yet achieves superior model convergence without significant compromise in test accuracy or generalization performance?  MILO Framework: In this study, we introduce MILO, a model-agnostic subset selection framework designed for efficient model training and tuning. MILO employs submodular measures [ 13 , 23 ], which capture higher-order interactions between data samples for subset selection. We utilize pre-trained large language models [ 55 ] and pre-trained vision transformers [25 ] as feature encoders due to their zero-shot feature encoding capabilities. These encoders compute the sample metrics in a nominal amount of time, and this computation is independent of the downstream model, rendering MILO model-agnostic. Figure 3 provides a visual representation of MILO for model training, comprising two steps: a) A pre-processing step that involves the selection of multiple subsets from the training dataset using "Stochastic-Greedy Exploration (SGE)" (refer to Section 3.1.1), and the construction of a probability distribution over the entire dataset for subset sampling through "Weighted Random Exploration (WRE)" (refer to Section 3.1.2); b) A model training scheme that involves training the model on a curriculum of easy-to-hard subsets (refer to Section 3.1.3) using the subsets selected and the probability distribution constructed in the pre-processing step. We also present a class-wise partitioning trick to significantly minimize the memory footprint of MILO (refer to Section 3.2). Effectiveness of MILO : Through extensive experiments on multiple real-world datasets, we empirically demonstrate the effectiveness of the MILO framework for efficient training and hyperparameter tuning. In Figure 2, we provide a summary of the speedup achieved by MILO compared to full data training and tuning, along with the corresponding relative performance. Our results show that MILO can train models 3x to 10x faster and tune hyperparameters 20x to 75x faster, with minimal loss in performance. Furthermore, we consistently outperform the subset selection baselines that were considered. Additionally, MILO exhibits significantly faster initial model convergence and maintains faster convergence throughout training compared to other baselines. These qualities make it highly effective for applications such as hyperparameter tuning and neural architecture search, where the ability to distinguish good models from bad models early in the tuning process is crucial for enhancing efficiency [38, 37]. Related Work: In recent years, data-subset selection strategies have achieved success in various machine learning applications such as speech recognition [ 68 , 67], machine translation [ 32 ], active learning [ 60 , 1 , 33 ], hyper-parameter tuning [ 30 ], continual learning [65 ], domain adaptation [20 ], and computer vision [22]. A recent empirical study [5] demonstrated that datasets frequently contain semantic redundancies. These redundancies can be removed during training without affecting performance. In response, data pruning methods [66 , 51, 62] have developed principled selection criteria for pruning redundant samples before model training, with minimal loss in performance. However, both existing data pruning and compute-efficient learning approaches are model-dependent, resulting in them being computationally expensive. In contrast, our method is model-agnostic and capable of identifying high-quality subsets by intelligent data exploration. Furthermore, our approach can achieve superior model convergence by utilizing an easy-to-hard curriculum, transitioning from representative to diverse subsets during training.	0	1
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2411.04425	DELIFT: Data Efficient Language Model Instruction Fine-Tuning	Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model’s responses to other samples, effectively measuring the informational value relative to the model’s current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.	Fine-tuning large language models (LLMs) is pivotal for adapting these powerful architectures (Devlin et al., 2019; Brown et al., 2020a; Touvron et al., 2023) to specialized tasks such as intricate reasoning, precise question-answering, and the seamless integration of new information (Ouyang et al., 2022). This transformation—from a general-purpose model to a task-specific agent—heavily relies on the quality and nature of the data employed during fine-tuning, which critically determines the model’s subsequent performance (Wei et al., 2022; Zhou et al., 2023; Hoffmann et al., 2024). The effectiveness of fine-tuning hinges on the quality, diversity, and relevance of the selected data (Gururangan et al., 2020; Wei et al., 2022; Zhou et al., 2023). High-quality data ensures accurate learning, diverse data enhances generalization, and relevant data aligns the model’s capabilities with specific application needs. However, optimizing data selection across different fine-tuning phases remains a significant challenge, leading to our central research question: How can we create a unified framework for efficient data selection across all fine-tuning stages of LLMs, while optimizing performance and maximizing data efficiency? To address this challenge, we present DELIFT (Data Efficient Language model Instruction Fine- Tuning), a novel, unified, and computationally efficient algorithm engineered to optimize data selection across all stages of the fine-tuning process. The key innovation of DELIFT lies in its pairwise utility metric, which assesses the informational value of data samples relative to both the model’s current capabilities and other samples within the dataset. This metric, combined with submodular optimization techniques, allows DELIFT to efficiently select optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. The typical fine-tuning process comprises three key stages: 1. Instruction Tuning: Enhances the model’s ability to follow general instructions (Mishra et al., 2022; Wei et al., 2022; Longpre et al., 2023); 2. Task-Specific Fine-Tuning: Refines the model’s expertise in specific domains (Gururangan et al., 2020; Cobbe et al., 2021); 3. Continual Fine-tuning: Enables the model to integrate new information while mitigating catastrophic forgetting (Madotto et al., 2021; Wu et al., 2024). DELIFT is able to optimize data selection processes across all three stages. Additionally, DELIFT offers significant benefits for In-Context Learning (ICL) (Brown et al., 2020b; Xue et al., 2024). By utilizing the selected subsets as the ICL example pool, DELIFT achieves similar or better performance compared to using the entire dataset, thereby enhancing data efficiency in ICL scenarios. This dual functionality is empirically validated in our experimental results. Existing data selection methodologies often fail to address the nuanced requirements of the aforementioned distinct fine-tuning stages. Many approaches are tailored to a single stage, lacking the adaptability needed for comprehensive fine-tuning (Xia et al., 2024; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024). Others depend on computationally intensive procedures, such as exhaustive gradient computations, rendering them impractical for large-scale models and datasets (Killamsetty et al., 2021b;a; Xia et al., 2024; Zhang et al., 2024). Additionally, some methods utilize features obtained from an independent model that are not specifically aligned with the model undergoing fine-tuning, reducing their effectiveness (Killamsetty et al., 2023; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024; Du et al., 2023). DELIFT addresses these limitations by adapting to the unique requirements of each fine-tuning stage. 1. Instruction Tuning: Selects diverse data to enhance general instruction-following capabilities; 2. Task-Specific Fine-Tuning: Prioritizes data that is aligned with the target task, to refine specialized expertise; 3. Continual Fine-tuning: Identifies novel, complementary information to expand the model’s knowledge base while safeguarding against catastrophic forgetting. Figure 1 illustrates how DELIFT optimizes data selection across these stages, demonstrating the selection and pruning processes in each fine-tuning phase. By leveraging submodular optimization techniques (Fujishige, 2005; Bilmes, 2022) and submodular information measures (Iyer et al., 2021), DELIFT efficiently selects optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. This approach effectively balances data utility and computational efficiency. Our key contributions are as follows: 1) Versatile Pairwise Utility Metric: A novel, easy-to-compute metric for assessing data informativeness, incorporating model feedback applicable across all fine-tuning stages. 2) Unified Data Selection Algorithm: DELIFT systematically optimizes data selection for instruction tuning, task-specific fine-tuning, and continual fine-tuning within a single framework. 3) Computational Efficiency: Circumvents resource-intensive operations, ensuring scalability to large datasets and models. DELIFT achieves at least 70% reduction in computational time compared to gradient-based methods on benchmark tasks. 4) Enhanced Performance with Reduced Data: Demonstrates the ability to reduce fine-tuning data size by up to 70% without compromising performance, and achieves comparable efficacy as to utilizing the full dataset. 5) Improvement over Existing Methods: Outperforms current data selection techniques by up to 26% in effectiveness across diverse tasks and model scales (see Section 4). The remainder of this paper is organized as follows: Section 2 provides background on fine-tuning LLMs and reviews related work. Section 3 details the methodology behind DELIFT, including the development of our pairwise utility metric and the submodular optimization process. Section 4 presents experimental results that showcase the effectiveness and efficiency of our method. Section 5 discusses the implications of our findings and potential future directions. Finally, we release our code base for further research.	https://arxiv.org/pdf/2103.00128	PRISM: A Rich Class of Parameterized Submodular Information Measures for Guided Subset Selection	With ever-increasing dataset sizes, subset selection techniques are becoming increasingly important for a plethora of tasks. It is often necessary to guide the subset selection to achieve certain desiderata, which includes focusing or targeting certain data points, while avoiding others. Examples of such problems include: i) targeted learning, where the goal is to find subsets with rare classes or rare attributes on which the model is underperforming, and ii) guided summarization, where data (e.g., image collection, text, document or video) is summarized for quicker human consumption with specific additional user intent. Motivated by such applications, we present PRISM, a rich class of PaRameterIzed Submodular information Measures. Through novel functions and their parameterizations, PRISM offers a variety of modeling capabilities that enable a trade-off between desired qualities of a subset like diversity or representation and similarity/dissimilarity with a set of data points. We demonstrate how PRISM can be applied to the two real-world problems mentioned above, which require guided subset selection. In doing so, we show that PRISM interestingly generalizes some past work, therein reinforcing its broad utility. Through extensive experiments on diverse datasets, we demonstrate the superiority of PRISM over the state-of-the-art in targeted learning and in guided imagecollection summarization. PRISM is available as a part of the SUBMODLIB (https://github.com/decile-team/submodlib) and TRUST (https://github.com/decile-team/trust) toolkits.	Recent times have seen explosive growth in data across several modalities, including text, images, and videos. This has given rise to the need for finding techniques for selecting effective smaller data subsets with specific characteristics for a variety of down-stream tasks. Often, we would like to guide the data selection to either target or avoid a certain set of data slices. One application is, what we call, targeted learning, where the goal is to select data points similar to data slices on which the model is currently performing poorly. These slices are data points that either belong to rare classes or have common rare attributes (e.g., color, background, etc.). An example of such a scenario is shown in Fig. 1(a), where a self-driving car model struggles in detecting “cars in a dark background“ because of a lack of such images in the training set. The targeted learning problem is to augment the training dataset with more of such rare images, with an aim to improve model performance. Another example is detecting cancers in biomedical imaging datasets, where the number of cancerous images are often a small fraction of the non-cancerous images. Another application comes from the summarization task, where an image collection, a video, or a text document is summarized for quicker human consumption by eliminating redundancy, while preserving the main content. While a number of applications require generic summarization (i.e., simply picking a representative and diverse subset of the massive dataset), it is often important to capture certain user intent in summarization. We call this guided summarization. Examples of guided summarization include: (i) query-focused summarization (Sharghi, Gong, and Shah 2016; Xiao et al. 2020), where a summary similar to a specific query is desired, and (ii) privacy-preserving summarization, where a summary dissimilar to a given private set of data points is desired (say, for privacy issues). See Fig. 1(b) for a pictorial illustration. PRISM Framework: We define PRISM through different instantiations and parameterizations of various submodular information measures (Sec. 2). These allow for modeling a spectrum of semantics required for guided subset selection, like relevance to a query set, irrelevance to a private set, and diversity among selected data points. We study the effect of parameter trade-off among these different semantics and present interesting insights. PRISM for Targeted Learning: We present a novel algorithm (Sec. 3.1, Algo. 1) to apply PRISM for targeted learning, which aims to improve a model’s performance on rare slices of data. Specifically, we show that submodular information measures are very effective in finding the examples from the rare classes in a large unlabeled set (akin to finding a needle in a haystack). On several image classification tasks, PRISM obtains ≈ 20-30% gain in accuracy of rare classes (≈ 12% more than existing approaches) by just adding a few additional labeled points from the rare classes. Furthermore, we show that PRISM is 20× to 50× more label-efficient compared to random sampling, and 2× to 4× more labelefficient compared to existing approaches (see Sec. 4.1). We also show that Algo. 1 generalizes some existing approaches for data subset selection, reinforcing its utility (Sec. 3.3). PRISM for Guided Summarization. We propose a learning framework for guided summarization using PRISM (Sec. 3.2). We show that PRISM offers a unified treatment to the different flavors of guided summarization (query-focused and privacy-preserving) and generalizes some existing approaches to summarization, again reinforcing its utility. We show that it outperforms other existing approaches on a real-world image collections dataset (Sec. 4.2).	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2411.04425	DELIFT: Data Efficient Language Model Instruction Fine-Tuning	Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model’s responses to other samples, effectively measuring the informational value relative to the model’s current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.	Fine-tuning large language models (LLMs) is pivotal for adapting these powerful architectures (Devlin et al., 2019; Brown et al., 2020a; Touvron et al., 2023) to specialized tasks such as intricate reasoning, precise question-answering, and the seamless integration of new information (Ouyang et al., 2022). This transformation—from a general-purpose model to a task-specific agent—heavily relies on the quality and nature of the data employed during fine-tuning, which critically determines the model’s subsequent performance (Wei et al., 2022; Zhou et al., 2023; Hoffmann et al., 2024). The effectiveness of fine-tuning hinges on the quality, diversity, and relevance of the selected data (Gururangan et al., 2020; Wei et al., 2022; Zhou et al., 2023). High-quality data ensures accurate learning, diverse data enhances generalization, and relevant data aligns the model’s capabilities with specific application needs. However, optimizing data selection across different fine-tuning phases remains a significant challenge, leading to our central research question: How can we create a unified framework for efficient data selection across all fine-tuning stages of LLMs, while optimizing performance and maximizing data efficiency? To address this challenge, we present DELIFT (Data Efficient Language model Instruction Fine- Tuning), a novel, unified, and computationally efficient algorithm engineered to optimize data selection across all stages of the fine-tuning process. The key innovation of DELIFT lies in its pairwise utility metric, which assesses the informational value of data samples relative to both the model’s current capabilities and other samples within the dataset. This metric, combined with submodular optimization techniques, allows DELIFT to efficiently select optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. The typical fine-tuning process comprises three key stages: 1. Instruction Tuning: Enhances the model’s ability to follow general instructions (Mishra et al., 2022; Wei et al., 2022; Longpre et al., 2023); 2. Task-Specific Fine-Tuning: Refines the model’s expertise in specific domains (Gururangan et al., 2020; Cobbe et al., 2021); 3. Continual Fine-tuning: Enables the model to integrate new information while mitigating catastrophic forgetting (Madotto et al., 2021; Wu et al., 2024). DELIFT is able to optimize data selection processes across all three stages. Additionally, DELIFT offers significant benefits for In-Context Learning (ICL) (Brown et al., 2020b; Xue et al., 2024). By utilizing the selected subsets as the ICL example pool, DELIFT achieves similar or better performance compared to using the entire dataset, thereby enhancing data efficiency in ICL scenarios. This dual functionality is empirically validated in our experimental results. Existing data selection methodologies often fail to address the nuanced requirements of the aforementioned distinct fine-tuning stages. Many approaches are tailored to a single stage, lacking the adaptability needed for comprehensive fine-tuning (Xia et al., 2024; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024). Others depend on computationally intensive procedures, such as exhaustive gradient computations, rendering them impractical for large-scale models and datasets (Killamsetty et al., 2021b;a; Xia et al., 2024; Zhang et al., 2024). Additionally, some methods utilize features obtained from an independent model that are not specifically aligned with the model undergoing fine-tuning, reducing their effectiveness (Killamsetty et al., 2023; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024; Du et al., 2023). DELIFT addresses these limitations by adapting to the unique requirements of each fine-tuning stage. 1. Instruction Tuning: Selects diverse data to enhance general instruction-following capabilities; 2. Task-Specific Fine-Tuning: Prioritizes data that is aligned with the target task, to refine specialized expertise; 3. Continual Fine-tuning: Identifies novel, complementary information to expand the model’s knowledge base while safeguarding against catastrophic forgetting. Figure 1 illustrates how DELIFT optimizes data selection across these stages, demonstrating the selection and pruning processes in each fine-tuning phase. By leveraging submodular optimization techniques (Fujishige, 2005; Bilmes, 2022) and submodular information measures (Iyer et al., 2021), DELIFT efficiently selects optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. This approach effectively balances data utility and computational efficiency. Our key contributions are as follows: 1) Versatile Pairwise Utility Metric: A novel, easy-to-compute metric for assessing data informativeness, incorporating model feedback applicable across all fine-tuning stages. 2) Unified Data Selection Algorithm: DELIFT systematically optimizes data selection for instruction tuning, task-specific fine-tuning, and continual fine-tuning within a single framework. 3) Computational Efficiency: Circumvents resource-intensive operations, ensuring scalability to large datasets and models. DELIFT achieves at least 70% reduction in computational time compared to gradient-based methods on benchmark tasks. 4) Enhanced Performance with Reduced Data: Demonstrates the ability to reduce fine-tuning data size by up to 70% without compromising performance, and achieves comparable efficacy as to utilizing the full dataset. 5) Improvement over Existing Methods: Outperforms current data selection techniques by up to 26% in effectiveness across diverse tasks and model scales (see Section 4). The remainder of this paper is organized as follows: Section 2 provides background on fine-tuning LLMs and reviews related work. Section 3 details the methodology behind DELIFT, including the development of our pairwise utility metric and the submodular optimization process. Section 4 presents experimental results that showcase the effectiveness and efficiency of our method. Section 5 discusses the implications of our findings and potential future directions. Finally, we release our code base for further research.	https://aclanthology.org/2024.findings-naacl.209.pdf	An End-to-End Submodular Framework for Data-Efficient In-Context Learning	Recent advancements in natural language tasks leverage the emergent In-Context Learning (ICL) ability of pretrained Large Language Models (LLMs). ICL enables LLMs to perform new tasks by utilizing a limited number of input-output examples as prompts. While ICL circumvents the costly step of finetuning LLMs, its effectiveness is heavily dependent on the quality and ordering of provided examples (called exemplars). In this work, we propose a two-stage data-efficient framework Div-S3 for exemplar selection for ICL. The first stage focuses on data annotation and employs a poolbased active learning approach to select a set of Diverse and informative exemplars from the target tasks’ unlabeled pool. Given a test input/query, the second stage uses Submodular Span Summarization (S3) to select the most relevant and non-redundant exemplars from the annotated pool of a limited budget. On 7 different NLP datasets and 5 LLMs of varying complexities, we show Div-S3 outperforms (1) existing active learning-based methods for data annotation for ICL and (2) similarity-based methods for test query-specific exemplars retrieval.	Pretrained large language models (LLMs) (Kenton and Toutanova, 2019; Brown et al., 2020; Chowdhery et al., 2022) have become foundational for a wide range of Natural Language Processing (NLP) tasks, demonstrating impressive success across various domains (Bommasani et al., 2021; Bubeck et al., 2023) through in-context learning (ICL) (Dong et al., 2022). ICL enables these pretrained LLMs to perform new tasks by using task-specific prompts containing a limited number of input-output demonstrations (also referred to as shots, exemplars, or prompts) in the natural language format. This approach facilitates deployment across different downstream tasks and reduces the need for labeled downstream training data since ICL does not require any task-specific training. The typical ICL procedure consists of two key components: (1) Exemplar annotation and retrieval (Wu et al., 2022; Köksal et al., 2022; Liu et al., 2022): This step involves annotating and retrieving exemplars that serve as context demonstrations. (2) Prompt template crafting (Sorensen et al., 2022; Deng et al., 2022): this step involves designing a prompt template to wrap these demonstrations in a comprehensible and coherent natural language instruction. Recent studies (Liu et al., 2022; Su et al., 2022; Margatina et al., 2023) show that providing exemplars most relevant to the current input instance is beneficial. Moreover, Zhao et al. (2021), Lu et al. (2022), and Liu et al. (2023) observe that LLMs attend more to the exemplars that are closer in the sequence to the input instance. Therefore, to achieve the best performance of ICL, the selection of exemplars and their ordering in the LLM prompt are crucial. In practice, an extensive collection of unlabeled exemplars is easily available (e.g., posts and discussions on forums like Stack Exchange or user-generated content on social media platforms), but manually annotating all exemplars would be exceptionally costly. To annotate and select the exemplars optimally for a given target task, we follow the two-stage approach shown in Figure 1: (1) Exemplar Annotation: select a subset of exemplars for annotation under a fixed budget (performed only once) and (2) Exemplar Retrieval: identify limited-sized exemplars in an ordering that are most influential for a given input instance from the annotated subset of exemplars. Intuitively, for the first stage, we aim to find the subset with maximal diversity and least redundancy so that, given any input, we can find corresponding labeled exemplars. For the second stage, in addition to the diversity requirement similar to the first stage, we emphasize the relevance of the exemplars to the given input query and order exemplars so that their relevance to the input query decreases as the exemplars are farther away from the input instance. We propose a framework Div-S3 based on submodular optimization that unifies the abovementioned two stages. For Exemplar Annotation, we model the problem as a submodular optimization problem under a cardinality constraint to find as Diverse a subset as possible within a budget. For Exemplar Retrieval, we formalize the problem as a Submodular Span Summarization (S3) problem (Kumari and Bilmes, 2021) with a knapsack constraint, which finds a diverse subset most relevant to the input query under a token length limit. Also, we naturally order the resulting exemplars based on the gains represented by the submodular function. The name of our proposed framework Div-S3 captures the optimization objectives used for both exemplar annotation (Div) and exemplar retrieval (S3) stages. In Fig. 2, we show a sample test query where using Div-S3 for exemplar selection leads to a more diverse and query-relevant exemplar set (more examples provided in Appendix D). Our framework is general, as any submodular function can be plugged into our method. For models beyond LMs, e.g., for text-image multi-modality models, we may use pre-existing submodular functions that are powerful for expressing diversity in the image domain. In addition, we account for relevance, diversity, and ordering for the exemplar retrieval stage, where one or two aspects typically get overlooked by previous methods. Empirically, we evaluate Div-S3 on 7 NLP tasks with 5 LLMs and show significantly improved performance compared to baselines. Our contributions are: 1. We propose an end-to-end framework DivS3 utilizing submodular optimization for performing data-efficient ICL using LLMs. Depending on budget requirements, Div-S3 provides the flexibility to set the budget either in terms of the number of exemplars to be used in the prompt or the LLM’s context window size. 2. We empirically validate the effectiveness of our framework on 7 different NLP tasks and show the transferability of results across LLMs of varying complexities. 3. We thoroughly analyze each component of Div-S3 by (a) studying S3 in a setting with no annotation budget constraint and (b) analyzing the sensitivity of the exemplars selected by Div-S3 to their position in the LLM’s prompt.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2402.04333	LESS: Selecting Influential data for Targeted Instruction Tuning	Instruction tuning has unlocked powerful capabilities in large language models (LLMs), using combined datasets to develop general-purpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application. To facilitate future work, we release code and data at princetonnlp/LESS.	Instruction tuning has made large language models (LLMs) adept at following human instructions (Ouyang et al., 2022) as versatile chatbots (OpenAI, 2022; 2023; Anthropic, 2023; Google, 2023). Recent efforts curating highly diverse and wide-ranging instruction tuning datasets (Taori et al., 2023; Wang et al.; Mukherjee et al., 2023; Xu et al., 2023, inter alia) induce remarkably strong generalization even from a small number of examples (Zhou et al., 2023). Regardless, it remains an open problem to understand how to best utilize these various datasets. Many real-world applications call for cultivating a specific suite of capabilities in LLMs (e.g., reasoning skills). However, training LLMs with mixed instruction tuning datasets can hinder the development of these specific capabilities. For example, Wang et al. (2023b) demonstrates that LLMs trained on a mix of instruction tuning datasets exhibit worse performance than those trained on a subset of the data. Additionally, considering the broad spectrum of user queries and the multitude of skills required to respond to them, there may not always be enough in-domain data available. Therefore, we hope to be able to effectively use the general instruction tuning data to improve specific capabilities. We frame this setting as targeted instruction tuning: Given just a handful of examples embodying a specific capability, how can we effectively select relevant fine-tuning data from a large collection of instruction datasets? We approach this problem by prioritizing training on data that directly minimizes loss on a target task instead of relying on surface form features (Gururangan et al., 2020; Xie et al., 2023b). Inspired by past works estimating the influence of individual training datapoints with gradient information (Pruthi et al., 2020; Han et al., 2023), we design an optimizer-aware approach to select such data. However, straightforward application of this influence formulation faces several challenges unique to the instruction tuning setting: (1) LLMs are traditionally fine-tuned with the Adam optimizer (Kingma & Ba, 2015) instead of the canonical SGD optimizer; (2) using sequence-level gradients of variable-length instruction data can derail the influence estimation; and (3) the large number of trainable parameters in LLMs makes the computation and storage of gradient information extremely resource-intensive. We address these concerns in LESS, an algorithm that performs Low-rank gradiEnt Similarity Search to select relevant instruction tuning data for a target application, which exhibits the following properties: 1. Compatible with instruction tuning with Adam (§2 and §3): LESS adapts the gradient features from classical influence formulations (Pruthi et al., 2020) to work with the Adam optimizer and variable-length instruction data. The optimization insights and influence formulation may be of independent interest as well. 2. Efficient (§4.1): LESS uses LoRA (Hu et al., 2021) and random projections (Johnson & Lindenstrauss, 1984) to construct a gradient datastore with lowdimensional, easily manipulable gradient features that permit efficient and effective dataset selection. The gradient datastore can be reused for new target tasks. 3. Transferable (§5.3): Data selected using small models’ gradient features induce strong performance in large models and models from different families, adding to the efficiency of LESS (Table 2). 4. Interpretable (§6.2): Qualitative analysis shows that LESS selects data with similar reasoning and skill types as the target task, whereas existing approaches often select data based on surface form cues (e.g., language or topic). We evaluate our approach on three diverse downstream datasets—MMLU (Hendrycks et al., 2020), TY- DIQA (Clark et al., 2020), and BBH (Suzgun et al., 2023)—each containing distinct subtasks that effectively simulate targeted instruction tuning scenarios. Results show that LESS often selects a small subset of the data (5%) that outperforms training on the full dataset, and the selected subset remains universally effective across model scales and families (Table 2). Comparisons with other data selection methods show that LESS is the only consistently effective approach, justifying its relatively high computational cost.	https://arxiv.org/pdf/1906.01827	Coresets for Data-Efficient Training of Machine Learning Models	Incremental gradient (IG) methods, such as stochastic gradient descent and its variants are commonly used for large scale optimization in machine learning. Despite the sustained effort to make IG methods more data-efficient, it remains an open question how to select a training data subset that can theoretically and practically perform on par with the full dataset. Here we develop CRAIG, a method to select a weighted subset (or coreset) of training data that closely estimates the full gradient by maximizing a submodular function. We prove that applying IG to this subset is guaranteed to converge to the (near)optimal solution with the same convergence rate as that of IG for convex optimization. As a result, CRAIG achieves a speedup that is inversely proportional to the size of the subset. To our knowledge, this is the first rigorous method for data-efficient training of general machine learning models. Our extensive set of experiments show that CRAIG, while achieving practically the same solution, speeds up various IG methods by up to 6x for logistic regression and 3x for training deep neural networks.	Mathematical optimization lies at the core of training largescale machine learning systems, and is now widely used over massive data sets with great practical success, assuming sufficient data resources are available. Achieving this success, however, also requires large amounts of (often GPU) computing, as well as concomitant financial expenditures and energy usage (Strubell et al., 2019). Significantly decreasing these costs without decreasing the learnt system’s resulting accuracy is one of the grand challenges of machine learning and artificial intelligence today (Asi & Duchi, 2019). Training machine learning models often reduces to optimizing a regularized empirical risk function. Given a convex loss l, and a μ-strongly convex regularizer r, one aims to find model parameter vector w∗ over the parameter space W that minimizes the loss f over the training data V : w∗ ∈ arg minw∈W f (w), f (w) := ∑ i∈V fi(w) + r(w), fi(w) = l(w, (xi, yi)), (1) where V = {1, . . . , n} is an index set of the training data, and functions fi : Rd �' R are associated with training examples (xi, yi), where xi ∈ Rd is the feature vector, and yi is the point i’s label. Standard Gradient Descent can find the minimizer of this problem, but requires repeated computations of the full gradient ∇f (w)—sum of the gradients over all training data points/functions i—and is therefore prohibitive for massive data sets. This issue is further exacerbated in case of deep neural networks where gradient computations (backpropagation) are expensive. Incremental Gradient (IG) methods, such as Stochastic Gradient Descent (SGD) and its accelerated variants, including SGD with momentum (Qian, 1999), Adagrad (Duchi et al., 2011), Adam (Kingma & Ba, 2014), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013) iteratively estimate the gradient on random subsets/batches of training data. While this provides an unbiased estimate of the full gradient, the randomized batches introduce variance in the gradient estimate (Hofmann et al., 2015), and therefore stochastic gradient methods are in general slow to converge (Johnson & Zhang, 2013; Defazio et al., 2014). The majority of the work speeding up IG methods has thus primarily focused on reducing the variance of the gradient estimate (SAGA (Defazio et al., 2014), SVRG (Johnson & Zhang, 2013), Katysha (Allen-Zhu, 2017)) or more carefully selecting the gradient stepsize (Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), Adam (Kingma & Ba, 2014)). However, the direction that remains largely unexplored is how to carefully select a small subset S ⊆ V of the fullCoresets for Data-efficient Training of Machine Learning Models PAGE 2 training data V , so that the model is trained only on the subset S while still (approximately) converging to the globally optimal solution (i.e., the model parameters that would be obtained if training/optimizing on the full V ). If such a subset S can be quickly found, then this would directly lead to a speedup of |V |/|S| (which can be very large if |S| ≪ |V |) per epoch of IG. There are four main challenges in finding such a subset S. First is that a guiding principle for selecting S is unclear. For example, selecting training points close to the decision boundary might allow the model to fine tune the decision boundary, while picking the most diverse set of data points would allow the model to get a better sense of the training data distribution. Second is that finding S must be fast, as otherwise identifying the set S may take longer than the actual optimization, and so no overall speed-up would be achieved. Third is that finding a subset S is not enough. One also has to decide on a gradient stepsize for each data point in S, as they affect the convergence. And last, while the method might work well empirically on some data sets, one also requires theoretical understanding and mathematical convergence guarantees. Here we develop Coresets for Accelerating Incremental Gradient descent (CRAIG), for selecting a subset of training data points to speed up training of large machine learning models. Our key idea is to select a weighted subset S of training data V that best approximates the full gradient of V . We prove that the subset S that minimizes an upper-bound on the error of estimating the full gradient maximizes a submodular facility location function. Hence, S can be efficiently found using a fast greedy algorithm. We also provide theoretical analysis of CRAIG and prove its convergence. Most importantly, we show that any incremental gradient method (IG) on S converges in the same number epochs as the same IG would on the full V , which means that we obtain a speed-up inversely proportional to the size of S. In particular, for a μ-strongly convex risk function and a subset S selected by CRAIG that estimates the full gradient by an error of at most ǫ, we prove that IG on S with diminishing stepsize αk = α/kτ at epoch k (with 0 < τ < 1 and 0 < α), converges to an 2Rǫ/μ2 neighborhood of the optimal solution at rate O(1/√k). Here, R = min{d0, (rγmaxC + ǫ)/μ} where d0 is the initial distance to the optimum, C is an upperbound on the norm of the gradients, r = |S|, and γmax is the largest weight for the elements in the subset obtained by CRAIG. Moreover, we prove that if in addition to the strong convexity, component functions have smooth gradients, IG with the same diminishing step size on subset S converges to a 2ǫ/μ neighborhood of the optimum solution at rate O(1/kτ ). The above implies that IG on S converges to the same solution and in the same number of epochs as IG on the full V . But because every epoch only uses a subset S of the data, it requires fewer gradient computations and thus leads to a |V |/|S| speedup over traditional IG methods, while still (approximately) converging to the optimal solution. We also note that CRAIG is complementary to various incremental gradient (IG) methods (SGD, SAGA, SVRG, Adam), and such methods can be used on the subset S found by CRAIG. We also demonstrate the effectiveness of CRAIG via an extensive set of experiments using logistic regression (a convex optimization problem) as well as training deep neural networks (non-convex optimization problems). We show that CRAIG speeds up incremental gradient methods, including SGD, SAGA, and SVRG. In particular, CRAIG while achieving practically the same loss and accuracy as the underlying incremental gradient descent methods, speeds up gradient methods by up to 6x for convex and 3x for non-convex loss functions.	0	1
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2402.04333	LESS: Selecting Influential data for Targeted Instruction Tuning	Instruction tuning has unlocked powerful capabilities in large language models (LLMs), using combined datasets to develop general-purpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application. To facilitate future work, we release code and data at princetonnlp/LESS.	Instruction tuning has made large language models (LLMs) adept at following human instructions (Ouyang et al., 2022) as versatile chatbots (OpenAI, 2022; 2023; Anthropic, 2023; Google, 2023). Recent efforts curating highly diverse and wide-ranging instruction tuning datasets (Taori et al., 2023; Wang et al.; Mukherjee et al., 2023; Xu et al., 2023, inter alia) induce remarkably strong generalization even from a small number of examples (Zhou et al., 2023). Regardless, it remains an open problem to understand how to best utilize these various datasets. Many real-world applications call for cultivating a specific suite of capabilities in LLMs (e.g., reasoning skills). However, training LLMs with mixed instruction tuning datasets can hinder the development of these specific capabilities. For example, Wang et al. (2023b) demonstrates that LLMs trained on a mix of instruction tuning datasets exhibit worse performance than those trained on a subset of the data. Additionally, considering the broad spectrum of user queries and the multitude of skills required to respond to them, there may not always be enough in-domain data available. Therefore, we hope to be able to effectively use the general instruction tuning data to improve specific capabilities. We frame this setting as targeted instruction tuning: Given just a handful of examples embodying a specific capability, how can we effectively select relevant fine-tuning data from a large collection of instruction datasets? We approach this problem by prioritizing training on data that directly minimizes loss on a target task instead of relying on surface form features (Gururangan et al., 2020; Xie et al., 2023b). Inspired by past works estimating the influence of individual training datapoints with gradient information (Pruthi et al., 2020; Han et al., 2023), we design an optimizer-aware approach to select such data. However, straightforward application of this influence formulation faces several challenges unique to the instruction tuning setting: (1) LLMs are traditionally fine-tuned with the Adam optimizer (Kingma & Ba, 2015) instead of the canonical SGD optimizer; (2) using sequence-level gradients of variable-length instruction data can derail the influence estimation; and (3) the large number of trainable parameters in LLMs makes the computation and storage of gradient information extremely resource-intensive. We address these concerns in LESS, an algorithm that performs Low-rank gradiEnt Similarity Search to select relevant instruction tuning data for a target application, which exhibits the following properties: 1. Compatible with instruction tuning with Adam (§2 and §3): LESS adapts the gradient features from classical influence formulations (Pruthi et al., 2020) to work with the Adam optimizer and variable-length instruction data. The optimization insights and influence formulation may be of independent interest as well. 2. Efficient (§4.1): LESS uses LoRA (Hu et al., 2021) and random projections (Johnson & Lindenstrauss, 1984) to construct a gradient datastore with lowdimensional, easily manipulable gradient features that permit efficient and effective dataset selection. The gradient datastore can be reused for new target tasks. 3. Transferable (§5.3): Data selected using small models’ gradient features induce strong performance in large models and models from different families, adding to the efficiency of LESS (Table 2). 4. Interpretable (§6.2): Qualitative analysis shows that LESS selects data with similar reasoning and skill types as the target task, whereas existing approaches often select data based on surface form cues (e.g., language or topic). We evaluate our approach on three diverse downstream datasets—MMLU (Hendrycks et al., 2020), TY- DIQA (Clark et al., 2020), and BBH (Suzgun et al., 2023)—each containing distinct subtasks that effectively simulate targeted instruction tuning scenarios. Results show that LESS often selects a small subset of the data (5%) that outperforms training on the full dataset, and the selected subset remains universally effective across model scales and families (Table 2). Comparisons with other data selection methods show that LESS is the only consistently effective approach, justifying its relatively high computational cost.	https://arxiv.org/pdf/2310.16776	DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection	Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT-UCS, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to identify a smaller, representative dataset that reduces the data needed to finetune PLMs for the text-generation task of textediting. We examine the efficacy of DEFT-UCS across multiple text-editing tasks, and compare to the state-of-the art text-editing model, CoEDIT. Our results demonstrate that DEFTUCS models are just as accurate as CoEDIT, across eight different datasets consisting of six different editing tasks, while finetuned on 70% less data.	How much data do we need to fine-tune a pretrained language model (PLM) for a specific downstream task? While successes in language modelling have led to numerous publicly available PLMs and ability to produce fine-tuned models for downstream tasks - the answer mostly remains, "as large as possible, and of good quality". For example, Alpaca, an instruction-following model, is trained with 52k data samples (Taori et al., 2023). Similarly, CoPoet, a collaborative poetry writing system is fine-tuned using 87k data samples (Chakrabarty et al., 2022). MetaMath, a mathreasoning LLM is fine-tuned with 395k data samples (Yu et al., 2023). Although fine-tuning PLMs on specific task results in performance gain, acquiring large amounts of data for fine-tuning is not easy for real-world applications which often require niche knowledge and domain expertise. Researchers have explored variety of methods primarily focused on improving the computational efficiency of fine-tuning, including parameterefficient fine-tuning approaches (PEFT) to reduce computational costs by optimizing parameter updates (Fu et al., 2023; Hu et al., 2021) as well as leveraging active-learning for iteratively selecting data samples during training (Su et al., 2022; Diao et al., 2023). Instead, our work focuses on improving the data efficiency of PLM fine-tuning without requiring iterative fine-tuning. Specifically, we explore how to fine-tune PLMs with significantly less data samples and without a cost to model performance. Related to language models, researchers have experimented with different core-set selection metrics (Paul et al., 2021; Sorscher et al., 2022) to improve the data efficiency during pre-training. Marion et al. (2023) demonstrated how perplexity, L2-Error Norm (EL2N) and memorization can be utilized to select smaller, good quality datasets for pre-training. Similarly, (Attendu and Corbeil, 2023) leverage EL2N to dynamically remove data samples with high EL2N between training epochs. However, these metrics assume access to task data and reference models to perform dataset pruning. In real world applications, utilizing such supervised, data-pruning metrics are less realistic since large amounts of annotated task-specific data may be costly to acquire. This leads us to our main research question: How can we leverage unsupervised data pruning to fine-tune PLMs for downstream tasks in a more data efficient manner? In this work, we introduce a new data-efficient fine-tuning framework, DEFT-UCS, that uses unsupervised core-set selection to minimize the amount of labelled data needed to fine-tune PLMs for the text-generation task of text-editing. Our framework is inspired by (Sorscher et al., 2022), who utilize clustering-based dataset pruning to reduce training samples for image-classification models, and to the best of our knowledge, our framework is the first to leverage unsupervised core-set selection for data-efficient fine-tuning of PLMs. We investigate the utility of DEFT-UCS in finetuning PLMs for text-generation across eight different datasets consisting of six different text-editing tasks, and compare DEFT-UCS models to the stateof-the-art text-editing model, CoEDIT(Raheja et al., 2023). Our contributions are as follows: - We introduce DEFT-UCS, a data-efficient-fine tuning framework that leverages unsupervised core-set selection via clustering to identify a smaller representative set of data needed to fine-tune PLMs. - We show that DEFT-UCS, utilizing only 32.5% of CoEDIT’s training data, is able to produce fine-tuned models with improved accuracy on four different text-editing tasks, and similar accuracy on two text-editing tasks compared to CoEDIT (Raheja et al., 2023). - We performed a human evaluation with 3 evaluators to assess the quality of text-edits from our DEFT-UCS model. Evaluators found edits generated by DEFT-UCS model as similar or preferred over CoEDIT (Raheja et al., 2023)	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2402.04333	LESS: Selecting Influential data for Targeted Instruction Tuning	Instruction tuning has unlocked powerful capabilities in large language models (LLMs), using combined datasets to develop general-purpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application. To facilitate future work, we release code and data at princetonnlp/LESS.	Instruction tuning has made large language models (LLMs) adept at following human instructions (Ouyang et al., 2022) as versatile chatbots (OpenAI, 2022; 2023; Anthropic, 2023; Google, 2023). Recent efforts curating highly diverse and wide-ranging instruction tuning datasets (Taori et al., 2023; Wang et al.; Mukherjee et al., 2023; Xu et al., 2023, inter alia) induce remarkably strong generalization even from a small number of examples (Zhou et al., 2023). Regardless, it remains an open problem to understand how to best utilize these various datasets. Many real-world applications call for cultivating a specific suite of capabilities in LLMs (e.g., reasoning skills). However, training LLMs with mixed instruction tuning datasets can hinder the development of these specific capabilities. For example, Wang et al. (2023b) demonstrates that LLMs trained on a mix of instruction tuning datasets exhibit worse performance than those trained on a subset of the data. Additionally, considering the broad spectrum of user queries and the multitude of skills required to respond to them, there may not always be enough in-domain data available. Therefore, we hope to be able to effectively use the general instruction tuning data to improve specific capabilities. We frame this setting as targeted instruction tuning: Given just a handful of examples embodying a specific capability, how can we effectively select relevant fine-tuning data from a large collection of instruction datasets? We approach this problem by prioritizing training on data that directly minimizes loss on a target task instead of relying on surface form features (Gururangan et al., 2020; Xie et al., 2023b). Inspired by past works estimating the influence of individual training datapoints with gradient information (Pruthi et al., 2020; Han et al., 2023), we design an optimizer-aware approach to select such data. However, straightforward application of this influence formulation faces several challenges unique to the instruction tuning setting: (1) LLMs are traditionally fine-tuned with the Adam optimizer (Kingma & Ba, 2015) instead of the canonical SGD optimizer; (2) using sequence-level gradients of variable-length instruction data can derail the influence estimation; and (3) the large number of trainable parameters in LLMs makes the computation and storage of gradient information extremely resource-intensive. We address these concerns in LESS, an algorithm that performs Low-rank gradiEnt Similarity Search to select relevant instruction tuning data for a target application, which exhibits the following properties: 1. Compatible with instruction tuning with Adam (§2 and §3): LESS adapts the gradient features from classical influence formulations (Pruthi et al., 2020) to work with the Adam optimizer and variable-length instruction data. The optimization insights and influence formulation may be of independent interest as well. 2. Efficient (§4.1): LESS uses LoRA (Hu et al., 2021) and random projections (Johnson & Lindenstrauss, 1984) to construct a gradient datastore with lowdimensional, easily manipulable gradient features that permit efficient and effective dataset selection. The gradient datastore can be reused for new target tasks. 3. Transferable (§5.3): Data selected using small models’ gradient features induce strong performance in large models and models from different families, adding to the efficiency of LESS (Table 2). 4. Interpretable (§6.2): Qualitative analysis shows that LESS selects data with similar reasoning and skill types as the target task, whereas existing approaches often select data based on surface form cues (e.g., language or topic). We evaluate our approach on three diverse downstream datasets—MMLU (Hendrycks et al., 2020), TY- DIQA (Clark et al., 2020), and BBH (Suzgun et al., 2023)—each containing distinct subtasks that effectively simulate targeted instruction tuning scenarios. Results show that LESS often selects a small subset of the data (5%) that outperforms training on the full dataset, and the selected subset remains universally effective across model scales and families (Table 2). Comparisons with other data selection methods show that LESS is the only consistently effective approach, justifying its relatively high computational cost.	https://arxiv.org/pdf/2403.08370	SMART: Submodular Data Mixture Strategy for Instruction Tuning	Instruction Tuning involves finetuning a language model on a collection of instructionformatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) — a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal mixing. Furthermore, SMART facilitates the creation of data mixtures based on a few representative subsets of tasks alone and through task pruning analysis, we reveal that in a limited budget setting, allocating budget among a subset of representative tasks yields superior performance compared to distributing the budget among all tasks. The code for reproducing our results is open-sourced at https://github.com/kowndinyarenduchintala/SMART.	One of the main goals of artificial intelligence (AI) research is to build machines that can communicate (Turing, 1950), and an essential part of communication is to understand and follow instructions. Large Language Models (LLMs), which are pre-trained over massive text corpora on next-tokenprediction objective, can perform a wide range of NLP tasks via "prompting" (Brown et al., 2020; Kojima et al., 2022; Almazrouei et al., 2023; Liu et al., 2023; Touvron et al., 2023). Instruction Tuning (Wei et al., 2021; Sanh et al., 2021; Chung et al., 2022) is an approach that further enhances the instruction-following ability and generalizability of pre-trained LLMs to unseen tasks. It involves fine-tuning an LLM on a collection of instruction-formatted instances (encompassing multiple tasks) - each consisting of an instruction (or task description), an optional input, the corresponding output (the ground truth) and optionally a few demonstrations/examples. It is a special case of multitask learning where the LLM is finetuned on a collection of instruction-formatted multitask datasets (Chung et al., 2022). Finetuning on multiple tasks simultaneously, allows the model to share and transfer information across tasks, resulting in a better common internal representation that is preferred by all tasks while suppressing taskdependent noise (Caruana, 1997). Consequently, the model learns to generalize to unseen tasks by discerning helpful cues from both implicitly and explicitly related tasks that it has previously seen. The performance enhancement from instruction tuning is heavily contingent on data quality, data quantity, and task composition (Wang et al., 2023b). Studies by Iyer et al. (2022) and Longpre et al. (2023) have shown that while scaling the number of tasks is important, the relative proportion of various tasks (mixture weighting) merits as much attention for optimal instruction tuning. Intuitively, we want the model to see enough data for a given task that it can perform well on it, but not to see so much data that it memorizes the training set (Raffel et al., 2020). Iyer et al. (2022) performed manual tuning of various benchmark proportions and decided on a final mixture, whereas Longpre et al. (2023) studied the impact of removing each benchmark from the finetuning mixture and relied on their practioners’ intuition from there on, to decide on the exact proportions of benchmarks. In this work, we would like to explore a more systematic approach to mixture weighting. Specifically, we are motivated by the fact that in a large multitask dataset like FLAN 2022 (Longpre et al., 2023), which has 1840 tasks, there will likely be many similar tasks leading to redundancies and not all of them may require sampling in equal proportions. For instance, there might be many tasks of the type Natural Language Inference (NLI), and it might be enough to sample relatively more instances from a few representative NLI tasks and less from the others. Furthermore, which samples we select from each task is also crucial because the samples should faithfully represent the task at hand. A random subset may fail to do this as it can miss out on essential corner cases. With this context, we focus on the following two fundamental research questions (RQs) that form the basis for our subsequent inquiry: - (RQ1) Given a huge multitask instructiontuning dataset and a limited fine-tuning budget which is defined by the total number of (prompt, response) instances that can be used for fine-tuning, how do we divide this budget among thousands of tasks present in the dataset? i.e., how many instances to sample from each task? and which instances to sample from each task? - (RQ2) Can we go a step further and strategically prune some tasks altogether and only fine-tune on a small subset of representative tasks without hurting the performance? If yes, what is the nature of this subset? To the best of our knowledge, there’s currently no principled approach to determining task compositions for instruction tuning, other than manual tuning and/or practioners’ intuition. As a first step towards addressing both of the above RQs, we first define a common subset selection problem (more formally stated in Section 3) as follows - Given a huge collection of M instructionformatted task datasets, a task budget M ′ ≤ M and a total budget (N ′) of (prompt, response) pairs, which M ′ tasks to select? and how many instances to select from each of these M ′ tasks and which instances to select? Note that RQ1 is an instance of this problem where M ′ = M . Constrained Submodular Maximization (Section 2) proves to be a good model for discovering representative subsets (or coresets) of a massive training dataset (or ground set) that acts as surrogate (i.e., achieves similar performance) and are much better than uniformly-at-random subsets. Intuitively, this is because submodular functions model information in subsets, and hence maximizing a submodular function subject to a constraint yields non-redundant subsets of the ground set. An essential feature of this model is that it returns weighted subsets, i.e., each sample in the coreset comes with an associated score, which indicates how important the sample is. Inspired by submodular functions, we propose our solution (Section 3) to the above subset selection problem for instruction tuning that works in two stages. In the first stage, we select a weighted subset of tasks from the full dataset where the weights will determine how many samples to select from each task. In the next stage, we select samples from each task based on the assigned task budgets. Note that the submodular functions used in each stage are not necessarily identical (Section 4.8). The main contributions of our work can be summarized as follows: - We introduce SMART — a novel data mixture strategy for instruction tuning that models the data mixture problem (Section 3) as a sequence of two cardinality-constrained submodular maximization problems and offer empirical evidence that it outperforms both examples proportional and equal mixing baselines (Section 4) as well as the mixture weights proposed by Longpre et al. (2023). - Existing works like Longpre et al. (2023) have reported a continuous increase in performance upon increasing the number of tasks (though the gains themselves may be diminishing). However, we posit that this depends on the order in which new tasks are incorporated and show empirically that in the case of SMART mixtures, a performance peak is observed with an initial addition of few representative tasks and upon adding more and more tasks, the performance is not sustained (Section 4.6). - We find that the nature of instances that should be selected in each task (i.e, whether a representative or diverse subset) also depends on the total task budget, M ′ (Section 4.8). For higher M ′s, each task on average gets the relatively low budget and selecting representative samples is more important; however for lower M ′s, when there is sufficient enough budget for each task, the need for diversity dominates	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2402.04333	LESS: Selecting Influential data for Targeted Instruction Tuning	Instruction tuning has unlocked powerful capabilities in large language models (LLMs), using combined datasets to develop general-purpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application. To facilitate future work, we release code and data at princetonnlp/LESS.	Instruction tuning has made large language models (LLMs) adept at following human instructions (Ouyang et al., 2022) as versatile chatbots (OpenAI, 2022; 2023; Anthropic, 2023; Google, 2023). Recent efforts curating highly diverse and wide-ranging instruction tuning datasets (Taori et al., 2023; Wang et al.; Mukherjee et al., 2023; Xu et al., 2023, inter alia) induce remarkably strong generalization even from a small number of examples (Zhou et al., 2023). Regardless, it remains an open problem to understand how to best utilize these various datasets. Many real-world applications call for cultivating a specific suite of capabilities in LLMs (e.g., reasoning skills). However, training LLMs with mixed instruction tuning datasets can hinder the development of these specific capabilities. For example, Wang et al. (2023b) demonstrates that LLMs trained on a mix of instruction tuning datasets exhibit worse performance than those trained on a subset of the data. Additionally, considering the broad spectrum of user queries and the multitude of skills required to respond to them, there may not always be enough in-domain data available. Therefore, we hope to be able to effectively use the general instruction tuning data to improve specific capabilities. We frame this setting as targeted instruction tuning: Given just a handful of examples embodying a specific capability, how can we effectively select relevant fine-tuning data from a large collection of instruction datasets? We approach this problem by prioritizing training on data that directly minimizes loss on a target task instead of relying on surface form features (Gururangan et al., 2020; Xie et al., 2023b). Inspired by past works estimating the influence of individual training datapoints with gradient information (Pruthi et al., 2020; Han et al., 2023), we design an optimizer-aware approach to select such data. However, straightforward application of this influence formulation faces several challenges unique to the instruction tuning setting: (1) LLMs are traditionally fine-tuned with the Adam optimizer (Kingma & Ba, 2015) instead of the canonical SGD optimizer; (2) using sequence-level gradients of variable-length instruction data can derail the influence estimation; and (3) the large number of trainable parameters in LLMs makes the computation and storage of gradient information extremely resource-intensive. We address these concerns in LESS, an algorithm that performs Low-rank gradiEnt Similarity Search to select relevant instruction tuning data for a target application, which exhibits the following properties: 1. Compatible with instruction tuning with Adam (§2 and §3): LESS adapts the gradient features from classical influence formulations (Pruthi et al., 2020) to work with the Adam optimizer and variable-length instruction data. The optimization insights and influence formulation may be of independent interest as well. 2. Efficient (§4.1): LESS uses LoRA (Hu et al., 2021) and random projections (Johnson & Lindenstrauss, 1984) to construct a gradient datastore with lowdimensional, easily manipulable gradient features that permit efficient and effective dataset selection. The gradient datastore can be reused for new target tasks. 3. Transferable (§5.3): Data selected using small models’ gradient features induce strong performance in large models and models from different families, adding to the efficiency of LESS (Table 2). 4. Interpretable (§6.2): Qualitative analysis shows that LESS selects data with similar reasoning and skill types as the target task, whereas existing approaches often select data based on surface form cues (e.g., language or topic). We evaluate our approach on three diverse downstream datasets—MMLU (Hendrycks et al., 2020), TY- DIQA (Clark et al., 2020), and BBH (Suzgun et al., 2023)—each containing distinct subtasks that effectively simulate targeted instruction tuning scenarios. Results show that LESS often selects a small subset of the data (5%) that outperforms training on the full dataset, and the selected subset remains universally effective across model scales and families (Table 2). Comparisons with other data selection methods show that LESS is the only consistently effective approach, justifying its relatively high computational cost.	https://arxiv.org/pdf/2012.10630	GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning	Large scale machine learning and deep models are extremely data-hungry. Unfortunately, obtaining large amounts of labeled data is expensive, and training state-of-the-art models (with hyperparameter tuning) requires significant computing resources and time. Secondly, real-world data is noisy and imbalanced. As a result, several recent papers try to make the training process more efficient and robust. However, most existing work either focuses on robustness or efficiency, but not both. In this work, we introduce GLISTER, a GeneraLIzation based data Subset selecTion for Efficient and Robust learning framework. We formulate GLISTER as a mixed discretecontinuous bi-level optimization problem to select a subset of the training data, which maximizes the log-likelihood on a held-out validation set. We then analyze GLISTER for simple classifiers such as gaussian and multinomial naive-bayes, k-nearest neighbor classifier, and linear regression and show connections to submodularity. Next, we propose an iterative online algorithm GLISTER-ONLINE, which performs data selection iteratively along with the parameter updates and can be applied to any loss-based learning algorithm. We then show that for a rich class of loss functions including cross-entropy, hinge-loss, squared-loss, and logistic-loss, the inner discrete data selection is an instance of (weakly) submodular optimization, and we analyze conditions for which GLISTER-ONLINE reduces the validation loss and converges. Finally, we propose GLISTER-ACTIVE, an extension to batch active learning, and we empirically demonstrate the performance of GLISTER on a wide range of tasks including, (a) data selection to reduce training time, (b) robust learning under label noise and imbalance settings, and (c) batch-active learning with several deep and shallow models. We show that our framework improves upon state of the art both in efficiency and accuracy (in cases (a) and (c)) and is more efficient compared to other state-ofthe-art robust learning algorithms in case (b). The code for GLISTERis at:https://github.com/dssresearch/GLISTER.	With the quest to achieve human-like performance for machine learning and deep learning systems, the cost of training and deploying machine learning models has been significantly increasing. The wasted computational and engineering energy becomes evident in deep learning algorithms, wherein extensive hyper-parameter tuning and network architecture search need to be done. This results in staggering compute costs and running times. As a result, efficient and robust machine learning is a very relevant and substantial research problem. In this paper, we shall focus on three goals: Goal 1: Train machine learning and deep learning models on effective subsets of data, thereby significantly reducing training time and compute while not sacrificing accuracy. Goal 2: To (iteratively) select effective subsets of labeled data to reduce the labeling cost. Goal 3: Select data subsets to remove noisy labels and class imbalance, which is increasingly common in operational machine learning settings. Most prior work discussed above, either study robustness or efficiency, but not both. For example, the data selection approaches such as (Wei, Iyer, and Bilmes 2015; Mirzasoleiman, Bilmes, and Leskovec 2020; Shinohara 2014) and others focus on approximating either gradients or performance on the training sets, and hence would not be suitable for scenarios such as label noise and imbalance. On the other hand, the approaches like (Ren et al. 2018; Jiang et al. 2018) and others, focus on robustness but are not necessarily efficient. For example, the approach of (Ren et al. 2018) requires 3x the standard (deep) training cost, to obtain a robust model. GLISTER is the first framework, to the best of our knowledge, which focuses on both efficiency and robustness. Our work is closely related to the approaches of (Wei, Iyer, and Bilmes 2015) and (Ren et al. 2018). We build upon the work of (Wei, Iyer, and Bilmes 2015), by first generalizing their framework beyond simple classifiers (like nearest neighbor and naive bayes), but with general loss functions. We do this by proposing an iterative algorithm GLISTER-ONLINE which does data selection via a meta-learning based approach along with parameter updates. Furthermore, we pose the problem as optimizing the validation set performance as opposed to training set performance, thereby encouraging generalization. Next, our approach also bears similarity to (Ren et al. 2018), except that we need to solve a discrete optimization problem instead of a meta-gradient update. Moreover, we do not run our data selection every iteration, thereby ensuring that we are significantly faster than a single training run. Finally, we extend our algorithm to the active learning scenario. We demonstrate that our framework is more efficient and accurate compared to existing data selection and active learning algorithms, and secondly, also generalizes well under noisy data, and class imbalance scenarios. In particular, we show that GLISTER achieves a 3x - 6x speedup on a wide range of models and datasets, with very small loss in accuracy.	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2402.04333	LESS: Selecting Influential data for Targeted Instruction Tuning	Instruction tuning has unlocked powerful capabilities in large language models (LLMs), using combined datasets to develop general-purpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application. To facilitate future work, we release code and data at princetonnlp/LESS.	Instruction tuning has made large language models (LLMs) adept at following human instructions (Ouyang et al., 2022) as versatile chatbots (OpenAI, 2022; 2023; Anthropic, 2023; Google, 2023). Recent efforts curating highly diverse and wide-ranging instruction tuning datasets (Taori et al., 2023; Wang et al.; Mukherjee et al., 2023; Xu et al., 2023, inter alia) induce remarkably strong generalization even from a small number of examples (Zhou et al., 2023). Regardless, it remains an open problem to understand how to best utilize these various datasets. Many real-world applications call for cultivating a specific suite of capabilities in LLMs (e.g., reasoning skills). However, training LLMs with mixed instruction tuning datasets can hinder the development of these specific capabilities. For example, Wang et al. (2023b) demonstrates that LLMs trained on a mix of instruction tuning datasets exhibit worse performance than those trained on a subset of the data. Additionally, considering the broad spectrum of user queries and the multitude of skills required to respond to them, there may not always be enough in-domain data available. Therefore, we hope to be able to effectively use the general instruction tuning data to improve specific capabilities. We frame this setting as targeted instruction tuning: Given just a handful of examples embodying a specific capability, how can we effectively select relevant fine-tuning data from a large collection of instruction datasets? We approach this problem by prioritizing training on data that directly minimizes loss on a target task instead of relying on surface form features (Gururangan et al., 2020; Xie et al., 2023b). Inspired by past works estimating the influence of individual training datapoints with gradient information (Pruthi et al., 2020; Han et al., 2023), we design an optimizer-aware approach to select such data. However, straightforward application of this influence formulation faces several challenges unique to the instruction tuning setting: (1) LLMs are traditionally fine-tuned with the Adam optimizer (Kingma & Ba, 2015) instead of the canonical SGD optimizer; (2) using sequence-level gradients of variable-length instruction data can derail the influence estimation; and (3) the large number of trainable parameters in LLMs makes the computation and storage of gradient information extremely resource-intensive. We address these concerns in LESS, an algorithm that performs Low-rank gradiEnt Similarity Search to select relevant instruction tuning data for a target application, which exhibits the following properties: 1. Compatible with instruction tuning with Adam (§2 and §3): LESS adapts the gradient features from classical influence formulations (Pruthi et al., 2020) to work with the Adam optimizer and variable-length instruction data. The optimization insights and influence formulation may be of independent interest as well. 2. Efficient (§4.1): LESS uses LoRA (Hu et al., 2021) and random projections (Johnson & Lindenstrauss, 1984) to construct a gradient datastore with lowdimensional, easily manipulable gradient features that permit efficient and effective dataset selection. The gradient datastore can be reused for new target tasks. 3. Transferable (§5.3): Data selected using small models’ gradient features induce strong performance in large models and models from different families, adding to the efficiency of LESS (Table 2). 4. Interpretable (§6.2): Qualitative analysis shows that LESS selects data with similar reasoning and skill types as the target task, whereas existing approaches often select data based on surface form cues (e.g., language or topic). We evaluate our approach on three diverse downstream datasets—MMLU (Hendrycks et al., 2020), TY- DIQA (Clark et al., 2020), and BBH (Suzgun et al., 2023)—each containing distinct subtasks that effectively simulate targeted instruction tuning scenarios. Results show that LESS often selects a small subset of the data (5%) that outperforms training on the full dataset, and the selected subset remains universally effective across model scales and families (Table 2). Comparisons with other data selection methods show that LESS is the only consistently effective approach, justifying its relatively high computational cost.	https://arxiv.org/pdf/2410.11303	TSDS: Data Selection for Task-Specific Model Finetuning	Finetuning foundation models for specific tasks is an emerging paradigm in modern machine learning. The efficacy of task-specific finetuning largely depends on the selection of appropriate training data. We present TSDS (Task-Specific Data Selection), a framework to select data for task-specific model finetuning, guided by a small but representative set of examples from the target task. To do so, we formulate data selection for task-specific finetuning as an optimization problem with a distribution alignment loss based on optimal transport to capture the discrepancy between the selected data and the target distribution. In addition, we add a regularizer to encourage the diversity of the selected data and incorporate kernel density estimation into the regularizer to reduce the negative effects of near-duplicates among the candidate data. We connect our optimization problem to nearest neighbor search and design efficient algorithms to compute the optimal solution based on approximate nearest neighbor search techniques. We evaluate our method on data selection for both continued pretraining and instruction tuning of language models. We show that instruction tuning using data selected by our method with a 1% selection ratio often outperforms using the full dataset and beats the baseline selection methods by 1.5 points in F1 score on average. Our code is available at https://github.com/ZifanL/TSDS.	inetuning foundation models [3 ] is the de-facto paradigm for building machine learning applications that focus on specific tasks. Models such as BERT [10] and LLaMA [ 43] are large-scale models pretrained on massive unlabeled data across a wide range of domains. Those models can be specialized to downstream tasks through finetuning. Finetuning can take a variety of forms depending on the target task. For instance, continued pretraining [ 17 ] extends the pretraining stage of a model on a dataset that is more closely related to a target domain. As another setting, instruction tuning [51 ] trains a generative foundation model on instruction-response pairs to improve its performance in responding to task-specific instructions. Finetuning foundation models can lead to significant improvement in downstream tasks, but the effectiveness heavily relies on the right choice of training data [ 17 , 30 , 48 , 47 ]. However, the data repositories that one considers during training of generative models tend to be large—consider for example the use of Common Crawl, which contains 250 billion web pages, or The Pile [14 ]—and hence, it is impractical to manually select the data that are distributed like the use cases in the target task. Therefore, automated task-specific data selection becomes critical. In this paper, we propose TSDS (Task-Specific Data Selection), a framework to select data for taskspecific model finetuning. We consider the scenario of finetuning a foundation model to customize it for a specific task characterized by a few representative examples. The input to our framework is the representative examples and a massive repository of candidate data. Guided by the representative examples, we select training data from the repository for task-specific finetuning. We identify the following requirements for our framework. (Distribution Alignment) First, the distribution of the selected data should match the distribution of the representative examples from the target task. Distribution alignment is essential for a model to learn the target distribution and enable data-efficient finetuning for the target task [ 40 ]. Many works [38 , 17, 2, 50 , 47 ] retrieve candidate examples that are most similar to the representative examples. Such heuristics do not ensure distribution alignment between the selected data and the representative examples. A recent work [ 48 ] selects data by importance resampling to match the target distribution but is limited to an n-gram feature space, which cannot capture high-level semantics. (Diversity) Second, the selected data should be diverse so that the model can learn a wide range of related knowledge rather than overfitting to specific examples. In practice, data repositories created by web crawling may contain a large portion of near-duplicates [13 , 28 ] that can compromise diversity and negatively impact model performance [ 28, 19 ]. For example, a study [ 13 ] on several snapshots of ClueWeb and Common Crawl shows that 14% to 52% of the documents are near-duplicates. Previous works [38, 17 , 2 , 50 , 48 , 47 ] on task-specific data selection overlook near-duplicates, leading to the over-representation of such examples in the selected data. We require our framework to ensure diversity in selection even when a large fraction of the candidate examples are near-duplicates. (Scalability) Finally, the selection algorithm should be efficient, considering the increasing scale of modern data repositories. The high volume of candidate data (e.g., 250 billion pages in Common Crawl) poses a great challenge to efficient selection. Our framework formulates task-specific data selection as an optimization problem that allows a smooth trade-off between two crucial objectives: distribution alignment and diversity. The solution to the optimization problem is a categorical distribution assigned to the candidates which we will sample from. In the optimization objective, we use optimal transport to measure the discrepancy between the distribution assigned to the candidates and the target distribution, encouraging the alignment between them. We show that the optimization problem admits efficient algorithms to compute the optimal solution. In addition, our framework supports distribution alignment in any metric space that supports efficient nearest-neighbor search, including model-agnostic semantic embedding and model-specific features such as gradients. Our contributions: 1) We formulate data selection for task-specific finetuning as an optimization problem based on optimal transport for distribution alignment, with a regularization term that encourages diversity. 2) We make our framework robust to near-duplicates by incorporating kernel density estimation [36 ] into the regularization term. 3) We show the connection between the optimal solution to the optimization problem and nearest neighbor search, which allows us to develop efficient algorithms employing approximate nearest-neighbor search techniques [23]. We conduct extensive experiments to validate the effectiveness of our framework. We focus on natural language processing tasks where foundation models have shown great advancements. We show that our framework beats the state-of-the-art baseline [47 ] by 1.5 points in F1 score on average with a selection ratio of 1% on instruction tuning for two modern large language models on three tasks. In addition, continued pretraining using domain-specific data selected by our framework outperforms the other selection methods by up to 3 F1 points on four classification tasks from various domains. We also demonstrate that our framework is robust to near-duplicates in the data repository, maintaining consistent performance when 1% of the candidate examples are duplicate for up to 1,000 times. Our method is efficient, taking 28 hours to preprocess a corpus of 150M examples and less than 1 hour for each task-specific selection.	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2402.04333	LESS: Selecting Influential data for Targeted Instruction Tuning	Instruction tuning has unlocked powerful capabilities in large language models (LLMs), using combined datasets to develop general-purpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application. To facilitate future work, we release code and data at princetonnlp/LESS.	Instruction tuning has made large language models (LLMs) adept at following human instructions (Ouyang et al., 2022) as versatile chatbots (OpenAI, 2022; 2023; Anthropic, 2023; Google, 2023). Recent efforts curating highly diverse and wide-ranging instruction tuning datasets (Taori et al., 2023; Wang et al.; Mukherjee et al., 2023; Xu et al., 2023, inter alia) induce remarkably strong generalization even from a small number of examples (Zhou et al., 2023). Regardless, it remains an open problem to understand how to best utilize these various datasets. Many real-world applications call for cultivating a specific suite of capabilities in LLMs (e.g., reasoning skills). However, training LLMs with mixed instruction tuning datasets can hinder the development of these specific capabilities. For example, Wang et al. (2023b) demonstrates that LLMs trained on a mix of instruction tuning datasets exhibit worse performance than those trained on a subset of the data. Additionally, considering the broad spectrum of user queries and the multitude of skills required to respond to them, there may not always be enough in-domain data available. Therefore, we hope to be able to effectively use the general instruction tuning data to improve specific capabilities. We frame this setting as targeted instruction tuning: Given just a handful of examples embodying a specific capability, how can we effectively select relevant fine-tuning data from a large collection of instruction datasets? We approach this problem by prioritizing training on data that directly minimizes loss on a target task instead of relying on surface form features (Gururangan et al., 2020; Xie et al., 2023b). Inspired by past works estimating the influence of individual training datapoints with gradient information (Pruthi et al., 2020; Han et al., 2023), we design an optimizer-aware approach to select such data. However, straightforward application of this influence formulation faces several challenges unique to the instruction tuning setting: (1) LLMs are traditionally fine-tuned with the Adam optimizer (Kingma & Ba, 2015) instead of the canonical SGD optimizer; (2) using sequence-level gradients of variable-length instruction data can derail the influence estimation; and (3) the large number of trainable parameters in LLMs makes the computation and storage of gradient information extremely resource-intensive. We address these concerns in LESS, an algorithm that performs Low-rank gradiEnt Similarity Search to select relevant instruction tuning data for a target application, which exhibits the following properties: 1. Compatible with instruction tuning with Adam (§2 and §3): LESS adapts the gradient features from classical influence formulations (Pruthi et al., 2020) to work with the Adam optimizer and variable-length instruction data. The optimization insights and influence formulation may be of independent interest as well. 2. Efficient (§4.1): LESS uses LoRA (Hu et al., 2021) and random projections (Johnson & Lindenstrauss, 1984) to construct a gradient datastore with lowdimensional, easily manipulable gradient features that permit efficient and effective dataset selection. The gradient datastore can be reused for new target tasks. 3. Transferable (§5.3): Data selected using small models’ gradient features induce strong performance in large models and models from different families, adding to the efficiency of LESS (Table 2). 4. Interpretable (§6.2): Qualitative analysis shows that LESS selects data with similar reasoning and skill types as the target task, whereas existing approaches often select data based on surface form cues (e.g., language or topic). We evaluate our approach on three diverse downstream datasets—MMLU (Hendrycks et al., 2020), TY- DIQA (Clark et al., 2020), and BBH (Suzgun et al., 2023)—each containing distinct subtasks that effectively simulate targeted instruction tuning scenarios. Results show that LESS often selects a small subset of the data (5%) that outperforms training on the full dataset, and the selected subset remains universally effective across model scales and families (Table 2). Comparisons with other data selection methods show that LESS is the only consistently effective approach, justifying its relatively high computational cost.	https://arxiv.org/pdf/2501.04155	MM-GEN: Enhancing Task Performance Through Targeted Multimodal Data Curation	Vision-language models (VLMs) are highly effective but often underperform on specialized tasks, for example Llava1.5 struggles on chart and diagram understanding, due to scarce task-specific training data. Existing training data, sourced from general-purpose datasets, fails to capture the nuanced details needed for these tasks. We introduce MMGEN, a scalable method that generates task-specific, highquality synthetic text for candidate images by leveraging stronger models. MM-GEN employs a three-stage targeted process: partitioning data into subgroups, generating targeted text based on task descriptions, and filtering out redundant and outlier data. Fine-tuning VLMs with data generated by MM-GEN leads to significant performance gains, including 29% on spatial reasoning and 15% on diagram understanding for Llava-1.5 (7B). Compared to human-curated caption data, MM-GEN achieves up to 1.6× better improvements for the original models, proving its effectiveness in enhancing task-specific VLM performance and bridging the gap between general-purpose datasets and specialized requirements. Code available at https://github.com/sjoshi804/MM-Gen.	While vision-language models (VLMs) demonstrate stateof-the-art performance on several multi-modal tasks [29], they are often on tasks that are simpler in nature [3]. These models still struggle with more complex tasks, e.g., those that require fine-grained understanding of details in images [3, 15, 21, 49]. We posit that the primary limitation for these VLMs is the quality and nature of the training data. VLMs are typically trained on large-scale image-text data scraped from the internet; while the images are rich and informative, the associated text descriptions can (i) have limited relevance to the image [39], or (ii) omit references to several specific details captured in the image [25]. Fig. 1 shows examples of such images and web-scraped captions. While the images are relevant for the tasks of chart understanding, spatial reasoning, and diagram understanding, respectively, the text fails to capture details essential for these tasks. While synthetic caption generation strategies proposed in prior work [25, 39, 57] can create more descriptive text annotations (by referring to more visual details), they are agnostic of the downstream target task. Consequently, they cannot ensure that relevant details are captured in the text annotations. Recently, Shi et al. [48] manually curated a task-specific dataset aimed at the task of multimodal mathematical question-answering by augmenting existing imagetext data with detailed textual annotations, based on their domain expertise, using strong VLMs. While effective, such a curation pipeline involves significant human effort and is not scalable [33, 59]. To address these limitations, we present MM-GEN, a highly general framework for automatically synthesizing taskrelevant text annotations for images by leveraging stronger VLMs (i.e., VLMs that perform well on the specific task) and requiring minimal human effort. MM-GEN takes as input a small set of examples from the target task (henceforth referred to as "reference samples"), a list of image types associated with the task, and a pool of task-relevant candidate images for training. In practice, these inputs can be easily obtained: a small number of reference samples and associated image types can be directly collected from the target task, and a task-relevant image pool can be found via image search with search engines or retrieved from largescale image-caption datasets [5, 46, 47]. Using the reference samples to specify the details of the task to the stronger VLM, MM-GEN generates text-annotations that are taskrelevant for the candidate images. Fig. 1 shows how the text generated by MM-GEN captures task-relevant details. This simple approach is highly effective, resulting in significant improvements across a variety of target tasks. Moreover, human-effort in this process is limited to providing (i) a small set of reference samples for the task, and (ii) a pool of candidate images. To further improve the quality and efficacy of the generated data, MM-GEN introduces a perplexity [4] based data-filtering approach to improve performance on target task using a high-value subset of the synthesized data. The components in MM-GEN are general and applicable to any image-text based target task enabling it to easily generalize across tasks and scale. We evaluate MM-GEN on improving VLMs' (e.g., Llava1.5 7B and 13B parameter versions) performance on finegrained image understanding tasks — chart understanding and reasoning, diagram understanding, and spatial reasoning on maps. The data curated by MM-GEN enables an absolute improvement over Llava-1.5 (7B) of 15%, 14% and 29%, respectively. We also see improvements over the much larger Llava-1.5 (13B) of 13%, , respectively. Moreover, MM-GEN’s filtering strategy helps in shrinking data volumes by up to 50% with no / minimal loss in performance. Empirical results show that models trained via MM-GEN data have a better performance than those trained via generated generic captions. MM-GEN data is also more effective than text annotations generated without task-specific reference examples, showing the importance of a targeted, data-centric approach for describing tasks. Finally, we analyze the effects of key design choices in MMGEN through ablation studies on e.g. size of the reference sample set, generating with / without partitioning, scaling number of in-context samples.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2402.04333	LESS: Selecting Influential data for Targeted Instruction Tuning	Instruction tuning has unlocked powerful capabilities in large language models (LLMs), using combined datasets to develop general-purpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application. To facilitate future work, we release code and data at princetonnlp/LESS.	Instruction tuning has made large language models (LLMs) adept at following human instructions (Ouyang et al., 2022) as versatile chatbots (OpenAI, 2022; 2023; Anthropic, 2023; Google, 2023). Recent efforts curating highly diverse and wide-ranging instruction tuning datasets (Taori et al., 2023; Wang et al.; Mukherjee et al., 2023; Xu et al., 2023, inter alia) induce remarkably strong generalization even from a small number of examples (Zhou et al., 2023). Regardless, it remains an open problem to understand how to best utilize these various datasets. Many real-world applications call for cultivating a specific suite of capabilities in LLMs (e.g., reasoning skills). However, training LLMs with mixed instruction tuning datasets can hinder the development of these specific capabilities. For example, Wang et al. (2023b) demonstrates that LLMs trained on a mix of instruction tuning datasets exhibit worse performance than those trained on a subset of the data. Additionally, considering the broad spectrum of user queries and the multitude of skills required to respond to them, there may not always be enough in-domain data available. Therefore, we hope to be able to effectively use the general instruction tuning data to improve specific capabilities. We frame this setting as targeted instruction tuning: Given just a handful of examples embodying a specific capability, how can we effectively select relevant fine-tuning data from a large collection of instruction datasets? We approach this problem by prioritizing training on data that directly minimizes loss on a target task instead of relying on surface form features (Gururangan et al., 2020; Xie et al., 2023b). Inspired by past works estimating the influence of individual training datapoints with gradient information (Pruthi et al., 2020; Han et al., 2023), we design an optimizer-aware approach to select such data. However, straightforward application of this influence formulation faces several challenges unique to the instruction tuning setting: (1) LLMs are traditionally fine-tuned with the Adam optimizer (Kingma & Ba, 2015) instead of the canonical SGD optimizer; (2) using sequence-level gradients of variable-length instruction data can derail the influence estimation; and (3) the large number of trainable parameters in LLMs makes the computation and storage of gradient information extremely resource-intensive. We address these concerns in LESS, an algorithm that performs Low-rank gradiEnt Similarity Search to select relevant instruction tuning data for a target application, which exhibits the following properties: 1. Compatible with instruction tuning with Adam (§2 and §3): LESS adapts the gradient features from classical influence formulations (Pruthi et al., 2020) to work with the Adam optimizer and variable-length instruction data. The optimization insights and influence formulation may be of independent interest as well. 2. Efficient (§4.1): LESS uses LoRA (Hu et al., 2021) and random projections (Johnson & Lindenstrauss, 1984) to construct a gradient datastore with lowdimensional, easily manipulable gradient features that permit efficient and effective dataset selection. The gradient datastore can be reused for new target tasks. 3. Transferable (§5.3): Data selected using small models’ gradient features induce strong performance in large models and models from different families, adding to the efficiency of LESS (Table 2). 4. Interpretable (§6.2): Qualitative analysis shows that LESS selects data with similar reasoning and skill types as the target task, whereas existing approaches often select data based on surface form cues (e.g., language or topic). We evaluate our approach on three diverse downstream datasets—MMLU (Hendrycks et al., 2020), TY- DIQA (Clark et al., 2020), and BBH (Suzgun et al., 2023)—each containing distinct subtasks that effectively simulate targeted instruction tuning scenarios. Results show that LESS often selects a small subset of the data (5%) that outperforms training on the full dataset, and the selected subset remains universally effective across model scales and families (Table 2). Comparisons with other data selection methods show that LESS is the only consistently effective approach, justifying its relatively high computational cost.	https://arxiv.org/pdf/1708.00489	Active Learning for Convolutional Neural Networks: A Core-Set Approach	Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (i.e. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.	Deep convolutional neural networks (CNNs) have shown unprecedented success in many areas of research in computer vision and pattern recognition, such as image classification, object detection, and scene segmentation. Although CNNs are universally successful in many tasks, they have a major drawback; they need a very large amount of labeled data to be able to learn their large number of parameters. More importantly, it is almost always better to have more data since the accuracy of CNNs is often not saturated with increasing dataset size. Hence, there is a constant desire to collect more and more data. Although this a desired behavior from an algorithmic perspective (higher representative power is typically better), labeling a dataset is a time consuming and an expensive task. These practical considerations raise a critical question: "what is the optimal way to choose data points to label such that the highest accuracy can be obtained given a fixed labeling budget." Active learning is one of the common paradigms to address this question. The goal of active learning is to find effective ways to choose data points to label, from a pool of unlabeled data points, in order to maximize the accuracy. Although it is not possible to obtain a universally good active learning strategy (Dasgupta, 2004), there exist many heuristics (Settles, 2010) which have been proven to be effective in practice. Active learning is typically an iterative process in which a model is learned at each iteration and a set of points is chosen to be labelled from a pool of unlabelled points using these aforementioned heuristics. We experiment with many of these heuristics in this paper and find them not effective when applied to CNNs. We argue that the main factor behind this ineffectiveness is the correlation caused via batch acquisition/sampling. In the classical setting, the active learning algorithms typically choose a single point at each iteration; however, this is not feasible for CNNs since i) a single point is likely to have no statistically significant impact on the accuracy due to the local optimization methods, and ii) each iteration requires a full training until convergence which makes it intractable to query labels one-by-one. Hence, it is necessary to query labels for a large subset at each iteration and it results in correlated samples even for moderately small subset sizes. In order to tailor an active learning method for the batch sampling case, we decided to define the active learning as core-set selection problem. Core-set selection problem aims to find a small subset given a large labeled dataset such that a model learned over the small subset is competitive over the whole dataset. Since we have no labels available, we perform the core-set selection without using the labels. In order to attack the unlabeled core-set problem for CNNs, we provide a rigorous bound between an average loss over any given subset of the dataset and the remaining data points via the geometry of the data points. As an active learning algorithm, we try to choose a subset such that this bound is minimized. Moreover, minimization of this bound turns out to be equivalent to the k-Center problem (Wolf, 2011) and we adopt an efficient approximate solution to this combinatorial optimization problem. We further study the behavior of our proposed algorithm empirically for the problem of image classification using three different datasets. Our empirical analysis demonstrates state-of-the-art performance by a large margin.	1	1
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2402.04333	LESS: Selecting Influential data for Targeted Instruction Tuning	Instruction tuning has unlocked powerful capabilities in large language models (LLMs), using combined datasets to develop general-purpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application. To facilitate future work, we release code and data at princetonnlp/LESS.	Instruction tuning has made large language models (LLMs) adept at following human instructions (Ouyang et al., 2022) as versatile chatbots (OpenAI, 2022; 2023; Anthropic, 2023; Google, 2023). Recent efforts curating highly diverse and wide-ranging instruction tuning datasets (Taori et al., 2023; Wang et al.; Mukherjee et al., 2023; Xu et al., 2023, inter alia) induce remarkably strong generalization even from a small number of examples (Zhou et al., 2023). Regardless, it remains an open problem to understand how to best utilize these various datasets. Many real-world applications call for cultivating a specific suite of capabilities in LLMs (e.g., reasoning skills). However, training LLMs with mixed instruction tuning datasets can hinder the development of these specific capabilities. For example, Wang et al. (2023b) demonstrates that LLMs trained on a mix of instruction tuning datasets exhibit worse performance than those trained on a subset of the data. Additionally, considering the broad spectrum of user queries and the multitude of skills required to respond to them, there may not always be enough in-domain data available. Therefore, we hope to be able to effectively use the general instruction tuning data to improve specific capabilities. We frame this setting as targeted instruction tuning: Given just a handful of examples embodying a specific capability, how can we effectively select relevant fine-tuning data from a large collection of instruction datasets? We approach this problem by prioritizing training on data that directly minimizes loss on a target task instead of relying on surface form features (Gururangan et al., 2020; Xie et al., 2023b). Inspired by past works estimating the influence of individual training datapoints with gradient information (Pruthi et al., 2020; Han et al., 2023), we design an optimizer-aware approach to select such data. However, straightforward application of this influence formulation faces several challenges unique to the instruction tuning setting: (1) LLMs are traditionally fine-tuned with the Adam optimizer (Kingma & Ba, 2015) instead of the canonical SGD optimizer; (2) using sequence-level gradients of variable-length instruction data can derail the influence estimation; and (3) the large number of trainable parameters in LLMs makes the computation and storage of gradient information extremely resource-intensive. We address these concerns in LESS, an algorithm that performs Low-rank gradiEnt Similarity Search to select relevant instruction tuning data for a target application, which exhibits the following properties: 1. Compatible with instruction tuning with Adam (§2 and §3): LESS adapts the gradient features from classical influence formulations (Pruthi et al., 2020) to work with the Adam optimizer and variable-length instruction data. The optimization insights and influence formulation may be of independent interest as well. 2. Efficient (§4.1): LESS uses LoRA (Hu et al., 2021) and random projections (Johnson & Lindenstrauss, 1984) to construct a gradient datastore with lowdimensional, easily manipulable gradient features that permit efficient and effective dataset selection. The gradient datastore can be reused for new target tasks. 3. Transferable (§5.3): Data selected using small models’ gradient features induce strong performance in large models and models from different families, adding to the efficiency of LESS (Table 2). 4. Interpretable (§6.2): Qualitative analysis shows that LESS selects data with similar reasoning and skill types as the target task, whereas existing approaches often select data based on surface form cues (e.g., language or topic). We evaluate our approach on three diverse downstream datasets—MMLU (Hendrycks et al., 2020), TY- DIQA (Clark et al., 2020), and BBH (Suzgun et al., 2023)—each containing distinct subtasks that effectively simulate targeted instruction tuning scenarios. Results show that LESS often selects a small subset of the data (5%) that outperforms training on the full dataset, and the selected subset remains universally effective across model scales and families (Table 2). Comparisons with other data selection methods show that LESS is the only consistently effective approach, justifying its relatively high computational cost.	https://arxiv.org/pdf/2301.13287	MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning	Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results indicate that MILO can train models 3 x -10x faster and tune hyperparameters 20 x -75x faster than full-dataset training or tuning without compromising performance.	Deep learning has achieved remarkable success in a multitude of machine learning tasks, including natural language processing, computer vision, and speech recognition in recent years. This success is partially due to the availability of massive training datasets and the capacity to train large-scale neural networks. However, training deep models on extensive datasets is computationally demanding, incurring significant financial costs and generating substantial CO2 emissions [63, 59 ]. Bhavya et al. [ 2] overview several research trajectories aimed at enhancing model convergence and reducing training time and costs, including data subset selection, curriculum learning, model architecture improvements, and optimization algorithm enhancements. Our work specifically focuses on selecting useful, generalizable data subsets for efficient deep neural network training. Recent research [66 , 5 , 21 ] suggests that many current training datasets are redundant, indicating that a non-redundant, informative subset could achieve comparable performance to training on the full dataset. To identify such informative samples, metrics such as prediction uncertainty [ 7], prediction flips [ 66 , 72 ], loss [42 ], or gradient/gradient-norm [ 21, 47 , 28, 27 , 29 ] have been applied. These metrics, calculated using either the downstream machine learning model or a lightweight surrogate [ 7], require a fully converged model, a requirement that runs counter to the goal of efficient training. To address this, existing subset selection algorithms [ 47 , 28 , 27, 29, 30 , 54 ] for efficient learning utilize the downstream model during training for heuristic computation and periodically update the subset as the model-dependent metrics for each sample evolve. Drawbacks of Model-Dependent Subset Selection: Despite the theoretical advantages of existing subset selection strategies[ 28 , 27, 47 ], they often fall short in computational efficiency compared to adaptive random subset selection, which selects random subsets periodically. This is primarily because traditional approaches rely on downstream models and typically require the calculation of sample metrics, such as gradients, before each subset selection step. Furthermore, these computationally demanding subset selection steps occur during model training. For instance, Figure 1 compares the convergence rate of the ResNet18 model on the CIFAR100 dataset, in terms of both time and epochs. Here, the model uses 10% subsets chosen every epoch by GradMatchPB [ 27 ], a state-of-the-art data subset selection strategy for efficient training, CraigPB [ 47 ], and Adaptive-Random (where a new 10% subset is randomly selected periodically). The selection of a new subset every epoch is done to demonstrate the maximum performance attainable by GradMatchPB and CraigPB. The results indicate that GradMatchPB achieves faster epoch convergence than both Adaptive-Random and CraigPB when selecting a new subset each epoch. However, due to the necessity for a computationally intensive subset selection step every epoch, both GradMatchPB and CraigPB demonstrate considerable inefficiency in terms of training time. In their respective studies, Killamsetty et al. [27 ] and Mirzasoleiman et al. [47] suggested selecting a new subset every R epochs to enhance training efficiency. However, this comes at the cost of the model’s convergence rate. Lastly, model-dependent subset selection requires a computationally intensive subset selection steps each time a new model is trained. Model-Agnostic Selection: Our primary insight is that a model-agnostic subset selection framework can circumvent the computationally intensive subset selection steps during model training by conducting subset selection in the preprocessing phase. By pre-selecting subsets and storing them as metadata with each dataset, we can train numerous models without incurring additional costs, effectively spreading out the expense of subset selection. In this work, we endeavor to answer the following question: Is it possible to develop a model-agnostic subset selection method that selects new subsets in a minimal amount of time, yet achieves superior model convergence without significant compromise in test accuracy or generalization performance?  MILO Framework: In this study, we introduce MILO, a model-agnostic subset selection framework designed for efficient model training and tuning. MILO employs submodular measures [ 13 , 23 ], which capture higher-order interactions between data samples for subset selection. We utilize pre-trained large language models [ 55 ] and pre-trained vision transformers [25 ] as feature encoders due to their zero-shot feature encoding capabilities. These encoders compute the sample metrics in a nominal amount of time, and this computation is independent of the downstream model, rendering MILO model-agnostic. Figure 3 provides a visual representation of MILO for model training, comprising two steps: a) A pre-processing step that involves the selection of multiple subsets from the training dataset using "Stochastic-Greedy Exploration (SGE)" (refer to Section 3.1.1), and the construction of a probability distribution over the entire dataset for subset sampling through "Weighted Random Exploration (WRE)" (refer to Section 3.1.2); b) A model training scheme that involves training the model on a curriculum of easy-to-hard subsets (refer to Section 3.1.3) using the subsets selected and the probability distribution constructed in the pre-processing step. We also present a class-wise partitioning trick to significantly minimize the memory footprint of MILO (refer to Section 3.2). Effectiveness of MILO : Through extensive experiments on multiple real-world datasets, we empirically demonstrate the effectiveness of the MILO framework for efficient training and hyperparameter tuning. In Figure 2, we provide a summary of the speedup achieved by MILO compared to full data training and tuning, along with the corresponding relative performance. Our results show that MILO can train models 3x to 10x faster and tune hyperparameters 20x to 75x faster, with minimal loss in performance. Furthermore, we consistently outperform the subset selection baselines that were considered. Additionally, MILO exhibits significantly faster initial model convergence and maintains faster convergence throughout training compared to other baselines. These qualities make it highly effective for applications such as hyperparameter tuning and neural architecture search, where the ability to distinguish good models from bad models early in the tuning process is crucial for enhancing efficiency [38, 37]. Related Work: In recent years, data-subset selection strategies have achieved success in various machine learning applications such as speech recognition [ 68 , 67], machine translation [ 32 ], active learning [ 60 , 1 , 33 ], hyper-parameter tuning [ 30 ], continual learning [65 ], domain adaptation [20 ], and computer vision [22]. A recent empirical study [5] demonstrated that datasets frequently contain semantic redundancies. These redundancies can be removed during training without affecting performance. In response, data pruning methods [66 , 51, 62] have developed principled selection criteria for pruning redundant samples before model training, with minimal loss in performance. However, both existing data pruning and compute-efficient learning approaches are model-dependent, resulting in them being computationally expensive. In contrast, our method is model-agnostic and capable of identifying high-quality subsets by intelligent data exploration. Furthermore, our approach can achieve superior model convergence by utilizing an easy-to-hard curriculum, transitioning from representative to diverse subsets during training.	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2402.04333	LESS: Selecting Influential data for Targeted Instruction Tuning	Instruction tuning has unlocked powerful capabilities in large language models (LLMs), using combined datasets to develop general-purpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application. To facilitate future work, we release code and data at princetonnlp/LESS.	Instruction tuning has made large language models (LLMs) adept at following human instructions (Ouyang et al., 2022) as versatile chatbots (OpenAI, 2022; 2023; Anthropic, 2023; Google, 2023). Recent efforts curating highly diverse and wide-ranging instruction tuning datasets (Taori et al., 2023; Wang et al.; Mukherjee et al., 2023; Xu et al., 2023, inter alia) induce remarkably strong generalization even from a small number of examples (Zhou et al., 2023). Regardless, it remains an open problem to understand how to best utilize these various datasets. Many real-world applications call for cultivating a specific suite of capabilities in LLMs (e.g., reasoning skills). However, training LLMs with mixed instruction tuning datasets can hinder the development of these specific capabilities. For example, Wang et al. (2023b) demonstrates that LLMs trained on a mix of instruction tuning datasets exhibit worse performance than those trained on a subset of the data. Additionally, considering the broad spectrum of user queries and the multitude of skills required to respond to them, there may not always be enough in-domain data available. Therefore, we hope to be able to effectively use the general instruction tuning data to improve specific capabilities. We frame this setting as targeted instruction tuning: Given just a handful of examples embodying a specific capability, how can we effectively select relevant fine-tuning data from a large collection of instruction datasets? We approach this problem by prioritizing training on data that directly minimizes loss on a target task instead of relying on surface form features (Gururangan et al., 2020; Xie et al., 2023b). Inspired by past works estimating the influence of individual training datapoints with gradient information (Pruthi et al., 2020; Han et al., 2023), we design an optimizer-aware approach to select such data. However, straightforward application of this influence formulation faces several challenges unique to the instruction tuning setting: (1) LLMs are traditionally fine-tuned with the Adam optimizer (Kingma & Ba, 2015) instead of the canonical SGD optimizer; (2) using sequence-level gradients of variable-length instruction data can derail the influence estimation; and (3) the large number of trainable parameters in LLMs makes the computation and storage of gradient information extremely resource-intensive. We address these concerns in LESS, an algorithm that performs Low-rank gradiEnt Similarity Search to select relevant instruction tuning data for a target application, which exhibits the following properties: 1. Compatible with instruction tuning with Adam (§2 and §3): LESS adapts the gradient features from classical influence formulations (Pruthi et al., 2020) to work with the Adam optimizer and variable-length instruction data. The optimization insights and influence formulation may be of independent interest as well. 2. Efficient (§4.1): LESS uses LoRA (Hu et al., 2021) and random projections (Johnson & Lindenstrauss, 1984) to construct a gradient datastore with lowdimensional, easily manipulable gradient features that permit efficient and effective dataset selection. The gradient datastore can be reused for new target tasks. 3. Transferable (§5.3): Data selected using small models’ gradient features induce strong performance in large models and models from different families, adding to the efficiency of LESS (Table 2). 4. Interpretable (§6.2): Qualitative analysis shows that LESS selects data with similar reasoning and skill types as the target task, whereas existing approaches often select data based on surface form cues (e.g., language or topic). We evaluate our approach on three diverse downstream datasets—MMLU (Hendrycks et al., 2020), TY- DIQA (Clark et al., 2020), and BBH (Suzgun et al., 2023)—each containing distinct subtasks that effectively simulate targeted instruction tuning scenarios. Results show that LESS often selects a small subset of the data (5%) that outperforms training on the full dataset, and the selected subset remains universally effective across model scales and families (Table 2). Comparisons with other data selection methods show that LESS is the only consistently effective approach, justifying its relatively high computational cost.	https://arxiv.org/pdf/2103.00128	PRISM: A Rich Class of Parameterized Submodular Information Measures for Guided Subset Selection	With ever-increasing dataset sizes, subset selection techniques are becoming increasingly important for a plethora of tasks. It is often necessary to guide the subset selection to achieve certain desiderata, which includes focusing or targeting certain data points, while avoiding others. Examples of such problems include: i) targeted learning, where the goal is to find subsets with rare classes or rare attributes on which the model is underperforming, and ii) guided summarization, where data (e.g., image collection, text, document or video) is summarized for quicker human consumption with specific additional user intent. Motivated by such applications, we present PRISM, a rich class of PaRameterIzed Submodular information Measures. Through novel functions and their parameterizations, PRISM offers a variety of modeling capabilities that enable a trade-off between desired qualities of a subset like diversity or representation and similarity/dissimilarity with a set of data points. We demonstrate how PRISM can be applied to the two real-world problems mentioned above, which require guided subset selection. In doing so, we show that PRISM interestingly generalizes some past work, therein reinforcing its broad utility. Through extensive experiments on diverse datasets, we demonstrate the superiority of PRISM over the state-of-the-art in targeted learning and in guided imagecollection summarization. PRISM is available as a part of the SUBMODLIB (https://github.com/decile-team/submodlib) and TRUST (https://github.com/decile-team/trust) toolkits.	Recent times have seen explosive growth in data across several modalities, including text, images, and videos. This has given rise to the need for finding techniques for selecting effective smaller data subsets with specific characteristics for a variety of down-stream tasks. Often, we would like to guide the data selection to either target or avoid a certain set of data slices. One application is, what we call, targeted learning, where the goal is to select data points similar to data slices on which the model is currently performing poorly. These slices are data points that either belong to rare classes or have common rare attributes (e.g., color, background, etc.). An example of such a scenario is shown in Fig. 1(a), where a self-driving car model struggles in detecting “cars in a dark background“ because of a lack of such images in the training set. The targeted learning problem is to augment the training dataset with more of such rare images, with an aim to improve model performance. Another example is detecting cancers in biomedical imaging datasets, where the number of cancerous images are often a small fraction of the non-cancerous images. Another application comes from the summarization task, where an image collection, a video, or a text document is summarized for quicker human consumption by eliminating redundancy, while preserving the main content. While a number of applications require generic summarization (i.e., simply picking a representative and diverse subset of the massive dataset), it is often important to capture certain user intent in summarization. We call this guided summarization. Examples of guided summarization include: (i) query-focused summarization (Sharghi, Gong, and Shah 2016; Xiao et al. 2020), where a summary similar to a specific query is desired, and (ii) privacy-preserving summarization, where a summary dissimilar to a given private set of data points is desired (say, for privacy issues). See Fig. 1(b) for a pictorial illustration. PRISM Framework: We define PRISM through different instantiations and parameterizations of various submodular information measures (Sec. 2). These allow for modeling a spectrum of semantics required for guided subset selection, like relevance to a query set, irrelevance to a private set, and diversity among selected data points. We study the effect of parameter trade-off among these different semantics and present interesting insights. PRISM for Targeted Learning: We present a novel algorithm (Sec. 3.1, Algo. 1) to apply PRISM for targeted learning, which aims to improve a model’s performance on rare slices of data. Specifically, we show that submodular information measures are very effective in finding the examples from the rare classes in a large unlabeled set (akin to finding a needle in a haystack). On several image classification tasks, PRISM obtains ≈ 20-30% gain in accuracy of rare classes (≈ 12% more than existing approaches) by just adding a few additional labeled points from the rare classes. Furthermore, we show that PRISM is 20× to 50× more label-efficient compared to random sampling, and 2× to 4× more labelefficient compared to existing approaches (see Sec. 4.1). We also show that Algo. 1 generalizes some existing approaches for data subset selection, reinforcing its utility (Sec. 3.3). PRISM for Guided Summarization. We propose a learning framework for guided summarization using PRISM (Sec. 3.2). We show that PRISM offers a unified treatment to the different flavors of guided summarization (query-focused and privacy-preserving) and generalizes some existing approaches to summarization, again reinforcing its utility. We show that it outperforms other existing approaches on a real-world image collections dataset (Sec. 4.2).	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2402.04333	LESS: Selecting Influential data for Targeted Instruction Tuning	Instruction tuning has unlocked powerful capabilities in large language models (LLMs), using combined datasets to develop general-purpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application. To facilitate future work, we release code and data at princetonnlp/LESS.	Instruction tuning has made large language models (LLMs) adept at following human instructions (Ouyang et al., 2022) as versatile chatbots (OpenAI, 2022; 2023; Anthropic, 2023; Google, 2023). Recent efforts curating highly diverse and wide-ranging instruction tuning datasets (Taori et al., 2023; Wang et al.; Mukherjee et al., 2023; Xu et al., 2023, inter alia) induce remarkably strong generalization even from a small number of examples (Zhou et al., 2023). Regardless, it remains an open problem to understand how to best utilize these various datasets. Many real-world applications call for cultivating a specific suite of capabilities in LLMs (e.g., reasoning skills). However, training LLMs with mixed instruction tuning datasets can hinder the development of these specific capabilities. For example, Wang et al. (2023b) demonstrates that LLMs trained on a mix of instruction tuning datasets exhibit worse performance than those trained on a subset of the data. Additionally, considering the broad spectrum of user queries and the multitude of skills required to respond to them, there may not always be enough in-domain data available. Therefore, we hope to be able to effectively use the general instruction tuning data to improve specific capabilities. We frame this setting as targeted instruction tuning: Given just a handful of examples embodying a specific capability, how can we effectively select relevant fine-tuning data from a large collection of instruction datasets? We approach this problem by prioritizing training on data that directly minimizes loss on a target task instead of relying on surface form features (Gururangan et al., 2020; Xie et al., 2023b). Inspired by past works estimating the influence of individual training datapoints with gradient information (Pruthi et al., 2020; Han et al., 2023), we design an optimizer-aware approach to select such data. However, straightforward application of this influence formulation faces several challenges unique to the instruction tuning setting: (1) LLMs are traditionally fine-tuned with the Adam optimizer (Kingma & Ba, 2015) instead of the canonical SGD optimizer; (2) using sequence-level gradients of variable-length instruction data can derail the influence estimation; and (3) the large number of trainable parameters in LLMs makes the computation and storage of gradient information extremely resource-intensive. We address these concerns in LESS, an algorithm that performs Low-rank gradiEnt Similarity Search to select relevant instruction tuning data for a target application, which exhibits the following properties: 1. Compatible with instruction tuning with Adam (§2 and §3): LESS adapts the gradient features from classical influence formulations (Pruthi et al., 2020) to work with the Adam optimizer and variable-length instruction data. The optimization insights and influence formulation may be of independent interest as well. 2. Efficient (§4.1): LESS uses LoRA (Hu et al., 2021) and random projections (Johnson & Lindenstrauss, 1984) to construct a gradient datastore with lowdimensional, easily manipulable gradient features that permit efficient and effective dataset selection. The gradient datastore can be reused for new target tasks. 3. Transferable (§5.3): Data selected using small models’ gradient features induce strong performance in large models and models from different families, adding to the efficiency of LESS (Table 2). 4. Interpretable (§6.2): Qualitative analysis shows that LESS selects data with similar reasoning and skill types as the target task, whereas existing approaches often select data based on surface form cues (e.g., language or topic). We evaluate our approach on three diverse downstream datasets—MMLU (Hendrycks et al., 2020), TY- DIQA (Clark et al., 2020), and BBH (Suzgun et al., 2023)—each containing distinct subtasks that effectively simulate targeted instruction tuning scenarios. Results show that LESS often selects a small subset of the data (5%) that outperforms training on the full dataset, and the selected subset remains universally effective across model scales and families (Table 2). Comparisons with other data selection methods show that LESS is the only consistently effective approach, justifying its relatively high computational cost.	https://aclanthology.org/2024.findings-naacl.209.pdf	An End-to-End Submodular Framework for Data-Efficient In-Context Learning	Recent advancements in natural language tasks leverage the emergent In-Context Learning (ICL) ability of pretrained Large Language Models (LLMs). ICL enables LLMs to perform new tasks by utilizing a limited number of input-output examples as prompts. While ICL circumvents the costly step of finetuning LLMs, its effectiveness is heavily dependent on the quality and ordering of provided examples (called exemplars). In this work, we propose a two-stage data-efficient framework Div-S3 for exemplar selection for ICL. The first stage focuses on data annotation and employs a poolbased active learning approach to select a set of Diverse and informative exemplars from the target tasks’ unlabeled pool. Given a test input/query, the second stage uses Submodular Span Summarization (S3) to select the most relevant and non-redundant exemplars from the annotated pool of a limited budget. On 7 different NLP datasets and 5 LLMs of varying complexities, we show Div-S3 outperforms (1) existing active learning-based methods for data annotation for ICL and (2) similarity-based methods for test query-specific exemplars retrieval.	Pretrained large language models (LLMs) (Kenton and Toutanova, 2019; Brown et al., 2020; Chowdhery et al., 2022) have become foundational for a wide range of Natural Language Processing (NLP) tasks, demonstrating impressive success across various domains (Bommasani et al., 2021; Bubeck et al., 2023) through in-context learning (ICL) (Dong et al., 2022). ICL enables these pretrained LLMs to perform new tasks by using task-specific prompts containing a limited number of input-output demonstrations (also referred to as shots, exemplars, or prompts) in the natural language format. This approach facilitates deployment across different downstream tasks and reduces the need for labeled downstream training data since ICL does not require any task-specific training. The typical ICL procedure consists of two key components: (1) Exemplar annotation and retrieval (Wu et al., 2022; Köksal et al., 2022; Liu et al., 2022): This step involves annotating and retrieving exemplars that serve as context demonstrations. (2) Prompt template crafting (Sorensen et al., 2022; Deng et al., 2022): this step involves designing a prompt template to wrap these demonstrations in a comprehensible and coherent natural language instruction. Recent studies (Liu et al., 2022; Su et al., 2022; Margatina et al., 2023) show that providing exemplars most relevant to the current input instance is beneficial. Moreover, Zhao et al. (2021), Lu et al. (2022), and Liu et al. (2023) observe that LLMs attend more to the exemplars that are closer in the sequence to the input instance. Therefore, to achieve the best performance of ICL, the selection of exemplars and their ordering in the LLM prompt are crucial. In practice, an extensive collection of unlabeled exemplars is easily available (e.g., posts and discussions on forums like Stack Exchange or user-generated content on social media platforms), but manually annotating all exemplars would be exceptionally costly. To annotate and select the exemplars optimally for a given target task, we follow the two-stage approach shown in Figure 1: (1) Exemplar Annotation: select a subset of exemplars for annotation under a fixed budget (performed only once) and (2) Exemplar Retrieval: identify limited-sized exemplars in an ordering that are most influential for a given input instance from the annotated subset of exemplars. Intuitively, for the first stage, we aim to find the subset with maximal diversity and least redundancy so that, given any input, we can find corresponding labeled exemplars. For the second stage, in addition to the diversity requirement similar to the first stage, we emphasize the relevance of the exemplars to the given input query and order exemplars so that their relevance to the input query decreases as the exemplars are farther away from the input instance. We propose a framework Div-S3 based on submodular optimization that unifies the abovementioned two stages. For Exemplar Annotation, we model the problem as a submodular optimization problem under a cardinality constraint to find as Diverse a subset as possible within a budget. For Exemplar Retrieval, we formalize the problem as a Submodular Span Summarization (S3) problem (Kumari and Bilmes, 2021) with a knapsack constraint, which finds a diverse subset most relevant to the input query under a token length limit. Also, we naturally order the resulting exemplars based on the gains represented by the submodular function. The name of our proposed framework Div-S3 captures the optimization objectives used for both exemplar annotation (Div) and exemplar retrieval (S3) stages. In Fig. 2, we show a sample test query where using Div-S3 for exemplar selection leads to a more diverse and query-relevant exemplar set (more examples provided in Appendix D). Our framework is general, as any submodular function can be plugged into our method. For models beyond LMs, e.g., for text-image multi-modality models, we may use pre-existing submodular functions that are powerful for expressing diversity in the image domain. In addition, we account for relevance, diversity, and ordering for the exemplar retrieval stage, where one or two aspects typically get overlooked by previous methods. Empirically, we evaluate Div-S3 on 7 NLP tasks with 5 LLMs and show significantly improved performance compared to baselines. Our contributions are: 1. We propose an end-to-end framework DivS3 utilizing submodular optimization for performing data-efficient ICL using LLMs. Depending on budget requirements, Div-S3 provides the flexibility to set the budget either in terms of the number of exemplars to be used in the prompt or the LLM’s context window size. 2. We empirically validate the effectiveness of our framework on 7 different NLP tasks and show the transferability of results across LLMs of varying complexities. 3. We thoroughly analyze each component of Div-S3 by (a) studying S3 in a setting with no annotation budget constraint and (b) analyzing the sensitivity of the exemplars selected by Div-S3 to their position in the LLM’s prompt.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/1906.01827	Coresets for Data-Efficient Training of Machine Learning Models	Incremental gradient (IG) methods, such as stochastic gradient descent and its variants are commonly used for large scale optimization in machine learning. Despite the sustained effort to make IG methods more data-efficient, it remains an open question how to select a training data subset that can theoretically and practically perform on par with the full dataset. Here we develop CRAIG, a method to select a weighted subset (or coreset) of training data that closely estimates the full gradient by maximizing a submodular function. We prove that applying IG to this subset is guaranteed to converge to the (near)optimal solution with the same convergence rate as that of IG for convex optimization. As a result, CRAIG achieves a speedup that is inversely proportional to the size of the subset. To our knowledge, this is the first rigorous method for data-efficient training of general machine learning models. Our extensive set of experiments show that CRAIG, while achieving practically the same solution, speeds up various IG methods by up to 6x for logistic regression and 3x for training deep neural networks.	Mathematical optimization lies at the core of training largescale machine learning systems, and is now widely used over massive data sets with great practical success, assuming sufficient data resources are available. Achieving this success, however, also requires large amounts of (often GPU) computing, as well as concomitant financial expenditures and energy usage (Strubell et al., 2019). Significantly decreasing these costs without decreasing the learnt system’s resulting accuracy is one of the grand challenges of machine learning and artificial intelligence today (Asi & Duchi, 2019). Training machine learning models often reduces to optimizing a regularized empirical risk function. Given a convex loss l, and a μ-strongly convex regularizer r, one aims to find model parameter vector w∗ over the parameter space W that minimizes the loss f over the training data V : w∗ ∈ arg minw∈W f (w), f (w) := ∑ i∈V fi(w) + r(w), fi(w) = l(w, (xi, yi)), (1) where V = {1, . . . , n} is an index set of the training data, and functions fi : Rd �' R are associated with training examples (xi, yi), where xi ∈ Rd is the feature vector, and yi is the point i’s label. Standard Gradient Descent can find the minimizer of this problem, but requires repeated computations of the full gradient ∇f (w)—sum of the gradients over all training data points/functions i—and is therefore prohibitive for massive data sets. This issue is further exacerbated in case of deep neural networks where gradient computations (backpropagation) are expensive. Incremental Gradient (IG) methods, such as Stochastic Gradient Descent (SGD) and its accelerated variants, including SGD with momentum (Qian, 1999), Adagrad (Duchi et al., 2011), Adam (Kingma & Ba, 2014), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013) iteratively estimate the gradient on random subsets/batches of training data. While this provides an unbiased estimate of the full gradient, the randomized batches introduce variance in the gradient estimate (Hofmann et al., 2015), and therefore stochastic gradient methods are in general slow to converge (Johnson & Zhang, 2013; Defazio et al., 2014). The majority of the work speeding up IG methods has thus primarily focused on reducing the variance of the gradient estimate (SAGA (Defazio et al., 2014), SVRG (Johnson & Zhang, 2013), Katysha (Allen-Zhu, 2017)) or more carefully selecting the gradient stepsize (Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), Adam (Kingma & Ba, 2014)). However, the direction that remains largely unexplored is how to carefully select a small subset S ⊆ V of the fullCoresets for Data-efficient Training of Machine Learning Models PAGE 2 training data V , so that the model is trained only on the subset S while still (approximately) converging to the globally optimal solution (i.e., the model parameters that would be obtained if training/optimizing on the full V ). If such a subset S can be quickly found, then this would directly lead to a speedup of |V |/|S| (which can be very large if |S| ≪ |V |) per epoch of IG. There are four main challenges in finding such a subset S. First is that a guiding principle for selecting S is unclear. For example, selecting training points close to the decision boundary might allow the model to fine tune the decision boundary, while picking the most diverse set of data points would allow the model to get a better sense of the training data distribution. Second is that finding S must be fast, as otherwise identifying the set S may take longer than the actual optimization, and so no overall speed-up would be achieved. Third is that finding a subset S is not enough. One also has to decide on a gradient stepsize for each data point in S, as they affect the convergence. And last, while the method might work well empirically on some data sets, one also requires theoretical understanding and mathematical convergence guarantees. Here we develop Coresets for Accelerating Incremental Gradient descent (CRAIG), for selecting a subset of training data points to speed up training of large machine learning models. Our key idea is to select a weighted subset S of training data V that best approximates the full gradient of V . We prove that the subset S that minimizes an upper-bound on the error of estimating the full gradient maximizes a submodular facility location function. Hence, S can be efficiently found using a fast greedy algorithm. We also provide theoretical analysis of CRAIG and prove its convergence. Most importantly, we show that any incremental gradient method (IG) on S converges in the same number epochs as the same IG would on the full V , which means that we obtain a speed-up inversely proportional to the size of S. In particular, for a μ-strongly convex risk function and a subset S selected by CRAIG that estimates the full gradient by an error of at most ǫ, we prove that IG on S with diminishing stepsize αk = α/kτ at epoch k (with 0 < τ < 1 and 0 < α), converges to an 2Rǫ/μ2 neighborhood of the optimal solution at rate O(1/√k). Here, R = min{d0, (rγmaxC + ǫ)/μ} where d0 is the initial distance to the optimum, C is an upperbound on the norm of the gradients, r = |S|, and γmax is the largest weight for the elements in the subset obtained by CRAIG. Moreover, we prove that if in addition to the strong convexity, component functions have smooth gradients, IG with the same diminishing step size on subset S converges to a 2ǫ/μ neighborhood of the optimum solution at rate O(1/kτ ). The above implies that IG on S converges to the same solution and in the same number of epochs as IG on the full V . But because every epoch only uses a subset S of the data, it requires fewer gradient computations and thus leads to a |V |/|S| speedup over traditional IG methods, while still (approximately) converging to the optimal solution. We also note that CRAIG is complementary to various incremental gradient (IG) methods (SGD, SAGA, SVRG, Adam), and such methods can be used on the subset S found by CRAIG. We also demonstrate the effectiveness of CRAIG via an extensive set of experiments using logistic regression (a convex optimization problem) as well as training deep neural networks (non-convex optimization problems). We show that CRAIG speeds up incremental gradient methods, including SGD, SAGA, and SVRG. In particular, CRAIG while achieving practically the same loss and accuracy as the underlying incremental gradient descent methods, speeds up gradient methods by up to 6x for convex and 3x for non-convex loss functions.	https://arxiv.org/pdf/2310.16776	DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection	Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT-UCS, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to identify a smaller, representative dataset that reduces the data needed to finetune PLMs for the text-generation task of textediting. We examine the efficacy of DEFT-UCS across multiple text-editing tasks, and compare to the state-of-the art text-editing model, CoEDIT. Our results demonstrate that DEFTUCS models are just as accurate as CoEDIT, across eight different datasets consisting of six different editing tasks, while finetuned on 70% less data.	How much data do we need to fine-tune a pretrained language model (PLM) for a specific downstream task? While successes in language modelling have led to numerous publicly available PLMs and ability to produce fine-tuned models for downstream tasks - the answer mostly remains, "as large as possible, and of good quality". For example, Alpaca, an instruction-following model, is trained with 52k data samples (Taori et al., 2023). Similarly, CoPoet, a collaborative poetry writing system is fine-tuned using 87k data samples (Chakrabarty et al., 2022). MetaMath, a mathreasoning LLM is fine-tuned with 395k data samples (Yu et al., 2023). Although fine-tuning PLMs on specific task results in performance gain, acquiring large amounts of data for fine-tuning is not easy for real-world applications which often require niche knowledge and domain expertise. Researchers have explored variety of methods primarily focused on improving the computational efficiency of fine-tuning, including parameterefficient fine-tuning approaches (PEFT) to reduce computational costs by optimizing parameter updates (Fu et al., 2023; Hu et al., 2021) as well as leveraging active-learning for iteratively selecting data samples during training (Su et al., 2022; Diao et al., 2023). Instead, our work focuses on improving the data efficiency of PLM fine-tuning without requiring iterative fine-tuning. Specifically, we explore how to fine-tune PLMs with significantly less data samples and without a cost to model performance. Related to language models, researchers have experimented with different core-set selection metrics (Paul et al., 2021; Sorscher et al., 2022) to improve the data efficiency during pre-training. Marion et al. (2023) demonstrated how perplexity, L2-Error Norm (EL2N) and memorization can be utilized to select smaller, good quality datasets for pre-training. Similarly, (Attendu and Corbeil, 2023) leverage EL2N to dynamically remove data samples with high EL2N between training epochs. However, these metrics assume access to task data and reference models to perform dataset pruning. In real world applications, utilizing such supervised, data-pruning metrics are less realistic since large amounts of annotated task-specific data may be costly to acquire. This leads us to our main research question: How can we leverage unsupervised data pruning to fine-tune PLMs for downstream tasks in a more data efficient manner? In this work, we introduce a new data-efficient fine-tuning framework, DEFT-UCS, that uses unsupervised core-set selection to minimize the amount of labelled data needed to fine-tune PLMs for the text-generation task of text-editing. Our framework is inspired by (Sorscher et al., 2022), who utilize clustering-based dataset pruning to reduce training samples for image-classification models, and to the best of our knowledge, our framework is the first to leverage unsupervised core-set selection for data-efficient fine-tuning of PLMs. We investigate the utility of DEFT-UCS in finetuning PLMs for text-generation across eight different datasets consisting of six different text-editing tasks, and compare DEFT-UCS models to the stateof-the-art text-editing model, CoEDIT(Raheja et al., 2023). Our contributions are as follows: - We introduce DEFT-UCS, a data-efficient-fine tuning framework that leverages unsupervised core-set selection via clustering to identify a smaller representative set of data needed to fine-tune PLMs. - We show that DEFT-UCS, utilizing only 32.5% of CoEDIT’s training data, is able to produce fine-tuned models with improved accuracy on four different text-editing tasks, and similar accuracy on two text-editing tasks compared to CoEDIT (Raheja et al., 2023). - We performed a human evaluation with 3 evaluators to assess the quality of text-edits from our DEFT-UCS model. Evaluators found edits generated by DEFT-UCS model as similar or preferred over CoEDIT (Raheja et al., 2023)	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/1906.01827	Coresets for Data-Efficient Training of Machine Learning Models	Incremental gradient (IG) methods, such as stochastic gradient descent and its variants are commonly used for large scale optimization in machine learning. Despite the sustained effort to make IG methods more data-efficient, it remains an open question how to select a training data subset that can theoretically and practically perform on par with the full dataset. Here we develop CRAIG, a method to select a weighted subset (or coreset) of training data that closely estimates the full gradient by maximizing a submodular function. We prove that applying IG to this subset is guaranteed to converge to the (near)optimal solution with the same convergence rate as that of IG for convex optimization. As a result, CRAIG achieves a speedup that is inversely proportional to the size of the subset. To our knowledge, this is the first rigorous method for data-efficient training of general machine learning models. Our extensive set of experiments show that CRAIG, while achieving practically the same solution, speeds up various IG methods by up to 6x for logistic regression and 3x for training deep neural networks.	Mathematical optimization lies at the core of training largescale machine learning systems, and is now widely used over massive data sets with great practical success, assuming sufficient data resources are available. Achieving this success, however, also requires large amounts of (often GPU) computing, as well as concomitant financial expenditures and energy usage (Strubell et al., 2019). Significantly decreasing these costs without decreasing the learnt system’s resulting accuracy is one of the grand challenges of machine learning and artificial intelligence today (Asi & Duchi, 2019). Training machine learning models often reduces to optimizing a regularized empirical risk function. Given a convex loss l, and a μ-strongly convex regularizer r, one aims to find model parameter vector w∗ over the parameter space W that minimizes the loss f over the training data V : w∗ ∈ arg minw∈W f (w), f (w) := ∑ i∈V fi(w) + r(w), fi(w) = l(w, (xi, yi)), (1) where V = {1, . . . , n} is an index set of the training data, and functions fi : Rd �' R are associated with training examples (xi, yi), where xi ∈ Rd is the feature vector, and yi is the point i’s label. Standard Gradient Descent can find the minimizer of this problem, but requires repeated computations of the full gradient ∇f (w)—sum of the gradients over all training data points/functions i—and is therefore prohibitive for massive data sets. This issue is further exacerbated in case of deep neural networks where gradient computations (backpropagation) are expensive. Incremental Gradient (IG) methods, such as Stochastic Gradient Descent (SGD) and its accelerated variants, including SGD with momentum (Qian, 1999), Adagrad (Duchi et al., 2011), Adam (Kingma & Ba, 2014), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013) iteratively estimate the gradient on random subsets/batches of training data. While this provides an unbiased estimate of the full gradient, the randomized batches introduce variance in the gradient estimate (Hofmann et al., 2015), and therefore stochastic gradient methods are in general slow to converge (Johnson & Zhang, 2013; Defazio et al., 2014). The majority of the work speeding up IG methods has thus primarily focused on reducing the variance of the gradient estimate (SAGA (Defazio et al., 2014), SVRG (Johnson & Zhang, 2013), Katysha (Allen-Zhu, 2017)) or more carefully selecting the gradient stepsize (Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), Adam (Kingma & Ba, 2014)). However, the direction that remains largely unexplored is how to carefully select a small subset S ⊆ V of the fullCoresets for Data-efficient Training of Machine Learning Models PAGE 2 training data V , so that the model is trained only on the subset S while still (approximately) converging to the globally optimal solution (i.e., the model parameters that would be obtained if training/optimizing on the full V ). If such a subset S can be quickly found, then this would directly lead to a speedup of |V |/|S| (which can be very large if |S| ≪ |V |) per epoch of IG. There are four main challenges in finding such a subset S. First is that a guiding principle for selecting S is unclear. For example, selecting training points close to the decision boundary might allow the model to fine tune the decision boundary, while picking the most diverse set of data points would allow the model to get a better sense of the training data distribution. Second is that finding S must be fast, as otherwise identifying the set S may take longer than the actual optimization, and so no overall speed-up would be achieved. Third is that finding a subset S is not enough. One also has to decide on a gradient stepsize for each data point in S, as they affect the convergence. And last, while the method might work well empirically on some data sets, one also requires theoretical understanding and mathematical convergence guarantees. Here we develop Coresets for Accelerating Incremental Gradient descent (CRAIG), for selecting a subset of training data points to speed up training of large machine learning models. Our key idea is to select a weighted subset S of training data V that best approximates the full gradient of V . We prove that the subset S that minimizes an upper-bound on the error of estimating the full gradient maximizes a submodular facility location function. Hence, S can be efficiently found using a fast greedy algorithm. We also provide theoretical analysis of CRAIG and prove its convergence. Most importantly, we show that any incremental gradient method (IG) on S converges in the same number epochs as the same IG would on the full V , which means that we obtain a speed-up inversely proportional to the size of S. In particular, for a μ-strongly convex risk function and a subset S selected by CRAIG that estimates the full gradient by an error of at most ǫ, we prove that IG on S with diminishing stepsize αk = α/kτ at epoch k (with 0 < τ < 1 and 0 < α), converges to an 2Rǫ/μ2 neighborhood of the optimal solution at rate O(1/√k). Here, R = min{d0, (rγmaxC + ǫ)/μ} where d0 is the initial distance to the optimum, C is an upperbound on the norm of the gradients, r = |S|, and γmax is the largest weight for the elements in the subset obtained by CRAIG. Moreover, we prove that if in addition to the strong convexity, component functions have smooth gradients, IG with the same diminishing step size on subset S converges to a 2ǫ/μ neighborhood of the optimum solution at rate O(1/kτ ). The above implies that IG on S converges to the same solution and in the same number of epochs as IG on the full V . But because every epoch only uses a subset S of the data, it requires fewer gradient computations and thus leads to a |V |/|S| speedup over traditional IG methods, while still (approximately) converging to the optimal solution. We also note that CRAIG is complementary to various incremental gradient (IG) methods (SGD, SAGA, SVRG, Adam), and such methods can be used on the subset S found by CRAIG. We also demonstrate the effectiveness of CRAIG via an extensive set of experiments using logistic regression (a convex optimization problem) as well as training deep neural networks (non-convex optimization problems). We show that CRAIG speeds up incremental gradient methods, including SGD, SAGA, and SVRG. In particular, CRAIG while achieving practically the same loss and accuracy as the underlying incremental gradient descent methods, speeds up gradient methods by up to 6x for convex and 3x for non-convex loss functions.	https://arxiv.org/pdf/2403.08370	SMART: Submodular Data Mixture Strategy for Instruction Tuning	Instruction Tuning involves finetuning a language model on a collection of instructionformatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) — a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal mixing. Furthermore, SMART facilitates the creation of data mixtures based on a few representative subsets of tasks alone and through task pruning analysis, we reveal that in a limited budget setting, allocating budget among a subset of representative tasks yields superior performance compared to distributing the budget among all tasks. The code for reproducing our results is open-sourced at https://github.com/kowndinyarenduchintala/SMART.	One of the main goals of artificial intelligence (AI) research is to build machines that can communicate (Turing, 1950), and an essential part of communication is to understand and follow instructions. Large Language Models (LLMs), which are pre-trained over massive text corpora on next-tokenprediction objective, can perform a wide range of NLP tasks via "prompting" (Brown et al., 2020; Kojima et al., 2022; Almazrouei et al., 2023; Liu et al., 2023; Touvron et al., 2023). Instruction Tuning (Wei et al., 2021; Sanh et al., 2021; Chung et al., 2022) is an approach that further enhances the instruction-following ability and generalizability of pre-trained LLMs to unseen tasks. It involves fine-tuning an LLM on a collection of instruction-formatted instances (encompassing multiple tasks) - each consisting of an instruction (or task description), an optional input, the corresponding output (the ground truth) and optionally a few demonstrations/examples. It is a special case of multitask learning where the LLM is finetuned on a collection of instruction-formatted multitask datasets (Chung et al., 2022). Finetuning on multiple tasks simultaneously, allows the model to share and transfer information across tasks, resulting in a better common internal representation that is preferred by all tasks while suppressing taskdependent noise (Caruana, 1997). Consequently, the model learns to generalize to unseen tasks by discerning helpful cues from both implicitly and explicitly related tasks that it has previously seen. The performance enhancement from instruction tuning is heavily contingent on data quality, data quantity, and task composition (Wang et al., 2023b). Studies by Iyer et al. (2022) and Longpre et al. (2023) have shown that while scaling the number of tasks is important, the relative proportion of various tasks (mixture weighting) merits as much attention for optimal instruction tuning. Intuitively, we want the model to see enough data for a given task that it can perform well on it, but not to see so much data that it memorizes the training set (Raffel et al., 2020). Iyer et al. (2022) performed manual tuning of various benchmark proportions and decided on a final mixture, whereas Longpre et al. (2023) studied the impact of removing each benchmark from the finetuning mixture and relied on their practioners’ intuition from there on, to decide on the exact proportions of benchmarks. In this work, we would like to explore a more systematic approach to mixture weighting. Specifically, we are motivated by the fact that in a large multitask dataset like FLAN 2022 (Longpre et al., 2023), which has 1840 tasks, there will likely be many similar tasks leading to redundancies and not all of them may require sampling in equal proportions. For instance, there might be many tasks of the type Natural Language Inference (NLI), and it might be enough to sample relatively more instances from a few representative NLI tasks and less from the others. Furthermore, which samples we select from each task is also crucial because the samples should faithfully represent the task at hand. A random subset may fail to do this as it can miss out on essential corner cases. With this context, we focus on the following two fundamental research questions (RQs) that form the basis for our subsequent inquiry: - (RQ1) Given a huge multitask instructiontuning dataset and a limited fine-tuning budget which is defined by the total number of (prompt, response) instances that can be used for fine-tuning, how do we divide this budget among thousands of tasks present in the dataset? i.e., how many instances to sample from each task? and which instances to sample from each task? - (RQ2) Can we go a step further and strategically prune some tasks altogether and only fine-tune on a small subset of representative tasks without hurting the performance? If yes, what is the nature of this subset? To the best of our knowledge, there’s currently no principled approach to determining task compositions for instruction tuning, other than manual tuning and/or practioners’ intuition. As a first step towards addressing both of the above RQs, we first define a common subset selection problem (more formally stated in Section 3) as follows - Given a huge collection of M instructionformatted task datasets, a task budget M ′ ≤ M and a total budget (N ′) of (prompt, response) pairs, which M ′ tasks to select? and how many instances to select from each of these M ′ tasks and which instances to select? Note that RQ1 is an instance of this problem where M ′ = M . Constrained Submodular Maximization (Section 2) proves to be a good model for discovering representative subsets (or coresets) of a massive training dataset (or ground set) that acts as surrogate (i.e., achieves similar performance) and are much better than uniformly-at-random subsets. Intuitively, this is because submodular functions model information in subsets, and hence maximizing a submodular function subject to a constraint yields non-redundant subsets of the ground set. An essential feature of this model is that it returns weighted subsets, i.e., each sample in the coreset comes with an associated score, which indicates how important the sample is. Inspired by submodular functions, we propose our solution (Section 3) to the above subset selection problem for instruction tuning that works in two stages. In the first stage, we select a weighted subset of tasks from the full dataset where the weights will determine how many samples to select from each task. In the next stage, we select samples from each task based on the assigned task budgets. Note that the submodular functions used in each stage are not necessarily identical (Section 4.8). The main contributions of our work can be summarized as follows: - We introduce SMART — a novel data mixture strategy for instruction tuning that models the data mixture problem (Section 3) as a sequence of two cardinality-constrained submodular maximization problems and offer empirical evidence that it outperforms both examples proportional and equal mixing baselines (Section 4) as well as the mixture weights proposed by Longpre et al. (2023). - Existing works like Longpre et al. (2023) have reported a continuous increase in performance upon increasing the number of tasks (though the gains themselves may be diminishing). However, we posit that this depends on the order in which new tasks are incorporated and show empirically that in the case of SMART mixtures, a performance peak is observed with an initial addition of few representative tasks and upon adding more and more tasks, the performance is not sustained (Section 4.6). - We find that the nature of instances that should be selected in each task (i.e, whether a representative or diverse subset) also depends on the total task budget, M ′ (Section 4.8). For higher M ′s, each task on average gets the relatively low budget and selecting representative samples is more important; however for lower M ′s, when there is sufficient enough budget for each task, the need for diversity dominates	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/1906.01827	Coresets for Data-Efficient Training of Machine Learning Models	Incremental gradient (IG) methods, such as stochastic gradient descent and its variants are commonly used for large scale optimization in machine learning. Despite the sustained effort to make IG methods more data-efficient, it remains an open question how to select a training data subset that can theoretically and practically perform on par with the full dataset. Here we develop CRAIG, a method to select a weighted subset (or coreset) of training data that closely estimates the full gradient by maximizing a submodular function. We prove that applying IG to this subset is guaranteed to converge to the (near)optimal solution with the same convergence rate as that of IG for convex optimization. As a result, CRAIG achieves a speedup that is inversely proportional to the size of the subset. To our knowledge, this is the first rigorous method for data-efficient training of general machine learning models. Our extensive set of experiments show that CRAIG, while achieving practically the same solution, speeds up various IG methods by up to 6x for logistic regression and 3x for training deep neural networks.	Mathematical optimization lies at the core of training largescale machine learning systems, and is now widely used over massive data sets with great practical success, assuming sufficient data resources are available. Achieving this success, however, also requires large amounts of (often GPU) computing, as well as concomitant financial expenditures and energy usage (Strubell et al., 2019). Significantly decreasing these costs without decreasing the learnt system’s resulting accuracy is one of the grand challenges of machine learning and artificial intelligence today (Asi & Duchi, 2019). Training machine learning models often reduces to optimizing a regularized empirical risk function. Given a convex loss l, and a μ-strongly convex regularizer r, one aims to find model parameter vector w∗ over the parameter space W that minimizes the loss f over the training data V : w∗ ∈ arg minw∈W f (w), f (w) := ∑ i∈V fi(w) + r(w), fi(w) = l(w, (xi, yi)), (1) where V = {1, . . . , n} is an index set of the training data, and functions fi : Rd �' R are associated with training examples (xi, yi), where xi ∈ Rd is the feature vector, and yi is the point i’s label. Standard Gradient Descent can find the minimizer of this problem, but requires repeated computations of the full gradient ∇f (w)—sum of the gradients over all training data points/functions i—and is therefore prohibitive for massive data sets. This issue is further exacerbated in case of deep neural networks where gradient computations (backpropagation) are expensive. Incremental Gradient (IG) methods, such as Stochastic Gradient Descent (SGD) and its accelerated variants, including SGD with momentum (Qian, 1999), Adagrad (Duchi et al., 2011), Adam (Kingma & Ba, 2014), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013) iteratively estimate the gradient on random subsets/batches of training data. While this provides an unbiased estimate of the full gradient, the randomized batches introduce variance in the gradient estimate (Hofmann et al., 2015), and therefore stochastic gradient methods are in general slow to converge (Johnson & Zhang, 2013; Defazio et al., 2014). The majority of the work speeding up IG methods has thus primarily focused on reducing the variance of the gradient estimate (SAGA (Defazio et al., 2014), SVRG (Johnson & Zhang, 2013), Katysha (Allen-Zhu, 2017)) or more carefully selecting the gradient stepsize (Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), Adam (Kingma & Ba, 2014)). However, the direction that remains largely unexplored is how to carefully select a small subset S ⊆ V of the fullCoresets for Data-efficient Training of Machine Learning Models PAGE 2 training data V , so that the model is trained only on the subset S while still (approximately) converging to the globally optimal solution (i.e., the model parameters that would be obtained if training/optimizing on the full V ). If such a subset S can be quickly found, then this would directly lead to a speedup of |V |/|S| (which can be very large if |S| ≪ |V |) per epoch of IG. There are four main challenges in finding such a subset S. First is that a guiding principle for selecting S is unclear. For example, selecting training points close to the decision boundary might allow the model to fine tune the decision boundary, while picking the most diverse set of data points would allow the model to get a better sense of the training data distribution. Second is that finding S must be fast, as otherwise identifying the set S may take longer than the actual optimization, and so no overall speed-up would be achieved. Third is that finding a subset S is not enough. One also has to decide on a gradient stepsize for each data point in S, as they affect the convergence. And last, while the method might work well empirically on some data sets, one also requires theoretical understanding and mathematical convergence guarantees. Here we develop Coresets for Accelerating Incremental Gradient descent (CRAIG), for selecting a subset of training data points to speed up training of large machine learning models. Our key idea is to select a weighted subset S of training data V that best approximates the full gradient of V . We prove that the subset S that minimizes an upper-bound on the error of estimating the full gradient maximizes a submodular facility location function. Hence, S can be efficiently found using a fast greedy algorithm. We also provide theoretical analysis of CRAIG and prove its convergence. Most importantly, we show that any incremental gradient method (IG) on S converges in the same number epochs as the same IG would on the full V , which means that we obtain a speed-up inversely proportional to the size of S. In particular, for a μ-strongly convex risk function and a subset S selected by CRAIG that estimates the full gradient by an error of at most ǫ, we prove that IG on S with diminishing stepsize αk = α/kτ at epoch k (with 0 < τ < 1 and 0 < α), converges to an 2Rǫ/μ2 neighborhood of the optimal solution at rate O(1/√k). Here, R = min{d0, (rγmaxC + ǫ)/μ} where d0 is the initial distance to the optimum, C is an upperbound on the norm of the gradients, r = |S|, and γmax is the largest weight for the elements in the subset obtained by CRAIG. Moreover, we prove that if in addition to the strong convexity, component functions have smooth gradients, IG with the same diminishing step size on subset S converges to a 2ǫ/μ neighborhood of the optimum solution at rate O(1/kτ ). The above implies that IG on S converges to the same solution and in the same number of epochs as IG on the full V . But because every epoch only uses a subset S of the data, it requires fewer gradient computations and thus leads to a |V |/|S| speedup over traditional IG methods, while still (approximately) converging to the optimal solution. We also note that CRAIG is complementary to various incremental gradient (IG) methods (SGD, SAGA, SVRG, Adam), and such methods can be used on the subset S found by CRAIG. We also demonstrate the effectiveness of CRAIG via an extensive set of experiments using logistic regression (a convex optimization problem) as well as training deep neural networks (non-convex optimization problems). We show that CRAIG speeds up incremental gradient methods, including SGD, SAGA, and SVRG. In particular, CRAIG while achieving practically the same loss and accuracy as the underlying incremental gradient descent methods, speeds up gradient methods by up to 6x for convex and 3x for non-convex loss functions.	https://arxiv.org/pdf/2012.10630	GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning	Large scale machine learning and deep models are extremely data-hungry. Unfortunately, obtaining large amounts of labeled data is expensive, and training state-of-the-art models (with hyperparameter tuning) requires significant computing resources and time. Secondly, real-world data is noisy and imbalanced. As a result, several recent papers try to make the training process more efficient and robust. However, most existing work either focuses on robustness or efficiency, but not both. In this work, we introduce GLISTER, a GeneraLIzation based data Subset selecTion for Efficient and Robust learning framework. We formulate GLISTER as a mixed discretecontinuous bi-level optimization problem to select a subset of the training data, which maximizes the log-likelihood on a held-out validation set. We then analyze GLISTER for simple classifiers such as gaussian and multinomial naive-bayes, k-nearest neighbor classifier, and linear regression and show connections to submodularity. Next, we propose an iterative online algorithm GLISTER-ONLINE, which performs data selection iteratively along with the parameter updates and can be applied to any loss-based learning algorithm. We then show that for a rich class of loss functions including cross-entropy, hinge-loss, squared-loss, and logistic-loss, the inner discrete data selection is an instance of (weakly) submodular optimization, and we analyze conditions for which GLISTER-ONLINE reduces the validation loss and converges. Finally, we propose GLISTER-ACTIVE, an extension to batch active learning, and we empirically demonstrate the performance of GLISTER on a wide range of tasks including, (a) data selection to reduce training time, (b) robust learning under label noise and imbalance settings, and (c) batch-active learning with several deep and shallow models. We show that our framework improves upon state of the art both in efficiency and accuracy (in cases (a) and (c)) and is more efficient compared to other state-ofthe-art robust learning algorithms in case (b). The code for GLISTERis at:https://github.com/dssresearch/GLISTER.	With the quest to achieve human-like performance for machine learning and deep learning systems, the cost of training and deploying machine learning models has been significantly increasing. The wasted computational and engineering energy becomes evident in deep learning algorithms, wherein extensive hyper-parameter tuning and network architecture search need to be done. This results in staggering compute costs and running times. As a result, efficient and robust machine learning is a very relevant and substantial research problem. In this paper, we shall focus on three goals: Goal 1: Train machine learning and deep learning models on effective subsets of data, thereby significantly reducing training time and compute while not sacrificing accuracy. Goal 2: To (iteratively) select effective subsets of labeled data to reduce the labeling cost. Goal 3: Select data subsets to remove noisy labels and class imbalance, which is increasingly common in operational machine learning settings. Most prior work discussed above, either study robustness or efficiency, but not both. For example, the data selection approaches such as (Wei, Iyer, and Bilmes 2015; Mirzasoleiman, Bilmes, and Leskovec 2020; Shinohara 2014) and others focus on approximating either gradients or performance on the training sets, and hence would not be suitable for scenarios such as label noise and imbalance. On the other hand, the approaches like (Ren et al. 2018; Jiang et al. 2018) and others, focus on robustness but are not necessarily efficient. For example, the approach of (Ren et al. 2018) requires 3x the standard (deep) training cost, to obtain a robust model. GLISTER is the first framework, to the best of our knowledge, which focuses on both efficiency and robustness. Our work is closely related to the approaches of (Wei, Iyer, and Bilmes 2015) and (Ren et al. 2018). We build upon the work of (Wei, Iyer, and Bilmes 2015), by first generalizing their framework beyond simple classifiers (like nearest neighbor and naive bayes), but with general loss functions. We do this by proposing an iterative algorithm GLISTER-ONLINE which does data selection via a meta-learning based approach along with parameter updates. Furthermore, we pose the problem as optimizing the validation set performance as opposed to training set performance, thereby encouraging generalization. Next, our approach also bears similarity to (Ren et al. 2018), except that we need to solve a discrete optimization problem instead of a meta-gradient update. Moreover, we do not run our data selection every iteration, thereby ensuring that we are significantly faster than a single training run. Finally, we extend our algorithm to the active learning scenario. We demonstrate that our framework is more efficient and accurate compared to existing data selection and active learning algorithms, and secondly, also generalizes well under noisy data, and class imbalance scenarios. In particular, we show that GLISTER achieves a 3x - 6x speedup on a wide range of models and datasets, with very small loss in accuracy.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/1906.01827	Coresets for Data-Efficient Training of Machine Learning Models	Incremental gradient (IG) methods, such as stochastic gradient descent and its variants are commonly used for large scale optimization in machine learning. Despite the sustained effort to make IG methods more data-efficient, it remains an open question how to select a training data subset that can theoretically and practically perform on par with the full dataset. Here we develop CRAIG, a method to select a weighted subset (or coreset) of training data that closely estimates the full gradient by maximizing a submodular function. We prove that applying IG to this subset is guaranteed to converge to the (near)optimal solution with the same convergence rate as that of IG for convex optimization. As a result, CRAIG achieves a speedup that is inversely proportional to the size of the subset. To our knowledge, this is the first rigorous method for data-efficient training of general machine learning models. Our extensive set of experiments show that CRAIG, while achieving practically the same solution, speeds up various IG methods by up to 6x for logistic regression and 3x for training deep neural networks.	Mathematical optimization lies at the core of training largescale machine learning systems, and is now widely used over massive data sets with great practical success, assuming sufficient data resources are available. Achieving this success, however, also requires large amounts of (often GPU) computing, as well as concomitant financial expenditures and energy usage (Strubell et al., 2019). Significantly decreasing these costs without decreasing the learnt system’s resulting accuracy is one of the grand challenges of machine learning and artificial intelligence today (Asi & Duchi, 2019). Training machine learning models often reduces to optimizing a regularized empirical risk function. Given a convex loss l, and a μ-strongly convex regularizer r, one aims to find model parameter vector w∗ over the parameter space W that minimizes the loss f over the training data V : w∗ ∈ arg minw∈W f (w), f (w) := ∑ i∈V fi(w) + r(w), fi(w) = l(w, (xi, yi)), (1) where V = {1, . . . , n} is an index set of the training data, and functions fi : Rd �' R are associated with training examples (xi, yi), where xi ∈ Rd is the feature vector, and yi is the point i’s label. Standard Gradient Descent can find the minimizer of this problem, but requires repeated computations of the full gradient ∇f (w)—sum of the gradients over all training data points/functions i—and is therefore prohibitive for massive data sets. This issue is further exacerbated in case of deep neural networks where gradient computations (backpropagation) are expensive. Incremental Gradient (IG) methods, such as Stochastic Gradient Descent (SGD) and its accelerated variants, including SGD with momentum (Qian, 1999), Adagrad (Duchi et al., 2011), Adam (Kingma & Ba, 2014), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013) iteratively estimate the gradient on random subsets/batches of training data. While this provides an unbiased estimate of the full gradient, the randomized batches introduce variance in the gradient estimate (Hofmann et al., 2015), and therefore stochastic gradient methods are in general slow to converge (Johnson & Zhang, 2013; Defazio et al., 2014). The majority of the work speeding up IG methods has thus primarily focused on reducing the variance of the gradient estimate (SAGA (Defazio et al., 2014), SVRG (Johnson & Zhang, 2013), Katysha (Allen-Zhu, 2017)) or more carefully selecting the gradient stepsize (Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), Adam (Kingma & Ba, 2014)). However, the direction that remains largely unexplored is how to carefully select a small subset S ⊆ V of the fullCoresets for Data-efficient Training of Machine Learning Models PAGE 2 training data V , so that the model is trained only on the subset S while still (approximately) converging to the globally optimal solution (i.e., the model parameters that would be obtained if training/optimizing on the full V ). If such a subset S can be quickly found, then this would directly lead to a speedup of |V |/|S| (which can be very large if |S| ≪ |V |) per epoch of IG. There are four main challenges in finding such a subset S. First is that a guiding principle for selecting S is unclear. For example, selecting training points close to the decision boundary might allow the model to fine tune the decision boundary, while picking the most diverse set of data points would allow the model to get a better sense of the training data distribution. Second is that finding S must be fast, as otherwise identifying the set S may take longer than the actual optimization, and so no overall speed-up would be achieved. Third is that finding a subset S is not enough. One also has to decide on a gradient stepsize for each data point in S, as they affect the convergence. And last, while the method might work well empirically on some data sets, one also requires theoretical understanding and mathematical convergence guarantees. Here we develop Coresets for Accelerating Incremental Gradient descent (CRAIG), for selecting a subset of training data points to speed up training of large machine learning models. Our key idea is to select a weighted subset S of training data V that best approximates the full gradient of V . We prove that the subset S that minimizes an upper-bound on the error of estimating the full gradient maximizes a submodular facility location function. Hence, S can be efficiently found using a fast greedy algorithm. We also provide theoretical analysis of CRAIG and prove its convergence. Most importantly, we show that any incremental gradient method (IG) on S converges in the same number epochs as the same IG would on the full V , which means that we obtain a speed-up inversely proportional to the size of S. In particular, for a μ-strongly convex risk function and a subset S selected by CRAIG that estimates the full gradient by an error of at most ǫ, we prove that IG on S with diminishing stepsize αk = α/kτ at epoch k (with 0 < τ < 1 and 0 < α), converges to an 2Rǫ/μ2 neighborhood of the optimal solution at rate O(1/√k). Here, R = min{d0, (rγmaxC + ǫ)/μ} where d0 is the initial distance to the optimum, C is an upperbound on the norm of the gradients, r = |S|, and γmax is the largest weight for the elements in the subset obtained by CRAIG. Moreover, we prove that if in addition to the strong convexity, component functions have smooth gradients, IG with the same diminishing step size on subset S converges to a 2ǫ/μ neighborhood of the optimum solution at rate O(1/kτ ). The above implies that IG on S converges to the same solution and in the same number of epochs as IG on the full V . But because every epoch only uses a subset S of the data, it requires fewer gradient computations and thus leads to a |V |/|S| speedup over traditional IG methods, while still (approximately) converging to the optimal solution. We also note that CRAIG is complementary to various incremental gradient (IG) methods (SGD, SAGA, SVRG, Adam), and such methods can be used on the subset S found by CRAIG. We also demonstrate the effectiveness of CRAIG via an extensive set of experiments using logistic regression (a convex optimization problem) as well as training deep neural networks (non-convex optimization problems). We show that CRAIG speeds up incremental gradient methods, including SGD, SAGA, and SVRG. In particular, CRAIG while achieving practically the same loss and accuracy as the underlying incremental gradient descent methods, speeds up gradient methods by up to 6x for convex and 3x for non-convex loss functions.	https://arxiv.org/pdf/2410.11303	TSDS: Data Selection for Task-Specific Model Finetuning	Finetuning foundation models for specific tasks is an emerging paradigm in modern machine learning. The efficacy of task-specific finetuning largely depends on the selection of appropriate training data. We present TSDS (Task-Specific Data Selection), a framework to select data for task-specific model finetuning, guided by a small but representative set of examples from the target task. To do so, we formulate data selection for task-specific finetuning as an optimization problem with a distribution alignment loss based on optimal transport to capture the discrepancy between the selected data and the target distribution. In addition, we add a regularizer to encourage the diversity of the selected data and incorporate kernel density estimation into the regularizer to reduce the negative effects of near-duplicates among the candidate data. We connect our optimization problem to nearest neighbor search and design efficient algorithms to compute the optimal solution based on approximate nearest neighbor search techniques. We evaluate our method on data selection for both continued pretraining and instruction tuning of language models. We show that instruction tuning using data selected by our method with a 1% selection ratio often outperforms using the full dataset and beats the baseline selection methods by 1.5 points in F1 score on average. Our code is available at https://github.com/ZifanL/TSDS.	inetuning foundation models [3 ] is the de-facto paradigm for building machine learning applications that focus on specific tasks. Models such as BERT [10] and LLaMA [ 43] are large-scale models pretrained on massive unlabeled data across a wide range of domains. Those models can be specialized to downstream tasks through finetuning. Finetuning can take a variety of forms depending on the target task. For instance, continued pretraining [ 17 ] extends the pretraining stage of a model on a dataset that is more closely related to a target domain. As another setting, instruction tuning [51 ] trains a generative foundation model on instruction-response pairs to improve its performance in responding to task-specific instructions. Finetuning foundation models can lead to significant improvement in downstream tasks, but the effectiveness heavily relies on the right choice of training data [ 17 , 30 , 48 , 47 ]. However, the data repositories that one considers during training of generative models tend to be large—consider for example the use of Common Crawl, which contains 250 billion web pages, or The Pile [14 ]—and hence, it is impractical to manually select the data that are distributed like the use cases in the target task. Therefore, automated task-specific data selection becomes critical. In this paper, we propose TSDS (Task-Specific Data Selection), a framework to select data for taskspecific model finetuning. We consider the scenario of finetuning a foundation model to customize it for a specific task characterized by a few representative examples. The input to our framework is the representative examples and a massive repository of candidate data. Guided by the representative examples, we select training data from the repository for task-specific finetuning. We identify the following requirements for our framework. (Distribution Alignment) First, the distribution of the selected data should match the distribution of the representative examples from the target task. Distribution alignment is essential for a model to learn the target distribution and enable data-efficient finetuning for the target task [ 40 ]. Many works [38 , 17, 2, 50 , 47 ] retrieve candidate examples that are most similar to the representative examples. Such heuristics do not ensure distribution alignment between the selected data and the representative examples. A recent work [ 48 ] selects data by importance resampling to match the target distribution but is limited to an n-gram feature space, which cannot capture high-level semantics. (Diversity) Second, the selected data should be diverse so that the model can learn a wide range of related knowledge rather than overfitting to specific examples. In practice, data repositories created by web crawling may contain a large portion of near-duplicates [13 , 28 ] that can compromise diversity and negatively impact model performance [ 28, 19 ]. For example, a study [ 13 ] on several snapshots of ClueWeb and Common Crawl shows that 14% to 52% of the documents are near-duplicates. Previous works [38, 17 , 2 , 50 , 48 , 47 ] on task-specific data selection overlook near-duplicates, leading to the over-representation of such examples in the selected data. We require our framework to ensure diversity in selection even when a large fraction of the candidate examples are near-duplicates. (Scalability) Finally, the selection algorithm should be efficient, considering the increasing scale of modern data repositories. The high volume of candidate data (e.g., 250 billion pages in Common Crawl) poses a great challenge to efficient selection. Our framework formulates task-specific data selection as an optimization problem that allows a smooth trade-off between two crucial objectives: distribution alignment and diversity. The solution to the optimization problem is a categorical distribution assigned to the candidates which we will sample from. In the optimization objective, we use optimal transport to measure the discrepancy between the distribution assigned to the candidates and the target distribution, encouraging the alignment between them. We show that the optimization problem admits efficient algorithms to compute the optimal solution. In addition, our framework supports distribution alignment in any metric space that supports efficient nearest-neighbor search, including model-agnostic semantic embedding and model-specific features such as gradients. Our contributions: 1) We formulate data selection for task-specific finetuning as an optimization problem based on optimal transport for distribution alignment, with a regularization term that encourages diversity. 2) We make our framework robust to near-duplicates by incorporating kernel density estimation [36 ] into the regularization term. 3) We show the connection between the optimal solution to the optimization problem and nearest neighbor search, which allows us to develop efficient algorithms employing approximate nearest-neighbor search techniques [23]. We conduct extensive experiments to validate the effectiveness of our framework. We focus on natural language processing tasks where foundation models have shown great advancements. We show that our framework beats the state-of-the-art baseline [47 ] by 1.5 points in F1 score on average with a selection ratio of 1% on instruction tuning for two modern large language models on three tasks. In addition, continued pretraining using domain-specific data selected by our framework outperforms the other selection methods by up to 3 F1 points on four classification tasks from various domains. We also demonstrate that our framework is robust to near-duplicates in the data repository, maintaining consistent performance when 1% of the candidate examples are duplicate for up to 1,000 times. Our method is efficient, taking 28 hours to preprocess a corpus of 150M examples and less than 1 hour for each task-specific selection.	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/1906.01827	Coresets for Data-Efficient Training of Machine Learning Models	Incremental gradient (IG) methods, such as stochastic gradient descent and its variants are commonly used for large scale optimization in machine learning. Despite the sustained effort to make IG methods more data-efficient, it remains an open question how to select a training data subset that can theoretically and practically perform on par with the full dataset. Here we develop CRAIG, a method to select a weighted subset (or coreset) of training data that closely estimates the full gradient by maximizing a submodular function. We prove that applying IG to this subset is guaranteed to converge to the (near)optimal solution with the same convergence rate as that of IG for convex optimization. As a result, CRAIG achieves a speedup that is inversely proportional to the size of the subset. To our knowledge, this is the first rigorous method for data-efficient training of general machine learning models. Our extensive set of experiments show that CRAIG, while achieving practically the same solution, speeds up various IG methods by up to 6x for logistic regression and 3x for training deep neural networks.	Mathematical optimization lies at the core of training largescale machine learning systems, and is now widely used over massive data sets with great practical success, assuming sufficient data resources are available. Achieving this success, however, also requires large amounts of (often GPU) computing, as well as concomitant financial expenditures and energy usage (Strubell et al., 2019). Significantly decreasing these costs without decreasing the learnt system’s resulting accuracy is one of the grand challenges of machine learning and artificial intelligence today (Asi & Duchi, 2019). Training machine learning models often reduces to optimizing a regularized empirical risk function. Given a convex loss l, and a μ-strongly convex regularizer r, one aims to find model parameter vector w∗ over the parameter space W that minimizes the loss f over the training data V : w∗ ∈ arg minw∈W f (w), f (w) := ∑ i∈V fi(w) + r(w), fi(w) = l(w, (xi, yi)), (1) where V = {1, . . . , n} is an index set of the training data, and functions fi : Rd �' R are associated with training examples (xi, yi), where xi ∈ Rd is the feature vector, and yi is the point i’s label. Standard Gradient Descent can find the minimizer of this problem, but requires repeated computations of the full gradient ∇f (w)—sum of the gradients over all training data points/functions i—and is therefore prohibitive for massive data sets. This issue is further exacerbated in case of deep neural networks where gradient computations (backpropagation) are expensive. Incremental Gradient (IG) methods, such as Stochastic Gradient Descent (SGD) and its accelerated variants, including SGD with momentum (Qian, 1999), Adagrad (Duchi et al., 2011), Adam (Kingma & Ba, 2014), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013) iteratively estimate the gradient on random subsets/batches of training data. While this provides an unbiased estimate of the full gradient, the randomized batches introduce variance in the gradient estimate (Hofmann et al., 2015), and therefore stochastic gradient methods are in general slow to converge (Johnson & Zhang, 2013; Defazio et al., 2014). The majority of the work speeding up IG methods has thus primarily focused on reducing the variance of the gradient estimate (SAGA (Defazio et al., 2014), SVRG (Johnson & Zhang, 2013), Katysha (Allen-Zhu, 2017)) or more carefully selecting the gradient stepsize (Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), Adam (Kingma & Ba, 2014)). However, the direction that remains largely unexplored is how to carefully select a small subset S ⊆ V of the fullCoresets for Data-efficient Training of Machine Learning Models PAGE 2 training data V , so that the model is trained only on the subset S while still (approximately) converging to the globally optimal solution (i.e., the model parameters that would be obtained if training/optimizing on the full V ). If such a subset S can be quickly found, then this would directly lead to a speedup of |V |/|S| (which can be very large if |S| ≪ |V |) per epoch of IG. There are four main challenges in finding such a subset S. First is that a guiding principle for selecting S is unclear. For example, selecting training points close to the decision boundary might allow the model to fine tune the decision boundary, while picking the most diverse set of data points would allow the model to get a better sense of the training data distribution. Second is that finding S must be fast, as otherwise identifying the set S may take longer than the actual optimization, and so no overall speed-up would be achieved. Third is that finding a subset S is not enough. One also has to decide on a gradient stepsize for each data point in S, as they affect the convergence. And last, while the method might work well empirically on some data sets, one also requires theoretical understanding and mathematical convergence guarantees. Here we develop Coresets for Accelerating Incremental Gradient descent (CRAIG), for selecting a subset of training data points to speed up training of large machine learning models. Our key idea is to select a weighted subset S of training data V that best approximates the full gradient of V . We prove that the subset S that minimizes an upper-bound on the error of estimating the full gradient maximizes a submodular facility location function. Hence, S can be efficiently found using a fast greedy algorithm. We also provide theoretical analysis of CRAIG and prove its convergence. Most importantly, we show that any incremental gradient method (IG) on S converges in the same number epochs as the same IG would on the full V , which means that we obtain a speed-up inversely proportional to the size of S. In particular, for a μ-strongly convex risk function and a subset S selected by CRAIG that estimates the full gradient by an error of at most ǫ, we prove that IG on S with diminishing stepsize αk = α/kτ at epoch k (with 0 < τ < 1 and 0 < α), converges to an 2Rǫ/μ2 neighborhood of the optimal solution at rate O(1/√k). Here, R = min{d0, (rγmaxC + ǫ)/μ} where d0 is the initial distance to the optimum, C is an upperbound on the norm of the gradients, r = |S|, and γmax is the largest weight for the elements in the subset obtained by CRAIG. Moreover, we prove that if in addition to the strong convexity, component functions have smooth gradients, IG with the same diminishing step size on subset S converges to a 2ǫ/μ neighborhood of the optimum solution at rate O(1/kτ ). The above implies that IG on S converges to the same solution and in the same number of epochs as IG on the full V . But because every epoch only uses a subset S of the data, it requires fewer gradient computations and thus leads to a |V |/|S| speedup over traditional IG methods, while still (approximately) converging to the optimal solution. We also note that CRAIG is complementary to various incremental gradient (IG) methods (SGD, SAGA, SVRG, Adam), and such methods can be used on the subset S found by CRAIG. We also demonstrate the effectiveness of CRAIG via an extensive set of experiments using logistic regression (a convex optimization problem) as well as training deep neural networks (non-convex optimization problems). We show that CRAIG speeds up incremental gradient methods, including SGD, SAGA, and SVRG. In particular, CRAIG while achieving practically the same loss and accuracy as the underlying incremental gradient descent methods, speeds up gradient methods by up to 6x for convex and 3x for non-convex loss functions.	https://arxiv.org/pdf/2501.04155	MM-GEN: Enhancing Task Performance Through Targeted Multimodal Data Curation	Vision-language models (VLMs) are highly effective but often underperform on specialized tasks, for example Llava1.5 struggles on chart and diagram understanding, due to scarce task-specific training data. Existing training data, sourced from general-purpose datasets, fails to capture the nuanced details needed for these tasks. We introduce MMGEN, a scalable method that generates task-specific, highquality synthetic text for candidate images by leveraging stronger models. MM-GEN employs a three-stage targeted process: partitioning data into subgroups, generating targeted text based on task descriptions, and filtering out redundant and outlier data. Fine-tuning VLMs with data generated by MM-GEN leads to significant performance gains, including 29% on spatial reasoning and 15% on diagram understanding for Llava-1.5 (7B). Compared to human-curated caption data, MM-GEN achieves up to 1.6× better improvements for the original models, proving its effectiveness in enhancing task-specific VLM performance and bridging the gap between general-purpose datasets and specialized requirements. Code available at https://github.com/sjoshi804/MM-Gen.	While vision-language models (VLMs) demonstrate stateof-the-art performance on several multi-modal tasks [29], they are often on tasks that are simpler in nature [3]. These models still struggle with more complex tasks, e.g., those that require fine-grained understanding of details in images [3, 15, 21, 49]. We posit that the primary limitation for these VLMs is the quality and nature of the training data. VLMs are typically trained on large-scale image-text data scraped from the internet; while the images are rich and informative, the associated text descriptions can (i) have limited relevance to the image [39], or (ii) omit references to several specific details captured in the image [25]. Fig. 1 shows examples of such images and web-scraped captions. While the images are relevant for the tasks of chart understanding, spatial reasoning, and diagram understanding, respectively, the text fails to capture details essential for these tasks. While synthetic caption generation strategies proposed in prior work [25, 39, 57] can create more descriptive text annotations (by referring to more visual details), they are agnostic of the downstream target task. Consequently, they cannot ensure that relevant details are captured in the text annotations. Recently, Shi et al. [48] manually curated a task-specific dataset aimed at the task of multimodal mathematical question-answering by augmenting existing imagetext data with detailed textual annotations, based on their domain expertise, using strong VLMs. While effective, such a curation pipeline involves significant human effort and is not scalable [33, 59]. To address these limitations, we present MM-GEN, a highly general framework for automatically synthesizing taskrelevant text annotations for images by leveraging stronger VLMs (i.e., VLMs that perform well on the specific task) and requiring minimal human effort. MM-GEN takes as input a small set of examples from the target task (henceforth referred to as "reference samples"), a list of image types associated with the task, and a pool of task-relevant candidate images for training. In practice, these inputs can be easily obtained: a small number of reference samples and associated image types can be directly collected from the target task, and a task-relevant image pool can be found via image search with search engines or retrieved from largescale image-caption datasets [5, 46, 47]. Using the reference samples to specify the details of the task to the stronger VLM, MM-GEN generates text-annotations that are taskrelevant for the candidate images. Fig. 1 shows how the text generated by MM-GEN captures task-relevant details. This simple approach is highly effective, resulting in significant improvements across a variety of target tasks. Moreover, human-effort in this process is limited to providing (i) a small set of reference samples for the task, and (ii) a pool of candidate images. To further improve the quality and efficacy of the generated data, MM-GEN introduces a perplexity [4] based data-filtering approach to improve performance on target task using a high-value subset of the synthesized data. The components in MM-GEN are general and applicable to any image-text based target task enabling it to easily generalize across tasks and scale. We evaluate MM-GEN on improving VLMs' (e.g., Llava1.5 7B and 13B parameter versions) performance on finegrained image understanding tasks — chart understanding and reasoning, diagram understanding, and spatial reasoning on maps. The data curated by MM-GEN enables an absolute improvement over Llava-1.5 (7B) of 15%, 14% and 29%, respectively. We also see improvements over the much larger Llava-1.5 (13B) of 13%, , respectively. Moreover, MM-GEN’s filtering strategy helps in shrinking data volumes by up to 50% with no / minimal loss in performance. Empirical results show that models trained via MM-GEN data have a better performance than those trained via generated generic captions. MM-GEN data is also more effective than text annotations generated without task-specific reference examples, showing the importance of a targeted, data-centric approach for describing tasks. Finally, we analyze the effects of key design choices in MMGEN through ablation studies on e.g. size of the reference sample set, generating with / without partitioning, scaling number of in-context samples.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/1906.01827	Coresets for Data-Efficient Training of Machine Learning Models	Incremental gradient (IG) methods, such as stochastic gradient descent and its variants are commonly used for large scale optimization in machine learning. Despite the sustained effort to make IG methods more data-efficient, it remains an open question how to select a training data subset that can theoretically and practically perform on par with the full dataset. Here we develop CRAIG, a method to select a weighted subset (or coreset) of training data that closely estimates the full gradient by maximizing a submodular function. We prove that applying IG to this subset is guaranteed to converge to the (near)optimal solution with the same convergence rate as that of IG for convex optimization. As a result, CRAIG achieves a speedup that is inversely proportional to the size of the subset. To our knowledge, this is the first rigorous method for data-efficient training of general machine learning models. Our extensive set of experiments show that CRAIG, while achieving practically the same solution, speeds up various IG methods by up to 6x for logistic regression and 3x for training deep neural networks.	Mathematical optimization lies at the core of training largescale machine learning systems, and is now widely used over massive data sets with great practical success, assuming sufficient data resources are available. Achieving this success, however, also requires large amounts of (often GPU) computing, as well as concomitant financial expenditures and energy usage (Strubell et al., 2019). Significantly decreasing these costs without decreasing the learnt system’s resulting accuracy is one of the grand challenges of machine learning and artificial intelligence today (Asi & Duchi, 2019). Training machine learning models often reduces to optimizing a regularized empirical risk function. Given a convex loss l, and a μ-strongly convex regularizer r, one aims to find model parameter vector w∗ over the parameter space W that minimizes the loss f over the training data V : w∗ ∈ arg minw∈W f (w), f (w) := ∑ i∈V fi(w) + r(w), fi(w) = l(w, (xi, yi)), (1) where V = {1, . . . , n} is an index set of the training data, and functions fi : Rd �' R are associated with training examples (xi, yi), where xi ∈ Rd is the feature vector, and yi is the point i’s label. Standard Gradient Descent can find the minimizer of this problem, but requires repeated computations of the full gradient ∇f (w)—sum of the gradients over all training data points/functions i—and is therefore prohibitive for massive data sets. This issue is further exacerbated in case of deep neural networks where gradient computations (backpropagation) are expensive. Incremental Gradient (IG) methods, such as Stochastic Gradient Descent (SGD) and its accelerated variants, including SGD with momentum (Qian, 1999), Adagrad (Duchi et al., 2011), Adam (Kingma & Ba, 2014), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013) iteratively estimate the gradient on random subsets/batches of training data. While this provides an unbiased estimate of the full gradient, the randomized batches introduce variance in the gradient estimate (Hofmann et al., 2015), and therefore stochastic gradient methods are in general slow to converge (Johnson & Zhang, 2013; Defazio et al., 2014). The majority of the work speeding up IG methods has thus primarily focused on reducing the variance of the gradient estimate (SAGA (Defazio et al., 2014), SVRG (Johnson & Zhang, 2013), Katysha (Allen-Zhu, 2017)) or more carefully selecting the gradient stepsize (Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), Adam (Kingma & Ba, 2014)). However, the direction that remains largely unexplored is how to carefully select a small subset S ⊆ V of the fullCoresets for Data-efficient Training of Machine Learning Models PAGE 2 training data V , so that the model is trained only on the subset S while still (approximately) converging to the globally optimal solution (i.e., the model parameters that would be obtained if training/optimizing on the full V ). If such a subset S can be quickly found, then this would directly lead to a speedup of |V |/|S| (which can be very large if |S| ≪ |V |) per epoch of IG. There are four main challenges in finding such a subset S. First is that a guiding principle for selecting S is unclear. For example, selecting training points close to the decision boundary might allow the model to fine tune the decision boundary, while picking the most diverse set of data points would allow the model to get a better sense of the training data distribution. Second is that finding S must be fast, as otherwise identifying the set S may take longer than the actual optimization, and so no overall speed-up would be achieved. Third is that finding a subset S is not enough. One also has to decide on a gradient stepsize for each data point in S, as they affect the convergence. And last, while the method might work well empirically on some data sets, one also requires theoretical understanding and mathematical convergence guarantees. Here we develop Coresets for Accelerating Incremental Gradient descent (CRAIG), for selecting a subset of training data points to speed up training of large machine learning models. Our key idea is to select a weighted subset S of training data V that best approximates the full gradient of V . We prove that the subset S that minimizes an upper-bound on the error of estimating the full gradient maximizes a submodular facility location function. Hence, S can be efficiently found using a fast greedy algorithm. We also provide theoretical analysis of CRAIG and prove its convergence. Most importantly, we show that any incremental gradient method (IG) on S converges in the same number epochs as the same IG would on the full V , which means that we obtain a speed-up inversely proportional to the size of S. In particular, for a μ-strongly convex risk function and a subset S selected by CRAIG that estimates the full gradient by an error of at most ǫ, we prove that IG on S with diminishing stepsize αk = α/kτ at epoch k (with 0 < τ < 1 and 0 < α), converges to an 2Rǫ/μ2 neighborhood of the optimal solution at rate O(1/√k). Here, R = min{d0, (rγmaxC + ǫ)/μ} where d0 is the initial distance to the optimum, C is an upperbound on the norm of the gradients, r = |S|, and γmax is the largest weight for the elements in the subset obtained by CRAIG. Moreover, we prove that if in addition to the strong convexity, component functions have smooth gradients, IG with the same diminishing step size on subset S converges to a 2ǫ/μ neighborhood of the optimum solution at rate O(1/kτ ). The above implies that IG on S converges to the same solution and in the same number of epochs as IG on the full V . But because every epoch only uses a subset S of the data, it requires fewer gradient computations and thus leads to a |V |/|S| speedup over traditional IG methods, while still (approximately) converging to the optimal solution. We also note that CRAIG is complementary to various incremental gradient (IG) methods (SGD, SAGA, SVRG, Adam), and such methods can be used on the subset S found by CRAIG. We also demonstrate the effectiveness of CRAIG via an extensive set of experiments using logistic regression (a convex optimization problem) as well as training deep neural networks (non-convex optimization problems). We show that CRAIG speeds up incremental gradient methods, including SGD, SAGA, and SVRG. In particular, CRAIG while achieving practically the same loss and accuracy as the underlying incremental gradient descent methods, speeds up gradient methods by up to 6x for convex and 3x for non-convex loss functions.	https://arxiv.org/pdf/1708.00489	Active Learning for Convolutional Neural Networks: A Core-Set Approach	Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (i.e. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.	Deep convolutional neural networks (CNNs) have shown unprecedented success in many areas of research in computer vision and pattern recognition, such as image classification, object detection, and scene segmentation. Although CNNs are universally successful in many tasks, they have a major drawback; they need a very large amount of labeled data to be able to learn their large number of parameters. More importantly, it is almost always better to have more data since the accuracy of CNNs is often not saturated with increasing dataset size. Hence, there is a constant desire to collect more and more data. Although this a desired behavior from an algorithmic perspective (higher representative power is typically better), labeling a dataset is a time consuming and an expensive task. These practical considerations raise a critical question: "what is the optimal way to choose data points to label such that the highest accuracy can be obtained given a fixed labeling budget." Active learning is one of the common paradigms to address this question. The goal of active learning is to find effective ways to choose data points to label, from a pool of unlabeled data points, in order to maximize the accuracy. Although it is not possible to obtain a universally good active learning strategy (Dasgupta, 2004), there exist many heuristics (Settles, 2010) which have been proven to be effective in practice. Active learning is typically an iterative process in which a model is learned at each iteration and a set of points is chosen to be labelled from a pool of unlabelled points using these aforementioned heuristics. We experiment with many of these heuristics in this paper and find them not effective when applied to CNNs. We argue that the main factor behind this ineffectiveness is the correlation caused via batch acquisition/sampling. In the classical setting, the active learning algorithms typically choose a single point at each iteration; however, this is not feasible for CNNs since i) a single point is likely to have no statistically significant impact on the accuracy due to the local optimization methods, and ii) each iteration requires a full training until convergence which makes it intractable to query labels one-by-one. Hence, it is necessary to query labels for a large subset at each iteration and it results in correlated samples even for moderately small subset sizes. In order to tailor an active learning method for the batch sampling case, we decided to define the active learning as core-set selection problem. Core-set selection problem aims to find a small subset given a large labeled dataset such that a model learned over the small subset is competitive over the whole dataset. Since we have no labels available, we perform the core-set selection without using the labels. In order to attack the unlabeled core-set problem for CNNs, we provide a rigorous bound between an average loss over any given subset of the dataset and the remaining data points via the geometry of the data points. As an active learning algorithm, we try to choose a subset such that this bound is minimized. Moreover, minimization of this bound turns out to be equivalent to the k-Center problem (Wolf, 2011) and we adopt an efficient approximate solution to this combinatorial optimization problem. We further study the behavior of our proposed algorithm empirically for the problem of image classification using three different datasets. Our empirical analysis demonstrates state-of-the-art performance by a large margin.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/1906.01827	Coresets for Data-Efficient Training of Machine Learning Models	Incremental gradient (IG) methods, such as stochastic gradient descent and its variants are commonly used for large scale optimization in machine learning. Despite the sustained effort to make IG methods more data-efficient, it remains an open question how to select a training data subset that can theoretically and practically perform on par with the full dataset. Here we develop CRAIG, a method to select a weighted subset (or coreset) of training data that closely estimates the full gradient by maximizing a submodular function. We prove that applying IG to this subset is guaranteed to converge to the (near)optimal solution with the same convergence rate as that of IG for convex optimization. As a result, CRAIG achieves a speedup that is inversely proportional to the size of the subset. To our knowledge, this is the first rigorous method for data-efficient training of general machine learning models. Our extensive set of experiments show that CRAIG, while achieving practically the same solution, speeds up various IG methods by up to 6x for logistic regression and 3x for training deep neural networks.	Mathematical optimization lies at the core of training largescale machine learning systems, and is now widely used over massive data sets with great practical success, assuming sufficient data resources are available. Achieving this success, however, also requires large amounts of (often GPU) computing, as well as concomitant financial expenditures and energy usage (Strubell et al., 2019). Significantly decreasing these costs without decreasing the learnt system’s resulting accuracy is one of the grand challenges of machine learning and artificial intelligence today (Asi & Duchi, 2019). Training machine learning models often reduces to optimizing a regularized empirical risk function. Given a convex loss l, and a μ-strongly convex regularizer r, one aims to find model parameter vector w∗ over the parameter space W that minimizes the loss f over the training data V : w∗ ∈ arg minw∈W f (w), f (w) := ∑ i∈V fi(w) + r(w), fi(w) = l(w, (xi, yi)), (1) where V = {1, . . . , n} is an index set of the training data, and functions fi : Rd �' R are associated with training examples (xi, yi), where xi ∈ Rd is the feature vector, and yi is the point i’s label. Standard Gradient Descent can find the minimizer of this problem, but requires repeated computations of the full gradient ∇f (w)—sum of the gradients over all training data points/functions i—and is therefore prohibitive for massive data sets. This issue is further exacerbated in case of deep neural networks where gradient computations (backpropagation) are expensive. Incremental Gradient (IG) methods, such as Stochastic Gradient Descent (SGD) and its accelerated variants, including SGD with momentum (Qian, 1999), Adagrad (Duchi et al., 2011), Adam (Kingma & Ba, 2014), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013) iteratively estimate the gradient on random subsets/batches of training data. While this provides an unbiased estimate of the full gradient, the randomized batches introduce variance in the gradient estimate (Hofmann et al., 2015), and therefore stochastic gradient methods are in general slow to converge (Johnson & Zhang, 2013; Defazio et al., 2014). The majority of the work speeding up IG methods has thus primarily focused on reducing the variance of the gradient estimate (SAGA (Defazio et al., 2014), SVRG (Johnson & Zhang, 2013), Katysha (Allen-Zhu, 2017)) or more carefully selecting the gradient stepsize (Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), Adam (Kingma & Ba, 2014)). However, the direction that remains largely unexplored is how to carefully select a small subset S ⊆ V of the fullCoresets for Data-efficient Training of Machine Learning Models PAGE 2 training data V , so that the model is trained only on the subset S while still (approximately) converging to the globally optimal solution (i.e., the model parameters that would be obtained if training/optimizing on the full V ). If such a subset S can be quickly found, then this would directly lead to a speedup of |V |/|S| (which can be very large if |S| ≪ |V |) per epoch of IG. There are four main challenges in finding such a subset S. First is that a guiding principle for selecting S is unclear. For example, selecting training points close to the decision boundary might allow the model to fine tune the decision boundary, while picking the most diverse set of data points would allow the model to get a better sense of the training data distribution. Second is that finding S must be fast, as otherwise identifying the set S may take longer than the actual optimization, and so no overall speed-up would be achieved. Third is that finding a subset S is not enough. One also has to decide on a gradient stepsize for each data point in S, as they affect the convergence. And last, while the method might work well empirically on some data sets, one also requires theoretical understanding and mathematical convergence guarantees. Here we develop Coresets for Accelerating Incremental Gradient descent (CRAIG), for selecting a subset of training data points to speed up training of large machine learning models. Our key idea is to select a weighted subset S of training data V that best approximates the full gradient of V . We prove that the subset S that minimizes an upper-bound on the error of estimating the full gradient maximizes a submodular facility location function. Hence, S can be efficiently found using a fast greedy algorithm. We also provide theoretical analysis of CRAIG and prove its convergence. Most importantly, we show that any incremental gradient method (IG) on S converges in the same number epochs as the same IG would on the full V , which means that we obtain a speed-up inversely proportional to the size of S. In particular, for a μ-strongly convex risk function and a subset S selected by CRAIG that estimates the full gradient by an error of at most ǫ, we prove that IG on S with diminishing stepsize αk = α/kτ at epoch k (with 0 < τ < 1 and 0 < α), converges to an 2Rǫ/μ2 neighborhood of the optimal solution at rate O(1/√k). Here, R = min{d0, (rγmaxC + ǫ)/μ} where d0 is the initial distance to the optimum, C is an upperbound on the norm of the gradients, r = |S|, and γmax is the largest weight for the elements in the subset obtained by CRAIG. Moreover, we prove that if in addition to the strong convexity, component functions have smooth gradients, IG with the same diminishing step size on subset S converges to a 2ǫ/μ neighborhood of the optimum solution at rate O(1/kτ ). The above implies that IG on S converges to the same solution and in the same number of epochs as IG on the full V . But because every epoch only uses a subset S of the data, it requires fewer gradient computations and thus leads to a |V |/|S| speedup over traditional IG methods, while still (approximately) converging to the optimal solution. We also note that CRAIG is complementary to various incremental gradient (IG) methods (SGD, SAGA, SVRG, Adam), and such methods can be used on the subset S found by CRAIG. We also demonstrate the effectiveness of CRAIG via an extensive set of experiments using logistic regression (a convex optimization problem) as well as training deep neural networks (non-convex optimization problems). We show that CRAIG speeds up incremental gradient methods, including SGD, SAGA, and SVRG. In particular, CRAIG while achieving practically the same loss and accuracy as the underlying incremental gradient descent methods, speeds up gradient methods by up to 6x for convex and 3x for non-convex loss functions.	https://arxiv.org/pdf/2301.13287	MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning	Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results indicate that MILO can train models 3 x -10x faster and tune hyperparameters 20 x -75x faster than full-dataset training or tuning without compromising performance.	Deep learning has achieved remarkable success in a multitude of machine learning tasks, including natural language processing, computer vision, and speech recognition in recent years. This success is partially due to the availability of massive training datasets and the capacity to train large-scale neural networks. However, training deep models on extensive datasets is computationally demanding, incurring significant financial costs and generating substantial CO2 emissions [63, 59 ]. Bhavya et al. [ 2] overview several research trajectories aimed at enhancing model convergence and reducing training time and costs, including data subset selection, curriculum learning, model architecture improvements, and optimization algorithm enhancements. Our work specifically focuses on selecting useful, generalizable data subsets for efficient deep neural network training. Recent research [66 , 5 , 21 ] suggests that many current training datasets are redundant, indicating that a non-redundant, informative subset could achieve comparable performance to training on the full dataset. To identify such informative samples, metrics such as prediction uncertainty [ 7], prediction flips [ 66 , 72 ], loss [42 ], or gradient/gradient-norm [ 21, 47 , 28, 27 , 29 ] have been applied. These metrics, calculated using either the downstream machine learning model or a lightweight surrogate [ 7], require a fully converged model, a requirement that runs counter to the goal of efficient training. To address this, existing subset selection algorithms [ 47 , 28 , 27, 29, 30 , 54 ] for efficient learning utilize the downstream model during training for heuristic computation and periodically update the subset as the model-dependent metrics for each sample evolve. Drawbacks of Model-Dependent Subset Selection: Despite the theoretical advantages of existing subset selection strategies[ 28 , 27, 47 ], they often fall short in computational efficiency compared to adaptive random subset selection, which selects random subsets periodically. This is primarily because traditional approaches rely on downstream models and typically require the calculation of sample metrics, such as gradients, before each subset selection step. Furthermore, these computationally demanding subset selection steps occur during model training. For instance, Figure 1 compares the convergence rate of the ResNet18 model on the CIFAR100 dataset, in terms of both time and epochs. Here, the model uses 10% subsets chosen every epoch by GradMatchPB [ 27 ], a state-of-the-art data subset selection strategy for efficient training, CraigPB [ 47 ], and Adaptive-Random (where a new 10% subset is randomly selected periodically). The selection of a new subset every epoch is done to demonstrate the maximum performance attainable by GradMatchPB and CraigPB. The results indicate that GradMatchPB achieves faster epoch convergence than both Adaptive-Random and CraigPB when selecting a new subset each epoch. However, due to the necessity for a computationally intensive subset selection step every epoch, both GradMatchPB and CraigPB demonstrate considerable inefficiency in terms of training time. In their respective studies, Killamsetty et al. [27 ] and Mirzasoleiman et al. [47] suggested selecting a new subset every R epochs to enhance training efficiency. However, this comes at the cost of the model’s convergence rate. Lastly, model-dependent subset selection requires a computationally intensive subset selection steps each time a new model is trained. Model-Agnostic Selection: Our primary insight is that a model-agnostic subset selection framework can circumvent the computationally intensive subset selection steps during model training by conducting subset selection in the preprocessing phase. By pre-selecting subsets and storing them as metadata with each dataset, we can train numerous models without incurring additional costs, effectively spreading out the expense of subset selection. In this work, we endeavor to answer the following question: Is it possible to develop a model-agnostic subset selection method that selects new subsets in a minimal amount of time, yet achieves superior model convergence without significant compromise in test accuracy or generalization performance?  MILO Framework: In this study, we introduce MILO, a model-agnostic subset selection framework designed for efficient model training and tuning. MILO employs submodular measures [ 13 , 23 ], which capture higher-order interactions between data samples for subset selection. We utilize pre-trained large language models [ 55 ] and pre-trained vision transformers [25 ] as feature encoders due to their zero-shot feature encoding capabilities. These encoders compute the sample metrics in a nominal amount of time, and this computation is independent of the downstream model, rendering MILO model-agnostic. Figure 3 provides a visual representation of MILO for model training, comprising two steps: a) A pre-processing step that involves the selection of multiple subsets from the training dataset using "Stochastic-Greedy Exploration (SGE)" (refer to Section 3.1.1), and the construction of a probability distribution over the entire dataset for subset sampling through "Weighted Random Exploration (WRE)" (refer to Section 3.1.2); b) A model training scheme that involves training the model on a curriculum of easy-to-hard subsets (refer to Section 3.1.3) using the subsets selected and the probability distribution constructed in the pre-processing step. We also present a class-wise partitioning trick to significantly minimize the memory footprint of MILO (refer to Section 3.2). Effectiveness of MILO : Through extensive experiments on multiple real-world datasets, we empirically demonstrate the effectiveness of the MILO framework for efficient training and hyperparameter tuning. In Figure 2, we provide a summary of the speedup achieved by MILO compared to full data training and tuning, along with the corresponding relative performance. Our results show that MILO can train models 3x to 10x faster and tune hyperparameters 20x to 75x faster, with minimal loss in performance. Furthermore, we consistently outperform the subset selection baselines that were considered. Additionally, MILO exhibits significantly faster initial model convergence and maintains faster convergence throughout training compared to other baselines. These qualities make it highly effective for applications such as hyperparameter tuning and neural architecture search, where the ability to distinguish good models from bad models early in the tuning process is crucial for enhancing efficiency [38, 37]. Related Work: In recent years, data-subset selection strategies have achieved success in various machine learning applications such as speech recognition [ 68 , 67], machine translation [ 32 ], active learning [ 60 , 1 , 33 ], hyper-parameter tuning [ 30 ], continual learning [65 ], domain adaptation [20 ], and computer vision [22]. A recent empirical study [5] demonstrated that datasets frequently contain semantic redundancies. These redundancies can be removed during training without affecting performance. In response, data pruning methods [66 , 51, 62] have developed principled selection criteria for pruning redundant samples before model training, with minimal loss in performance. However, both existing data pruning and compute-efficient learning approaches are model-dependent, resulting in them being computationally expensive. In contrast, our method is model-agnostic and capable of identifying high-quality subsets by intelligent data exploration. Furthermore, our approach can achieve superior model convergence by utilizing an easy-to-hard curriculum, transitioning from representative to diverse subsets during training.	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/1906.01827	Coresets for Data-Efficient Training of Machine Learning Models	Incremental gradient (IG) methods, such as stochastic gradient descent and its variants are commonly used for large scale optimization in machine learning. Despite the sustained effort to make IG methods more data-efficient, it remains an open question how to select a training data subset that can theoretically and practically perform on par with the full dataset. Here we develop CRAIG, a method to select a weighted subset (or coreset) of training data that closely estimates the full gradient by maximizing a submodular function. We prove that applying IG to this subset is guaranteed to converge to the (near)optimal solution with the same convergence rate as that of IG for convex optimization. As a result, CRAIG achieves a speedup that is inversely proportional to the size of the subset. To our knowledge, this is the first rigorous method for data-efficient training of general machine learning models. Our extensive set of experiments show that CRAIG, while achieving practically the same solution, speeds up various IG methods by up to 6x for logistic regression and 3x for training deep neural networks.	Mathematical optimization lies at the core of training largescale machine learning systems, and is now widely used over massive data sets with great practical success, assuming sufficient data resources are available. Achieving this success, however, also requires large amounts of (often GPU) computing, as well as concomitant financial expenditures and energy usage (Strubell et al., 2019). Significantly decreasing these costs without decreasing the learnt system’s resulting accuracy is one of the grand challenges of machine learning and artificial intelligence today (Asi & Duchi, 2019). Training machine learning models often reduces to optimizing a regularized empirical risk function. Given a convex loss l, and a μ-strongly convex regularizer r, one aims to find model parameter vector w∗ over the parameter space W that minimizes the loss f over the training data V : w∗ ∈ arg minw∈W f (w), f (w) := ∑ i∈V fi(w) + r(w), fi(w) = l(w, (xi, yi)), (1) where V = {1, . . . , n} is an index set of the training data, and functions fi : Rd �' R are associated with training examples (xi, yi), where xi ∈ Rd is the feature vector, and yi is the point i’s label. Standard Gradient Descent can find the minimizer of this problem, but requires repeated computations of the full gradient ∇f (w)—sum of the gradients over all training data points/functions i—and is therefore prohibitive for massive data sets. This issue is further exacerbated in case of deep neural networks where gradient computations (backpropagation) are expensive. Incremental Gradient (IG) methods, such as Stochastic Gradient Descent (SGD) and its accelerated variants, including SGD with momentum (Qian, 1999), Adagrad (Duchi et al., 2011), Adam (Kingma & Ba, 2014), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013) iteratively estimate the gradient on random subsets/batches of training data. While this provides an unbiased estimate of the full gradient, the randomized batches introduce variance in the gradient estimate (Hofmann et al., 2015), and therefore stochastic gradient methods are in general slow to converge (Johnson & Zhang, 2013; Defazio et al., 2014). The majority of the work speeding up IG methods has thus primarily focused on reducing the variance of the gradient estimate (SAGA (Defazio et al., 2014), SVRG (Johnson & Zhang, 2013), Katysha (Allen-Zhu, 2017)) or more carefully selecting the gradient stepsize (Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), Adam (Kingma & Ba, 2014)). However, the direction that remains largely unexplored is how to carefully select a small subset S ⊆ V of the fullCoresets for Data-efficient Training of Machine Learning Models PAGE 2 training data V , so that the model is trained only on the subset S while still (approximately) converging to the globally optimal solution (i.e., the model parameters that would be obtained if training/optimizing on the full V ). If such a subset S can be quickly found, then this would directly lead to a speedup of |V |/|S| (which can be very large if |S| ≪ |V |) per epoch of IG. There are four main challenges in finding such a subset S. First is that a guiding principle for selecting S is unclear. For example, selecting training points close to the decision boundary might allow the model to fine tune the decision boundary, while picking the most diverse set of data points would allow the model to get a better sense of the training data distribution. Second is that finding S must be fast, as otherwise identifying the set S may take longer than the actual optimization, and so no overall speed-up would be achieved. Third is that finding a subset S is not enough. One also has to decide on a gradient stepsize for each data point in S, as they affect the convergence. And last, while the method might work well empirically on some data sets, one also requires theoretical understanding and mathematical convergence guarantees. Here we develop Coresets for Accelerating Incremental Gradient descent (CRAIG), for selecting a subset of training data points to speed up training of large machine learning models. Our key idea is to select a weighted subset S of training data V that best approximates the full gradient of V . We prove that the subset S that minimizes an upper-bound on the error of estimating the full gradient maximizes a submodular facility location function. Hence, S can be efficiently found using a fast greedy algorithm. We also provide theoretical analysis of CRAIG and prove its convergence. Most importantly, we show that any incremental gradient method (IG) on S converges in the same number epochs as the same IG would on the full V , which means that we obtain a speed-up inversely proportional to the size of S. In particular, for a μ-strongly convex risk function and a subset S selected by CRAIG that estimates the full gradient by an error of at most ǫ, we prove that IG on S with diminishing stepsize αk = α/kτ at epoch k (with 0 < τ < 1 and 0 < α), converges to an 2Rǫ/μ2 neighborhood of the optimal solution at rate O(1/√k). Here, R = min{d0, (rγmaxC + ǫ)/μ} where d0 is the initial distance to the optimum, C is an upperbound on the norm of the gradients, r = |S|, and γmax is the largest weight for the elements in the subset obtained by CRAIG. Moreover, we prove that if in addition to the strong convexity, component functions have smooth gradients, IG with the same diminishing step size on subset S converges to a 2ǫ/μ neighborhood of the optimum solution at rate O(1/kτ ). The above implies that IG on S converges to the same solution and in the same number of epochs as IG on the full V . But because every epoch only uses a subset S of the data, it requires fewer gradient computations and thus leads to a |V |/|S| speedup over traditional IG methods, while still (approximately) converging to the optimal solution. We also note that CRAIG is complementary to various incremental gradient (IG) methods (SGD, SAGA, SVRG, Adam), and such methods can be used on the subset S found by CRAIG. We also demonstrate the effectiveness of CRAIG via an extensive set of experiments using logistic regression (a convex optimization problem) as well as training deep neural networks (non-convex optimization problems). We show that CRAIG speeds up incremental gradient methods, including SGD, SAGA, and SVRG. In particular, CRAIG while achieving practically the same loss and accuracy as the underlying incremental gradient descent methods, speeds up gradient methods by up to 6x for convex and 3x for non-convex loss functions.	https://arxiv.org/pdf/2103.00128	PRISM: A Rich Class of Parameterized Submodular Information Measures for Guided Subset Selection	With ever-increasing dataset sizes, subset selection techniques are becoming increasingly important for a plethora of tasks. It is often necessary to guide the subset selection to achieve certain desiderata, which includes focusing or targeting certain data points, while avoiding others. Examples of such problems include: i) targeted learning, where the goal is to find subsets with rare classes or rare attributes on which the model is underperforming, and ii) guided summarization, where data (e.g., image collection, text, document or video) is summarized for quicker human consumption with specific additional user intent. Motivated by such applications, we present PRISM, a rich class of PaRameterIzed Submodular information Measures. Through novel functions and their parameterizations, PRISM offers a variety of modeling capabilities that enable a trade-off between desired qualities of a subset like diversity or representation and similarity/dissimilarity with a set of data points. We demonstrate how PRISM can be applied to the two real-world problems mentioned above, which require guided subset selection. In doing so, we show that PRISM interestingly generalizes some past work, therein reinforcing its broad utility. Through extensive experiments on diverse datasets, we demonstrate the superiority of PRISM over the state-of-the-art in targeted learning and in guided imagecollection summarization. PRISM is available as a part of the SUBMODLIB (https://github.com/decile-team/submodlib) and TRUST (https://github.com/decile-team/trust) toolkits.	Recent times have seen explosive growth in data across several modalities, including text, images, and videos. This has given rise to the need for finding techniques for selecting effective smaller data subsets with specific characteristics for a variety of down-stream tasks. Often, we would like to guide the data selection to either target or avoid a certain set of data slices. One application is, what we call, targeted learning, where the goal is to select data points similar to data slices on which the model is currently performing poorly. These slices are data points that either belong to rare classes or have common rare attributes (e.g., color, background, etc.). An example of such a scenario is shown in Fig. 1(a), where a self-driving car model struggles in detecting “cars in a dark background“ because of a lack of such images in the training set. The targeted learning problem is to augment the training dataset with more of such rare images, with an aim to improve model performance. Another example is detecting cancers in biomedical imaging datasets, where the number of cancerous images are often a small fraction of the non-cancerous images. Another application comes from the summarization task, where an image collection, a video, or a text document is summarized for quicker human consumption by eliminating redundancy, while preserving the main content. While a number of applications require generic summarization (i.e., simply picking a representative and diverse subset of the massive dataset), it is often important to capture certain user intent in summarization. We call this guided summarization. Examples of guided summarization include: (i) query-focused summarization (Sharghi, Gong, and Shah 2016; Xiao et al. 2020), where a summary similar to a specific query is desired, and (ii) privacy-preserving summarization, where a summary dissimilar to a given private set of data points is desired (say, for privacy issues). See Fig. 1(b) for a pictorial illustration. PRISM Framework: We define PRISM through different instantiations and parameterizations of various submodular information measures (Sec. 2). These allow for modeling a spectrum of semantics required for guided subset selection, like relevance to a query set, irrelevance to a private set, and diversity among selected data points. We study the effect of parameter trade-off among these different semantics and present interesting insights. PRISM for Targeted Learning: We present a novel algorithm (Sec. 3.1, Algo. 1) to apply PRISM for targeted learning, which aims to improve a model’s performance on rare slices of data. Specifically, we show that submodular information measures are very effective in finding the examples from the rare classes in a large unlabeled set (akin to finding a needle in a haystack). On several image classification tasks, PRISM obtains ≈ 20-30% gain in accuracy of rare classes (≈ 12% more than existing approaches) by just adding a few additional labeled points from the rare classes. Furthermore, we show that PRISM is 20× to 50× more label-efficient compared to random sampling, and 2× to 4× more labelefficient compared to existing approaches (see Sec. 4.1). We also show that Algo. 1 generalizes some existing approaches for data subset selection, reinforcing its utility (Sec. 3.3). PRISM for Guided Summarization. We propose a learning framework for guided summarization using PRISM (Sec. 3.2). We show that PRISM offers a unified treatment to the different flavors of guided summarization (query-focused and privacy-preserving) and generalizes some existing approaches to summarization, again reinforcing its utility. We show that it outperforms other existing approaches on a real-world image collections dataset (Sec. 4.2).	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/1906.01827	Coresets for Data-Efficient Training of Machine Learning Models	Incremental gradient (IG) methods, such as stochastic gradient descent and its variants are commonly used for large scale optimization in machine learning. Despite the sustained effort to make IG methods more data-efficient, it remains an open question how to select a training data subset that can theoretically and practically perform on par with the full dataset. Here we develop CRAIG, a method to select a weighted subset (or coreset) of training data that closely estimates the full gradient by maximizing a submodular function. We prove that applying IG to this subset is guaranteed to converge to the (near)optimal solution with the same convergence rate as that of IG for convex optimization. As a result, CRAIG achieves a speedup that is inversely proportional to the size of the subset. To our knowledge, this is the first rigorous method for data-efficient training of general machine learning models. Our extensive set of experiments show that CRAIG, while achieving practically the same solution, speeds up various IG methods by up to 6x for logistic regression and 3x for training deep neural networks.	Mathematical optimization lies at the core of training largescale machine learning systems, and is now widely used over massive data sets with great practical success, assuming sufficient data resources are available. Achieving this success, however, also requires large amounts of (often GPU) computing, as well as concomitant financial expenditures and energy usage (Strubell et al., 2019). Significantly decreasing these costs without decreasing the learnt system’s resulting accuracy is one of the grand challenges of machine learning and artificial intelligence today (Asi & Duchi, 2019). Training machine learning models often reduces to optimizing a regularized empirical risk function. Given a convex loss l, and a μ-strongly convex regularizer r, one aims to find model parameter vector w∗ over the parameter space W that minimizes the loss f over the training data V : w∗ ∈ arg minw∈W f (w), f (w) := ∑ i∈V fi(w) + r(w), fi(w) = l(w, (xi, yi)), (1) where V = {1, . . . , n} is an index set of the training data, and functions fi : Rd �' R are associated with training examples (xi, yi), where xi ∈ Rd is the feature vector, and yi is the point i’s label. Standard Gradient Descent can find the minimizer of this problem, but requires repeated computations of the full gradient ∇f (w)—sum of the gradients over all training data points/functions i—and is therefore prohibitive for massive data sets. This issue is further exacerbated in case of deep neural networks where gradient computations (backpropagation) are expensive. Incremental Gradient (IG) methods, such as Stochastic Gradient Descent (SGD) and its accelerated variants, including SGD with momentum (Qian, 1999), Adagrad (Duchi et al., 2011), Adam (Kingma & Ba, 2014), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013) iteratively estimate the gradient on random subsets/batches of training data. While this provides an unbiased estimate of the full gradient, the randomized batches introduce variance in the gradient estimate (Hofmann et al., 2015), and therefore stochastic gradient methods are in general slow to converge (Johnson & Zhang, 2013; Defazio et al., 2014). The majority of the work speeding up IG methods has thus primarily focused on reducing the variance of the gradient estimate (SAGA (Defazio et al., 2014), SVRG (Johnson & Zhang, 2013), Katysha (Allen-Zhu, 2017)) or more carefully selecting the gradient stepsize (Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), Adam (Kingma & Ba, 2014)). However, the direction that remains largely unexplored is how to carefully select a small subset S ⊆ V of the fullCoresets for Data-efficient Training of Machine Learning Models PAGE 2 training data V , so that the model is trained only on the subset S while still (approximately) converging to the globally optimal solution (i.e., the model parameters that would be obtained if training/optimizing on the full V ). If such a subset S can be quickly found, then this would directly lead to a speedup of |V |/|S| (which can be very large if |S| ≪ |V |) per epoch of IG. There are four main challenges in finding such a subset S. First is that a guiding principle for selecting S is unclear. For example, selecting training points close to the decision boundary might allow the model to fine tune the decision boundary, while picking the most diverse set of data points would allow the model to get a better sense of the training data distribution. Second is that finding S must be fast, as otherwise identifying the set S may take longer than the actual optimization, and so no overall speed-up would be achieved. Third is that finding a subset S is not enough. One also has to decide on a gradient stepsize for each data point in S, as they affect the convergence. And last, while the method might work well empirically on some data sets, one also requires theoretical understanding and mathematical convergence guarantees. Here we develop Coresets for Accelerating Incremental Gradient descent (CRAIG), for selecting a subset of training data points to speed up training of large machine learning models. Our key idea is to select a weighted subset S of training data V that best approximates the full gradient of V . We prove that the subset S that minimizes an upper-bound on the error of estimating the full gradient maximizes a submodular facility location function. Hence, S can be efficiently found using a fast greedy algorithm. We also provide theoretical analysis of CRAIG and prove its convergence. Most importantly, we show that any incremental gradient method (IG) on S converges in the same number epochs as the same IG would on the full V , which means that we obtain a speed-up inversely proportional to the size of S. In particular, for a μ-strongly convex risk function and a subset S selected by CRAIG that estimates the full gradient by an error of at most ǫ, we prove that IG on S with diminishing stepsize αk = α/kτ at epoch k (with 0 < τ < 1 and 0 < α), converges to an 2Rǫ/μ2 neighborhood of the optimal solution at rate O(1/√k). Here, R = min{d0, (rγmaxC + ǫ)/μ} where d0 is the initial distance to the optimum, C is an upperbound on the norm of the gradients, r = |S|, and γmax is the largest weight for the elements in the subset obtained by CRAIG. Moreover, we prove that if in addition to the strong convexity, component functions have smooth gradients, IG with the same diminishing step size on subset S converges to a 2ǫ/μ neighborhood of the optimum solution at rate O(1/kτ ). The above implies that IG on S converges to the same solution and in the same number of epochs as IG on the full V . But because every epoch only uses a subset S of the data, it requires fewer gradient computations and thus leads to a |V |/|S| speedup over traditional IG methods, while still (approximately) converging to the optimal solution. We also note that CRAIG is complementary to various incremental gradient (IG) methods (SGD, SAGA, SVRG, Adam), and such methods can be used on the subset S found by CRAIG. We also demonstrate the effectiveness of CRAIG via an extensive set of experiments using logistic regression (a convex optimization problem) as well as training deep neural networks (non-convex optimization problems). We show that CRAIG speeds up incremental gradient methods, including SGD, SAGA, and SVRG. In particular, CRAIG while achieving practically the same loss and accuracy as the underlying incremental gradient descent methods, speeds up gradient methods by up to 6x for convex and 3x for non-convex loss functions.	https://aclanthology.org/2024.findings-naacl.209.pdf	An End-to-End Submodular Framework for Data-Efficient In-Context Learning	Recent advancements in natural language tasks leverage the emergent In-Context Learning (ICL) ability of pretrained Large Language Models (LLMs). ICL enables LLMs to perform new tasks by utilizing a limited number of input-output examples as prompts. While ICL circumvents the costly step of finetuning LLMs, its effectiveness is heavily dependent on the quality and ordering of provided examples (called exemplars). In this work, we propose a two-stage data-efficient framework Div-S3 for exemplar selection for ICL. The first stage focuses on data annotation and employs a poolbased active learning approach to select a set of Diverse and informative exemplars from the target tasks’ unlabeled pool. Given a test input/query, the second stage uses Submodular Span Summarization (S3) to select the most relevant and non-redundant exemplars from the annotated pool of a limited budget. On 7 different NLP datasets and 5 LLMs of varying complexities, we show Div-S3 outperforms (1) existing active learning-based methods for data annotation for ICL and (2) similarity-based methods for test query-specific exemplars retrieval.	Pretrained large language models (LLMs) (Kenton and Toutanova, 2019; Brown et al., 2020; Chowdhery et al., 2022) have become foundational for a wide range of Natural Language Processing (NLP) tasks, demonstrating impressive success across various domains (Bommasani et al., 2021; Bubeck et al., 2023) through in-context learning (ICL) (Dong et al., 2022). ICL enables these pretrained LLMs to perform new tasks by using task-specific prompts containing a limited number of input-output demonstrations (also referred to as shots, exemplars, or prompts) in the natural language format. This approach facilitates deployment across different downstream tasks and reduces the need for labeled downstream training data since ICL does not require any task-specific training. The typical ICL procedure consists of two key components: (1) Exemplar annotation and retrieval (Wu et al., 2022; Köksal et al., 2022; Liu et al., 2022): This step involves annotating and retrieving exemplars that serve as context demonstrations. (2) Prompt template crafting (Sorensen et al., 2022; Deng et al., 2022): this step involves designing a prompt template to wrap these demonstrations in a comprehensible and coherent natural language instruction. Recent studies (Liu et al., 2022; Su et al., 2022; Margatina et al., 2023) show that providing exemplars most relevant to the current input instance is beneficial. Moreover, Zhao et al. (2021), Lu et al. (2022), and Liu et al. (2023) observe that LLMs attend more to the exemplars that are closer in the sequence to the input instance. Therefore, to achieve the best performance of ICL, the selection of exemplars and their ordering in the LLM prompt are crucial. In practice, an extensive collection of unlabeled exemplars is easily available (e.g., posts and discussions on forums like Stack Exchange or user-generated content on social media platforms), but manually annotating all exemplars would be exceptionally costly. To annotate and select the exemplars optimally for a given target task, we follow the two-stage approach shown in Figure 1: (1) Exemplar Annotation: select a subset of exemplars for annotation under a fixed budget (performed only once) and (2) Exemplar Retrieval: identify limited-sized exemplars in an ordering that are most influential for a given input instance from the annotated subset of exemplars. Intuitively, for the first stage, we aim to find the subset with maximal diversity and least redundancy so that, given any input, we can find corresponding labeled exemplars. For the second stage, in addition to the diversity requirement similar to the first stage, we emphasize the relevance of the exemplars to the given input query and order exemplars so that their relevance to the input query decreases as the exemplars are farther away from the input instance. We propose a framework Div-S3 based on submodular optimization that unifies the abovementioned two stages. For Exemplar Annotation, we model the problem as a submodular optimization problem under a cardinality constraint to find as Diverse a subset as possible within a budget. For Exemplar Retrieval, we formalize the problem as a Submodular Span Summarization (S3) problem (Kumari and Bilmes, 2021) with a knapsack constraint, which finds a diverse subset most relevant to the input query under a token length limit. Also, we naturally order the resulting exemplars based on the gains represented by the submodular function. The name of our proposed framework Div-S3 captures the optimization objectives used for both exemplar annotation (Div) and exemplar retrieval (S3) stages. In Fig. 2, we show a sample test query where using Div-S3 for exemplar selection leads to a more diverse and query-relevant exemplar set (more examples provided in Appendix D). Our framework is general, as any submodular function can be plugged into our method. For models beyond LMs, e.g., for text-image multi-modality models, we may use pre-existing submodular functions that are powerful for expressing diversity in the image domain. In addition, we account for relevance, diversity, and ordering for the exemplar retrieval stage, where one or two aspects typically get overlooked by previous methods. Empirically, we evaluate Div-S3 on 7 NLP tasks with 5 LLMs and show significantly improved performance compared to baselines. Our contributions are: 1. We propose an end-to-end framework DivS3 utilizing submodular optimization for performing data-efficient ICL using LLMs. Depending on budget requirements, Div-S3 provides the flexibility to set the budget either in terms of the number of exemplars to be used in the prompt or the LLM’s context window size. 2. We empirically validate the effectiveness of our framework on 7 different NLP tasks and show the transferability of results across LLMs of varying complexities. 3. We thoroughly analyze each component of Div-S3 by (a) studying S3 in a setting with no annotation budget constraint and (b) analyzing the sensitivity of the exemplars selected by Div-S3 to their position in the LLM’s prompt.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2310.16776	DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection	Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT-UCS, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to identify a smaller, representative dataset that reduces the data needed to finetune PLMs for the text-generation task of textediting. We examine the efficacy of DEFT-UCS across multiple text-editing tasks, and compare to the state-of-the art text-editing model, CoEDIT. Our results demonstrate that DEFTUCS models are just as accurate as CoEDIT, across eight different datasets consisting of six different editing tasks, while finetuned on 70% less data.	How much data do we need to fine-tune a pretrained language model (PLM) for a specific downstream task? While successes in language modelling have led to numerous publicly available PLMs and ability to produce fine-tuned models for downstream tasks - the answer mostly remains, "as large as possible, and of good quality". For example, Alpaca, an instruction-following model, is trained with 52k data samples (Taori et al., 2023). Similarly, CoPoet, a collaborative poetry writing system is fine-tuned using 87k data samples (Chakrabarty et al., 2022). MetaMath, a mathreasoning LLM is fine-tuned with 395k data samples (Yu et al., 2023). Although fine-tuning PLMs on specific task results in performance gain, acquiring large amounts of data for fine-tuning is not easy for real-world applications which often require niche knowledge and domain expertise. Researchers have explored variety of methods primarily focused on improving the computational efficiency of fine-tuning, including parameterefficient fine-tuning approaches (PEFT) to reduce computational costs by optimizing parameter updates (Fu et al., 2023; Hu et al., 2021) as well as leveraging active-learning for iteratively selecting data samples during training (Su et al., 2022; Diao et al., 2023). Instead, our work focuses on improving the data efficiency of PLM fine-tuning without requiring iterative fine-tuning. Specifically, we explore how to fine-tune PLMs with significantly less data samples and without a cost to model performance. Related to language models, researchers have experimented with different core-set selection metrics (Paul et al., 2021; Sorscher et al., 2022) to improve the data efficiency during pre-training. Marion et al. (2023) demonstrated how perplexity, L2-Error Norm (EL2N) and memorization can be utilized to select smaller, good quality datasets for pre-training. Similarly, (Attendu and Corbeil, 2023) leverage EL2N to dynamically remove data samples with high EL2N between training epochs. However, these metrics assume access to task data and reference models to perform dataset pruning. In real world applications, utilizing such supervised, data-pruning metrics are less realistic since large amounts of annotated task-specific data may be costly to acquire. This leads us to our main research question: How can we leverage unsupervised data pruning to fine-tune PLMs for downstream tasks in a more data efficient manner? In this work, we introduce a new data-efficient fine-tuning framework, DEFT-UCS, that uses unsupervised core-set selection to minimize the amount of labelled data needed to fine-tune PLMs for the text-generation task of text-editing. Our framework is inspired by (Sorscher et al., 2022), who utilize clustering-based dataset pruning to reduce training samples for image-classification models, and to the best of our knowledge, our framework is the first to leverage unsupervised core-set selection for data-efficient fine-tuning of PLMs. We investigate the utility of DEFT-UCS in finetuning PLMs for text-generation across eight different datasets consisting of six different text-editing tasks, and compare DEFT-UCS models to the stateof-the-art text-editing model, CoEDIT(Raheja et al., 2023). Our contributions are as follows: - We introduce DEFT-UCS, a data-efficient-fine tuning framework that leverages unsupervised core-set selection via clustering to identify a smaller representative set of data needed to fine-tune PLMs. - We show that DEFT-UCS, utilizing only 32.5% of CoEDIT’s training data, is able to produce fine-tuned models with improved accuracy on four different text-editing tasks, and similar accuracy on two text-editing tasks compared to CoEDIT (Raheja et al., 2023). - We performed a human evaluation with 3 evaluators to assess the quality of text-edits from our DEFT-UCS model. Evaluators found edits generated by DEFT-UCS model as similar or preferred over CoEDIT (Raheja et al., 2023)	https://arxiv.org/pdf/2403.08370	SMART: Submodular Data Mixture Strategy for Instruction Tuning	Instruction Tuning involves finetuning a language model on a collection of instructionformatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) — a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal mixing. Furthermore, SMART facilitates the creation of data mixtures based on a few representative subsets of tasks alone and through task pruning analysis, we reveal that in a limited budget setting, allocating budget among a subset of representative tasks yields superior performance compared to distributing the budget among all tasks. The code for reproducing our results is open-sourced at https://github.com/kowndinyarenduchintala/SMART.	One of the main goals of artificial intelligence (AI) research is to build machines that can communicate (Turing, 1950), and an essential part of communication is to understand and follow instructions. Large Language Models (LLMs), which are pre-trained over massive text corpora on next-tokenprediction objective, can perform a wide range of NLP tasks via "prompting" (Brown et al., 2020; Kojima et al., 2022; Almazrouei et al., 2023; Liu et al., 2023; Touvron et al., 2023). Instruction Tuning (Wei et al., 2021; Sanh et al., 2021; Chung et al., 2022) is an approach that further enhances the instruction-following ability and generalizability of pre-trained LLMs to unseen tasks. It involves fine-tuning an LLM on a collection of instruction-formatted instances (encompassing multiple tasks) - each consisting of an instruction (or task description), an optional input, the corresponding output (the ground truth) and optionally a few demonstrations/examples. It is a special case of multitask learning where the LLM is finetuned on a collection of instruction-formatted multitask datasets (Chung et al., 2022). Finetuning on multiple tasks simultaneously, allows the model to share and transfer information across tasks, resulting in a better common internal representation that is preferred by all tasks while suppressing taskdependent noise (Caruana, 1997). Consequently, the model learns to generalize to unseen tasks by discerning helpful cues from both implicitly and explicitly related tasks that it has previously seen. The performance enhancement from instruction tuning is heavily contingent on data quality, data quantity, and task composition (Wang et al., 2023b). Studies by Iyer et al. (2022) and Longpre et al. (2023) have shown that while scaling the number of tasks is important, the relative proportion of various tasks (mixture weighting) merits as much attention for optimal instruction tuning. Intuitively, we want the model to see enough data for a given task that it can perform well on it, but not to see so much data that it memorizes the training set (Raffel et al., 2020). Iyer et al. (2022) performed manual tuning of various benchmark proportions and decided on a final mixture, whereas Longpre et al. (2023) studied the impact of removing each benchmark from the finetuning mixture and relied on their practioners’ intuition from there on, to decide on the exact proportions of benchmarks. In this work, we would like to explore a more systematic approach to mixture weighting. Specifically, we are motivated by the fact that in a large multitask dataset like FLAN 2022 (Longpre et al., 2023), which has 1840 tasks, there will likely be many similar tasks leading to redundancies and not all of them may require sampling in equal proportions. For instance, there might be many tasks of the type Natural Language Inference (NLI), and it might be enough to sample relatively more instances from a few representative NLI tasks and less from the others. Furthermore, which samples we select from each task is also crucial because the samples should faithfully represent the task at hand. A random subset may fail to do this as it can miss out on essential corner cases. With this context, we focus on the following two fundamental research questions (RQs) that form the basis for our subsequent inquiry: - (RQ1) Given a huge multitask instructiontuning dataset and a limited fine-tuning budget which is defined by the total number of (prompt, response) instances that can be used for fine-tuning, how do we divide this budget among thousands of tasks present in the dataset? i.e., how many instances to sample from each task? and which instances to sample from each task? - (RQ2) Can we go a step further and strategically prune some tasks altogether and only fine-tune on a small subset of representative tasks without hurting the performance? If yes, what is the nature of this subset? To the best of our knowledge, there’s currently no principled approach to determining task compositions for instruction tuning, other than manual tuning and/or practioners’ intuition. As a first step towards addressing both of the above RQs, we first define a common subset selection problem (more formally stated in Section 3) as follows - Given a huge collection of M instructionformatted task datasets, a task budget M ′ ≤ M and a total budget (N ′) of (prompt, response) pairs, which M ′ tasks to select? and how many instances to select from each of these M ′ tasks and which instances to select? Note that RQ1 is an instance of this problem where M ′ = M . Constrained Submodular Maximization (Section 2) proves to be a good model for discovering representative subsets (or coresets) of a massive training dataset (or ground set) that acts as surrogate (i.e., achieves similar performance) and are much better than uniformly-at-random subsets. Intuitively, this is because submodular functions model information in subsets, and hence maximizing a submodular function subject to a constraint yields non-redundant subsets of the ground set. An essential feature of this model is that it returns weighted subsets, i.e., each sample in the coreset comes with an associated score, which indicates how important the sample is. Inspired by submodular functions, we propose our solution (Section 3) to the above subset selection problem for instruction tuning that works in two stages. In the first stage, we select a weighted subset of tasks from the full dataset where the weights will determine how many samples to select from each task. In the next stage, we select samples from each task based on the assigned task budgets. Note that the submodular functions used in each stage are not necessarily identical (Section 4.8). The main contributions of our work can be summarized as follows: - We introduce SMART — a novel data mixture strategy for instruction tuning that models the data mixture problem (Section 3) as a sequence of two cardinality-constrained submodular maximization problems and offer empirical evidence that it outperforms both examples proportional and equal mixing baselines (Section 4) as well as the mixture weights proposed by Longpre et al. (2023). - Existing works like Longpre et al. (2023) have reported a continuous increase in performance upon increasing the number of tasks (though the gains themselves may be diminishing). However, we posit that this depends on the order in which new tasks are incorporated and show empirically that in the case of SMART mixtures, a performance peak is observed with an initial addition of few representative tasks and upon adding more and more tasks, the performance is not sustained (Section 4.6). - We find that the nature of instances that should be selected in each task (i.e, whether a representative or diverse subset) also depends on the total task budget, M ′ (Section 4.8). For higher M ′s, each task on average gets the relatively low budget and selecting representative samples is more important; however for lower M ′s, when there is sufficient enough budget for each task, the need for diversity dominates	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2310.16776	DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection	Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT-UCS, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to identify a smaller, representative dataset that reduces the data needed to finetune PLMs for the text-generation task of textediting. We examine the efficacy of DEFT-UCS across multiple text-editing tasks, and compare to the state-of-the art text-editing model, CoEDIT. Our results demonstrate that DEFTUCS models are just as accurate as CoEDIT, across eight different datasets consisting of six different editing tasks, while finetuned on 70% less data.	How much data do we need to fine-tune a pretrained language model (PLM) for a specific downstream task? While successes in language modelling have led to numerous publicly available PLMs and ability to produce fine-tuned models for downstream tasks - the answer mostly remains, "as large as possible, and of good quality". For example, Alpaca, an instruction-following model, is trained with 52k data samples (Taori et al., 2023). Similarly, CoPoet, a collaborative poetry writing system is fine-tuned using 87k data samples (Chakrabarty et al., 2022). MetaMath, a mathreasoning LLM is fine-tuned with 395k data samples (Yu et al., 2023). Although fine-tuning PLMs on specific task results in performance gain, acquiring large amounts of data for fine-tuning is not easy for real-world applications which often require niche knowledge and domain expertise. Researchers have explored variety of methods primarily focused on improving the computational efficiency of fine-tuning, including parameterefficient fine-tuning approaches (PEFT) to reduce computational costs by optimizing parameter updates (Fu et al., 2023; Hu et al., 2021) as well as leveraging active-learning for iteratively selecting data samples during training (Su et al., 2022; Diao et al., 2023). Instead, our work focuses on improving the data efficiency of PLM fine-tuning without requiring iterative fine-tuning. Specifically, we explore how to fine-tune PLMs with significantly less data samples and without a cost to model performance. Related to language models, researchers have experimented with different core-set selection metrics (Paul et al., 2021; Sorscher et al., 2022) to improve the data efficiency during pre-training. Marion et al. (2023) demonstrated how perplexity, L2-Error Norm (EL2N) and memorization can be utilized to select smaller, good quality datasets for pre-training. Similarly, (Attendu and Corbeil, 2023) leverage EL2N to dynamically remove data samples with high EL2N between training epochs. However, these metrics assume access to task data and reference models to perform dataset pruning. In real world applications, utilizing such supervised, data-pruning metrics are less realistic since large amounts of annotated task-specific data may be costly to acquire. This leads us to our main research question: How can we leverage unsupervised data pruning to fine-tune PLMs for downstream tasks in a more data efficient manner? In this work, we introduce a new data-efficient fine-tuning framework, DEFT-UCS, that uses unsupervised core-set selection to minimize the amount of labelled data needed to fine-tune PLMs for the text-generation task of text-editing. Our framework is inspired by (Sorscher et al., 2022), who utilize clustering-based dataset pruning to reduce training samples for image-classification models, and to the best of our knowledge, our framework is the first to leverage unsupervised core-set selection for data-efficient fine-tuning of PLMs. We investigate the utility of DEFT-UCS in finetuning PLMs for text-generation across eight different datasets consisting of six different text-editing tasks, and compare DEFT-UCS models to the stateof-the-art text-editing model, CoEDIT(Raheja et al., 2023). Our contributions are as follows: - We introduce DEFT-UCS, a data-efficient-fine tuning framework that leverages unsupervised core-set selection via clustering to identify a smaller representative set of data needed to fine-tune PLMs. - We show that DEFT-UCS, utilizing only 32.5% of CoEDIT’s training data, is able to produce fine-tuned models with improved accuracy on four different text-editing tasks, and similar accuracy on two text-editing tasks compared to CoEDIT (Raheja et al., 2023). - We performed a human evaluation with 3 evaluators to assess the quality of text-edits from our DEFT-UCS model. Evaluators found edits generated by DEFT-UCS model as similar or preferred over CoEDIT (Raheja et al., 2023)	https://arxiv.org/pdf/2012.10630	GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning	Large scale machine learning and deep models are extremely data-hungry. Unfortunately, obtaining large amounts of labeled data is expensive, and training state-of-the-art models (with hyperparameter tuning) requires significant computing resources and time. Secondly, real-world data is noisy and imbalanced. As a result, several recent papers try to make the training process more efficient and robust. However, most existing work either focuses on robustness or efficiency, but not both. In this work, we introduce GLISTER, a GeneraLIzation based data Subset selecTion for Efficient and Robust learning framework. We formulate GLISTER as a mixed discretecontinuous bi-level optimization problem to select a subset of the training data, which maximizes the log-likelihood on a held-out validation set. We then analyze GLISTER for simple classifiers such as gaussian and multinomial naive-bayes, k-nearest neighbor classifier, and linear regression and show connections to submodularity. Next, we propose an iterative online algorithm GLISTER-ONLINE, which performs data selection iteratively along with the parameter updates and can be applied to any loss-based learning algorithm. We then show that for a rich class of loss functions including cross-entropy, hinge-loss, squared-loss, and logistic-loss, the inner discrete data selection is an instance of (weakly) submodular optimization, and we analyze conditions for which GLISTER-ONLINE reduces the validation loss and converges. Finally, we propose GLISTER-ACTIVE, an extension to batch active learning, and we empirically demonstrate the performance of GLISTER on a wide range of tasks including, (a) data selection to reduce training time, (b) robust learning under label noise and imbalance settings, and (c) batch-active learning with several deep and shallow models. We show that our framework improves upon state of the art both in efficiency and accuracy (in cases (a) and (c)) and is more efficient compared to other state-ofthe-art robust learning algorithms in case (b). The code for GLISTERis at:https://github.com/dssresearch/GLISTER.	With the quest to achieve human-like performance for machine learning and deep learning systems, the cost of training and deploying machine learning models has been significantly increasing. The wasted computational and engineering energy becomes evident in deep learning algorithms, wherein extensive hyper-parameter tuning and network architecture search need to be done. This results in staggering compute costs and running times. As a result, efficient and robust machine learning is a very relevant and substantial research problem. In this paper, we shall focus on three goals: Goal 1: Train machine learning and deep learning models on effective subsets of data, thereby significantly reducing training time and compute while not sacrificing accuracy. Goal 2: To (iteratively) select effective subsets of labeled data to reduce the labeling cost. Goal 3: Select data subsets to remove noisy labels and class imbalance, which is increasingly common in operational machine learning settings. Most prior work discussed above, either study robustness or efficiency, but not both. For example, the data selection approaches such as (Wei, Iyer, and Bilmes 2015; Mirzasoleiman, Bilmes, and Leskovec 2020; Shinohara 2014) and others focus on approximating either gradients or performance on the training sets, and hence would not be suitable for scenarios such as label noise and imbalance. On the other hand, the approaches like (Ren et al. 2018; Jiang et al. 2018) and others, focus on robustness but are not necessarily efficient. For example, the approach of (Ren et al. 2018) requires 3x the standard (deep) training cost, to obtain a robust model. GLISTER is the first framework, to the best of our knowledge, which focuses on both efficiency and robustness. Our work is closely related to the approaches of (Wei, Iyer, and Bilmes 2015) and (Ren et al. 2018). We build upon the work of (Wei, Iyer, and Bilmes 2015), by first generalizing their framework beyond simple classifiers (like nearest neighbor and naive bayes), but with general loss functions. We do this by proposing an iterative algorithm GLISTER-ONLINE which does data selection via a meta-learning based approach along with parameter updates. Furthermore, we pose the problem as optimizing the validation set performance as opposed to training set performance, thereby encouraging generalization. Next, our approach also bears similarity to (Ren et al. 2018), except that we need to solve a discrete optimization problem instead of a meta-gradient update. Moreover, we do not run our data selection every iteration, thereby ensuring that we are significantly faster than a single training run. Finally, we extend our algorithm to the active learning scenario. We demonstrate that our framework is more efficient and accurate compared to existing data selection and active learning algorithms, and secondly, also generalizes well under noisy data, and class imbalance scenarios. In particular, we show that GLISTER achieves a 3x - 6x speedup on a wide range of models and datasets, with very small loss in accuracy.	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2310.16776	DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection	Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT-UCS, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to identify a smaller, representative dataset that reduces the data needed to finetune PLMs for the text-generation task of textediting. We examine the efficacy of DEFT-UCS across multiple text-editing tasks, and compare to the state-of-the art text-editing model, CoEDIT. Our results demonstrate that DEFTUCS models are just as accurate as CoEDIT, across eight different datasets consisting of six different editing tasks, while finetuned on 70% less data.	How much data do we need to fine-tune a pretrained language model (PLM) for a specific downstream task? While successes in language modelling have led to numerous publicly available PLMs and ability to produce fine-tuned models for downstream tasks - the answer mostly remains, "as large as possible, and of good quality". For example, Alpaca, an instruction-following model, is trained with 52k data samples (Taori et al., 2023). Similarly, CoPoet, a collaborative poetry writing system is fine-tuned using 87k data samples (Chakrabarty et al., 2022). MetaMath, a mathreasoning LLM is fine-tuned with 395k data samples (Yu et al., 2023). Although fine-tuning PLMs on specific task results in performance gain, acquiring large amounts of data for fine-tuning is not easy for real-world applications which often require niche knowledge and domain expertise. Researchers have explored variety of methods primarily focused on improving the computational efficiency of fine-tuning, including parameterefficient fine-tuning approaches (PEFT) to reduce computational costs by optimizing parameter updates (Fu et al., 2023; Hu et al., 2021) as well as leveraging active-learning for iteratively selecting data samples during training (Su et al., 2022; Diao et al., 2023). Instead, our work focuses on improving the data efficiency of PLM fine-tuning without requiring iterative fine-tuning. Specifically, we explore how to fine-tune PLMs with significantly less data samples and without a cost to model performance. Related to language models, researchers have experimented with different core-set selection metrics (Paul et al., 2021; Sorscher et al., 2022) to improve the data efficiency during pre-training. Marion et al. (2023) demonstrated how perplexity, L2-Error Norm (EL2N) and memorization can be utilized to select smaller, good quality datasets for pre-training. Similarly, (Attendu and Corbeil, 2023) leverage EL2N to dynamically remove data samples with high EL2N between training epochs. However, these metrics assume access to task data and reference models to perform dataset pruning. In real world applications, utilizing such supervised, data-pruning metrics are less realistic since large amounts of annotated task-specific data may be costly to acquire. This leads us to our main research question: How can we leverage unsupervised data pruning to fine-tune PLMs for downstream tasks in a more data efficient manner? In this work, we introduce a new data-efficient fine-tuning framework, DEFT-UCS, that uses unsupervised core-set selection to minimize the amount of labelled data needed to fine-tune PLMs for the text-generation task of text-editing. Our framework is inspired by (Sorscher et al., 2022), who utilize clustering-based dataset pruning to reduce training samples for image-classification models, and to the best of our knowledge, our framework is the first to leverage unsupervised core-set selection for data-efficient fine-tuning of PLMs. We investigate the utility of DEFT-UCS in finetuning PLMs for text-generation across eight different datasets consisting of six different text-editing tasks, and compare DEFT-UCS models to the stateof-the-art text-editing model, CoEDIT(Raheja et al., 2023). Our contributions are as follows: - We introduce DEFT-UCS, a data-efficient-fine tuning framework that leverages unsupervised core-set selection via clustering to identify a smaller representative set of data needed to fine-tune PLMs. - We show that DEFT-UCS, utilizing only 32.5% of CoEDIT’s training data, is able to produce fine-tuned models with improved accuracy on four different text-editing tasks, and similar accuracy on two text-editing tasks compared to CoEDIT (Raheja et al., 2023). - We performed a human evaluation with 3 evaluators to assess the quality of text-edits from our DEFT-UCS model. Evaluators found edits generated by DEFT-UCS model as similar or preferred over CoEDIT (Raheja et al., 2023)	https://arxiv.org/pdf/2410.11303	TSDS: Data Selection for Task-Specific Model Finetuning	Finetuning foundation models for specific tasks is an emerging paradigm in modern machine learning. The efficacy of task-specific finetuning largely depends on the selection of appropriate training data. We present TSDS (Task-Specific Data Selection), a framework to select data for task-specific model finetuning, guided by a small but representative set of examples from the target task. To do so, we formulate data selection for task-specific finetuning as an optimization problem with a distribution alignment loss based on optimal transport to capture the discrepancy between the selected data and the target distribution. In addition, we add a regularizer to encourage the diversity of the selected data and incorporate kernel density estimation into the regularizer to reduce the negative effects of near-duplicates among the candidate data. We connect our optimization problem to nearest neighbor search and design efficient algorithms to compute the optimal solution based on approximate nearest neighbor search techniques. We evaluate our method on data selection for both continued pretraining and instruction tuning of language models. We show that instruction tuning using data selected by our method with a 1% selection ratio often outperforms using the full dataset and beats the baseline selection methods by 1.5 points in F1 score on average. Our code is available at https://github.com/ZifanL/TSDS.	inetuning foundation models [3 ] is the de-facto paradigm for building machine learning applications that focus on specific tasks. Models such as BERT [10] and LLaMA [ 43] are large-scale models pretrained on massive unlabeled data across a wide range of domains. Those models can be specialized to downstream tasks through finetuning. Finetuning can take a variety of forms depending on the target task. For instance, continued pretraining [ 17 ] extends the pretraining stage of a model on a dataset that is more closely related to a target domain. As another setting, instruction tuning [51 ] trains a generative foundation model on instruction-response pairs to improve its performance in responding to task-specific instructions. Finetuning foundation models can lead to significant improvement in downstream tasks, but the effectiveness heavily relies on the right choice of training data [ 17 , 30 , 48 , 47 ]. However, the data repositories that one considers during training of generative models tend to be large—consider for example the use of Common Crawl, which contains 250 billion web pages, or The Pile [14 ]—and hence, it is impractical to manually select the data that are distributed like the use cases in the target task. Therefore, automated task-specific data selection becomes critical. In this paper, we propose TSDS (Task-Specific Data Selection), a framework to select data for taskspecific model finetuning. We consider the scenario of finetuning a foundation model to customize it for a specific task characterized by a few representative examples. The input to our framework is the representative examples and a massive repository of candidate data. Guided by the representative examples, we select training data from the repository for task-specific finetuning. We identify the following requirements for our framework. (Distribution Alignment) First, the distribution of the selected data should match the distribution of the representative examples from the target task. Distribution alignment is essential for a model to learn the target distribution and enable data-efficient finetuning for the target task [ 40 ]. Many works [38 , 17, 2, 50 , 47 ] retrieve candidate examples that are most similar to the representative examples. Such heuristics do not ensure distribution alignment between the selected data and the representative examples. A recent work [ 48 ] selects data by importance resampling to match the target distribution but is limited to an n-gram feature space, which cannot capture high-level semantics. (Diversity) Second, the selected data should be diverse so that the model can learn a wide range of related knowledge rather than overfitting to specific examples. In practice, data repositories created by web crawling may contain a large portion of near-duplicates [13 , 28 ] that can compromise diversity and negatively impact model performance [ 28, 19 ]. For example, a study [ 13 ] on several snapshots of ClueWeb and Common Crawl shows that 14% to 52% of the documents are near-duplicates. Previous works [38, 17 , 2 , 50 , 48 , 47 ] on task-specific data selection overlook near-duplicates, leading to the over-representation of such examples in the selected data. We require our framework to ensure diversity in selection even when a large fraction of the candidate examples are near-duplicates. (Scalability) Finally, the selection algorithm should be efficient, considering the increasing scale of modern data repositories. The high volume of candidate data (e.g., 250 billion pages in Common Crawl) poses a great challenge to efficient selection. Our framework formulates task-specific data selection as an optimization problem that allows a smooth trade-off between two crucial objectives: distribution alignment and diversity. The solution to the optimization problem is a categorical distribution assigned to the candidates which we will sample from. In the optimization objective, we use optimal transport to measure the discrepancy between the distribution assigned to the candidates and the target distribution, encouraging the alignment between them. We show that the optimization problem admits efficient algorithms to compute the optimal solution. In addition, our framework supports distribution alignment in any metric space that supports efficient nearest-neighbor search, including model-agnostic semantic embedding and model-specific features such as gradients. Our contributions: 1) We formulate data selection for task-specific finetuning as an optimization problem based on optimal transport for distribution alignment, with a regularization term that encourages diversity. 2) We make our framework robust to near-duplicates by incorporating kernel density estimation [36 ] into the regularization term. 3) We show the connection between the optimal solution to the optimization problem and nearest neighbor search, which allows us to develop efficient algorithms employing approximate nearest-neighbor search techniques [23]. We conduct extensive experiments to validate the effectiveness of our framework. We focus on natural language processing tasks where foundation models have shown great advancements. We show that our framework beats the state-of-the-art baseline [47 ] by 1.5 points in F1 score on average with a selection ratio of 1% on instruction tuning for two modern large language models on three tasks. In addition, continued pretraining using domain-specific data selected by our framework outperforms the other selection methods by up to 3 F1 points on four classification tasks from various domains. We also demonstrate that our framework is robust to near-duplicates in the data repository, maintaining consistent performance when 1% of the candidate examples are duplicate for up to 1,000 times. Our method is efficient, taking 28 hours to preprocess a corpus of 150M examples and less than 1 hour for each task-specific selection.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2310.16776	DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection	Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT-UCS, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to identify a smaller, representative dataset that reduces the data needed to finetune PLMs for the text-generation task of textediting. We examine the efficacy of DEFT-UCS across multiple text-editing tasks, and compare to the state-of-the art text-editing model, CoEDIT. Our results demonstrate that DEFTUCS models are just as accurate as CoEDIT, across eight different datasets consisting of six different editing tasks, while finetuned on 70% less data.	How much data do we need to fine-tune a pretrained language model (PLM) for a specific downstream task? While successes in language modelling have led to numerous publicly available PLMs and ability to produce fine-tuned models for downstream tasks - the answer mostly remains, "as large as possible, and of good quality". For example, Alpaca, an instruction-following model, is trained with 52k data samples (Taori et al., 2023). Similarly, CoPoet, a collaborative poetry writing system is fine-tuned using 87k data samples (Chakrabarty et al., 2022). MetaMath, a mathreasoning LLM is fine-tuned with 395k data samples (Yu et al., 2023). Although fine-tuning PLMs on specific task results in performance gain, acquiring large amounts of data for fine-tuning is not easy for real-world applications which often require niche knowledge and domain expertise. Researchers have explored variety of methods primarily focused on improving the computational efficiency of fine-tuning, including parameterefficient fine-tuning approaches (PEFT) to reduce computational costs by optimizing parameter updates (Fu et al., 2023; Hu et al., 2021) as well as leveraging active-learning for iteratively selecting data samples during training (Su et al., 2022; Diao et al., 2023). Instead, our work focuses on improving the data efficiency of PLM fine-tuning without requiring iterative fine-tuning. Specifically, we explore how to fine-tune PLMs with significantly less data samples and without a cost to model performance. Related to language models, researchers have experimented with different core-set selection metrics (Paul et al., 2021; Sorscher et al., 2022) to improve the data efficiency during pre-training. Marion et al. (2023) demonstrated how perplexity, L2-Error Norm (EL2N) and memorization can be utilized to select smaller, good quality datasets for pre-training. Similarly, (Attendu and Corbeil, 2023) leverage EL2N to dynamically remove data samples with high EL2N between training epochs. However, these metrics assume access to task data and reference models to perform dataset pruning. In real world applications, utilizing such supervised, data-pruning metrics are less realistic since large amounts of annotated task-specific data may be costly to acquire. This leads us to our main research question: How can we leverage unsupervised data pruning to fine-tune PLMs for downstream tasks in a more data efficient manner? In this work, we introduce a new data-efficient fine-tuning framework, DEFT-UCS, that uses unsupervised core-set selection to minimize the amount of labelled data needed to fine-tune PLMs for the text-generation task of text-editing. Our framework is inspired by (Sorscher et al., 2022), who utilize clustering-based dataset pruning to reduce training samples for image-classification models, and to the best of our knowledge, our framework is the first to leverage unsupervised core-set selection for data-efficient fine-tuning of PLMs. We investigate the utility of DEFT-UCS in finetuning PLMs for text-generation across eight different datasets consisting of six different text-editing tasks, and compare DEFT-UCS models to the stateof-the-art text-editing model, CoEDIT(Raheja et al., 2023). Our contributions are as follows: - We introduce DEFT-UCS, a data-efficient-fine tuning framework that leverages unsupervised core-set selection via clustering to identify a smaller representative set of data needed to fine-tune PLMs. - We show that DEFT-UCS, utilizing only 32.5% of CoEDIT’s training data, is able to produce fine-tuned models with improved accuracy on four different text-editing tasks, and similar accuracy on two text-editing tasks compared to CoEDIT (Raheja et al., 2023). - We performed a human evaluation with 3 evaluators to assess the quality of text-edits from our DEFT-UCS model. Evaluators found edits generated by DEFT-UCS model as similar or preferred over CoEDIT (Raheja et al., 2023)	https://arxiv.org/pdf/2501.04155	MM-GEN: Enhancing Task Performance Through Targeted Multimodal Data Curation	Vision-language models (VLMs) are highly effective but often underperform on specialized tasks, for example Llava1.5 struggles on chart and diagram understanding, due to scarce task-specific training data. Existing training data, sourced from general-purpose datasets, fails to capture the nuanced details needed for these tasks. We introduce MMGEN, a scalable method that generates task-specific, highquality synthetic text for candidate images by leveraging stronger models. MM-GEN employs a three-stage targeted process: partitioning data into subgroups, generating targeted text based on task descriptions, and filtering out redundant and outlier data. Fine-tuning VLMs with data generated by MM-GEN leads to significant performance gains, including 29% on spatial reasoning and 15% on diagram understanding for Llava-1.5 (7B). Compared to human-curated caption data, MM-GEN achieves up to 1.6× better improvements for the original models, proving its effectiveness in enhancing task-specific VLM performance and bridging the gap between general-purpose datasets and specialized requirements. Code available at https://github.com/sjoshi804/MM-Gen.	While vision-language models (VLMs) demonstrate stateof-the-art performance on several multi-modal tasks [29], they are often on tasks that are simpler in nature [3]. These models still struggle with more complex tasks, e.g., those that require fine-grained understanding of details in images [3, 15, 21, 49]. We posit that the primary limitation for these VLMs is the quality and nature of the training data. VLMs are typically trained on large-scale image-text data scraped from the internet; while the images are rich and informative, the associated text descriptions can (i) have limited relevance to the image [39], or (ii) omit references to several specific details captured in the image [25]. Fig. 1 shows examples of such images and web-scraped captions. While the images are relevant for the tasks of chart understanding, spatial reasoning, and diagram understanding, respectively, the text fails to capture details essential for these tasks. While synthetic caption generation strategies proposed in prior work [25, 39, 57] can create more descriptive text annotations (by referring to more visual details), they are agnostic of the downstream target task. Consequently, they cannot ensure that relevant details are captured in the text annotations. Recently, Shi et al. [48] manually curated a task-specific dataset aimed at the task of multimodal mathematical question-answering by augmenting existing imagetext data with detailed textual annotations, based on their domain expertise, using strong VLMs. While effective, such a curation pipeline involves significant human effort and is not scalable [33, 59]. To address these limitations, we present MM-GEN, a highly general framework for automatically synthesizing taskrelevant text annotations for images by leveraging stronger VLMs (i.e., VLMs that perform well on the specific task) and requiring minimal human effort. MM-GEN takes as input a small set of examples from the target task (henceforth referred to as "reference samples"), a list of image types associated with the task, and a pool of task-relevant candidate images for training. In practice, these inputs can be easily obtained: a small number of reference samples and associated image types can be directly collected from the target task, and a task-relevant image pool can be found via image search with search engines or retrieved from largescale image-caption datasets [5, 46, 47]. Using the reference samples to specify the details of the task to the stronger VLM, MM-GEN generates text-annotations that are taskrelevant for the candidate images. Fig. 1 shows how the text generated by MM-GEN captures task-relevant details. This simple approach is highly effective, resulting in significant improvements across a variety of target tasks. Moreover, human-effort in this process is limited to providing (i) a small set of reference samples for the task, and (ii) a pool of candidate images. To further improve the quality and efficacy of the generated data, MM-GEN introduces a perplexity [4] based data-filtering approach to improve performance on target task using a high-value subset of the synthesized data. The components in MM-GEN are general and applicable to any image-text based target task enabling it to easily generalize across tasks and scale. We evaluate MM-GEN on improving VLMs' (e.g., Llava1.5 7B and 13B parameter versions) performance on finegrained image understanding tasks — chart understanding and reasoning, diagram understanding, and spatial reasoning on maps. The data curated by MM-GEN enables an absolute improvement over Llava-1.5 (7B) of 15%, 14% and 29%, respectively. We also see improvements over the much larger Llava-1.5 (13B) of 13%, , respectively. Moreover, MM-GEN’s filtering strategy helps in shrinking data volumes by up to 50% with no / minimal loss in performance. Empirical results show that models trained via MM-GEN data have a better performance than those trained via generated generic captions. MM-GEN data is also more effective than text annotations generated without task-specific reference examples, showing the importance of a targeted, data-centric approach for describing tasks. Finally, we analyze the effects of key design choices in MMGEN through ablation studies on e.g. size of the reference sample set, generating with / without partitioning, scaling number of in-context samples.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2310.16776	DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection	Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT-UCS, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to identify a smaller, representative dataset that reduces the data needed to finetune PLMs for the text-generation task of textediting. We examine the efficacy of DEFT-UCS across multiple text-editing tasks, and compare to the state-of-the art text-editing model, CoEDIT. Our results demonstrate that DEFTUCS models are just as accurate as CoEDIT, across eight different datasets consisting of six different editing tasks, while finetuned on 70% less data.	How much data do we need to fine-tune a pretrained language model (PLM) for a specific downstream task? While successes in language modelling have led to numerous publicly available PLMs and ability to produce fine-tuned models for downstream tasks - the answer mostly remains, "as large as possible, and of good quality". For example, Alpaca, an instruction-following model, is trained with 52k data samples (Taori et al., 2023). Similarly, CoPoet, a collaborative poetry writing system is fine-tuned using 87k data samples (Chakrabarty et al., 2022). MetaMath, a mathreasoning LLM is fine-tuned with 395k data samples (Yu et al., 2023). Although fine-tuning PLMs on specific task results in performance gain, acquiring large amounts of data for fine-tuning is not easy for real-world applications which often require niche knowledge and domain expertise. Researchers have explored variety of methods primarily focused on improving the computational efficiency of fine-tuning, including parameterefficient fine-tuning approaches (PEFT) to reduce computational costs by optimizing parameter updates (Fu et al., 2023; Hu et al., 2021) as well as leveraging active-learning for iteratively selecting data samples during training (Su et al., 2022; Diao et al., 2023). Instead, our work focuses on improving the data efficiency of PLM fine-tuning without requiring iterative fine-tuning. Specifically, we explore how to fine-tune PLMs with significantly less data samples and without a cost to model performance. Related to language models, researchers have experimented with different core-set selection metrics (Paul et al., 2021; Sorscher et al., 2022) to improve the data efficiency during pre-training. Marion et al. (2023) demonstrated how perplexity, L2-Error Norm (EL2N) and memorization can be utilized to select smaller, good quality datasets for pre-training. Similarly, (Attendu and Corbeil, 2023) leverage EL2N to dynamically remove data samples with high EL2N between training epochs. However, these metrics assume access to task data and reference models to perform dataset pruning. In real world applications, utilizing such supervised, data-pruning metrics are less realistic since large amounts of annotated task-specific data may be costly to acquire. This leads us to our main research question: How can we leverage unsupervised data pruning to fine-tune PLMs for downstream tasks in a more data efficient manner? In this work, we introduce a new data-efficient fine-tuning framework, DEFT-UCS, that uses unsupervised core-set selection to minimize the amount of labelled data needed to fine-tune PLMs for the text-generation task of text-editing. Our framework is inspired by (Sorscher et al., 2022), who utilize clustering-based dataset pruning to reduce training samples for image-classification models, and to the best of our knowledge, our framework is the first to leverage unsupervised core-set selection for data-efficient fine-tuning of PLMs. We investigate the utility of DEFT-UCS in finetuning PLMs for text-generation across eight different datasets consisting of six different text-editing tasks, and compare DEFT-UCS models to the stateof-the-art text-editing model, CoEDIT(Raheja et al., 2023). Our contributions are as follows: - We introduce DEFT-UCS, a data-efficient-fine tuning framework that leverages unsupervised core-set selection via clustering to identify a smaller representative set of data needed to fine-tune PLMs. - We show that DEFT-UCS, utilizing only 32.5% of CoEDIT’s training data, is able to produce fine-tuned models with improved accuracy on four different text-editing tasks, and similar accuracy on two text-editing tasks compared to CoEDIT (Raheja et al., 2023). - We performed a human evaluation with 3 evaluators to assess the quality of text-edits from our DEFT-UCS model. Evaluators found edits generated by DEFT-UCS model as similar or preferred over CoEDIT (Raheja et al., 2023)	https://arxiv.org/pdf/1708.00489	Active Learning for Convolutional Neural Networks: A Core-Set Approach	Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (i.e. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.	Deep convolutional neural networks (CNNs) have shown unprecedented success in many areas of research in computer vision and pattern recognition, such as image classification, object detection, and scene segmentation. Although CNNs are universally successful in many tasks, they have a major drawback; they need a very large amount of labeled data to be able to learn their large number of parameters. More importantly, it is almost always better to have more data since the accuracy of CNNs is often not saturated with increasing dataset size. Hence, there is a constant desire to collect more and more data. Although this a desired behavior from an algorithmic perspective (higher representative power is typically better), labeling a dataset is a time consuming and an expensive task. These practical considerations raise a critical question: "what is the optimal way to choose data points to label such that the highest accuracy can be obtained given a fixed labeling budget." Active learning is one of the common paradigms to address this question. The goal of active learning is to find effective ways to choose data points to label, from a pool of unlabeled data points, in order to maximize the accuracy. Although it is not possible to obtain a universally good active learning strategy (Dasgupta, 2004), there exist many heuristics (Settles, 2010) which have been proven to be effective in practice. Active learning is typically an iterative process in which a model is learned at each iteration and a set of points is chosen to be labelled from a pool of unlabelled points using these aforementioned heuristics. We experiment with many of these heuristics in this paper and find them not effective when applied to CNNs. We argue that the main factor behind this ineffectiveness is the correlation caused via batch acquisition/sampling. In the classical setting, the active learning algorithms typically choose a single point at each iteration; however, this is not feasible for CNNs since i) a single point is likely to have no statistically significant impact on the accuracy due to the local optimization methods, and ii) each iteration requires a full training until convergence which makes it intractable to query labels one-by-one. Hence, it is necessary to query labels for a large subset at each iteration and it results in correlated samples even for moderately small subset sizes. In order to tailor an active learning method for the batch sampling case, we decided to define the active learning as core-set selection problem. Core-set selection problem aims to find a small subset given a large labeled dataset such that a model learned over the small subset is competitive over the whole dataset. Since we have no labels available, we perform the core-set selection without using the labels. In order to attack the unlabeled core-set problem for CNNs, we provide a rigorous bound between an average loss over any given subset of the dataset and the remaining data points via the geometry of the data points. As an active learning algorithm, we try to choose a subset such that this bound is minimized. Moreover, minimization of this bound turns out to be equivalent to the k-Center problem (Wolf, 2011) and we adopt an efficient approximate solution to this combinatorial optimization problem. We further study the behavior of our proposed algorithm empirically for the problem of image classification using three different datasets. Our empirical analysis demonstrates state-of-the-art performance by a large margin.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2310.16776	DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection	Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT-UCS, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to identify a smaller, representative dataset that reduces the data needed to finetune PLMs for the text-generation task of textediting. We examine the efficacy of DEFT-UCS across multiple text-editing tasks, and compare to the state-of-the art text-editing model, CoEDIT. Our results demonstrate that DEFTUCS models are just as accurate as CoEDIT, across eight different datasets consisting of six different editing tasks, while finetuned on 70% less data.	How much data do we need to fine-tune a pretrained language model (PLM) for a specific downstream task? While successes in language modelling have led to numerous publicly available PLMs and ability to produce fine-tuned models for downstream tasks - the answer mostly remains, "as large as possible, and of good quality". For example, Alpaca, an instruction-following model, is trained with 52k data samples (Taori et al., 2023). Similarly, CoPoet, a collaborative poetry writing system is fine-tuned using 87k data samples (Chakrabarty et al., 2022). MetaMath, a mathreasoning LLM is fine-tuned with 395k data samples (Yu et al., 2023). Although fine-tuning PLMs on specific task results in performance gain, acquiring large amounts of data for fine-tuning is not easy for real-world applications which often require niche knowledge and domain expertise. Researchers have explored variety of methods primarily focused on improving the computational efficiency of fine-tuning, including parameterefficient fine-tuning approaches (PEFT) to reduce computational costs by optimizing parameter updates (Fu et al., 2023; Hu et al., 2021) as well as leveraging active-learning for iteratively selecting data samples during training (Su et al., 2022; Diao et al., 2023). Instead, our work focuses on improving the data efficiency of PLM fine-tuning without requiring iterative fine-tuning. Specifically, we explore how to fine-tune PLMs with significantly less data samples and without a cost to model performance. Related to language models, researchers have experimented with different core-set selection metrics (Paul et al., 2021; Sorscher et al., 2022) to improve the data efficiency during pre-training. Marion et al. (2023) demonstrated how perplexity, L2-Error Norm (EL2N) and memorization can be utilized to select smaller, good quality datasets for pre-training. Similarly, (Attendu and Corbeil, 2023) leverage EL2N to dynamically remove data samples with high EL2N between training epochs. However, these metrics assume access to task data and reference models to perform dataset pruning. In real world applications, utilizing such supervised, data-pruning metrics are less realistic since large amounts of annotated task-specific data may be costly to acquire. This leads us to our main research question: How can we leverage unsupervised data pruning to fine-tune PLMs for downstream tasks in a more data efficient manner? In this work, we introduce a new data-efficient fine-tuning framework, DEFT-UCS, that uses unsupervised core-set selection to minimize the amount of labelled data needed to fine-tune PLMs for the text-generation task of text-editing. Our framework is inspired by (Sorscher et al., 2022), who utilize clustering-based dataset pruning to reduce training samples for image-classification models, and to the best of our knowledge, our framework is the first to leverage unsupervised core-set selection for data-efficient fine-tuning of PLMs. We investigate the utility of DEFT-UCS in finetuning PLMs for text-generation across eight different datasets consisting of six different text-editing tasks, and compare DEFT-UCS models to the stateof-the-art text-editing model, CoEDIT(Raheja et al., 2023). Our contributions are as follows: - We introduce DEFT-UCS, a data-efficient-fine tuning framework that leverages unsupervised core-set selection via clustering to identify a smaller representative set of data needed to fine-tune PLMs. - We show that DEFT-UCS, utilizing only 32.5% of CoEDIT’s training data, is able to produce fine-tuned models with improved accuracy on four different text-editing tasks, and similar accuracy on two text-editing tasks compared to CoEDIT (Raheja et al., 2023). - We performed a human evaluation with 3 evaluators to assess the quality of text-edits from our DEFT-UCS model. Evaluators found edits generated by DEFT-UCS model as similar or preferred over CoEDIT (Raheja et al., 2023)	https://arxiv.org/pdf/2301.13287	MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning	Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results indicate that MILO can train models 3 x -10x faster and tune hyperparameters 20 x -75x faster than full-dataset training or tuning without compromising performance.	Deep learning has achieved remarkable success in a multitude of machine learning tasks, including natural language processing, computer vision, and speech recognition in recent years. This success is partially due to the availability of massive training datasets and the capacity to train large-scale neural networks. However, training deep models on extensive datasets is computationally demanding, incurring significant financial costs and generating substantial CO2 emissions [63, 59 ]. Bhavya et al. [ 2] overview several research trajectories aimed at enhancing model convergence and reducing training time and costs, including data subset selection, curriculum learning, model architecture improvements, and optimization algorithm enhancements. Our work specifically focuses on selecting useful, generalizable data subsets for efficient deep neural network training. Recent research [66 , 5 , 21 ] suggests that many current training datasets are redundant, indicating that a non-redundant, informative subset could achieve comparable performance to training on the full dataset. To identify such informative samples, metrics such as prediction uncertainty [ 7], prediction flips [ 66 , 72 ], loss [42 ], or gradient/gradient-norm [ 21, 47 , 28, 27 , 29 ] have been applied. These metrics, calculated using either the downstream machine learning model or a lightweight surrogate [ 7], require a fully converged model, a requirement that runs counter to the goal of efficient training. To address this, existing subset selection algorithms [ 47 , 28 , 27, 29, 30 , 54 ] for efficient learning utilize the downstream model during training for heuristic computation and periodically update the subset as the model-dependent metrics for each sample evolve. Drawbacks of Model-Dependent Subset Selection: Despite the theoretical advantages of existing subset selection strategies[ 28 , 27, 47 ], they often fall short in computational efficiency compared to adaptive random subset selection, which selects random subsets periodically. This is primarily because traditional approaches rely on downstream models and typically require the calculation of sample metrics, such as gradients, before each subset selection step. Furthermore, these computationally demanding subset selection steps occur during model training. For instance, Figure 1 compares the convergence rate of the ResNet18 model on the CIFAR100 dataset, in terms of both time and epochs. Here, the model uses 10% subsets chosen every epoch by GradMatchPB [ 27 ], a state-of-the-art data subset selection strategy for efficient training, CraigPB [ 47 ], and Adaptive-Random (where a new 10% subset is randomly selected periodically). The selection of a new subset every epoch is done to demonstrate the maximum performance attainable by GradMatchPB and CraigPB. The results indicate that GradMatchPB achieves faster epoch convergence than both Adaptive-Random and CraigPB when selecting a new subset each epoch. However, due to the necessity for a computationally intensive subset selection step every epoch, both GradMatchPB and CraigPB demonstrate considerable inefficiency in terms of training time. In their respective studies, Killamsetty et al. [27 ] and Mirzasoleiman et al. [47] suggested selecting a new subset every R epochs to enhance training efficiency. However, this comes at the cost of the model’s convergence rate. Lastly, model-dependent subset selection requires a computationally intensive subset selection steps each time a new model is trained. Model-Agnostic Selection: Our primary insight is that a model-agnostic subset selection framework can circumvent the computationally intensive subset selection steps during model training by conducting subset selection in the preprocessing phase. By pre-selecting subsets and storing them as metadata with each dataset, we can train numerous models without incurring additional costs, effectively spreading out the expense of subset selection. In this work, we endeavor to answer the following question: Is it possible to develop a model-agnostic subset selection method that selects new subsets in a minimal amount of time, yet achieves superior model convergence without significant compromise in test accuracy or generalization performance?  MILO Framework: In this study, we introduce MILO, a model-agnostic subset selection framework designed for efficient model training and tuning. MILO employs submodular measures [ 13 , 23 ], which capture higher-order interactions between data samples for subset selection. We utilize pre-trained large language models [ 55 ] and pre-trained vision transformers [25 ] as feature encoders due to their zero-shot feature encoding capabilities. These encoders compute the sample metrics in a nominal amount of time, and this computation is independent of the downstream model, rendering MILO model-agnostic. Figure 3 provides a visual representation of MILO for model training, comprising two steps: a) A pre-processing step that involves the selection of multiple subsets from the training dataset using "Stochastic-Greedy Exploration (SGE)" (refer to Section 3.1.1), and the construction of a probability distribution over the entire dataset for subset sampling through "Weighted Random Exploration (WRE)" (refer to Section 3.1.2); b) A model training scheme that involves training the model on a curriculum of easy-to-hard subsets (refer to Section 3.1.3) using the subsets selected and the probability distribution constructed in the pre-processing step. We also present a class-wise partitioning trick to significantly minimize the memory footprint of MILO (refer to Section 3.2). Effectiveness of MILO : Through extensive experiments on multiple real-world datasets, we empirically demonstrate the effectiveness of the MILO framework for efficient training and hyperparameter tuning. In Figure 2, we provide a summary of the speedup achieved by MILO compared to full data training and tuning, along with the corresponding relative performance. Our results show that MILO can train models 3x to 10x faster and tune hyperparameters 20x to 75x faster, with minimal loss in performance. Furthermore, we consistently outperform the subset selection baselines that were considered. Additionally, MILO exhibits significantly faster initial model convergence and maintains faster convergence throughout training compared to other baselines. These qualities make it highly effective for applications such as hyperparameter tuning and neural architecture search, where the ability to distinguish good models from bad models early in the tuning process is crucial for enhancing efficiency [38, 37]. Related Work: In recent years, data-subset selection strategies have achieved success in various machine learning applications such as speech recognition [ 68 , 67], machine translation [ 32 ], active learning [ 60 , 1 , 33 ], hyper-parameter tuning [ 30 ], continual learning [65 ], domain adaptation [20 ], and computer vision [22]. A recent empirical study [5] demonstrated that datasets frequently contain semantic redundancies. These redundancies can be removed during training without affecting performance. In response, data pruning methods [66 , 51, 62] have developed principled selection criteria for pruning redundant samples before model training, with minimal loss in performance. However, both existing data pruning and compute-efficient learning approaches are model-dependent, resulting in them being computationally expensive. In contrast, our method is model-agnostic and capable of identifying high-quality subsets by intelligent data exploration. Furthermore, our approach can achieve superior model convergence by utilizing an easy-to-hard curriculum, transitioning from representative to diverse subsets during training.	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2310.16776	DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection	Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT-UCS, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to identify a smaller, representative dataset that reduces the data needed to finetune PLMs for the text-generation task of textediting. We examine the efficacy of DEFT-UCS across multiple text-editing tasks, and compare to the state-of-the art text-editing model, CoEDIT. Our results demonstrate that DEFTUCS models are just as accurate as CoEDIT, across eight different datasets consisting of six different editing tasks, while finetuned on 70% less data.	How much data do we need to fine-tune a pretrained language model (PLM) for a specific downstream task? While successes in language modelling have led to numerous publicly available PLMs and ability to produce fine-tuned models for downstream tasks - the answer mostly remains, "as large as possible, and of good quality". For example, Alpaca, an instruction-following model, is trained with 52k data samples (Taori et al., 2023). Similarly, CoPoet, a collaborative poetry writing system is fine-tuned using 87k data samples (Chakrabarty et al., 2022). MetaMath, a mathreasoning LLM is fine-tuned with 395k data samples (Yu et al., 2023). Although fine-tuning PLMs on specific task results in performance gain, acquiring large amounts of data for fine-tuning is not easy for real-world applications which often require niche knowledge and domain expertise. Researchers have explored variety of methods primarily focused on improving the computational efficiency of fine-tuning, including parameterefficient fine-tuning approaches (PEFT) to reduce computational costs by optimizing parameter updates (Fu et al., 2023; Hu et al., 2021) as well as leveraging active-learning for iteratively selecting data samples during training (Su et al., 2022; Diao et al., 2023). Instead, our work focuses on improving the data efficiency of PLM fine-tuning without requiring iterative fine-tuning. Specifically, we explore how to fine-tune PLMs with significantly less data samples and without a cost to model performance. Related to language models, researchers have experimented with different core-set selection metrics (Paul et al., 2021; Sorscher et al., 2022) to improve the data efficiency during pre-training. Marion et al. (2023) demonstrated how perplexity, L2-Error Norm (EL2N) and memorization can be utilized to select smaller, good quality datasets for pre-training. Similarly, (Attendu and Corbeil, 2023) leverage EL2N to dynamically remove data samples with high EL2N between training epochs. However, these metrics assume access to task data and reference models to perform dataset pruning. In real world applications, utilizing such supervised, data-pruning metrics are less realistic since large amounts of annotated task-specific data may be costly to acquire. This leads us to our main research question: How can we leverage unsupervised data pruning to fine-tune PLMs for downstream tasks in a more data efficient manner? In this work, we introduce a new data-efficient fine-tuning framework, DEFT-UCS, that uses unsupervised core-set selection to minimize the amount of labelled data needed to fine-tune PLMs for the text-generation task of text-editing. Our framework is inspired by (Sorscher et al., 2022), who utilize clustering-based dataset pruning to reduce training samples for image-classification models, and to the best of our knowledge, our framework is the first to leverage unsupervised core-set selection for data-efficient fine-tuning of PLMs. We investigate the utility of DEFT-UCS in finetuning PLMs for text-generation across eight different datasets consisting of six different text-editing tasks, and compare DEFT-UCS models to the stateof-the-art text-editing model, CoEDIT(Raheja et al., 2023). Our contributions are as follows: - We introduce DEFT-UCS, a data-efficient-fine tuning framework that leverages unsupervised core-set selection via clustering to identify a smaller representative set of data needed to fine-tune PLMs. - We show that DEFT-UCS, utilizing only 32.5% of CoEDIT’s training data, is able to produce fine-tuned models with improved accuracy on four different text-editing tasks, and similar accuracy on two text-editing tasks compared to CoEDIT (Raheja et al., 2023). - We performed a human evaluation with 3 evaluators to assess the quality of text-edits from our DEFT-UCS model. Evaluators found edits generated by DEFT-UCS model as similar or preferred over CoEDIT (Raheja et al., 2023)	https://arxiv.org/pdf/2103.00128	PRISM: A Rich Class of Parameterized Submodular Information Measures for Guided Subset Selection	With ever-increasing dataset sizes, subset selection techniques are becoming increasingly important for a plethora of tasks. It is often necessary to guide the subset selection to achieve certain desiderata, which includes focusing or targeting certain data points, while avoiding others. Examples of such problems include: i) targeted learning, where the goal is to find subsets with rare classes or rare attributes on which the model is underperforming, and ii) guided summarization, where data (e.g., image collection, text, document or video) is summarized for quicker human consumption with specific additional user intent. Motivated by such applications, we present PRISM, a rich class of PaRameterIzed Submodular information Measures. Through novel functions and their parameterizations, PRISM offers a variety of modeling capabilities that enable a trade-off between desired qualities of a subset like diversity or representation and similarity/dissimilarity with a set of data points. We demonstrate how PRISM can be applied to the two real-world problems mentioned above, which require guided subset selection. In doing so, we show that PRISM interestingly generalizes some past work, therein reinforcing its broad utility. Through extensive experiments on diverse datasets, we demonstrate the superiority of PRISM over the state-of-the-art in targeted learning and in guided imagecollection summarization. PRISM is available as a part of the SUBMODLIB (https://github.com/decile-team/submodlib) and TRUST (https://github.com/decile-team/trust) toolkits.	Recent times have seen explosive growth in data across several modalities, including text, images, and videos. This has given rise to the need for finding techniques for selecting effective smaller data subsets with specific characteristics for a variety of down-stream tasks. Often, we would like to guide the data selection to either target or avoid a certain set of data slices. One application is, what we call, targeted learning, where the goal is to select data points similar to data slices on which the model is currently performing poorly. These slices are data points that either belong to rare classes or have common rare attributes (e.g., color, background, etc.). An example of such a scenario is shown in Fig. 1(a), where a self-driving car model struggles in detecting “cars in a dark background“ because of a lack of such images in the training set. The targeted learning problem is to augment the training dataset with more of such rare images, with an aim to improve model performance. Another example is detecting cancers in biomedical imaging datasets, where the number of cancerous images are often a small fraction of the non-cancerous images. Another application comes from the summarization task, where an image collection, a video, or a text document is summarized for quicker human consumption by eliminating redundancy, while preserving the main content. While a number of applications require generic summarization (i.e., simply picking a representative and diverse subset of the massive dataset), it is often important to capture certain user intent in summarization. We call this guided summarization. Examples of guided summarization include: (i) query-focused summarization (Sharghi, Gong, and Shah 2016; Xiao et al. 2020), where a summary similar to a specific query is desired, and (ii) privacy-preserving summarization, where a summary dissimilar to a given private set of data points is desired (say, for privacy issues). See Fig. 1(b) for a pictorial illustration. PRISM Framework: We define PRISM through different instantiations and parameterizations of various submodular information measures (Sec. 2). These allow for modeling a spectrum of semantics required for guided subset selection, like relevance to a query set, irrelevance to a private set, and diversity among selected data points. We study the effect of parameter trade-off among these different semantics and present interesting insights. PRISM for Targeted Learning: We present a novel algorithm (Sec. 3.1, Algo. 1) to apply PRISM for targeted learning, which aims to improve a model’s performance on rare slices of data. Specifically, we show that submodular information measures are very effective in finding the examples from the rare classes in a large unlabeled set (akin to finding a needle in a haystack). On several image classification tasks, PRISM obtains ≈ 20-30% gain in accuracy of rare classes (≈ 12% more than existing approaches) by just adding a few additional labeled points from the rare classes. Furthermore, we show that PRISM is 20× to 50× more label-efficient compared to random sampling, and 2× to 4× more labelefficient compared to existing approaches (see Sec. 4.1). We also show that Algo. 1 generalizes some existing approaches for data subset selection, reinforcing its utility (Sec. 3.3). PRISM for Guided Summarization. We propose a learning framework for guided summarization using PRISM (Sec. 3.2). We show that PRISM offers a unified treatment to the different flavors of guided summarization (query-focused and privacy-preserving) and generalizes some existing approaches to summarization, again reinforcing its utility. We show that it outperforms other existing approaches on a real-world image collections dataset (Sec. 4.2).	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2310.16776	DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection	Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT-UCS, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to identify a smaller, representative dataset that reduces the data needed to finetune PLMs for the text-generation task of textediting. We examine the efficacy of DEFT-UCS across multiple text-editing tasks, and compare to the state-of-the art text-editing model, CoEDIT. Our results demonstrate that DEFTUCS models are just as accurate as CoEDIT, across eight different datasets consisting of six different editing tasks, while finetuned on 70% less data.	How much data do we need to fine-tune a pretrained language model (PLM) for a specific downstream task? While successes in language modelling have led to numerous publicly available PLMs and ability to produce fine-tuned models for downstream tasks - the answer mostly remains, "as large as possible, and of good quality". For example, Alpaca, an instruction-following model, is trained with 52k data samples (Taori et al., 2023). Similarly, CoPoet, a collaborative poetry writing system is fine-tuned using 87k data samples (Chakrabarty et al., 2022). MetaMath, a mathreasoning LLM is fine-tuned with 395k data samples (Yu et al., 2023). Although fine-tuning PLMs on specific task results in performance gain, acquiring large amounts of data for fine-tuning is not easy for real-world applications which often require niche knowledge and domain expertise. Researchers have explored variety of methods primarily focused on improving the computational efficiency of fine-tuning, including parameterefficient fine-tuning approaches (PEFT) to reduce computational costs by optimizing parameter updates (Fu et al., 2023; Hu et al., 2021) as well as leveraging active-learning for iteratively selecting data samples during training (Su et al., 2022; Diao et al., 2023). Instead, our work focuses on improving the data efficiency of PLM fine-tuning without requiring iterative fine-tuning. Specifically, we explore how to fine-tune PLMs with significantly less data samples and without a cost to model performance. Related to language models, researchers have experimented with different core-set selection metrics (Paul et al., 2021; Sorscher et al., 2022) to improve the data efficiency during pre-training. Marion et al. (2023) demonstrated how perplexity, L2-Error Norm (EL2N) and memorization can be utilized to select smaller, good quality datasets for pre-training. Similarly, (Attendu and Corbeil, 2023) leverage EL2N to dynamically remove data samples with high EL2N between training epochs. However, these metrics assume access to task data and reference models to perform dataset pruning. In real world applications, utilizing such supervised, data-pruning metrics are less realistic since large amounts of annotated task-specific data may be costly to acquire. This leads us to our main research question: How can we leverage unsupervised data pruning to fine-tune PLMs for downstream tasks in a more data efficient manner? In this work, we introduce a new data-efficient fine-tuning framework, DEFT-UCS, that uses unsupervised core-set selection to minimize the amount of labelled data needed to fine-tune PLMs for the text-generation task of text-editing. Our framework is inspired by (Sorscher et al., 2022), who utilize clustering-based dataset pruning to reduce training samples for image-classification models, and to the best of our knowledge, our framework is the first to leverage unsupervised core-set selection for data-efficient fine-tuning of PLMs. We investigate the utility of DEFT-UCS in finetuning PLMs for text-generation across eight different datasets consisting of six different text-editing tasks, and compare DEFT-UCS models to the stateof-the-art text-editing model, CoEDIT(Raheja et al., 2023). Our contributions are as follows: - We introduce DEFT-UCS, a data-efficient-fine tuning framework that leverages unsupervised core-set selection via clustering to identify a smaller representative set of data needed to fine-tune PLMs. - We show that DEFT-UCS, utilizing only 32.5% of CoEDIT’s training data, is able to produce fine-tuned models with improved accuracy on four different text-editing tasks, and similar accuracy on two text-editing tasks compared to CoEDIT (Raheja et al., 2023). - We performed a human evaluation with 3 evaluators to assess the quality of text-edits from our DEFT-UCS model. Evaluators found edits generated by DEFT-UCS model as similar or preferred over CoEDIT (Raheja et al., 2023)	https://aclanthology.org/2024.findings-naacl.209.pdf	An End-to-End Submodular Framework for Data-Efficient In-Context Learning	Recent advancements in natural language tasks leverage the emergent In-Context Learning (ICL) ability of pretrained Large Language Models (LLMs). ICL enables LLMs to perform new tasks by utilizing a limited number of input-output examples as prompts. While ICL circumvents the costly step of finetuning LLMs, its effectiveness is heavily dependent on the quality and ordering of provided examples (called exemplars). In this work, we propose a two-stage data-efficient framework Div-S3 for exemplar selection for ICL. The first stage focuses on data annotation and employs a poolbased active learning approach to select a set of Diverse and informative exemplars from the target tasks’ unlabeled pool. Given a test input/query, the second stage uses Submodular Span Summarization (S3) to select the most relevant and non-redundant exemplars from the annotated pool of a limited budget. On 7 different NLP datasets and 5 LLMs of varying complexities, we show Div-S3 outperforms (1) existing active learning-based methods for data annotation for ICL and (2) similarity-based methods for test query-specific exemplars retrieval.	Pretrained large language models (LLMs) (Kenton and Toutanova, 2019; Brown et al., 2020; Chowdhery et al., 2022) have become foundational for a wide range of Natural Language Processing (NLP) tasks, demonstrating impressive success across various domains (Bommasani et al., 2021; Bubeck et al., 2023) through in-context learning (ICL) (Dong et al., 2022). ICL enables these pretrained LLMs to perform new tasks by using task-specific prompts containing a limited number of input-output demonstrations (also referred to as shots, exemplars, or prompts) in the natural language format. This approach facilitates deployment across different downstream tasks and reduces the need for labeled downstream training data since ICL does not require any task-specific training. The typical ICL procedure consists of two key components: (1) Exemplar annotation and retrieval (Wu et al., 2022; Köksal et al., 2022; Liu et al., 2022): This step involves annotating and retrieving exemplars that serve as context demonstrations. (2) Prompt template crafting (Sorensen et al., 2022; Deng et al., 2022): this step involves designing a prompt template to wrap these demonstrations in a comprehensible and coherent natural language instruction. Recent studies (Liu et al., 2022; Su et al., 2022; Margatina et al., 2023) show that providing exemplars most relevant to the current input instance is beneficial. Moreover, Zhao et al. (2021), Lu et al. (2022), and Liu et al. (2023) observe that LLMs attend more to the exemplars that are closer in the sequence to the input instance. Therefore, to achieve the best performance of ICL, the selection of exemplars and their ordering in the LLM prompt are crucial. In practice, an extensive collection of unlabeled exemplars is easily available (e.g., posts and discussions on forums like Stack Exchange or user-generated content on social media platforms), but manually annotating all exemplars would be exceptionally costly. To annotate and select the exemplars optimally for a given target task, we follow the two-stage approach shown in Figure 1: (1) Exemplar Annotation: select a subset of exemplars for annotation under a fixed budget (performed only once) and (2) Exemplar Retrieval: identify limited-sized exemplars in an ordering that are most influential for a given input instance from the annotated subset of exemplars. Intuitively, for the first stage, we aim to find the subset with maximal diversity and least redundancy so that, given any input, we can find corresponding labeled exemplars. For the second stage, in addition to the diversity requirement similar to the first stage, we emphasize the relevance of the exemplars to the given input query and order exemplars so that their relevance to the input query decreases as the exemplars are farther away from the input instance. We propose a framework Div-S3 based on submodular optimization that unifies the abovementioned two stages. For Exemplar Annotation, we model the problem as a submodular optimization problem under a cardinality constraint to find as Diverse a subset as possible within a budget. For Exemplar Retrieval, we formalize the problem as a Submodular Span Summarization (S3) problem (Kumari and Bilmes, 2021) with a knapsack constraint, which finds a diverse subset most relevant to the input query under a token length limit. Also, we naturally order the resulting exemplars based on the gains represented by the submodular function. The name of our proposed framework Div-S3 captures the optimization objectives used for both exemplar annotation (Div) and exemplar retrieval (S3) stages. In Fig. 2, we show a sample test query where using Div-S3 for exemplar selection leads to a more diverse and query-relevant exemplar set (more examples provided in Appendix D). Our framework is general, as any submodular function can be plugged into our method. For models beyond LMs, e.g., for text-image multi-modality models, we may use pre-existing submodular functions that are powerful for expressing diversity in the image domain. In addition, we account for relevance, diversity, and ordering for the exemplar retrieval stage, where one or two aspects typically get overlooked by previous methods. Empirically, we evaluate Div-S3 on 7 NLP tasks with 5 LLMs and show significantly improved performance compared to baselines. Our contributions are: 1. We propose an end-to-end framework DivS3 utilizing submodular optimization for performing data-efficient ICL using LLMs. Depending on budget requirements, Div-S3 provides the flexibility to set the budget either in terms of the number of exemplars to be used in the prompt or the LLM’s context window size. 2. We empirically validate the effectiveness of our framework on 7 different NLP tasks and show the transferability of results across LLMs of varying complexities. 3. We thoroughly analyze each component of Div-S3 by (a) studying S3 in a setting with no annotation budget constraint and (b) analyzing the sensitivity of the exemplars selected by Div-S3 to their position in the LLM’s prompt.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2403.08370	SMART: Submodular Data Mixture Strategy for Instruction Tuning	Instruction Tuning involves finetuning a language model on a collection of instructionformatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) — a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal mixing. Furthermore, SMART facilitates the creation of data mixtures based on a few representative subsets of tasks alone and through task pruning analysis, we reveal that in a limited budget setting, allocating budget among a subset of representative tasks yields superior performance compared to distributing the budget among all tasks. The code for reproducing our results is open-sourced at https://github.com/kowndinyarenduchintala/SMART.	One of the main goals of artificial intelligence (AI) research is to build machines that can communicate (Turing, 1950), and an essential part of communication is to understand and follow instructions. Large Language Models (LLMs), which are pre-trained over massive text corpora on next-tokenprediction objective, can perform a wide range of NLP tasks via "prompting" (Brown et al., 2020; Kojima et al., 2022; Almazrouei et al., 2023; Liu et al., 2023; Touvron et al., 2023). Instruction Tuning (Wei et al., 2021; Sanh et al., 2021; Chung et al., 2022) is an approach that further enhances the instruction-following ability and generalizability of pre-trained LLMs to unseen tasks. It involves fine-tuning an LLM on a collection of instruction-formatted instances (encompassing multiple tasks) - each consisting of an instruction (or task description), an optional input, the corresponding output (the ground truth) and optionally a few demonstrations/examples. It is a special case of multitask learning where the LLM is finetuned on a collection of instruction-formatted multitask datasets (Chung et al., 2022). Finetuning on multiple tasks simultaneously, allows the model to share and transfer information across tasks, resulting in a better common internal representation that is preferred by all tasks while suppressing taskdependent noise (Caruana, 1997). Consequently, the model learns to generalize to unseen tasks by discerning helpful cues from both implicitly and explicitly related tasks that it has previously seen. The performance enhancement from instruction tuning is heavily contingent on data quality, data quantity, and task composition (Wang et al., 2023b). Studies by Iyer et al. (2022) and Longpre et al. (2023) have shown that while scaling the number of tasks is important, the relative proportion of various tasks (mixture weighting) merits as much attention for optimal instruction tuning. Intuitively, we want the model to see enough data for a given task that it can perform well on it, but not to see so much data that it memorizes the training set (Raffel et al., 2020). Iyer et al. (2022) performed manual tuning of various benchmark proportions and decided on a final mixture, whereas Longpre et al. (2023) studied the impact of removing each benchmark from the finetuning mixture and relied on their practioners’ intuition from there on, to decide on the exact proportions of benchmarks. In this work, we would like to explore a more systematic approach to mixture weighting. Specifically, we are motivated by the fact that in a large multitask dataset like FLAN 2022 (Longpre et al., 2023), which has 1840 tasks, there will likely be many similar tasks leading to redundancies and not all of them may require sampling in equal proportions. For instance, there might be many tasks of the type Natural Language Inference (NLI), and it might be enough to sample relatively more instances from a few representative NLI tasks and less from the others. Furthermore, which samples we select from each task is also crucial because the samples should faithfully represent the task at hand. A random subset may fail to do this as it can miss out on essential corner cases. With this context, we focus on the following two fundamental research questions (RQs) that form the basis for our subsequent inquiry: - (RQ1) Given a huge multitask instructiontuning dataset and a limited fine-tuning budget which is defined by the total number of (prompt, response) instances that can be used for fine-tuning, how do we divide this budget among thousands of tasks present in the dataset? i.e., how many instances to sample from each task? and which instances to sample from each task? - (RQ2) Can we go a step further and strategically prune some tasks altogether and only fine-tune on a small subset of representative tasks without hurting the performance? If yes, what is the nature of this subset? To the best of our knowledge, there’s currently no principled approach to determining task compositions for instruction tuning, other than manual tuning and/or practioners’ intuition. As a first step towards addressing both of the above RQs, we first define a common subset selection problem (more formally stated in Section 3) as follows - Given a huge collection of M instructionformatted task datasets, a task budget M ′ ≤ M and a total budget (N ′) of (prompt, response) pairs, which M ′ tasks to select? and how many instances to select from each of these M ′ tasks and which instances to select? Note that RQ1 is an instance of this problem where M ′ = M . Constrained Submodular Maximization (Section 2) proves to be a good model for discovering representative subsets (or coresets) of a massive training dataset (or ground set) that acts as surrogate (i.e., achieves similar performance) and are much better than uniformly-at-random subsets. Intuitively, this is because submodular functions model information in subsets, and hence maximizing a submodular function subject to a constraint yields non-redundant subsets of the ground set. An essential feature of this model is that it returns weighted subsets, i.e., each sample in the coreset comes with an associated score, which indicates how important the sample is. Inspired by submodular functions, we propose our solution (Section 3) to the above subset selection problem for instruction tuning that works in two stages. In the first stage, we select a weighted subset of tasks from the full dataset where the weights will determine how many samples to select from each task. In the next stage, we select samples from each task based on the assigned task budgets. Note that the submodular functions used in each stage are not necessarily identical (Section 4.8). The main contributions of our work can be summarized as follows: - We introduce SMART — a novel data mixture strategy for instruction tuning that models the data mixture problem (Section 3) as a sequence of two cardinality-constrained submodular maximization problems and offer empirical evidence that it outperforms both examples proportional and equal mixing baselines (Section 4) as well as the mixture weights proposed by Longpre et al. (2023). - Existing works like Longpre et al. (2023) have reported a continuous increase in performance upon increasing the number of tasks (though the gains themselves may be diminishing). However, we posit that this depends on the order in which new tasks are incorporated and show empirically that in the case of SMART mixtures, a performance peak is observed with an initial addition of few representative tasks and upon adding more and more tasks, the performance is not sustained (Section 4.6). - We find that the nature of instances that should be selected in each task (i.e, whether a representative or diverse subset) also depends on the total task budget, M ′ (Section 4.8). For higher M ′s, each task on average gets the relatively low budget and selecting representative samples is more important; however for lower M ′s, when there is sufficient enough budget for each task, the need for diversity dominates	https://arxiv.org/pdf/2012.10630	GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning	Large scale machine learning and deep models are extremely data-hungry. Unfortunately, obtaining large amounts of labeled data is expensive, and training state-of-the-art models (with hyperparameter tuning) requires significant computing resources and time. Secondly, real-world data is noisy and imbalanced. As a result, several recent papers try to make the training process more efficient and robust. However, most existing work either focuses on robustness or efficiency, but not both. In this work, we introduce GLISTER, a GeneraLIzation based data Subset selecTion for Efficient and Robust learning framework. We formulate GLISTER as a mixed discretecontinuous bi-level optimization problem to select a subset of the training data, which maximizes the log-likelihood on a held-out validation set. We then analyze GLISTER for simple classifiers such as gaussian and multinomial naive-bayes, k-nearest neighbor classifier, and linear regression and show connections to submodularity. Next, we propose an iterative online algorithm GLISTER-ONLINE, which performs data selection iteratively along with the parameter updates and can be applied to any loss-based learning algorithm. We then show that for a rich class of loss functions including cross-entropy, hinge-loss, squared-loss, and logistic-loss, the inner discrete data selection is an instance of (weakly) submodular optimization, and we analyze conditions for which GLISTER-ONLINE reduces the validation loss and converges. Finally, we propose GLISTER-ACTIVE, an extension to batch active learning, and we empirically demonstrate the performance of GLISTER on a wide range of tasks including, (a) data selection to reduce training time, (b) robust learning under label noise and imbalance settings, and (c) batch-active learning with several deep and shallow models. We show that our framework improves upon state of the art both in efficiency and accuracy (in cases (a) and (c)) and is more efficient compared to other state-ofthe-art robust learning algorithms in case (b). The code for GLISTERis at:https://github.com/dssresearch/GLISTER.	With the quest to achieve human-like performance for machine learning and deep learning systems, the cost of training and deploying machine learning models has been significantly increasing. The wasted computational and engineering energy becomes evident in deep learning algorithms, wherein extensive hyper-parameter tuning and network architecture search need to be done. This results in staggering compute costs and running times. As a result, efficient and robust machine learning is a very relevant and substantial research problem. In this paper, we shall focus on three goals: Goal 1: Train machine learning and deep learning models on effective subsets of data, thereby significantly reducing training time and compute while not sacrificing accuracy. Goal 2: To (iteratively) select effective subsets of labeled data to reduce the labeling cost. Goal 3: Select data subsets to remove noisy labels and class imbalance, which is increasingly common in operational machine learning settings. Most prior work discussed above, either study robustness or efficiency, but not both. For example, the data selection approaches such as (Wei, Iyer, and Bilmes 2015; Mirzasoleiman, Bilmes, and Leskovec 2020; Shinohara 2014) and others focus on approximating either gradients or performance on the training sets, and hence would not be suitable for scenarios such as label noise and imbalance. On the other hand, the approaches like (Ren et al. 2018; Jiang et al. 2018) and others, focus on robustness but are not necessarily efficient. For example, the approach of (Ren et al. 2018) requires 3x the standard (deep) training cost, to obtain a robust model. GLISTER is the first framework, to the best of our knowledge, which focuses on both efficiency and robustness. Our work is closely related to the approaches of (Wei, Iyer, and Bilmes 2015) and (Ren et al. 2018). We build upon the work of (Wei, Iyer, and Bilmes 2015), by first generalizing their framework beyond simple classifiers (like nearest neighbor and naive bayes), but with general loss functions. We do this by proposing an iterative algorithm GLISTER-ONLINE which does data selection via a meta-learning based approach along with parameter updates. Furthermore, we pose the problem as optimizing the validation set performance as opposed to training set performance, thereby encouraging generalization. Next, our approach also bears similarity to (Ren et al. 2018), except that we need to solve a discrete optimization problem instead of a meta-gradient update. Moreover, we do not run our data selection every iteration, thereby ensuring that we are significantly faster than a single training run. Finally, we extend our algorithm to the active learning scenario. We demonstrate that our framework is more efficient and accurate compared to existing data selection and active learning algorithms, and secondly, also generalizes well under noisy data, and class imbalance scenarios. In particular, we show that GLISTER achieves a 3x - 6x speedup on a wide range of models and datasets, with very small loss in accuracy.	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2403.08370	SMART: Submodular Data Mixture Strategy for Instruction Tuning	Instruction Tuning involves finetuning a language model on a collection of instructionformatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) — a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal mixing. Furthermore, SMART facilitates the creation of data mixtures based on a few representative subsets of tasks alone and through task pruning analysis, we reveal that in a limited budget setting, allocating budget among a subset of representative tasks yields superior performance compared to distributing the budget among all tasks. The code for reproducing our results is open-sourced at https://github.com/kowndinyarenduchintala/SMART.	One of the main goals of artificial intelligence (AI) research is to build machines that can communicate (Turing, 1950), and an essential part of communication is to understand and follow instructions. Large Language Models (LLMs), which are pre-trained over massive text corpora on next-tokenprediction objective, can perform a wide range of NLP tasks via "prompting" (Brown et al., 2020; Kojima et al., 2022; Almazrouei et al., 2023; Liu et al., 2023; Touvron et al., 2023). Instruction Tuning (Wei et al., 2021; Sanh et al., 2021; Chung et al., 2022) is an approach that further enhances the instruction-following ability and generalizability of pre-trained LLMs to unseen tasks. It involves fine-tuning an LLM on a collection of instruction-formatted instances (encompassing multiple tasks) - each consisting of an instruction (or task description), an optional input, the corresponding output (the ground truth) and optionally a few demonstrations/examples. It is a special case of multitask learning where the LLM is finetuned on a collection of instruction-formatted multitask datasets (Chung et al., 2022). Finetuning on multiple tasks simultaneously, allows the model to share and transfer information across tasks, resulting in a better common internal representation that is preferred by all tasks while suppressing taskdependent noise (Caruana, 1997). Consequently, the model learns to generalize to unseen tasks by discerning helpful cues from both implicitly and explicitly related tasks that it has previously seen. The performance enhancement from instruction tuning is heavily contingent on data quality, data quantity, and task composition (Wang et al., 2023b). Studies by Iyer et al. (2022) and Longpre et al. (2023) have shown that while scaling the number of tasks is important, the relative proportion of various tasks (mixture weighting) merits as much attention for optimal instruction tuning. Intuitively, we want the model to see enough data for a given task that it can perform well on it, but not to see so much data that it memorizes the training set (Raffel et al., 2020). Iyer et al. (2022) performed manual tuning of various benchmark proportions and decided on a final mixture, whereas Longpre et al. (2023) studied the impact of removing each benchmark from the finetuning mixture and relied on their practioners’ intuition from there on, to decide on the exact proportions of benchmarks. In this work, we would like to explore a more systematic approach to mixture weighting. Specifically, we are motivated by the fact that in a large multitask dataset like FLAN 2022 (Longpre et al., 2023), which has 1840 tasks, there will likely be many similar tasks leading to redundancies and not all of them may require sampling in equal proportions. For instance, there might be many tasks of the type Natural Language Inference (NLI), and it might be enough to sample relatively more instances from a few representative NLI tasks and less from the others. Furthermore, which samples we select from each task is also crucial because the samples should faithfully represent the task at hand. A random subset may fail to do this as it can miss out on essential corner cases. With this context, we focus on the following two fundamental research questions (RQs) that form the basis for our subsequent inquiry: - (RQ1) Given a huge multitask instructiontuning dataset and a limited fine-tuning budget which is defined by the total number of (prompt, response) instances that can be used for fine-tuning, how do we divide this budget among thousands of tasks present in the dataset? i.e., how many instances to sample from each task? and which instances to sample from each task? - (RQ2) Can we go a step further and strategically prune some tasks altogether and only fine-tune on a small subset of representative tasks without hurting the performance? If yes, what is the nature of this subset? To the best of our knowledge, there’s currently no principled approach to determining task compositions for instruction tuning, other than manual tuning and/or practioners’ intuition. As a first step towards addressing both of the above RQs, we first define a common subset selection problem (more formally stated in Section 3) as follows - Given a huge collection of M instructionformatted task datasets, a task budget M ′ ≤ M and a total budget (N ′) of (prompt, response) pairs, which M ′ tasks to select? and how many instances to select from each of these M ′ tasks and which instances to select? Note that RQ1 is an instance of this problem where M ′ = M . Constrained Submodular Maximization (Section 2) proves to be a good model for discovering representative subsets (or coresets) of a massive training dataset (or ground set) that acts as surrogate (i.e., achieves similar performance) and are much better than uniformly-at-random subsets. Intuitively, this is because submodular functions model information in subsets, and hence maximizing a submodular function subject to a constraint yields non-redundant subsets of the ground set. An essential feature of this model is that it returns weighted subsets, i.e., each sample in the coreset comes with an associated score, which indicates how important the sample is. Inspired by submodular functions, we propose our solution (Section 3) to the above subset selection problem for instruction tuning that works in two stages. In the first stage, we select a weighted subset of tasks from the full dataset where the weights will determine how many samples to select from each task. In the next stage, we select samples from each task based on the assigned task budgets. Note that the submodular functions used in each stage are not necessarily identical (Section 4.8). The main contributions of our work can be summarized as follows: - We introduce SMART — a novel data mixture strategy for instruction tuning that models the data mixture problem (Section 3) as a sequence of two cardinality-constrained submodular maximization problems and offer empirical evidence that it outperforms both examples proportional and equal mixing baselines (Section 4) as well as the mixture weights proposed by Longpre et al. (2023). - Existing works like Longpre et al. (2023) have reported a continuous increase in performance upon increasing the number of tasks (though the gains themselves may be diminishing). However, we posit that this depends on the order in which new tasks are incorporated and show empirically that in the case of SMART mixtures, a performance peak is observed with an initial addition of few representative tasks and upon adding more and more tasks, the performance is not sustained (Section 4.6). - We find that the nature of instances that should be selected in each task (i.e, whether a representative or diverse subset) also depends on the total task budget, M ′ (Section 4.8). For higher M ′s, each task on average gets the relatively low budget and selecting representative samples is more important; however for lower M ′s, when there is sufficient enough budget for each task, the need for diversity dominates	https://arxiv.org/pdf/2410.11303	TSDS: Data Selection for Task-Specific Model Finetuning	Finetuning foundation models for specific tasks is an emerging paradigm in modern machine learning. The efficacy of task-specific finetuning largely depends on the selection of appropriate training data. We present TSDS (Task-Specific Data Selection), a framework to select data for task-specific model finetuning, guided by a small but representative set of examples from the target task. To do so, we formulate data selection for task-specific finetuning as an optimization problem with a distribution alignment loss based on optimal transport to capture the discrepancy between the selected data and the target distribution. In addition, we add a regularizer to encourage the diversity of the selected data and incorporate kernel density estimation into the regularizer to reduce the negative effects of near-duplicates among the candidate data. We connect our optimization problem to nearest neighbor search and design efficient algorithms to compute the optimal solution based on approximate nearest neighbor search techniques. We evaluate our method on data selection for both continued pretraining and instruction tuning of language models. We show that instruction tuning using data selected by our method with a 1% selection ratio often outperforms using the full dataset and beats the baseline selection methods by 1.5 points in F1 score on average. Our code is available at https://github.com/ZifanL/TSDS.	inetuning foundation models [3 ] is the de-facto paradigm for building machine learning applications that focus on specific tasks. Models such as BERT [10] and LLaMA [ 43] are large-scale models pretrained on massive unlabeled data across a wide range of domains. Those models can be specialized to downstream tasks through finetuning. Finetuning can take a variety of forms depending on the target task. For instance, continued pretraining [ 17 ] extends the pretraining stage of a model on a dataset that is more closely related to a target domain. As another setting, instruction tuning [51 ] trains a generative foundation model on instruction-response pairs to improve its performance in responding to task-specific instructions. Finetuning foundation models can lead to significant improvement in downstream tasks, but the effectiveness heavily relies on the right choice of training data [ 17 , 30 , 48 , 47 ]. However, the data repositories that one considers during training of generative models tend to be large—consider for example the use of Common Crawl, which contains 250 billion web pages, or The Pile [14 ]—and hence, it is impractical to manually select the data that are distributed like the use cases in the target task. Therefore, automated task-specific data selection becomes critical. In this paper, we propose TSDS (Task-Specific Data Selection), a framework to select data for taskspecific model finetuning. We consider the scenario of finetuning a foundation model to customize it for a specific task characterized by a few representative examples. The input to our framework is the representative examples and a massive repository of candidate data. Guided by the representative examples, we select training data from the repository for task-specific finetuning. We identify the following requirements for our framework. (Distribution Alignment) First, the distribution of the selected data should match the distribution of the representative examples from the target task. Distribution alignment is essential for a model to learn the target distribution and enable data-efficient finetuning for the target task [ 40 ]. Many works [38 , 17, 2, 50 , 47 ] retrieve candidate examples that are most similar to the representative examples. Such heuristics do not ensure distribution alignment between the selected data and the representative examples. A recent work [ 48 ] selects data by importance resampling to match the target distribution but is limited to an n-gram feature space, which cannot capture high-level semantics. (Diversity) Second, the selected data should be diverse so that the model can learn a wide range of related knowledge rather than overfitting to specific examples. In practice, data repositories created by web crawling may contain a large portion of near-duplicates [13 , 28 ] that can compromise diversity and negatively impact model performance [ 28, 19 ]. For example, a study [ 13 ] on several snapshots of ClueWeb and Common Crawl shows that 14% to 52% of the documents are near-duplicates. Previous works [38, 17 , 2 , 50 , 48 , 47 ] on task-specific data selection overlook near-duplicates, leading to the over-representation of such examples in the selected data. We require our framework to ensure diversity in selection even when a large fraction of the candidate examples are near-duplicates. (Scalability) Finally, the selection algorithm should be efficient, considering the increasing scale of modern data repositories. The high volume of candidate data (e.g., 250 billion pages in Common Crawl) poses a great challenge to efficient selection. Our framework formulates task-specific data selection as an optimization problem that allows a smooth trade-off between two crucial objectives: distribution alignment and diversity. The solution to the optimization problem is a categorical distribution assigned to the candidates which we will sample from. In the optimization objective, we use optimal transport to measure the discrepancy between the distribution assigned to the candidates and the target distribution, encouraging the alignment between them. We show that the optimization problem admits efficient algorithms to compute the optimal solution. In addition, our framework supports distribution alignment in any metric space that supports efficient nearest-neighbor search, including model-agnostic semantic embedding and model-specific features such as gradients. Our contributions: 1) We formulate data selection for task-specific finetuning as an optimization problem based on optimal transport for distribution alignment, with a regularization term that encourages diversity. 2) We make our framework robust to near-duplicates by incorporating kernel density estimation [36 ] into the regularization term. 3) We show the connection between the optimal solution to the optimization problem and nearest neighbor search, which allows us to develop efficient algorithms employing approximate nearest-neighbor search techniques [23]. We conduct extensive experiments to validate the effectiveness of our framework. We focus on natural language processing tasks where foundation models have shown great advancements. We show that our framework beats the state-of-the-art baseline [47 ] by 1.5 points in F1 score on average with a selection ratio of 1% on instruction tuning for two modern large language models on three tasks. In addition, continued pretraining using domain-specific data selected by our framework outperforms the other selection methods by up to 3 F1 points on four classification tasks from various domains. We also demonstrate that our framework is robust to near-duplicates in the data repository, maintaining consistent performance when 1% of the candidate examples are duplicate for up to 1,000 times. Our method is efficient, taking 28 hours to preprocess a corpus of 150M examples and less than 1 hour for each task-specific selection.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2403.08370	SMART: Submodular Data Mixture Strategy for Instruction Tuning	Instruction Tuning involves finetuning a language model on a collection of instructionformatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) — a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal mixing. Furthermore, SMART facilitates the creation of data mixtures based on a few representative subsets of tasks alone and through task pruning analysis, we reveal that in a limited budget setting, allocating budget among a subset of representative tasks yields superior performance compared to distributing the budget among all tasks. The code for reproducing our results is open-sourced at https://github.com/kowndinyarenduchintala/SMART.	One of the main goals of artificial intelligence (AI) research is to build machines that can communicate (Turing, 1950), and an essential part of communication is to understand and follow instructions. Large Language Models (LLMs), which are pre-trained over massive text corpora on next-tokenprediction objective, can perform a wide range of NLP tasks via "prompting" (Brown et al., 2020; Kojima et al., 2022; Almazrouei et al., 2023; Liu et al., 2023; Touvron et al., 2023). Instruction Tuning (Wei et al., 2021; Sanh et al., 2021; Chung et al., 2022) is an approach that further enhances the instruction-following ability and generalizability of pre-trained LLMs to unseen tasks. It involves fine-tuning an LLM on a collection of instruction-formatted instances (encompassing multiple tasks) - each consisting of an instruction (or task description), an optional input, the corresponding output (the ground truth) and optionally a few demonstrations/examples. It is a special case of multitask learning where the LLM is finetuned on a collection of instruction-formatted multitask datasets (Chung et al., 2022). Finetuning on multiple tasks simultaneously, allows the model to share and transfer information across tasks, resulting in a better common internal representation that is preferred by all tasks while suppressing taskdependent noise (Caruana, 1997). Consequently, the model learns to generalize to unseen tasks by discerning helpful cues from both implicitly and explicitly related tasks that it has previously seen. The performance enhancement from instruction tuning is heavily contingent on data quality, data quantity, and task composition (Wang et al., 2023b). Studies by Iyer et al. (2022) and Longpre et al. (2023) have shown that while scaling the number of tasks is important, the relative proportion of various tasks (mixture weighting) merits as much attention for optimal instruction tuning. Intuitively, we want the model to see enough data for a given task that it can perform well on it, but not to see so much data that it memorizes the training set (Raffel et al., 2020). Iyer et al. (2022) performed manual tuning of various benchmark proportions and decided on a final mixture, whereas Longpre et al. (2023) studied the impact of removing each benchmark from the finetuning mixture and relied on their practioners’ intuition from there on, to decide on the exact proportions of benchmarks. In this work, we would like to explore a more systematic approach to mixture weighting. Specifically, we are motivated by the fact that in a large multitask dataset like FLAN 2022 (Longpre et al., 2023), which has 1840 tasks, there will likely be many similar tasks leading to redundancies and not all of them may require sampling in equal proportions. For instance, there might be many tasks of the type Natural Language Inference (NLI), and it might be enough to sample relatively more instances from a few representative NLI tasks and less from the others. Furthermore, which samples we select from each task is also crucial because the samples should faithfully represent the task at hand. A random subset may fail to do this as it can miss out on essential corner cases. With this context, we focus on the following two fundamental research questions (RQs) that form the basis for our subsequent inquiry: - (RQ1) Given a huge multitask instructiontuning dataset and a limited fine-tuning budget which is defined by the total number of (prompt, response) instances that can be used for fine-tuning, how do we divide this budget among thousands of tasks present in the dataset? i.e., how many instances to sample from each task? and which instances to sample from each task? - (RQ2) Can we go a step further and strategically prune some tasks altogether and only fine-tune on a small subset of representative tasks without hurting the performance? If yes, what is the nature of this subset? To the best of our knowledge, there’s currently no principled approach to determining task compositions for instruction tuning, other than manual tuning and/or practioners’ intuition. As a first step towards addressing both of the above RQs, we first define a common subset selection problem (more formally stated in Section 3) as follows - Given a huge collection of M instructionformatted task datasets, a task budget M ′ ≤ M and a total budget (N ′) of (prompt, response) pairs, which M ′ tasks to select? and how many instances to select from each of these M ′ tasks and which instances to select? Note that RQ1 is an instance of this problem where M ′ = M . Constrained Submodular Maximization (Section 2) proves to be a good model for discovering representative subsets (or coresets) of a massive training dataset (or ground set) that acts as surrogate (i.e., achieves similar performance) and are much better than uniformly-at-random subsets. Intuitively, this is because submodular functions model information in subsets, and hence maximizing a submodular function subject to a constraint yields non-redundant subsets of the ground set. An essential feature of this model is that it returns weighted subsets, i.e., each sample in the coreset comes with an associated score, which indicates how important the sample is. Inspired by submodular functions, we propose our solution (Section 3) to the above subset selection problem for instruction tuning that works in two stages. In the first stage, we select a weighted subset of tasks from the full dataset where the weights will determine how many samples to select from each task. In the next stage, we select samples from each task based on the assigned task budgets. Note that the submodular functions used in each stage are not necessarily identical (Section 4.8). The main contributions of our work can be summarized as follows: - We introduce SMART — a novel data mixture strategy for instruction tuning that models the data mixture problem (Section 3) as a sequence of two cardinality-constrained submodular maximization problems and offer empirical evidence that it outperforms both examples proportional and equal mixing baselines (Section 4) as well as the mixture weights proposed by Longpre et al. (2023). - Existing works like Longpre et al. (2023) have reported a continuous increase in performance upon increasing the number of tasks (though the gains themselves may be diminishing). However, we posit that this depends on the order in which new tasks are incorporated and show empirically that in the case of SMART mixtures, a performance peak is observed with an initial addition of few representative tasks and upon adding more and more tasks, the performance is not sustained (Section 4.6). - We find that the nature of instances that should be selected in each task (i.e, whether a representative or diverse subset) also depends on the total task budget, M ′ (Section 4.8). For higher M ′s, each task on average gets the relatively low budget and selecting representative samples is more important; however for lower M ′s, when there is sufficient enough budget for each task, the need for diversity dominates	https://arxiv.org/pdf/2501.04155	MM-GEN: Enhancing Task Performance Through Targeted Multimodal Data Curation	Vision-language models (VLMs) are highly effective but often underperform on specialized tasks, for example Llava1.5 struggles on chart and diagram understanding, due to scarce task-specific training data. Existing training data, sourced from general-purpose datasets, fails to capture the nuanced details needed for these tasks. We introduce MMGEN, a scalable method that generates task-specific, highquality synthetic text for candidate images by leveraging stronger models. MM-GEN employs a three-stage targeted process: partitioning data into subgroups, generating targeted text based on task descriptions, and filtering out redundant and outlier data. Fine-tuning VLMs with data generated by MM-GEN leads to significant performance gains, including 29% on spatial reasoning and 15% on diagram understanding for Llava-1.5 (7B). Compared to human-curated caption data, MM-GEN achieves up to 1.6× better improvements for the original models, proving its effectiveness in enhancing task-specific VLM performance and bridging the gap between general-purpose datasets and specialized requirements. Code available at https://github.com/sjoshi804/MM-Gen.	While vision-language models (VLMs) demonstrate stateof-the-art performance on several multi-modal tasks [29], they are often on tasks that are simpler in nature [3]. These models still struggle with more complex tasks, e.g., those that require fine-grained understanding of details in images [3, 15, 21, 49]. We posit that the primary limitation for these VLMs is the quality and nature of the training data. VLMs are typically trained on large-scale image-text data scraped from the internet; while the images are rich and informative, the associated text descriptions can (i) have limited relevance to the image [39], or (ii) omit references to several specific details captured in the image [25]. Fig. 1 shows examples of such images and web-scraped captions. While the images are relevant for the tasks of chart understanding, spatial reasoning, and diagram understanding, respectively, the text fails to capture details essential for these tasks. While synthetic caption generation strategies proposed in prior work [25, 39, 57] can create more descriptive text annotations (by referring to more visual details), they are agnostic of the downstream target task. Consequently, they cannot ensure that relevant details are captured in the text annotations. Recently, Shi et al. [48] manually curated a task-specific dataset aimed at the task of multimodal mathematical question-answering by augmenting existing imagetext data with detailed textual annotations, based on their domain expertise, using strong VLMs. While effective, such a curation pipeline involves significant human effort and is not scalable [33, 59]. To address these limitations, we present MM-GEN, a highly general framework for automatically synthesizing taskrelevant text annotations for images by leveraging stronger VLMs (i.e., VLMs that perform well on the specific task) and requiring minimal human effort. MM-GEN takes as input a small set of examples from the target task (henceforth referred to as "reference samples"), a list of image types associated with the task, and a pool of task-relevant candidate images for training. In practice, these inputs can be easily obtained: a small number of reference samples and associated image types can be directly collected from the target task, and a task-relevant image pool can be found via image search with search engines or retrieved from largescale image-caption datasets [5, 46, 47]. Using the reference samples to specify the details of the task to the stronger VLM, MM-GEN generates text-annotations that are taskrelevant for the candidate images. Fig. 1 shows how the text generated by MM-GEN captures task-relevant details. This simple approach is highly effective, resulting in significant improvements across a variety of target tasks. Moreover, human-effort in this process is limited to providing (i) a small set of reference samples for the task, and (ii) a pool of candidate images. To further improve the quality and efficacy of the generated data, MM-GEN introduces a perplexity [4] based data-filtering approach to improve performance on target task using a high-value subset of the synthesized data. The components in MM-GEN are general and applicable to any image-text based target task enabling it to easily generalize across tasks and scale. We evaluate MM-GEN on improving VLMs' (e.g., Llava1.5 7B and 13B parameter versions) performance on finegrained image understanding tasks — chart understanding and reasoning, diagram understanding, and spatial reasoning on maps. The data curated by MM-GEN enables an absolute improvement over Llava-1.5 (7B) of 15%, 14% and 29%, respectively. We also see improvements over the much larger Llava-1.5 (13B) of 13%, , respectively. Moreover, MM-GEN’s filtering strategy helps in shrinking data volumes by up to 50% with no / minimal loss in performance. Empirical results show that models trained via MM-GEN data have a better performance than those trained via generated generic captions. MM-GEN data is also more effective than text annotations generated without task-specific reference examples, showing the importance of a targeted, data-centric approach for describing tasks. Finally, we analyze the effects of key design choices in MMGEN through ablation studies on e.g. size of the reference sample set, generating with / without partitioning, scaling number of in-context samples.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2403.08370	SMART: Submodular Data Mixture Strategy for Instruction Tuning	Instruction Tuning involves finetuning a language model on a collection of instructionformatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) — a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal mixing. Furthermore, SMART facilitates the creation of data mixtures based on a few representative subsets of tasks alone and through task pruning analysis, we reveal that in a limited budget setting, allocating budget among a subset of representative tasks yields superior performance compared to distributing the budget among all tasks. The code for reproducing our results is open-sourced at https://github.com/kowndinyarenduchintala/SMART.	One of the main goals of artificial intelligence (AI) research is to build machines that can communicate (Turing, 1950), and an essential part of communication is to understand and follow instructions. Large Language Models (LLMs), which are pre-trained over massive text corpora on next-tokenprediction objective, can perform a wide range of NLP tasks via "prompting" (Brown et al., 2020; Kojima et al., 2022; Almazrouei et al., 2023; Liu et al., 2023; Touvron et al., 2023). Instruction Tuning (Wei et al., 2021; Sanh et al., 2021; Chung et al., 2022) is an approach that further enhances the instruction-following ability and generalizability of pre-trained LLMs to unseen tasks. It involves fine-tuning an LLM on a collection of instruction-formatted instances (encompassing multiple tasks) - each consisting of an instruction (or task description), an optional input, the corresponding output (the ground truth) and optionally a few demonstrations/examples. It is a special case of multitask learning where the LLM is finetuned on a collection of instruction-formatted multitask datasets (Chung et al., 2022). Finetuning on multiple tasks simultaneously, allows the model to share and transfer information across tasks, resulting in a better common internal representation that is preferred by all tasks while suppressing taskdependent noise (Caruana, 1997). Consequently, the model learns to generalize to unseen tasks by discerning helpful cues from both implicitly and explicitly related tasks that it has previously seen. The performance enhancement from instruction tuning is heavily contingent on data quality, data quantity, and task composition (Wang et al., 2023b). Studies by Iyer et al. (2022) and Longpre et al. (2023) have shown that while scaling the number of tasks is important, the relative proportion of various tasks (mixture weighting) merits as much attention for optimal instruction tuning. Intuitively, we want the model to see enough data for a given task that it can perform well on it, but not to see so much data that it memorizes the training set (Raffel et al., 2020). Iyer et al. (2022) performed manual tuning of various benchmark proportions and decided on a final mixture, whereas Longpre et al. (2023) studied the impact of removing each benchmark from the finetuning mixture and relied on their practioners’ intuition from there on, to decide on the exact proportions of benchmarks. In this work, we would like to explore a more systematic approach to mixture weighting. Specifically, we are motivated by the fact that in a large multitask dataset like FLAN 2022 (Longpre et al., 2023), which has 1840 tasks, there will likely be many similar tasks leading to redundancies and not all of them may require sampling in equal proportions. For instance, there might be many tasks of the type Natural Language Inference (NLI), and it might be enough to sample relatively more instances from a few representative NLI tasks and less from the others. Furthermore, which samples we select from each task is also crucial because the samples should faithfully represent the task at hand. A random subset may fail to do this as it can miss out on essential corner cases. With this context, we focus on the following two fundamental research questions (RQs) that form the basis for our subsequent inquiry: - (RQ1) Given a huge multitask instructiontuning dataset and a limited fine-tuning budget which is defined by the total number of (prompt, response) instances that can be used for fine-tuning, how do we divide this budget among thousands of tasks present in the dataset? i.e., how many instances to sample from each task? and which instances to sample from each task? - (RQ2) Can we go a step further and strategically prune some tasks altogether and only fine-tune on a small subset of representative tasks without hurting the performance? If yes, what is the nature of this subset? To the best of our knowledge, there’s currently no principled approach to determining task compositions for instruction tuning, other than manual tuning and/or practioners’ intuition. As a first step towards addressing both of the above RQs, we first define a common subset selection problem (more formally stated in Section 3) as follows - Given a huge collection of M instructionformatted task datasets, a task budget M ′ ≤ M and a total budget (N ′) of (prompt, response) pairs, which M ′ tasks to select? and how many instances to select from each of these M ′ tasks and which instances to select? Note that RQ1 is an instance of this problem where M ′ = M . Constrained Submodular Maximization (Section 2) proves to be a good model for discovering representative subsets (or coresets) of a massive training dataset (or ground set) that acts as surrogate (i.e., achieves similar performance) and are much better than uniformly-at-random subsets. Intuitively, this is because submodular functions model information in subsets, and hence maximizing a submodular function subject to a constraint yields non-redundant subsets of the ground set. An essential feature of this model is that it returns weighted subsets, i.e., each sample in the coreset comes with an associated score, which indicates how important the sample is. Inspired by submodular functions, we propose our solution (Section 3) to the above subset selection problem for instruction tuning that works in two stages. In the first stage, we select a weighted subset of tasks from the full dataset where the weights will determine how many samples to select from each task. In the next stage, we select samples from each task based on the assigned task budgets. Note that the submodular functions used in each stage are not necessarily identical (Section 4.8). The main contributions of our work can be summarized as follows: - We introduce SMART — a novel data mixture strategy for instruction tuning that models the data mixture problem (Section 3) as a sequence of two cardinality-constrained submodular maximization problems and offer empirical evidence that it outperforms both examples proportional and equal mixing baselines (Section 4) as well as the mixture weights proposed by Longpre et al. (2023). - Existing works like Longpre et al. (2023) have reported a continuous increase in performance upon increasing the number of tasks (though the gains themselves may be diminishing). However, we posit that this depends on the order in which new tasks are incorporated and show empirically that in the case of SMART mixtures, a performance peak is observed with an initial addition of few representative tasks and upon adding more and more tasks, the performance is not sustained (Section 4.6). - We find that the nature of instances that should be selected in each task (i.e, whether a representative or diverse subset) also depends on the total task budget, M ′ (Section 4.8). For higher M ′s, each task on average gets the relatively low budget and selecting representative samples is more important; however for lower M ′s, when there is sufficient enough budget for each task, the need for diversity dominates	https://arxiv.org/pdf/1708.00489	Active Learning for Convolutional Neural Networks: A Core-Set Approach	Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (i.e. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.	Deep convolutional neural networks (CNNs) have shown unprecedented success in many areas of research in computer vision and pattern recognition, such as image classification, object detection, and scene segmentation. Although CNNs are universally successful in many tasks, they have a major drawback; they need a very large amount of labeled data to be able to learn their large number of parameters. More importantly, it is almost always better to have more data since the accuracy of CNNs is often not saturated with increasing dataset size. Hence, there is a constant desire to collect more and more data. Although this a desired behavior from an algorithmic perspective (higher representative power is typically better), labeling a dataset is a time consuming and an expensive task. These practical considerations raise a critical question: "what is the optimal way to choose data points to label such that the highest accuracy can be obtained given a fixed labeling budget." Active learning is one of the common paradigms to address this question. The goal of active learning is to find effective ways to choose data points to label, from a pool of unlabeled data points, in order to maximize the accuracy. Although it is not possible to obtain a universally good active learning strategy (Dasgupta, 2004), there exist many heuristics (Settles, 2010) which have been proven to be effective in practice. Active learning is typically an iterative process in which a model is learned at each iteration and a set of points is chosen to be labelled from a pool of unlabelled points using these aforementioned heuristics. We experiment with many of these heuristics in this paper and find them not effective when applied to CNNs. We argue that the main factor behind this ineffectiveness is the correlation caused via batch acquisition/sampling. In the classical setting, the active learning algorithms typically choose a single point at each iteration; however, this is not feasible for CNNs since i) a single point is likely to have no statistically significant impact on the accuracy due to the local optimization methods, and ii) each iteration requires a full training until convergence which makes it intractable to query labels one-by-one. Hence, it is necessary to query labels for a large subset at each iteration and it results in correlated samples even for moderately small subset sizes. In order to tailor an active learning method for the batch sampling case, we decided to define the active learning as core-set selection problem. Core-set selection problem aims to find a small subset given a large labeled dataset such that a model learned over the small subset is competitive over the whole dataset. Since we have no labels available, we perform the core-set selection without using the labels. In order to attack the unlabeled core-set problem for CNNs, we provide a rigorous bound between an average loss over any given subset of the dataset and the remaining data points via the geometry of the data points. As an active learning algorithm, we try to choose a subset such that this bound is minimized. Moreover, minimization of this bound turns out to be equivalent to the k-Center problem (Wolf, 2011) and we adopt an efficient approximate solution to this combinatorial optimization problem. We further study the behavior of our proposed algorithm empirically for the problem of image classification using three different datasets. Our empirical analysis demonstrates state-of-the-art performance by a large margin.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2403.08370	SMART: Submodular Data Mixture Strategy for Instruction Tuning	Instruction Tuning involves finetuning a language model on a collection of instructionformatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) — a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal mixing. Furthermore, SMART facilitates the creation of data mixtures based on a few representative subsets of tasks alone and through task pruning analysis, we reveal that in a limited budget setting, allocating budget among a subset of representative tasks yields superior performance compared to distributing the budget among all tasks. The code for reproducing our results is open-sourced at https://github.com/kowndinyarenduchintala/SMART.	One of the main goals of artificial intelligence (AI) research is to build machines that can communicate (Turing, 1950), and an essential part of communication is to understand and follow instructions. Large Language Models (LLMs), which are pre-trained over massive text corpora on next-tokenprediction objective, can perform a wide range of NLP tasks via "prompting" (Brown et al., 2020; Kojima et al., 2022; Almazrouei et al., 2023; Liu et al., 2023; Touvron et al., 2023). Instruction Tuning (Wei et al., 2021; Sanh et al., 2021; Chung et al., 2022) is an approach that further enhances the instruction-following ability and generalizability of pre-trained LLMs to unseen tasks. It involves fine-tuning an LLM on a collection of instruction-formatted instances (encompassing multiple tasks) - each consisting of an instruction (or task description), an optional input, the corresponding output (the ground truth) and optionally a few demonstrations/examples. It is a special case of multitask learning where the LLM is finetuned on a collection of instruction-formatted multitask datasets (Chung et al., 2022). Finetuning on multiple tasks simultaneously, allows the model to share and transfer information across tasks, resulting in a better common internal representation that is preferred by all tasks while suppressing taskdependent noise (Caruana, 1997). Consequently, the model learns to generalize to unseen tasks by discerning helpful cues from both implicitly and explicitly related tasks that it has previously seen. The performance enhancement from instruction tuning is heavily contingent on data quality, data quantity, and task composition (Wang et al., 2023b). Studies by Iyer et al. (2022) and Longpre et al. (2023) have shown that while scaling the number of tasks is important, the relative proportion of various tasks (mixture weighting) merits as much attention for optimal instruction tuning. Intuitively, we want the model to see enough data for a given task that it can perform well on it, but not to see so much data that it memorizes the training set (Raffel et al., 2020). Iyer et al. (2022) performed manual tuning of various benchmark proportions and decided on a final mixture, whereas Longpre et al. (2023) studied the impact of removing each benchmark from the finetuning mixture and relied on their practioners’ intuition from there on, to decide on the exact proportions of benchmarks. In this work, we would like to explore a more systematic approach to mixture weighting. Specifically, we are motivated by the fact that in a large multitask dataset like FLAN 2022 (Longpre et al., 2023), which has 1840 tasks, there will likely be many similar tasks leading to redundancies and not all of them may require sampling in equal proportions. For instance, there might be many tasks of the type Natural Language Inference (NLI), and it might be enough to sample relatively more instances from a few representative NLI tasks and less from the others. Furthermore, which samples we select from each task is also crucial because the samples should faithfully represent the task at hand. A random subset may fail to do this as it can miss out on essential corner cases. With this context, we focus on the following two fundamental research questions (RQs) that form the basis for our subsequent inquiry: - (RQ1) Given a huge multitask instructiontuning dataset and a limited fine-tuning budget which is defined by the total number of (prompt, response) instances that can be used for fine-tuning, how do we divide this budget among thousands of tasks present in the dataset? i.e., how many instances to sample from each task? and which instances to sample from each task? - (RQ2) Can we go a step further and strategically prune some tasks altogether and only fine-tune on a small subset of representative tasks without hurting the performance? If yes, what is the nature of this subset? To the best of our knowledge, there’s currently no principled approach to determining task compositions for instruction tuning, other than manual tuning and/or practioners’ intuition. As a first step towards addressing both of the above RQs, we first define a common subset selection problem (more formally stated in Section 3) as follows - Given a huge collection of M instructionformatted task datasets, a task budget M ′ ≤ M and a total budget (N ′) of (prompt, response) pairs, which M ′ tasks to select? and how many instances to select from each of these M ′ tasks and which instances to select? Note that RQ1 is an instance of this problem where M ′ = M . Constrained Submodular Maximization (Section 2) proves to be a good model for discovering representative subsets (or coresets) of a massive training dataset (or ground set) that acts as surrogate (i.e., achieves similar performance) and are much better than uniformly-at-random subsets. Intuitively, this is because submodular functions model information in subsets, and hence maximizing a submodular function subject to a constraint yields non-redundant subsets of the ground set. An essential feature of this model is that it returns weighted subsets, i.e., each sample in the coreset comes with an associated score, which indicates how important the sample is. Inspired by submodular functions, we propose our solution (Section 3) to the above subset selection problem for instruction tuning that works in two stages. In the first stage, we select a weighted subset of tasks from the full dataset where the weights will determine how many samples to select from each task. In the next stage, we select samples from each task based on the assigned task budgets. Note that the submodular functions used in each stage are not necessarily identical (Section 4.8). The main contributions of our work can be summarized as follows: - We introduce SMART — a novel data mixture strategy for instruction tuning that models the data mixture problem (Section 3) as a sequence of two cardinality-constrained submodular maximization problems and offer empirical evidence that it outperforms both examples proportional and equal mixing baselines (Section 4) as well as the mixture weights proposed by Longpre et al. (2023). - Existing works like Longpre et al. (2023) have reported a continuous increase in performance upon increasing the number of tasks (though the gains themselves may be diminishing). However, we posit that this depends on the order in which new tasks are incorporated and show empirically that in the case of SMART mixtures, a performance peak is observed with an initial addition of few representative tasks and upon adding more and more tasks, the performance is not sustained (Section 4.6). - We find that the nature of instances that should be selected in each task (i.e, whether a representative or diverse subset) also depends on the total task budget, M ′ (Section 4.8). For higher M ′s, each task on average gets the relatively low budget and selecting representative samples is more important; however for lower M ′s, when there is sufficient enough budget for each task, the need for diversity dominates	https://arxiv.org/pdf/2301.13287	MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning	Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results indicate that MILO can train models 3 x -10x faster and tune hyperparameters 20 x -75x faster than full-dataset training or tuning without compromising performance.	Deep learning has achieved remarkable success in a multitude of machine learning tasks, including natural language processing, computer vision, and speech recognition in recent years. This success is partially due to the availability of massive training datasets and the capacity to train large-scale neural networks. However, training deep models on extensive datasets is computationally demanding, incurring significant financial costs and generating substantial CO2 emissions [63, 59 ]. Bhavya et al. [ 2] overview several research trajectories aimed at enhancing model convergence and reducing training time and costs, including data subset selection, curriculum learning, model architecture improvements, and optimization algorithm enhancements. Our work specifically focuses on selecting useful, generalizable data subsets for efficient deep neural network training. Recent research [66 , 5 , 21 ] suggests that many current training datasets are redundant, indicating that a non-redundant, informative subset could achieve comparable performance to training on the full dataset. To identify such informative samples, metrics such as prediction uncertainty [ 7], prediction flips [ 66 , 72 ], loss [42 ], or gradient/gradient-norm [ 21, 47 , 28, 27 , 29 ] have been applied. These metrics, calculated using either the downstream machine learning model or a lightweight surrogate [ 7], require a fully converged model, a requirement that runs counter to the goal of efficient training. To address this, existing subset selection algorithms [ 47 , 28 , 27, 29, 30 , 54 ] for efficient learning utilize the downstream model during training for heuristic computation and periodically update the subset as the model-dependent metrics for each sample evolve. Drawbacks of Model-Dependent Subset Selection: Despite the theoretical advantages of existing subset selection strategies[ 28 , 27, 47 ], they often fall short in computational efficiency compared to adaptive random subset selection, which selects random subsets periodically. This is primarily because traditional approaches rely on downstream models and typically require the calculation of sample metrics, such as gradients, before each subset selection step. Furthermore, these computationally demanding subset selection steps occur during model training. For instance, Figure 1 compares the convergence rate of the ResNet18 model on the CIFAR100 dataset, in terms of both time and epochs. Here, the model uses 10% subsets chosen every epoch by GradMatchPB [ 27 ], a state-of-the-art data subset selection strategy for efficient training, CraigPB [ 47 ], and Adaptive-Random (where a new 10% subset is randomly selected periodically). The selection of a new subset every epoch is done to demonstrate the maximum performance attainable by GradMatchPB and CraigPB. The results indicate that GradMatchPB achieves faster epoch convergence than both Adaptive-Random and CraigPB when selecting a new subset each epoch. However, due to the necessity for a computationally intensive subset selection step every epoch, both GradMatchPB and CraigPB demonstrate considerable inefficiency in terms of training time. In their respective studies, Killamsetty et al. [27 ] and Mirzasoleiman et al. [47] suggested selecting a new subset every R epochs to enhance training efficiency. However, this comes at the cost of the model’s convergence rate. Lastly, model-dependent subset selection requires a computationally intensive subset selection steps each time a new model is trained. Model-Agnostic Selection: Our primary insight is that a model-agnostic subset selection framework can circumvent the computationally intensive subset selection steps during model training by conducting subset selection in the preprocessing phase. By pre-selecting subsets and storing them as metadata with each dataset, we can train numerous models without incurring additional costs, effectively spreading out the expense of subset selection. In this work, we endeavor to answer the following question: Is it possible to develop a model-agnostic subset selection method that selects new subsets in a minimal amount of time, yet achieves superior model convergence without significant compromise in test accuracy or generalization performance?  MILO Framework: In this study, we introduce MILO, a model-agnostic subset selection framework designed for efficient model training and tuning. MILO employs submodular measures [ 13 , 23 ], which capture higher-order interactions between data samples for subset selection. We utilize pre-trained large language models [ 55 ] and pre-trained vision transformers [25 ] as feature encoders due to their zero-shot feature encoding capabilities. These encoders compute the sample metrics in a nominal amount of time, and this computation is independent of the downstream model, rendering MILO model-agnostic. Figure 3 provides a visual representation of MILO for model training, comprising two steps: a) A pre-processing step that involves the selection of multiple subsets from the training dataset using "Stochastic-Greedy Exploration (SGE)" (refer to Section 3.1.1), and the construction of a probability distribution over the entire dataset for subset sampling through "Weighted Random Exploration (WRE)" (refer to Section 3.1.2); b) A model training scheme that involves training the model on a curriculum of easy-to-hard subsets (refer to Section 3.1.3) using the subsets selected and the probability distribution constructed in the pre-processing step. We also present a class-wise partitioning trick to significantly minimize the memory footprint of MILO (refer to Section 3.2). Effectiveness of MILO : Through extensive experiments on multiple real-world datasets, we empirically demonstrate the effectiveness of the MILO framework for efficient training and hyperparameter tuning. In Figure 2, we provide a summary of the speedup achieved by MILO compared to full data training and tuning, along with the corresponding relative performance. Our results show that MILO can train models 3x to 10x faster and tune hyperparameters 20x to 75x faster, with minimal loss in performance. Furthermore, we consistently outperform the subset selection baselines that were considered. Additionally, MILO exhibits significantly faster initial model convergence and maintains faster convergence throughout training compared to other baselines. These qualities make it highly effective for applications such as hyperparameter tuning and neural architecture search, where the ability to distinguish good models from bad models early in the tuning process is crucial for enhancing efficiency [38, 37]. Related Work: In recent years, data-subset selection strategies have achieved success in various machine learning applications such as speech recognition [ 68 , 67], machine translation [ 32 ], active learning [ 60 , 1 , 33 ], hyper-parameter tuning [ 30 ], continual learning [65 ], domain adaptation [20 ], and computer vision [22]. A recent empirical study [5] demonstrated that datasets frequently contain semantic redundancies. These redundancies can be removed during training without affecting performance. In response, data pruning methods [66 , 51, 62] have developed principled selection criteria for pruning redundant samples before model training, with minimal loss in performance. However, both existing data pruning and compute-efficient learning approaches are model-dependent, resulting in them being computationally expensive. In contrast, our method is model-agnostic and capable of identifying high-quality subsets by intelligent data exploration. Furthermore, our approach can achieve superior model convergence by utilizing an easy-to-hard curriculum, transitioning from representative to diverse subsets during training.	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2403.08370	SMART: Submodular Data Mixture Strategy for Instruction Tuning	Instruction Tuning involves finetuning a language model on a collection of instructionformatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) — a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal mixing. Furthermore, SMART facilitates the creation of data mixtures based on a few representative subsets of tasks alone and through task pruning analysis, we reveal that in a limited budget setting, allocating budget among a subset of representative tasks yields superior performance compared to distributing the budget among all tasks. The code for reproducing our results is open-sourced at https://github.com/kowndinyarenduchintala/SMART.	One of the main goals of artificial intelligence (AI) research is to build machines that can communicate (Turing, 1950), and an essential part of communication is to understand and follow instructions. Large Language Models (LLMs), which are pre-trained over massive text corpora on next-tokenprediction objective, can perform a wide range of NLP tasks via "prompting" (Brown et al., 2020; Kojima et al., 2022; Almazrouei et al., 2023; Liu et al., 2023; Touvron et al., 2023). Instruction Tuning (Wei et al., 2021; Sanh et al., 2021; Chung et al., 2022) is an approach that further enhances the instruction-following ability and generalizability of pre-trained LLMs to unseen tasks. It involves fine-tuning an LLM on a collection of instruction-formatted instances (encompassing multiple tasks) - each consisting of an instruction (or task description), an optional input, the corresponding output (the ground truth) and optionally a few demonstrations/examples. It is a special case of multitask learning where the LLM is finetuned on a collection of instruction-formatted multitask datasets (Chung et al., 2022). Finetuning on multiple tasks simultaneously, allows the model to share and transfer information across tasks, resulting in a better common internal representation that is preferred by all tasks while suppressing taskdependent noise (Caruana, 1997). Consequently, the model learns to generalize to unseen tasks by discerning helpful cues from both implicitly and explicitly related tasks that it has previously seen. The performance enhancement from instruction tuning is heavily contingent on data quality, data quantity, and task composition (Wang et al., 2023b). Studies by Iyer et al. (2022) and Longpre et al. (2023) have shown that while scaling the number of tasks is important, the relative proportion of various tasks (mixture weighting) merits as much attention for optimal instruction tuning. Intuitively, we want the model to see enough data for a given task that it can perform well on it, but not to see so much data that it memorizes the training set (Raffel et al., 2020). Iyer et al. (2022) performed manual tuning of various benchmark proportions and decided on a final mixture, whereas Longpre et al. (2023) studied the impact of removing each benchmark from the finetuning mixture and relied on their practioners’ intuition from there on, to decide on the exact proportions of benchmarks. In this work, we would like to explore a more systematic approach to mixture weighting. Specifically, we are motivated by the fact that in a large multitask dataset like FLAN 2022 (Longpre et al., 2023), which has 1840 tasks, there will likely be many similar tasks leading to redundancies and not all of them may require sampling in equal proportions. For instance, there might be many tasks of the type Natural Language Inference (NLI), and it might be enough to sample relatively more instances from a few representative NLI tasks and less from the others. Furthermore, which samples we select from each task is also crucial because the samples should faithfully represent the task at hand. A random subset may fail to do this as it can miss out on essential corner cases. With this context, we focus on the following two fundamental research questions (RQs) that form the basis for our subsequent inquiry: - (RQ1) Given a huge multitask instructiontuning dataset and a limited fine-tuning budget which is defined by the total number of (prompt, response) instances that can be used for fine-tuning, how do we divide this budget among thousands of tasks present in the dataset? i.e., how many instances to sample from each task? and which instances to sample from each task? - (RQ2) Can we go a step further and strategically prune some tasks altogether and only fine-tune on a small subset of representative tasks without hurting the performance? If yes, what is the nature of this subset? To the best of our knowledge, there’s currently no principled approach to determining task compositions for instruction tuning, other than manual tuning and/or practioners’ intuition. As a first step towards addressing both of the above RQs, we first define a common subset selection problem (more formally stated in Section 3) as follows - Given a huge collection of M instructionformatted task datasets, a task budget M ′ ≤ M and a total budget (N ′) of (prompt, response) pairs, which M ′ tasks to select? and how many instances to select from each of these M ′ tasks and which instances to select? Note that RQ1 is an instance of this problem where M ′ = M . Constrained Submodular Maximization (Section 2) proves to be a good model for discovering representative subsets (or coresets) of a massive training dataset (or ground set) that acts as surrogate (i.e., achieves similar performance) and are much better than uniformly-at-random subsets. Intuitively, this is because submodular functions model information in subsets, and hence maximizing a submodular function subject to a constraint yields non-redundant subsets of the ground set. An essential feature of this model is that it returns weighted subsets, i.e., each sample in the coreset comes with an associated score, which indicates how important the sample is. Inspired by submodular functions, we propose our solution (Section 3) to the above subset selection problem for instruction tuning that works in two stages. In the first stage, we select a weighted subset of tasks from the full dataset where the weights will determine how many samples to select from each task. In the next stage, we select samples from each task based on the assigned task budgets. Note that the submodular functions used in each stage are not necessarily identical (Section 4.8). The main contributions of our work can be summarized as follows: - We introduce SMART — a novel data mixture strategy for instruction tuning that models the data mixture problem (Section 3) as a sequence of two cardinality-constrained submodular maximization problems and offer empirical evidence that it outperforms both examples proportional and equal mixing baselines (Section 4) as well as the mixture weights proposed by Longpre et al. (2023). - Existing works like Longpre et al. (2023) have reported a continuous increase in performance upon increasing the number of tasks (though the gains themselves may be diminishing). However, we posit that this depends on the order in which new tasks are incorporated and show empirically that in the case of SMART mixtures, a performance peak is observed with an initial addition of few representative tasks and upon adding more and more tasks, the performance is not sustained (Section 4.6). - We find that the nature of instances that should be selected in each task (i.e, whether a representative or diverse subset) also depends on the total task budget, M ′ (Section 4.8). For higher M ′s, each task on average gets the relatively low budget and selecting representative samples is more important; however for lower M ′s, when there is sufficient enough budget for each task, the need for diversity dominates	https://arxiv.org/pdf/2103.00128	PRISM: A Rich Class of Parameterized Submodular Information Measures for Guided Subset Selection	With ever-increasing dataset sizes, subset selection techniques are becoming increasingly important for a plethora of tasks. It is often necessary to guide the subset selection to achieve certain desiderata, which includes focusing or targeting certain data points, while avoiding others. Examples of such problems include: i) targeted learning, where the goal is to find subsets with rare classes or rare attributes on which the model is underperforming, and ii) guided summarization, where data (e.g., image collection, text, document or video) is summarized for quicker human consumption with specific additional user intent. Motivated by such applications, we present PRISM, a rich class of PaRameterIzed Submodular information Measures. Through novel functions and their parameterizations, PRISM offers a variety of modeling capabilities that enable a trade-off between desired qualities of a subset like diversity or representation and similarity/dissimilarity with a set of data points. We demonstrate how PRISM can be applied to the two real-world problems mentioned above, which require guided subset selection. In doing so, we show that PRISM interestingly generalizes some past work, therein reinforcing its broad utility. Through extensive experiments on diverse datasets, we demonstrate the superiority of PRISM over the state-of-the-art in targeted learning and in guided imagecollection summarization. PRISM is available as a part of the SUBMODLIB (https://github.com/decile-team/submodlib) and TRUST (https://github.com/decile-team/trust) toolkits.	Recent times have seen explosive growth in data across several modalities, including text, images, and videos. This has given rise to the need for finding techniques for selecting effective smaller data subsets with specific characteristics for a variety of down-stream tasks. Often, we would like to guide the data selection to either target or avoid a certain set of data slices. One application is, what we call, targeted learning, where the goal is to select data points similar to data slices on which the model is currently performing poorly. These slices are data points that either belong to rare classes or have common rare attributes (e.g., color, background, etc.). An example of such a scenario is shown in Fig. 1(a), where a self-driving car model struggles in detecting “cars in a dark background“ because of a lack of such images in the training set. The targeted learning problem is to augment the training dataset with more of such rare images, with an aim to improve model performance. Another example is detecting cancers in biomedical imaging datasets, where the number of cancerous images are often a small fraction of the non-cancerous images. Another application comes from the summarization task, where an image collection, a video, or a text document is summarized for quicker human consumption by eliminating redundancy, while preserving the main content. While a number of applications require generic summarization (i.e., simply picking a representative and diverse subset of the massive dataset), it is often important to capture certain user intent in summarization. We call this guided summarization. Examples of guided summarization include: (i) query-focused summarization (Sharghi, Gong, and Shah 2016; Xiao et al. 2020), where a summary similar to a specific query is desired, and (ii) privacy-preserving summarization, where a summary dissimilar to a given private set of data points is desired (say, for privacy issues). See Fig. 1(b) for a pictorial illustration. PRISM Framework: We define PRISM through different instantiations and parameterizations of various submodular information measures (Sec. 2). These allow for modeling a spectrum of semantics required for guided subset selection, like relevance to a query set, irrelevance to a private set, and diversity among selected data points. We study the effect of parameter trade-off among these different semantics and present interesting insights. PRISM for Targeted Learning: We present a novel algorithm (Sec. 3.1, Algo. 1) to apply PRISM for targeted learning, which aims to improve a model’s performance on rare slices of data. Specifically, we show that submodular information measures are very effective in finding the examples from the rare classes in a large unlabeled set (akin to finding a needle in a haystack). On several image classification tasks, PRISM obtains ≈ 20-30% gain in accuracy of rare classes (≈ 12% more than existing approaches) by just adding a few additional labeled points from the rare classes. Furthermore, we show that PRISM is 20× to 50× more label-efficient compared to random sampling, and 2× to 4× more labelefficient compared to existing approaches (see Sec. 4.1). We also show that Algo. 1 generalizes some existing approaches for data subset selection, reinforcing its utility (Sec. 3.3). PRISM for Guided Summarization. We propose a learning framework for guided summarization using PRISM (Sec. 3.2). We show that PRISM offers a unified treatment to the different flavors of guided summarization (query-focused and privacy-preserving) and generalizes some existing approaches to summarization, again reinforcing its utility. We show that it outperforms other existing approaches on a real-world image collections dataset (Sec. 4.2).	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2403.08370	SMART: Submodular Data Mixture Strategy for Instruction Tuning	Instruction Tuning involves finetuning a language model on a collection of instructionformatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) — a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal mixing. Furthermore, SMART facilitates the creation of data mixtures based on a few representative subsets of tasks alone and through task pruning analysis, we reveal that in a limited budget setting, allocating budget among a subset of representative tasks yields superior performance compared to distributing the budget among all tasks. The code for reproducing our results is open-sourced at https://github.com/kowndinyarenduchintala/SMART.	One of the main goals of artificial intelligence (AI) research is to build machines that can communicate (Turing, 1950), and an essential part of communication is to understand and follow instructions. Large Language Models (LLMs), which are pre-trained over massive text corpora on next-tokenprediction objective, can perform a wide range of NLP tasks via "prompting" (Brown et al., 2020; Kojima et al., 2022; Almazrouei et al., 2023; Liu et al., 2023; Touvron et al., 2023). Instruction Tuning (Wei et al., 2021; Sanh et al., 2021; Chung et al., 2022) is an approach that further enhances the instruction-following ability and generalizability of pre-trained LLMs to unseen tasks. It involves fine-tuning an LLM on a collection of instruction-formatted instances (encompassing multiple tasks) - each consisting of an instruction (or task description), an optional input, the corresponding output (the ground truth) and optionally a few demonstrations/examples. It is a special case of multitask learning where the LLM is finetuned on a collection of instruction-formatted multitask datasets (Chung et al., 2022). Finetuning on multiple tasks simultaneously, allows the model to share and transfer information across tasks, resulting in a better common internal representation that is preferred by all tasks while suppressing taskdependent noise (Caruana, 1997). Consequently, the model learns to generalize to unseen tasks by discerning helpful cues from both implicitly and explicitly related tasks that it has previously seen. The performance enhancement from instruction tuning is heavily contingent on data quality, data quantity, and task composition (Wang et al., 2023b). Studies by Iyer et al. (2022) and Longpre et al. (2023) have shown that while scaling the number of tasks is important, the relative proportion of various tasks (mixture weighting) merits as much attention for optimal instruction tuning. Intuitively, we want the model to see enough data for a given task that it can perform well on it, but not to see so much data that it memorizes the training set (Raffel et al., 2020). Iyer et al. (2022) performed manual tuning of various benchmark proportions and decided on a final mixture, whereas Longpre et al. (2023) studied the impact of removing each benchmark from the finetuning mixture and relied on their practioners’ intuition from there on, to decide on the exact proportions of benchmarks. In this work, we would like to explore a more systematic approach to mixture weighting. Specifically, we are motivated by the fact that in a large multitask dataset like FLAN 2022 (Longpre et al., 2023), which has 1840 tasks, there will likely be many similar tasks leading to redundancies and not all of them may require sampling in equal proportions. For instance, there might be many tasks of the type Natural Language Inference (NLI), and it might be enough to sample relatively more instances from a few representative NLI tasks and less from the others. Furthermore, which samples we select from each task is also crucial because the samples should faithfully represent the task at hand. A random subset may fail to do this as it can miss out on essential corner cases. With this context, we focus on the following two fundamental research questions (RQs) that form the basis for our subsequent inquiry: - (RQ1) Given a huge multitask instructiontuning dataset and a limited fine-tuning budget which is defined by the total number of (prompt, response) instances that can be used for fine-tuning, how do we divide this budget among thousands of tasks present in the dataset? i.e., how many instances to sample from each task? and which instances to sample from each task? - (RQ2) Can we go a step further and strategically prune some tasks altogether and only fine-tune on a small subset of representative tasks without hurting the performance? If yes, what is the nature of this subset? To the best of our knowledge, there’s currently no principled approach to determining task compositions for instruction tuning, other than manual tuning and/or practioners’ intuition. As a first step towards addressing both of the above RQs, we first define a common subset selection problem (more formally stated in Section 3) as follows - Given a huge collection of M instructionformatted task datasets, a task budget M ′ ≤ M and a total budget (N ′) of (prompt, response) pairs, which M ′ tasks to select? and how many instances to select from each of these M ′ tasks and which instances to select? Note that RQ1 is an instance of this problem where M ′ = M . Constrained Submodular Maximization (Section 2) proves to be a good model for discovering representative subsets (or coresets) of a massive training dataset (or ground set) that acts as surrogate (i.e., achieves similar performance) and are much better than uniformly-at-random subsets. Intuitively, this is because submodular functions model information in subsets, and hence maximizing a submodular function subject to a constraint yields non-redundant subsets of the ground set. An essential feature of this model is that it returns weighted subsets, i.e., each sample in the coreset comes with an associated score, which indicates how important the sample is. Inspired by submodular functions, we propose our solution (Section 3) to the above subset selection problem for instruction tuning that works in two stages. In the first stage, we select a weighted subset of tasks from the full dataset where the weights will determine how many samples to select from each task. In the next stage, we select samples from each task based on the assigned task budgets. Note that the submodular functions used in each stage are not necessarily identical (Section 4.8). The main contributions of our work can be summarized as follows: - We introduce SMART — a novel data mixture strategy for instruction tuning that models the data mixture problem (Section 3) as a sequence of two cardinality-constrained submodular maximization problems and offer empirical evidence that it outperforms both examples proportional and equal mixing baselines (Section 4) as well as the mixture weights proposed by Longpre et al. (2023). - Existing works like Longpre et al. (2023) have reported a continuous increase in performance upon increasing the number of tasks (though the gains themselves may be diminishing). However, we posit that this depends on the order in which new tasks are incorporated and show empirically that in the case of SMART mixtures, a performance peak is observed with an initial addition of few representative tasks and upon adding more and more tasks, the performance is not sustained (Section 4.6). - We find that the nature of instances that should be selected in each task (i.e, whether a representative or diverse subset) also depends on the total task budget, M ′ (Section 4.8). For higher M ′s, each task on average gets the relatively low budget and selecting representative samples is more important; however for lower M ′s, when there is sufficient enough budget for each task, the need for diversity dominates	https://aclanthology.org/2024.findings-naacl.209.pdf	An End-to-End Submodular Framework for Data-Efficient In-Context Learning	Recent advancements in natural language tasks leverage the emergent In-Context Learning (ICL) ability of pretrained Large Language Models (LLMs). ICL enables LLMs to perform new tasks by utilizing a limited number of input-output examples as prompts. While ICL circumvents the costly step of finetuning LLMs, its effectiveness is heavily dependent on the quality and ordering of provided examples (called exemplars). In this work, we propose a two-stage data-efficient framework Div-S3 for exemplar selection for ICL. The first stage focuses on data annotation and employs a poolbased active learning approach to select a set of Diverse and informative exemplars from the target tasks’ unlabeled pool. Given a test input/query, the second stage uses Submodular Span Summarization (S3) to select the most relevant and non-redundant exemplars from the annotated pool of a limited budget. On 7 different NLP datasets and 5 LLMs of varying complexities, we show Div-S3 outperforms (1) existing active learning-based methods for data annotation for ICL and (2) similarity-based methods for test query-specific exemplars retrieval.	Pretrained large language models (LLMs) (Kenton and Toutanova, 2019; Brown et al., 2020; Chowdhery et al., 2022) have become foundational for a wide range of Natural Language Processing (NLP) tasks, demonstrating impressive success across various domains (Bommasani et al., 2021; Bubeck et al., 2023) through in-context learning (ICL) (Dong et al., 2022). ICL enables these pretrained LLMs to perform new tasks by using task-specific prompts containing a limited number of input-output demonstrations (also referred to as shots, exemplars, or prompts) in the natural language format. This approach facilitates deployment across different downstream tasks and reduces the need for labeled downstream training data since ICL does not require any task-specific training. The typical ICL procedure consists of two key components: (1) Exemplar annotation and retrieval (Wu et al., 2022; Köksal et al., 2022; Liu et al., 2022): This step involves annotating and retrieving exemplars that serve as context demonstrations. (2) Prompt template crafting (Sorensen et al., 2022; Deng et al., 2022): this step involves designing a prompt template to wrap these demonstrations in a comprehensible and coherent natural language instruction. Recent studies (Liu et al., 2022; Su et al., 2022; Margatina et al., 2023) show that providing exemplars most relevant to the current input instance is beneficial. Moreover, Zhao et al. (2021), Lu et al. (2022), and Liu et al. (2023) observe that LLMs attend more to the exemplars that are closer in the sequence to the input instance. Therefore, to achieve the best performance of ICL, the selection of exemplars and their ordering in the LLM prompt are crucial. In practice, an extensive collection of unlabeled exemplars is easily available (e.g., posts and discussions on forums like Stack Exchange or user-generated content on social media platforms), but manually annotating all exemplars would be exceptionally costly. To annotate and select the exemplars optimally for a given target task, we follow the two-stage approach shown in Figure 1: (1) Exemplar Annotation: select a subset of exemplars for annotation under a fixed budget (performed only once) and (2) Exemplar Retrieval: identify limited-sized exemplars in an ordering that are most influential for a given input instance from the annotated subset of exemplars. Intuitively, for the first stage, we aim to find the subset with maximal diversity and least redundancy so that, given any input, we can find corresponding labeled exemplars. For the second stage, in addition to the diversity requirement similar to the first stage, we emphasize the relevance of the exemplars to the given input query and order exemplars so that their relevance to the input query decreases as the exemplars are farther away from the input instance. We propose a framework Div-S3 based on submodular optimization that unifies the abovementioned two stages. For Exemplar Annotation, we model the problem as a submodular optimization problem under a cardinality constraint to find as Diverse a subset as possible within a budget. For Exemplar Retrieval, we formalize the problem as a Submodular Span Summarization (S3) problem (Kumari and Bilmes, 2021) with a knapsack constraint, which finds a diverse subset most relevant to the input query under a token length limit. Also, we naturally order the resulting exemplars based on the gains represented by the submodular function. The name of our proposed framework Div-S3 captures the optimization objectives used for both exemplar annotation (Div) and exemplar retrieval (S3) stages. In Fig. 2, we show a sample test query where using Div-S3 for exemplar selection leads to a more diverse and query-relevant exemplar set (more examples provided in Appendix D). Our framework is general, as any submodular function can be plugged into our method. For models beyond LMs, e.g., for text-image multi-modality models, we may use pre-existing submodular functions that are powerful for expressing diversity in the image domain. In addition, we account for relevance, diversity, and ordering for the exemplar retrieval stage, where one or two aspects typically get overlooked by previous methods. Empirically, we evaluate Div-S3 on 7 NLP tasks with 5 LLMs and show significantly improved performance compared to baselines. Our contributions are: 1. We propose an end-to-end framework DivS3 utilizing submodular optimization for performing data-efficient ICL using LLMs. Depending on budget requirements, Div-S3 provides the flexibility to set the budget either in terms of the number of exemplars to be used in the prompt or the LLM’s context window size. 2. We empirically validate the effectiveness of our framework on 7 different NLP tasks and show the transferability of results across LLMs of varying complexities. 3. We thoroughly analyze each component of Div-S3 by (a) studying S3 in a setting with no annotation budget constraint and (b) analyzing the sensitivity of the exemplars selected by Div-S3 to their position in the LLM’s prompt.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2012.10630	GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning	Large scale machine learning and deep models are extremely data-hungry. Unfortunately, obtaining large amounts of labeled data is expensive, and training state-of-the-art models (with hyperparameter tuning) requires significant computing resources and time. Secondly, real-world data is noisy and imbalanced. As a result, several recent papers try to make the training process more efficient and robust. However, most existing work either focuses on robustness or efficiency, but not both. In this work, we introduce GLISTER, a GeneraLIzation based data Subset selecTion for Efficient and Robust learning framework. We formulate GLISTER as a mixed discretecontinuous bi-level optimization problem to select a subset of the training data, which maximizes the log-likelihood on a held-out validation set. We then analyze GLISTER for simple classifiers such as gaussian and multinomial naive-bayes, k-nearest neighbor classifier, and linear regression and show connections to submodularity. Next, we propose an iterative online algorithm GLISTER-ONLINE, which performs data selection iteratively along with the parameter updates and can be applied to any loss-based learning algorithm. We then show that for a rich class of loss functions including cross-entropy, hinge-loss, squared-loss, and logistic-loss, the inner discrete data selection is an instance of (weakly) submodular optimization, and we analyze conditions for which GLISTER-ONLINE reduces the validation loss and converges. Finally, we propose GLISTER-ACTIVE, an extension to batch active learning, and we empirically demonstrate the performance of GLISTER on a wide range of tasks including, (a) data selection to reduce training time, (b) robust learning under label noise and imbalance settings, and (c) batch-active learning with several deep and shallow models. We show that our framework improves upon state of the art both in efficiency and accuracy (in cases (a) and (c)) and is more efficient compared to other state-ofthe-art robust learning algorithms in case (b). The code for GLISTERis at: https://github.com/dssresearch/GLISTER.	With the quest to achieve human-like performance for machine learning and deep learning systems, the cost of training and deploying machine learning models has been significantly increasing. The wasted computational and engineering energy becomes evident in deep learning algorithms, wherein extensive hyper-parameter tuning and network architecture search need to be done. This results in staggering compute costs and running times. As a result, efficient and robust machine learning is a very relevant and substantial research problem. In this paper, we shall focus on three goals: Goal 1: Train machine learning and deep learning models on effective subsets of data, thereby significantly reducing training time and compute while not sacrificing accuracy. Goal 2: To (iteratively) select effective subsets of labeled data to reduce the labeling cost. Goal 3: Select data subsets to remove noisy labels and class imbalance, which is increasingly common in operational machine learning settings. Most prior work discussed above, either study robustness or efficiency, but not both. For example, the data selection approaches such as (Wei, Iyer, and Bilmes 2015; Mirzasoleiman, Bilmes, and Leskovec 2020; Shinohara 2014) and others focus on approximating either gradients or performance on the training sets, and hence would not be suitable for scenarios such as label noise and imbalance. On the other hand, the approaches like (Ren et al. 2018; Jiang et al. 2018) and others, focus on robustness but are not necessarily efficient. For example, the approach of (Ren et al. 2018) requires 3x the standard (deep) training cost, to obtain a robust model. GLISTER is the first framework, to the best of our knowledge, which focuses on both efficiency and robustness. Our work is closely related to the approaches of (Wei, Iyer, and Bilmes 2015) and (Ren et al. 2018). We build upon the work of (Wei, Iyer, and Bilmes 2015), by first generalizing their framework beyond simple classifiers (like nearest neighbor and naive bayes), but with general loss functions. We do this by proposing an iterative algorithm GLISTER-ONLINE which does data selection via a meta-learning based approach along with parameter updates. Furthermore, we pose the problem as optimizing the validation set performance as opposed to training set performance, thereby encouraging generalization. Next, our approach also bears similarity to (Ren et al. 2018), except that we need to solve a discrete optimization problem instead of a meta-gradient update. Moreover, we do not run our data selection every iteration, thereby ensuring that we are significantly faster than a single training run. Finally, we extend our algorithm to the active learning scenario. We demonstrate that our framework is more efficient and accurate compared to existing data selection and active learning algorithms, and secondly, also generalizes well under noisy data, and class imbalance scenarios. In particular, we show that GLISTER achieves a 3x - 6x speedup on a wide range of models and datasets, with very small loss in accuracy.	https://arxiv.org/pdf/2410.11303	TSDS: Data Selection for Task-Specific Model Finetuning	Finetuning foundation models for specific tasks is an emerging paradigm in modern machine learning. The efficacy of task-specific finetuning largely depends on the selection of appropriate training data. We present TSDS (Task-Specific Data Selection), a framework to select data for task-specific model finetuning, guided by a small but representative set of examples from the target task. To do so, we formulate data selection for task-specific finetuning as an optimization problem with a distribution alignment loss based on optimal transport to capture the discrepancy between the selected data and the target distribution. In addition, we add a regularizer to encourage the diversity of the selected data and incorporate kernel density estimation into the regularizer to reduce the negative effects of near-duplicates among the candidate data. We connect our optimization problem to nearest neighbor search and design efficient algorithms to compute the optimal solution based on approximate nearest neighbor search techniques. We evaluate our method on data selection for both continued pretraining and instruction tuning of language models. We show that instruction tuning using data selected by our method with a 1% selection ratio often outperforms using the full dataset and beats the baseline selection methods by 1.5 points in F1 score on average. Our code is available at https://github.com/ZifanL/TSDS.	inetuning foundation models [3 ] is the de-facto paradigm for building machine learning applications that focus on specific tasks. Models such as BERT [10] and LLaMA [ 43] are large-scale models pretrained on massive unlabeled data across a wide range of domains. Those models can be specialized to downstream tasks through finetuning. Finetuning can take a variety of forms depending on the target task. For instance, continued pretraining [ 17 ] extends the pretraining stage of a model on a dataset that is more closely related to a target domain. As another setting, instruction tuning [51 ] trains a generative foundation model on instruction-response pairs to improve its performance in responding to task-specific instructions. Finetuning foundation models can lead to significant improvement in downstream tasks, but the effectiveness heavily relies on the right choice of training data [ 17 , 30 , 48 , 47 ]. However, the data repositories that one considers during training of generative models tend to be large—consider for example the use of Common Crawl, which contains 250 billion web pages, or The Pile [14 ]—and hence, it is impractical to manually select the data that are distributed like the use cases in the target task. Therefore, automated task-specific data selection becomes critical. In this paper, we propose TSDS (Task-Specific Data Selection), a framework to select data for taskspecific model finetuning. We consider the scenario of finetuning a foundation model to customize it for a specific task characterized by a few representative examples. The input to our framework is the representative examples and a massive repository of candidate data. Guided by the representative examples, we select training data from the repository for task-specific finetuning. We identify the following requirements for our framework. (Distribution Alignment) First, the distribution of the selected data should match the distribution of the representative examples from the target task. Distribution alignment is essential for a model to learn the target distribution and enable data-efficient finetuning for the target task [ 40 ]. Many works [38 , 17, 2, 50 , 47 ] retrieve candidate examples that are most similar to the representative examples. Such heuristics do not ensure distribution alignment between the selected data and the representative examples. A recent work [ 48 ] selects data by importance resampling to match the target distribution but is limited to an n-gram feature space, which cannot capture high-level semantics. (Diversity) Second, the selected data should be diverse so that the model can learn a wide range of related knowledge rather than overfitting to specific examples. In practice, data repositories created by web crawling may contain a large portion of near-duplicates [13 , 28 ] that can compromise diversity and negatively impact model performance [ 28, 19 ]. For example, a study [ 13 ] on several snapshots of ClueWeb and Common Crawl shows that 14% to 52% of the documents are near-duplicates. Previous works [38, 17 , 2 , 50 , 48 , 47 ] on task-specific data selection overlook near-duplicates, leading to the over-representation of such examples in the selected data. We require our framework to ensure diversity in selection even when a large fraction of the candidate examples are near-duplicates. (Scalability) Finally, the selection algorithm should be efficient, considering the increasing scale of modern data repositories. The high volume of candidate data (e.g., 250 billion pages in Common Crawl) poses a great challenge to efficient selection. Our framework formulates task-specific data selection as an optimization problem that allows a smooth trade-off between two crucial objectives: distribution alignment and diversity. The solution to the optimization problem is a categorical distribution assigned to the candidates which we will sample from. In the optimization objective, we use optimal transport to measure the discrepancy between the distribution assigned to the candidates and the target distribution, encouraging the alignment between them. We show that the optimization problem admits efficient algorithms to compute the optimal solution. In addition, our framework supports distribution alignment in any metric space that supports efficient nearest-neighbor search, including model-agnostic semantic embedding and model-specific features such as gradients. Our contributions: 1) We formulate data selection for task-specific finetuning as an optimization problem based on optimal transport for distribution alignment, with a regularization term that encourages diversity. 2) We make our framework robust to near-duplicates by incorporating kernel density estimation [36 ] into the regularization term. 3) We show the connection between the optimal solution to the optimization problem and nearest neighbor search, which allows us to develop efficient algorithms employing approximate nearest-neighbor search techniques [23]. We conduct extensive experiments to validate the effectiveness of our framework. We focus on natural language processing tasks where foundation models have shown great advancements. We show that our framework beats the state-of-the-art baseline [47 ] by 1.5 points in F1 score on average with a selection ratio of 1% on instruction tuning for two modern large language models on three tasks. In addition, continued pretraining using domain-specific data selected by our framework outperforms the other selection methods by up to 3 F1 points on four classification tasks from various domains. We also demonstrate that our framework is robust to near-duplicates in the data repository, maintaining consistent performance when 1% of the candidate examples are duplicate for up to 1,000 times. Our method is efficient, taking 28 hours to preprocess a corpus of 150M examples and less than 1 hour for each task-specific selection.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2012.10630	GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning	Large scale machine learning and deep models are extremely data-hungry. Unfortunately, obtaining large amounts of labeled data is expensive, and training state-of-the-art models (with hyperparameter tuning) requires significant computing resources and time. Secondly, real-world data is noisy and imbalanced. As a result, several recent papers try to make the training process more efficient and robust. However, most existing work either focuses on robustness or efficiency, but not both. In this work, we introduce GLISTER, a GeneraLIzation based data Subset selecTion for Efficient and Robust learning framework. We formulate GLISTER as a mixed discretecontinuous bi-level optimization problem to select a subset of the training data, which maximizes the log-likelihood on a held-out validation set. We then analyze GLISTER for simple classifiers such as gaussian and multinomial naive-bayes, k-nearest neighbor classifier, and linear regression and show connections to submodularity. Next, we propose an iterative online algorithm GLISTER-ONLINE, which performs data selection iteratively along with the parameter updates and can be applied to any loss-based learning algorithm. We then show that for a rich class of loss functions including cross-entropy, hinge-loss, squared-loss, and logistic-loss, the inner discrete data selection is an instance of (weakly) submodular optimization, and we analyze conditions for which GLISTER-ONLINE reduces the validation loss and converges. Finally, we propose GLISTER-ACTIVE, an extension to batch active learning, and we empirically demonstrate the performance of GLISTER on a wide range of tasks including, (a) data selection to reduce training time, (b) robust learning under label noise and imbalance settings, and (c) batch-active learning with several deep and shallow models. We show that our framework improves upon state of the art both in efficiency and accuracy (in cases (a) and (c)) and is more efficient compared to other state-ofthe-art robust learning algorithms in case (b). The code for GLISTERis at: https://github.com/dssresearch/GLISTER.	With the quest to achieve human-like performance for machine learning and deep learning systems, the cost of training and deploying machine learning models has been significantly increasing. The wasted computational and engineering energy becomes evident in deep learning algorithms, wherein extensive hyper-parameter tuning and network architecture search need to be done. This results in staggering compute costs and running times. As a result, efficient and robust machine learning is a very relevant and substantial research problem. In this paper, we shall focus on three goals: Goal 1: Train machine learning and deep learning models on effective subsets of data, thereby significantly reducing training time and compute while not sacrificing accuracy. Goal 2: To (iteratively) select effective subsets of labeled data to reduce the labeling cost. Goal 3: Select data subsets to remove noisy labels and class imbalance, which is increasingly common in operational machine learning settings. Most prior work discussed above, either study robustness or efficiency, but not both. For example, the data selection approaches such as (Wei, Iyer, and Bilmes 2015; Mirzasoleiman, Bilmes, and Leskovec 2020; Shinohara 2014) and others focus on approximating either gradients or performance on the training sets, and hence would not be suitable for scenarios such as label noise and imbalance. On the other hand, the approaches like (Ren et al. 2018; Jiang et al. 2018) and others, focus on robustness but are not necessarily efficient. For example, the approach of (Ren et al. 2018) requires 3x the standard (deep) training cost, to obtain a robust model. GLISTER is the first framework, to the best of our knowledge, which focuses on both efficiency and robustness. Our work is closely related to the approaches of (Wei, Iyer, and Bilmes 2015) and (Ren et al. 2018). We build upon the work of (Wei, Iyer, and Bilmes 2015), by first generalizing their framework beyond simple classifiers (like nearest neighbor and naive bayes), but with general loss functions. We do this by proposing an iterative algorithm GLISTER-ONLINE which does data selection via a meta-learning based approach along with parameter updates. Furthermore, we pose the problem as optimizing the validation set performance as opposed to training set performance, thereby encouraging generalization. Next, our approach also bears similarity to (Ren et al. 2018), except that we need to solve a discrete optimization problem instead of a meta-gradient update. Moreover, we do not run our data selection every iteration, thereby ensuring that we are significantly faster than a single training run. Finally, we extend our algorithm to the active learning scenario. We demonstrate that our framework is more efficient and accurate compared to existing data selection and active learning algorithms, and secondly, also generalizes well under noisy data, and class imbalance scenarios. In particular, we show that GLISTER achieves a 3x - 6x speedup on a wide range of models and datasets, with very small loss in accuracy.	https://arxiv.org/pdf/2501.04155	MM-GEN: Enhancing Task Performance Through Targeted Multimodal Data Curation	Vision-language models (VLMs) are highly effective but often underperform on specialized tasks, for example Llava1.5 struggles on chart and diagram understanding, due to scarce task-specific training data. Existing training data, sourced from general-purpose datasets, fails to capture the nuanced details needed for these tasks. We introduce MMGEN, a scalable method that generates task-specific, highquality synthetic text for candidate images by leveraging stronger models. MM-GEN employs a three-stage targeted process: partitioning data into subgroups, generating targeted text based on task descriptions, and filtering out redundant and outlier data. Fine-tuning VLMs with data generated by MM-GEN leads to significant performance gains, including 29% on spatial reasoning and 15% on diagram understanding for Llava-1.5 (7B). Compared to human-curated caption data, MM-GEN achieves up to 1.6× better improvements for the original models, proving its effectiveness in enhancing task-specific VLM performance and bridging the gap between general-purpose datasets and specialized requirements. Code available at https://github.com/sjoshi804/MM-Gen.	While vision-language models (VLMs) demonstrate stateof-the-art performance on several multi-modal tasks [29], they are often on tasks that are simpler in nature [3]. These models still struggle with more complex tasks, e.g., those that require fine-grained understanding of details in images [3, 15, 21, 49]. We posit that the primary limitation for these VLMs is the quality and nature of the training data. VLMs are typically trained on large-scale image-text data scraped from the internet; while the images are rich and informative, the associated text descriptions can (i) have limited relevance to the image [39], or (ii) omit references to several specific details captured in the image [25]. Fig. 1 shows examples of such images and web-scraped captions. While the images are relevant for the tasks of chart understanding, spatial reasoning, and diagram understanding, respectively, the text fails to capture details essential for these tasks. While synthetic caption generation strategies proposed in prior work [25, 39, 57] can create more descriptive text annotations (by referring to more visual details), they are agnostic of the downstream target task. Consequently, they cannot ensure that relevant details are captured in the text annotations. Recently, Shi et al. [48] manually curated a task-specific dataset aimed at the task of multimodal mathematical question-answering by augmenting existing imagetext data with detailed textual annotations, based on their domain expertise, using strong VLMs. While effective, such a curation pipeline involves significant human effort and is not scalable [33, 59]. To address these limitations, we present MM-GEN, a highly general framework for automatically synthesizing taskrelevant text annotations for images by leveraging stronger VLMs (i.e., VLMs that perform well on the specific task) and requiring minimal human effort. MM-GEN takes as input a small set of examples from the target task (henceforth referred to as "reference samples"), a list of image types associated with the task, and a pool of task-relevant candidate images for training. In practice, these inputs can be easily obtained: a small number of reference samples and associated image types can be directly collected from the target task, and a task-relevant image pool can be found via image search with search engines or retrieved from largescale image-caption datasets [5, 46, 47]. Using the reference samples to specify the details of the task to the stronger VLM, MM-GEN generates text-annotations that are taskrelevant for the candidate images. Fig. 1 shows how the text generated by MM-GEN captures task-relevant details. This simple approach is highly effective, resulting in significant improvements across a variety of target tasks. Moreover, human-effort in this process is limited to providing (i) a small set of reference samples for the task, and (ii) a pool of candidate images. To further improve the quality and efficacy of the generated data, MM-GEN introduces a perplexity [4] based data-filtering approach to improve performance on target task using a high-value subset of the synthesized data. The components in MM-GEN are general and applicable to any image-text based target task enabling it to easily generalize across tasks and scale. We evaluate MM-GEN on improving VLMs' (e.g., Llava1.5 7B and 13B parameter versions) performance on finegrained image understanding tasks — chart understanding and reasoning, diagram understanding, and spatial reasoning on maps. The data curated by MM-GEN enables an absolute improvement over Llava-1.5 (7B) of 15%, 14% and 29%, respectively. We also see improvements over the much larger Llava-1.5 (13B) of 13%, , respectively. Moreover, MM-GEN’s filtering strategy helps in shrinking data volumes by up to 50% with no / minimal loss in performance. Empirical results show that models trained via MM-GEN data have a better performance than those trained via generated generic captions. MM-GEN data is also more effective than text annotations generated without task-specific reference examples, showing the importance of a targeted, data-centric approach for describing tasks. Finally, we analyze the effects of key design choices in MMGEN through ablation studies on e.g. size of the reference sample set, generating with / without partitioning, scaling number of in-context samples.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2012.10630	GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning	Large scale machine learning and deep models are extremely data-hungry. Unfortunately, obtaining large amounts of labeled data is expensive, and training state-of-the-art models (with hyperparameter tuning) requires significant computing resources and time. Secondly, real-world data is noisy and imbalanced. As a result, several recent papers try to make the training process more efficient and robust. However, most existing work either focuses on robustness or efficiency, but not both. In this work, we introduce GLISTER, a GeneraLIzation based data Subset selecTion for Efficient and Robust learning framework. We formulate GLISTER as a mixed discretecontinuous bi-level optimization problem to select a subset of the training data, which maximizes the log-likelihood on a held-out validation set. We then analyze GLISTER for simple classifiers such as gaussian and multinomial naive-bayes, k-nearest neighbor classifier, and linear regression and show connections to submodularity. Next, we propose an iterative online algorithm GLISTER-ONLINE, which performs data selection iteratively along with the parameter updates and can be applied to any loss-based learning algorithm. We then show that for a rich class of loss functions including cross-entropy, hinge-loss, squared-loss, and logistic-loss, the inner discrete data selection is an instance of (weakly) submodular optimization, and we analyze conditions for which GLISTER-ONLINE reduces the validation loss and converges. Finally, we propose GLISTER-ACTIVE, an extension to batch active learning, and we empirically demonstrate the performance of GLISTER on a wide range of tasks including, (a) data selection to reduce training time, (b) robust learning under label noise and imbalance settings, and (c) batch-active learning with several deep and shallow models. We show that our framework improves upon state of the art both in efficiency and accuracy (in cases (a) and (c)) and is more efficient compared to other state-ofthe-art robust learning algorithms in case (b). The code for GLISTERis at: https://github.com/dssresearch/GLISTER.	With the quest to achieve human-like performance for machine learning and deep learning systems, the cost of training and deploying machine learning models has been significantly increasing. The wasted computational and engineering energy becomes evident in deep learning algorithms, wherein extensive hyper-parameter tuning and network architecture search need to be done. This results in staggering compute costs and running times. As a result, efficient and robust machine learning is a very relevant and substantial research problem. In this paper, we shall focus on three goals: Goal 1: Train machine learning and deep learning models on effective subsets of data, thereby significantly reducing training time and compute while not sacrificing accuracy. Goal 2: To (iteratively) select effective subsets of labeled data to reduce the labeling cost. Goal 3: Select data subsets to remove noisy labels and class imbalance, which is increasingly common in operational machine learning settings. Most prior work discussed above, either study robustness or efficiency, but not both. For example, the data selection approaches such as (Wei, Iyer, and Bilmes 2015; Mirzasoleiman, Bilmes, and Leskovec 2020; Shinohara 2014) and others focus on approximating either gradients or performance on the training sets, and hence would not be suitable for scenarios such as label noise and imbalance. On the other hand, the approaches like (Ren et al. 2018; Jiang et al. 2018) and others, focus on robustness but are not necessarily efficient. For example, the approach of (Ren et al. 2018) requires 3x the standard (deep) training cost, to obtain a robust model. GLISTER is the first framework, to the best of our knowledge, which focuses on both efficiency and robustness. Our work is closely related to the approaches of (Wei, Iyer, and Bilmes 2015) and (Ren et al. 2018). We build upon the work of (Wei, Iyer, and Bilmes 2015), by first generalizing their framework beyond simple classifiers (like nearest neighbor and naive bayes), but with general loss functions. We do this by proposing an iterative algorithm GLISTER-ONLINE which does data selection via a meta-learning based approach along with parameter updates. Furthermore, we pose the problem as optimizing the validation set performance as opposed to training set performance, thereby encouraging generalization. Next, our approach also bears similarity to (Ren et al. 2018), except that we need to solve a discrete optimization problem instead of a meta-gradient update. Moreover, we do not run our data selection every iteration, thereby ensuring that we are significantly faster than a single training run. Finally, we extend our algorithm to the active learning scenario. We demonstrate that our framework is more efficient and accurate compared to existing data selection and active learning algorithms, and secondly, also generalizes well under noisy data, and class imbalance scenarios. In particular, we show that GLISTER achieves a 3x - 6x speedup on a wide range of models and datasets, with very small loss in accuracy.	https://arxiv.org/pdf/1708.00489	Active Learning for Convolutional Neural Networks: A Core-Set Approach	Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (i.e. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.	Deep convolutional neural networks (CNNs) have shown unprecedented success in many areas of research in computer vision and pattern recognition, such as image classification, object detection, and scene segmentation. Although CNNs are universally successful in many tasks, they have a major drawback; they need a very large amount of labeled data to be able to learn their large number of parameters. More importantly, it is almost always better to have more data since the accuracy of CNNs is often not saturated with increasing dataset size. Hence, there is a constant desire to collect more and more data. Although this a desired behavior from an algorithmic perspective (higher representative power is typically better), labeling a dataset is a time consuming and an expensive task. These practical considerations raise a critical question: "what is the optimal way to choose data points to label such that the highest accuracy can be obtained given a fixed labeling budget." Active learning is one of the common paradigms to address this question. The goal of active learning is to find effective ways to choose data points to label, from a pool of unlabeled data points, in order to maximize the accuracy. Although it is not possible to obtain a universally good active learning strategy (Dasgupta, 2004), there exist many heuristics (Settles, 2010) which have been proven to be effective in practice. Active learning is typically an iterative process in which a model is learned at each iteration and a set of points is chosen to be labelled from a pool of unlabelled points using these aforementioned heuristics. We experiment with many of these heuristics in this paper and find them not effective when applied to CNNs. We argue that the main factor behind this ineffectiveness is the correlation caused via batch acquisition/sampling. In the classical setting, the active learning algorithms typically choose a single point at each iteration; however, this is not feasible for CNNs since i) a single point is likely to have no statistically significant impact on the accuracy due to the local optimization methods, and ii) each iteration requires a full training until convergence which makes it intractable to query labels one-by-one. Hence, it is necessary to query labels for a large subset at each iteration and it results in correlated samples even for moderately small subset sizes. In order to tailor an active learning method for the batch sampling case, we decided to define the active learning as core-set selection problem. Core-set selection problem aims to find a small subset given a large labeled dataset such that a model learned over the small subset is competitive over the whole dataset. Since we have no labels available, we perform the core-set selection without using the labels. In order to attack the unlabeled core-set problem for CNNs, we provide a rigorous bound between an average loss over any given subset of the dataset and the remaining data points via the geometry of the data points. As an active learning algorithm, we try to choose a subset such that this bound is minimized. Moreover, minimization of this bound turns out to be equivalent to the k-Center problem (Wolf, 2011) and we adopt an efficient approximate solution to this combinatorial optimization problem. We further study the behavior of our proposed algorithm empirically for the problem of image classification using three different datasets. Our empirical analysis demonstrates state-of-the-art performance by a large margin.	1	1
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2012.10630	GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning	Large scale machine learning and deep models are extremely data-hungry. Unfortunately, obtaining large amounts of labeled data is expensive, and training state-of-the-art models (with hyperparameter tuning) requires significant computing resources and time. Secondly, real-world data is noisy and imbalanced. As a result, several recent papers try to make the training process more efficient and robust. However, most existing work either focuses on robustness or efficiency, but not both. In this work, we introduce GLISTER, a GeneraLIzation based data Subset selecTion for Efficient and Robust learning framework. We formulate GLISTER as a mixed discretecontinuous bi-level optimization problem to select a subset of the training data, which maximizes the log-likelihood on a held-out validation set. We then analyze GLISTER for simple classifiers such as gaussian and multinomial naive-bayes, k-nearest neighbor classifier, and linear regression and show connections to submodularity. Next, we propose an iterative online algorithm GLISTER-ONLINE, which performs data selection iteratively along with the parameter updates and can be applied to any loss-based learning algorithm. We then show that for a rich class of loss functions including cross-entropy, hinge-loss, squared-loss, and logistic-loss, the inner discrete data selection is an instance of (weakly) submodular optimization, and we analyze conditions for which GLISTER-ONLINE reduces the validation loss and converges. Finally, we propose GLISTER-ACTIVE, an extension to batch active learning, and we empirically demonstrate the performance of GLISTER on a wide range of tasks including, (a) data selection to reduce training time, (b) robust learning under label noise and imbalance settings, and (c) batch-active learning with several deep and shallow models. We show that our framework improves upon state of the art both in efficiency and accuracy (in cases (a) and (c)) and is more efficient compared to other state-ofthe-art robust learning algorithms in case (b). The code for GLISTERis at: https://github.com/dssresearch/GLISTER.	With the quest to achieve human-like performance for machine learning and deep learning systems, the cost of training and deploying machine learning models has been significantly increasing. The wasted computational and engineering energy becomes evident in deep learning algorithms, wherein extensive hyper-parameter tuning and network architecture search need to be done. This results in staggering compute costs and running times. As a result, efficient and robust machine learning is a very relevant and substantial research problem. In this paper, we shall focus on three goals: Goal 1: Train machine learning and deep learning models on effective subsets of data, thereby significantly reducing training time and compute while not sacrificing accuracy. Goal 2: To (iteratively) select effective subsets of labeled data to reduce the labeling cost. Goal 3: Select data subsets to remove noisy labels and class imbalance, which is increasingly common in operational machine learning settings. Most prior work discussed above, either study robustness or efficiency, but not both. For example, the data selection approaches such as (Wei, Iyer, and Bilmes 2015; Mirzasoleiman, Bilmes, and Leskovec 2020; Shinohara 2014) and others focus on approximating either gradients or performance on the training sets, and hence would not be suitable for scenarios such as label noise and imbalance. On the other hand, the approaches like (Ren et al. 2018; Jiang et al. 2018) and others, focus on robustness but are not necessarily efficient. For example, the approach of (Ren et al. 2018) requires 3x the standard (deep) training cost, to obtain a robust model. GLISTER is the first framework, to the best of our knowledge, which focuses on both efficiency and robustness. Our work is closely related to the approaches of (Wei, Iyer, and Bilmes 2015) and (Ren et al. 2018). We build upon the work of (Wei, Iyer, and Bilmes 2015), by first generalizing their framework beyond simple classifiers (like nearest neighbor and naive bayes), but with general loss functions. We do this by proposing an iterative algorithm GLISTER-ONLINE which does data selection via a meta-learning based approach along with parameter updates. Furthermore, we pose the problem as optimizing the validation set performance as opposed to training set performance, thereby encouraging generalization. Next, our approach also bears similarity to (Ren et al. 2018), except that we need to solve a discrete optimization problem instead of a meta-gradient update. Moreover, we do not run our data selection every iteration, thereby ensuring that we are significantly faster than a single training run. Finally, we extend our algorithm to the active learning scenario. We demonstrate that our framework is more efficient and accurate compared to existing data selection and active learning algorithms, and secondly, also generalizes well under noisy data, and class imbalance scenarios. In particular, we show that GLISTER achieves a 3x - 6x speedup on a wide range of models and datasets, with very small loss in accuracy.	https://arxiv.org/pdf/2301.13287	MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning	Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results indicate that MILO can train models 3 x -10x faster and tune hyperparameters 20 x -75x faster than full-dataset training or tuning without compromising performance.	Deep learning has achieved remarkable success in a multitude of machine learning tasks, including natural language processing, computer vision, and speech recognition in recent years. This success is partially due to the availability of massive training datasets and the capacity to train large-scale neural networks. However, training deep models on extensive datasets is computationally demanding, incurring significant financial costs and generating substantial CO2 emissions [63, 59 ]. Bhavya et al. [ 2] overview several research trajectories aimed at enhancing model convergence and reducing training time and costs, including data subset selection, curriculum learning, model architecture improvements, and optimization algorithm enhancements. Our work specifically focuses on selecting useful, generalizable data subsets for efficient deep neural network training. Recent research [66 , 5 , 21 ] suggests that many current training datasets are redundant, indicating that a non-redundant, informative subset could achieve comparable performance to training on the full dataset. To identify such informative samples, metrics such as prediction uncertainty [ 7], prediction flips [ 66 , 72 ], loss [42 ], or gradient/gradient-norm [ 21, 47 , 28, 27 , 29 ] have been applied. These metrics, calculated using either the downstream machine learning model or a lightweight surrogate [ 7], require a fully converged model, a requirement that runs counter to the goal of efficient training. To address this, existing subset selection algorithms [ 47 , 28 , 27, 29, 30 , 54 ] for efficient learning utilize the downstream model during training for heuristic computation and periodically update the subset as the model-dependent metrics for each sample evolve. Drawbacks of Model-Dependent Subset Selection: Despite the theoretical advantages of existing subset selection strategies[ 28 , 27, 47 ], they often fall short in computational efficiency compared to adaptive random subset selection, which selects random subsets periodically. This is primarily because traditional approaches rely on downstream models and typically require the calculation of sample metrics, such as gradients, before each subset selection step. Furthermore, these computationally demanding subset selection steps occur during model training. For instance, Figure 1 compares the convergence rate of the ResNet18 model on the CIFAR100 dataset, in terms of both time and epochs. Here, the model uses 10% subsets chosen every epoch by GradMatchPB [ 27 ], a state-of-the-art data subset selection strategy for efficient training, CraigPB [ 47 ], and Adaptive-Random (where a new 10% subset is randomly selected periodically). The selection of a new subset every epoch is done to demonstrate the maximum performance attainable by GradMatchPB and CraigPB. The results indicate that GradMatchPB achieves faster epoch convergence than both Adaptive-Random and CraigPB when selecting a new subset each epoch. However, due to the necessity for a computationally intensive subset selection step every epoch, both GradMatchPB and CraigPB demonstrate considerable inefficiency in terms of training time. In their respective studies, Killamsetty et al. [27 ] and Mirzasoleiman et al. [47] suggested selecting a new subset every R epochs to enhance training efficiency. However, this comes at the cost of the model’s convergence rate. Lastly, model-dependent subset selection requires a computationally intensive subset selection steps each time a new model is trained. Model-Agnostic Selection: Our primary insight is that a model-agnostic subset selection framework can circumvent the computationally intensive subset selection steps during model training by conducting subset selection in the preprocessing phase. By pre-selecting subsets and storing them as metadata with each dataset, we can train numerous models without incurring additional costs, effectively spreading out the expense of subset selection. In this work, we endeavor to answer the following question: Is it possible to develop a model-agnostic subset selection method that selects new subsets in a minimal amount of time, yet achieves superior model convergence without significant compromise in test accuracy or generalization performance?  MILO Framework: In this study, we introduce MILO, a model-agnostic subset selection framework designed for efficient model training and tuning. MILO employs submodular measures [ 13 , 23 ], which capture higher-order interactions between data samples for subset selection. We utilize pre-trained large language models [ 55 ] and pre-trained vision transformers [25 ] as feature encoders due to their zero-shot feature encoding capabilities. These encoders compute the sample metrics in a nominal amount of time, and this computation is independent of the downstream model, rendering MILO model-agnostic. Figure 3 provides a visual representation of MILO for model training, comprising two steps: a) A pre-processing step that involves the selection of multiple subsets from the training dataset using "Stochastic-Greedy Exploration (SGE)" (refer to Section 3.1.1), and the construction of a probability distribution over the entire dataset for subset sampling through "Weighted Random Exploration (WRE)" (refer to Section 3.1.2); b) A model training scheme that involves training the model on a curriculum of easy-to-hard subsets (refer to Section 3.1.3) using the subsets selected and the probability distribution constructed in the pre-processing step. We also present a class-wise partitioning trick to significantly minimize the memory footprint of MILO (refer to Section 3.2). Effectiveness of MILO : Through extensive experiments on multiple real-world datasets, we empirically demonstrate the effectiveness of the MILO framework for efficient training and hyperparameter tuning. In Figure 2, we provide a summary of the speedup achieved by MILO compared to full data training and tuning, along with the corresponding relative performance. Our results show that MILO can train models 3x to 10x faster and tune hyperparameters 20x to 75x faster, with minimal loss in performance. Furthermore, we consistently outperform the subset selection baselines that were considered. Additionally, MILO exhibits significantly faster initial model convergence and maintains faster convergence throughout training compared to other baselines. These qualities make it highly effective for applications such as hyperparameter tuning and neural architecture search, where the ability to distinguish good models from bad models early in the tuning process is crucial for enhancing efficiency [38, 37]. Related Work: In recent years, data-subset selection strategies have achieved success in various machine learning applications such as speech recognition [ 68 , 67], machine translation [ 32 ], active learning [ 60 , 1 , 33 ], hyper-parameter tuning [ 30 ], continual learning [65 ], domain adaptation [20 ], and computer vision [22]. A recent empirical study [5] demonstrated that datasets frequently contain semantic redundancies. These redundancies can be removed during training without affecting performance. In response, data pruning methods [66 , 51, 62] have developed principled selection criteria for pruning redundant samples before model training, with minimal loss in performance. However, both existing data pruning and compute-efficient learning approaches are model-dependent, resulting in them being computationally expensive. In contrast, our method is model-agnostic and capable of identifying high-quality subsets by intelligent data exploration. Furthermore, our approach can achieve superior model convergence by utilizing an easy-to-hard curriculum, transitioning from representative to diverse subsets during training.	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2012.10630	GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning	Large scale machine learning and deep models are extremely data-hungry. Unfortunately, obtaining large amounts of labeled data is expensive, and training state-of-the-art models (with hyperparameter tuning) requires significant computing resources and time. Secondly, real-world data is noisy and imbalanced. As a result, several recent papers try to make the training process more efficient and robust. However, most existing work either focuses on robustness or efficiency, but not both. In this work, we introduce GLISTER, a GeneraLIzation based data Subset selecTion for Efficient and Robust learning framework. We formulate GLISTER as a mixed discretecontinuous bi-level optimization problem to select a subset of the training data, which maximizes the log-likelihood on a held-out validation set. We then analyze GLISTER for simple classifiers such as gaussian and multinomial naive-bayes, k-nearest neighbor classifier, and linear regression and show connections to submodularity. Next, we propose an iterative online algorithm GLISTER-ONLINE, which performs data selection iteratively along with the parameter updates and can be applied to any loss-based learning algorithm. We then show that for a rich class of loss functions including cross-entropy, hinge-loss, squared-loss, and logistic-loss, the inner discrete data selection is an instance of (weakly) submodular optimization, and we analyze conditions for which GLISTER-ONLINE reduces the validation loss and converges. Finally, we propose GLISTER-ACTIVE, an extension to batch active learning, and we empirically demonstrate the performance of GLISTER on a wide range of tasks including, (a) data selection to reduce training time, (b) robust learning under label noise and imbalance settings, and (c) batch-active learning with several deep and shallow models. We show that our framework improves upon state of the art both in efficiency and accuracy (in cases (a) and (c)) and is more efficient compared to other state-ofthe-art robust learning algorithms in case (b). The code for GLISTERis at: https://github.com/dssresearch/GLISTER.	With the quest to achieve human-like performance for machine learning and deep learning systems, the cost of training and deploying machine learning models has been significantly increasing. The wasted computational and engineering energy becomes evident in deep learning algorithms, wherein extensive hyper-parameter tuning and network architecture search need to be done. This results in staggering compute costs and running times. As a result, efficient and robust machine learning is a very relevant and substantial research problem. In this paper, we shall focus on three goals: Goal 1: Train machine learning and deep learning models on effective subsets of data, thereby significantly reducing training time and compute while not sacrificing accuracy. Goal 2: To (iteratively) select effective subsets of labeled data to reduce the labeling cost. Goal 3: Select data subsets to remove noisy labels and class imbalance, which is increasingly common in operational machine learning settings. Most prior work discussed above, either study robustness or efficiency, but not both. For example, the data selection approaches such as (Wei, Iyer, and Bilmes 2015; Mirzasoleiman, Bilmes, and Leskovec 2020; Shinohara 2014) and others focus on approximating either gradients or performance on the training sets, and hence would not be suitable for scenarios such as label noise and imbalance. On the other hand, the approaches like (Ren et al. 2018; Jiang et al. 2018) and others, focus on robustness but are not necessarily efficient. For example, the approach of (Ren et al. 2018) requires 3x the standard (deep) training cost, to obtain a robust model. GLISTER is the first framework, to the best of our knowledge, which focuses on both efficiency and robustness. Our work is closely related to the approaches of (Wei, Iyer, and Bilmes 2015) and (Ren et al. 2018). We build upon the work of (Wei, Iyer, and Bilmes 2015), by first generalizing their framework beyond simple classifiers (like nearest neighbor and naive bayes), but with general loss functions. We do this by proposing an iterative algorithm GLISTER-ONLINE which does data selection via a meta-learning based approach along with parameter updates. Furthermore, we pose the problem as optimizing the validation set performance as opposed to training set performance, thereby encouraging generalization. Next, our approach also bears similarity to (Ren et al. 2018), except that we need to solve a discrete optimization problem instead of a meta-gradient update. Moreover, we do not run our data selection every iteration, thereby ensuring that we are significantly faster than a single training run. Finally, we extend our algorithm to the active learning scenario. We demonstrate that our framework is more efficient and accurate compared to existing data selection and active learning algorithms, and secondly, also generalizes well under noisy data, and class imbalance scenarios. In particular, we show that GLISTER achieves a 3x - 6x speedup on a wide range of models and datasets, with very small loss in accuracy.	https://arxiv.org/pdf/2103.00128	PRISM: A Rich Class of Parameterized Submodular Information Measures for Guided Subset Selection	With ever-increasing dataset sizes, subset selection techniques are becoming increasingly important for a plethora of tasks. It is often necessary to guide the subset selection to achieve certain desiderata, which includes focusing or targeting certain data points, while avoiding others. Examples of such problems include: i) targeted learning, where the goal is to find subsets with rare classes or rare attributes on which the model is underperforming, and ii) guided summarization, where data (e.g., image collection, text, document or video) is summarized for quicker human consumption with specific additional user intent. Motivated by such applications, we present PRISM, a rich class of PaRameterIzed Submodular information Measures. Through novel functions and their parameterizations, PRISM offers a variety of modeling capabilities that enable a trade-off between desired qualities of a subset like diversity or representation and similarity/dissimilarity with a set of data points. We demonstrate how PRISM can be applied to the two real-world problems mentioned above, which require guided subset selection. In doing so, we show that PRISM interestingly generalizes some past work, therein reinforcing its broad utility. Through extensive experiments on diverse datasets, we demonstrate the superiority of PRISM over the state-of-the-art in targeted learning and in guided imagecollection summarization. PRISM is available as a part of the SUBMODLIB (https://github.com/decile-team/submodlib) and TRUST (https://github.com/decile-team/trust) toolkits.	Recent times have seen explosive growth in data across several modalities, including text, images, and videos. This has given rise to the need for finding techniques for selecting effective smaller data subsets with specific characteristics for a variety of down-stream tasks. Often, we would like to guide the data selection to either target or avoid a certain set of data slices. One application is, what we call, targeted learning, where the goal is to select data points similar to data slices on which the model is currently performing poorly. These slices are data points that either belong to rare classes or have common rare attributes (e.g., color, background, etc.). An example of such a scenario is shown in Fig. 1(a), where a self-driving car model struggles in detecting “cars in a dark background“ because of a lack of such images in the training set. The targeted learning problem is to augment the training dataset with more of such rare images, with an aim to improve model performance. Another example is detecting cancers in biomedical imaging datasets, where the number of cancerous images are often a small fraction of the non-cancerous images. Another application comes from the summarization task, where an image collection, a video, or a text document is summarized for quicker human consumption by eliminating redundancy, while preserving the main content. While a number of applications require generic summarization (i.e., simply picking a representative and diverse subset of the massive dataset), it is often important to capture certain user intent in summarization. We call this guided summarization. Examples of guided summarization include: (i) query-focused summarization (Sharghi, Gong, and Shah 2016; Xiao et al. 2020), where a summary similar to a specific query is desired, and (ii) privacy-preserving summarization, where a summary dissimilar to a given private set of data points is desired (say, for privacy issues). See Fig. 1(b) for a pictorial illustration. PRISM Framework: We define PRISM through different instantiations and parameterizations of various submodular information measures (Sec. 2). These allow for modeling a spectrum of semantics required for guided subset selection, like relevance to a query set, irrelevance to a private set, and diversity among selected data points. We study the effect of parameter trade-off among these different semantics and present interesting insights. PRISM for Targeted Learning: We present a novel algorithm (Sec. 3.1, Algo. 1) to apply PRISM for targeted learning, which aims to improve a model’s performance on rare slices of data. Specifically, we show that submodular information measures are very effective in finding the examples from the rare classes in a large unlabeled set (akin to finding a needle in a haystack). On several image classification tasks, PRISM obtains ≈ 20-30% gain in accuracy of rare classes (≈ 12% more than existing approaches) by just adding a few additional labeled points from the rare classes. Furthermore, we show that PRISM is 20× to 50× more label-efficient compared to random sampling, and 2× to 4× more labelefficient compared to existing approaches (see Sec. 4.1). We also show that Algo. 1 generalizes some existing approaches for data subset selection, reinforcing its utility (Sec. 3.3). PRISM for Guided Summarization. We propose a learning framework for guided summarization using PRISM (Sec. 3.2). We show that PRISM offers a unified treatment to the different flavors of guided summarization (query-focused and privacy-preserving) and generalizes some existing approaches to summarization, again reinforcing its utility. We show that it outperforms other existing approaches on a real-world image collections dataset (Sec. 4.2).	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2012.10630	GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning	Large scale machine learning and deep models are extremely data-hungry. Unfortunately, obtaining large amounts of labeled data is expensive, and training state-of-the-art models (with hyperparameter tuning) requires significant computing resources and time. Secondly, real-world data is noisy and imbalanced. As a result, several recent papers try to make the training process more efficient and robust. However, most existing work either focuses on robustness or efficiency, but not both. In this work, we introduce GLISTER, a GeneraLIzation based data Subset selecTion for Efficient and Robust learning framework. We formulate GLISTER as a mixed discretecontinuous bi-level optimization problem to select a subset of the training data, which maximizes the log-likelihood on a held-out validation set. We then analyze GLISTER for simple classifiers such as gaussian and multinomial naive-bayes, k-nearest neighbor classifier, and linear regression and show connections to submodularity. Next, we propose an iterative online algorithm GLISTER-ONLINE, which performs data selection iteratively along with the parameter updates and can be applied to any loss-based learning algorithm. We then show that for a rich class of loss functions including cross-entropy, hinge-loss, squared-loss, and logistic-loss, the inner discrete data selection is an instance of (weakly) submodular optimization, and we analyze conditions for which GLISTER-ONLINE reduces the validation loss and converges. Finally, we propose GLISTER-ACTIVE, an extension to batch active learning, and we empirically demonstrate the performance of GLISTER on a wide range of tasks including, (a) data selection to reduce training time, (b) robust learning under label noise and imbalance settings, and (c) batch-active learning with several deep and shallow models. We show that our framework improves upon state of the art both in efficiency and accuracy (in cases (a) and (c)) and is more efficient compared to other state-ofthe-art robust learning algorithms in case (b). The code for GLISTERis at:https://github.com/dssresearch/GLISTER.	With the quest to achieve human-like performance for machine learning and deep learning systems, the cost of training and deploying machine learning models has been significantly increasing. The wasted computational and engineering energy becomes evident in deep learning algorithms, wherein extensive hyper-parameter tuning and network architecture search need to be done. This results in staggering compute costs and running times. As a result, efficient and robust machine learning is a very relevant and substantial research problem. In this paper, we shall focus on three goals: Goal 1: Train machine learning and deep learning models on effective subsets of data, thereby significantly reducing training time and compute while not sacrificing accuracy. Goal 2: To (iteratively) select effective subsets of labeled data to reduce the labeling cost. Goal 3: Select data subsets to remove noisy labels and class imbalance, which is increasingly common in operational machine learning settings. Most prior work discussed above, either study robustness or efficiency, but not both. For example, the data selection approaches such as (Wei, Iyer, and Bilmes 2015; Mirzasoleiman, Bilmes, and Leskovec 2020; Shinohara 2014) and others focus on approximating either gradients or performance on the training sets, and hence would not be suitable for scenarios such as label noise and imbalance. On the other hand, the approaches like (Ren et al. 2018; Jiang et al. 2018) and others, focus on robustness but are not necessarily efficient. For example, the approach of (Ren et al. 2018) requires 3x the standard (deep) training cost, to obtain a robust model. GLISTER is the first framework, to the best of our knowledge, which focuses on both efficiency and robustness. Our work is closely related to the approaches of (Wei, Iyer, and Bilmes 2015) and (Ren et al. 2018). We build upon the work of (Wei, Iyer, and Bilmes 2015), by first generalizing their framework beyond simple classifiers (like nearest neighbor and naive bayes), but with general loss functions. We do this by proposing an iterative algorithm GLISTER-ONLINE which does data selection via a meta-learning based approach along with parameter updates. Furthermore, we pose the problem as optimizing the validation set performance as opposed to training set performance, thereby encouraging generalization. Next, our approach also bears similarity to (Ren et al. 2018), except that we need to solve a discrete optimization problem instead of a meta-gradient update. Moreover, we do not run our data selection every iteration, thereby ensuring that we are significantly faster than a single training run. Finally, we extend our algorithm to the active learning scenario. We demonstrate that our framework is more efficient and accurate compared to existing data selection and active learning algorithms, and secondly, also generalizes well under noisy data, and class imbalance scenarios. In particular, we show that GLISTER achieves a 3x - 6x speedup on a wide range of models and datasets, with very small loss in accuracy.	https://aclanthology.org/2024.findings-naacl.209.pdf	An End-to-End Submodular Framework for Data-Efficient In-Context Learning	Recent advancements in natural language tasks leverage the emergent In-Context Learning (ICL) ability of pretrained Large Language Models (LLMs). ICL enables LLMs to perform new tasks by utilizing a limited number of input-output examples as prompts. While ICL circumvents the costly step of finetuning LLMs, its effectiveness is heavily dependent on the quality and ordering of provided examples (called exemplars). In this work, we propose a two-stage data-efficient framework Div-S3 for exemplar selection for ICL. The first stage focuses on data annotation and employs a poolbased active learning approach to select a set of Diverse and informative exemplars from the target tasks’ unlabeled pool. Given a test input/query, the second stage uses Submodular Span Summarization (S3) to select the most relevant and non-redundant exemplars from the annotated pool of a limited budget. On 7 different NLP datasets and 5 LLMs of varying complexities, we show Div-S3 outperforms (1) existing active learning-based methods for data annotation for ICL and (2) similarity-based methods for test query-specific exemplars retrieval.	Pretrained large language models (LLMs) (Kenton and Toutanova, 2019; Brown et al., 2020; Chowdhery et al., 2022) have become foundational for a wide range of Natural Language Processing (NLP) tasks, demonstrating impressive success across various domains (Bommasani et al., 2021; Bubeck et al., 2023) through in-context learning (ICL) (Dong et al., 2022). ICL enables these pretrained LLMs to perform new tasks by using task-specific prompts containing a limited number of input-output demonstrations (also referred to as shots, exemplars, or prompts) in the natural language format. This approach facilitates deployment across different downstream tasks and reduces the need for labeled downstream training data since ICL does not require any task-specific training. The typical ICL procedure consists of two key components: (1) Exemplar annotation and retrieval (Wu et al., 2022; Köksal et al., 2022; Liu et al., 2022): This step involves annotating and retrieving exemplars that serve as context demonstrations. (2) Prompt template crafting (Sorensen et al., 2022; Deng et al., 2022): this step involves designing a prompt template to wrap these demonstrations in a comprehensible and coherent natural language instruction. Recent studies (Liu et al., 2022; Su et al., 2022; Margatina et al., 2023) show that providing exemplars most relevant to the current input instance is beneficial. Moreover, Zhao et al. (2021), Lu et al. (2022), and Liu et al. (2023) observe that LLMs attend more to the exemplars that are closer in the sequence to the input instance. Therefore, to achieve the best performance of ICL, the selection of exemplars and their ordering in the LLM prompt are crucial. In practice, an extensive collection of unlabeled exemplars is easily available (e.g., posts and discussions on forums like Stack Exchange or user-generated content on social media platforms), but manually annotating all exemplars would be exceptionally costly. To annotate and select the exemplars optimally for a given target task, we follow the two-stage approach shown in Figure 1: (1) Exemplar Annotation: select a subset of exemplars for annotation under a fixed budget (performed only once) and (2) Exemplar Retrieval: identify limited-sized exemplars in an ordering that are most influential for a given input instance from the annotated subset of exemplars. Intuitively, for the first stage, we aim to find the subset with maximal diversity and least redundancy so that, given any input, we can find corresponding labeled exemplars. For the second stage, in addition to the diversity requirement similar to the first stage, we emphasize the relevance of the exemplars to the given input query and order exemplars so that their relevance to the input query decreases as the exemplars are farther away from the input instance. We propose a framework Div-S3 based on submodular optimization that unifies the abovementioned two stages. For Exemplar Annotation, we model the problem as a submodular optimization problem under a cardinality constraint to find as Diverse a subset as possible within a budget. For Exemplar Retrieval, we formalize the problem as a Submodular Span Summarization (S3) problem (Kumari and Bilmes, 2021) with a knapsack constraint, which finds a diverse subset most relevant to the input query under a token length limit. Also, we naturally order the resulting exemplars based on the gains represented by the submodular function. The name of our proposed framework Div-S3 captures the optimization objectives used for both exemplar annotation (Div) and exemplar retrieval (S3) stages. In Fig. 2, we show a sample test query where using Div-S3 for exemplar selection leads to a more diverse and query-relevant exemplar set (more examples provided in Appendix D). Our framework is general, as any submodular function can be plugged into our method. For models beyond LMs, e.g., for text-image multi-modality models, we may use pre-existing submodular functions that are powerful for expressing diversity in the image domain. In addition, we account for relevance, diversity, and ordering for the exemplar retrieval stage, where one or two aspects typically get overlooked by previous methods. Empirically, we evaluate Div-S3 on 7 NLP tasks with 5 LLMs and show significantly improved performance compared to baselines. Our contributions are: 1. We propose an end-to-end framework DivS3 utilizing submodular optimization for performing data-efficient ICL using LLMs. Depending on budget requirements, Div-S3 provides the flexibility to set the budget either in terms of the number of exemplars to be used in the prompt or the LLM’s context window size. 2. We empirically validate the effectiveness of our framework on 7 different NLP tasks and show the transferability of results across LLMs of varying complexities. 3. We thoroughly analyze each component of Div-S3 by (a) studying S3 in a setting with no annotation budget constraint and (b) analyzing the sensitivity of the exemplars selected by Div-S3 to their position in the LLM’s prompt.	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2410.11303	TSDS: Data Selection for Task-Specific Model Finetuning	Finetuning foundation models for specific tasks is an emerging paradigm in modern machine learning. The efficacy of task-specific finetuning largely depends on the selection of appropriate training data. We present TSDS (Task-Specific Data Selection), a framework to select data for task-specific model finetuning, guided by a small but representative set of examples from the target task. To do so, we formulate data selection for task-specific finetuning as an optimization problem with a distribution alignment loss based on optimal transport to capture the discrepancy between the selected data and the target distribution. In addition, we add a regularizer to encourage the diversity of the selected data and incorporate kernel density estimation into the regularizer to reduce the negative effects of near-duplicates among the candidate data. We connect our optimization problem to nearest neighbor search and design efficient algorithms to compute the optimal solution based on approximate nearest neighbor search techniques. We evaluate our method on data selection for both continued pretraining and instruction tuning of language models. We show that instruction tuning using data selected by our method with a 1% selection ratio often outperforms using the full dataset and beats the baseline selection methods by 1.5 points in F1 score on average. Our code is available at https://github.com/ZifanL/TSDS.	inetuning foundation models [3 ] is the de-facto paradigm for building machine learning applications that focus on specific tasks. Models such as BERT [10] and LLaMA [ 43] are large-scale models pretrained on massive unlabeled data across a wide range of domains. Those models can be specialized to downstream tasks through finetuning. Finetuning can take a variety of forms depending on the target task. For instance, continued pretraining [ 17 ] extends the pretraining stage of a model on a dataset that is more closely related to a target domain. As another setting, instruction tuning [51 ] trains a generative foundation model on instruction-response pairs to improve its performance in responding to task-specific instructions. Finetuning foundation models can lead to significant improvement in downstream tasks, but the effectiveness heavily relies on the right choice of training data [ 17 , 30 , 48 , 47 ]. However, the data repositories that one considers during training of generative models tend to be large—consider for example the use of Common Crawl, which contains 250 billion web pages, or The Pile [14 ]—and hence, it is impractical to manually select the data that are distributed like the use cases in the target task. Therefore, automated task-specific data selection becomes critical. In this paper, we propose TSDS (Task-Specific Data Selection), a framework to select data for taskspecific model finetuning. We consider the scenario of finetuning a foundation model to customize it for a specific task characterized by a few representative examples. The input to our framework is the representative examples and a massive repository of candidate data. Guided by the representative examples, we select training data from the repository for task-specific finetuning. We identify the following requirements for our framework. (Distribution Alignment) First, the distribution of the selected data should match the distribution of the representative examples from the target task. Distribution alignment is essential for a model to learn the target distribution and enable data-efficient finetuning for the target task [ 40 ]. Many works [38 , 17, 2, 50 , 47 ] retrieve candidate examples that are most similar to the representative examples. Such heuristics do not ensure distribution alignment between the selected data and the representative examples. A recent work [ 48 ] selects data by importance resampling to match the target distribution but is limited to an n-gram feature space, which cannot capture high-level semantics. (Diversity) Second, the selected data should be diverse so that the model can learn a wide range of related knowledge rather than overfitting to specific examples. In practice, data repositories created by web crawling may contain a large portion of near-duplicates [13 , 28 ] that can compromise diversity and negatively impact model performance [ 28, 19 ]. For example, a study [ 13 ] on several snapshots of ClueWeb and Common Crawl shows that 14% to 52% of the documents are near-duplicates. Previous works [38, 17 , 2 , 50 , 48 , 47 ] on task-specific data selection overlook near-duplicates, leading to the over-representation of such examples in the selected data. We require our framework to ensure diversity in selection even when a large fraction of the candidate examples are near-duplicates. (Scalability) Finally, the selection algorithm should be efficient, considering the increasing scale of modern data repositories. The high volume of candidate data (e.g., 250 billion pages in Common Crawl) poses a great challenge to efficient selection. Our framework formulates task-specific data selection as an optimization problem that allows a smooth trade-off between two crucial objectives: distribution alignment and diversity. The solution to the optimization problem is a categorical distribution assigned to the candidates which we will sample from. In the optimization objective, we use optimal transport to measure the discrepancy between the distribution assigned to the candidates and the target distribution, encouraging the alignment between them. We show that the optimization problem admits efficient algorithms to compute the optimal solution. In addition, our framework supports distribution alignment in any metric space that supports efficient nearest-neighbor search, including model-agnostic semantic embedding and model-specific features such as gradients. Our contributions: 1) We formulate data selection for task-specific finetuning as an optimization problem based on optimal transport for distribution alignment, with a regularization term that encourages diversity. 2) We make our framework robust to near-duplicates by incorporating kernel density estimation [36 ] into the regularization term. 3) We show the connection between the optimal solution to the optimization problem and nearest neighbor search, which allows us to develop efficient algorithms employing approximate nearest-neighbor search techniques [23]. We conduct extensive experiments to validate the effectiveness of our framework. We focus on natural language processing tasks where foundation models have shown great advancements. We show that our framework beats the state-of-the-art baseline [47 ] by 1.5 points in F1 score on average with a selection ratio of 1% on instruction tuning for two modern large language models on three tasks. In addition, continued pretraining using domain-specific data selected by our framework outperforms the other selection methods by up to 3 F1 points on four classification tasks from various domains. We also demonstrate that our framework is robust to near-duplicates in the data repository, maintaining consistent performance when 1% of the candidate examples are duplicate for up to 1,000 times. Our method is efficient, taking 28 hours to preprocess a corpus of 150M examples and less than 1 hour for each task-specific selection.	https://arxiv.org/pdf/2501.04155	MM-GEN: Enhancing Task Performance Through Targeted Multimodal Data Curation	Vision-language models (VLMs) are highly effective but often underperform on specialized tasks, for example Llava1.5 struggles on chart and diagram understanding, due to scarce task-specific training data. Existing training data, sourced from general-purpose datasets, fails to capture the nuanced details needed for these tasks. We introduce MMGEN, a scalable method that generates task-specific, highquality synthetic text for candidate images by leveraging stronger models. MM-GEN employs a three-stage targeted process: partitioning data into subgroups, generating targeted text based on task descriptions, and filtering out redundant and outlier data. Fine-tuning VLMs with data generated by MM-GEN leads to significant performance gains, including 29% on spatial reasoning and 15% on diagram understanding for Llava-1.5 (7B). Compared to human-curated caption data, MM-GEN achieves up to 1.6× better improvements for the original models, proving its effectiveness in enhancing task-specific VLM performance and bridging the gap between general-purpose datasets and specialized requirements. Code available at https://github.com/sjoshi804/MM-Gen.	While vision-language models (VLMs) demonstrate stateof-the-art performance on several multi-modal tasks [29], they are often on tasks that are simpler in nature [3]. These models still struggle with more complex tasks, e.g., those that require fine-grained understanding of details in images [3, 15, 21, 49]. We posit that the primary limitation for these VLMs is the quality and nature of the training data. VLMs are typically trained on large-scale image-text data scraped from the internet; while the images are rich and informative, the associated text descriptions can (i) have limited relevance to the image [39], or (ii) omit references to several specific details captured in the image [25]. Fig. 1 shows examples of such images and web-scraped captions. While the images are relevant for the tasks of chart understanding, spatial reasoning, and diagram understanding, respectively, the text fails to capture details essential for these tasks. While synthetic caption generation strategies proposed in prior work [25, 39, 57] can create more descriptive text annotations (by referring to more visual details), they are agnostic of the downstream target task. Consequently, they cannot ensure that relevant details are captured in the text annotations. Recently, Shi et al. [48] manually curated a task-specific dataset aimed at the task of multimodal mathematical question-answering by augmenting existing imagetext data with detailed textual annotations, based on their domain expertise, using strong VLMs. While effective, such a curation pipeline involves significant human effort and is not scalable [33, 59]. To address these limitations, we present MM-GEN, a highly general framework for automatically synthesizing taskrelevant text annotations for images by leveraging stronger VLMs (i.e., VLMs that perform well on the specific task) and requiring minimal human effort. MM-GEN takes as input a small set of examples from the target task (henceforth referred to as "reference samples"), a list of image types associated with the task, and a pool of task-relevant candidate images for training. In practice, these inputs can be easily obtained: a small number of reference samples and associated image types can be directly collected from the target task, and a task-relevant image pool can be found via image search with search engines or retrieved from largescale image-caption datasets [5, 46, 47]. Using the reference samples to specify the details of the task to the stronger VLM, MM-GEN generates text-annotations that are taskrelevant for the candidate images. Fig. 1 shows how the text generated by MM-GEN captures task-relevant details. This simple approach is highly effective, resulting in significant improvements across a variety of target tasks. Moreover, human-effort in this process is limited to providing (i) a small set of reference samples for the task, and (ii) a pool of candidate images. To further improve the quality and efficacy of the generated data, MM-GEN introduces a perplexity [4] based data-filtering approach to improve performance on target task using a high-value subset of the synthesized data. The components in MM-GEN are general and applicable to any image-text based target task enabling it to easily generalize across tasks and scale. We evaluate MM-GEN on improving VLMs' (e.g., Llava1.5 7B and 13B parameter versions) performance on finegrained image understanding tasks — chart understanding and reasoning, diagram understanding, and spatial reasoning on maps. The data curated by MM-GEN enables an absolute improvement over Llava-1.5 (7B) of 15%, 14% and 29%, respectively. We also see improvements over the much larger Llava-1.5 (13B) of 13%, , respectively. Moreover, MM-GEN’s filtering strategy helps in shrinking data volumes by up to 50% with no / minimal loss in performance. Empirical results show that models trained via MM-GEN data have a better performance than those trained via generated generic captions. MM-GEN data is also more effective than text annotations generated without task-specific reference examples, showing the importance of a targeted, data-centric approach for describing tasks. Finally, we analyze the effects of key design choices in MMGEN through ablation studies on e.g. size of the reference sample set, generating with / without partitioning, scaling number of in-context samples.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2410.11303	TSDS: Data Selection for Task-Specific Model Finetuning	Finetuning foundation models for specific tasks is an emerging paradigm in modern machine learning. The efficacy of task-specific finetuning largely depends on the selection of appropriate training data. We present TSDS (Task-Specific Data Selection), a framework to select data for task-specific model finetuning, guided by a small but representative set of examples from the target task. To do so, we formulate data selection for task-specific finetuning as an optimization problem with a distribution alignment loss based on optimal transport to capture the discrepancy between the selected data and the target distribution. In addition, we add a regularizer to encourage the diversity of the selected data and incorporate kernel density estimation into the regularizer to reduce the negative effects of near-duplicates among the candidate data. We connect our optimization problem to nearest neighbor search and design efficient algorithms to compute the optimal solution based on approximate nearest neighbor search techniques. We evaluate our method on data selection for both continued pretraining and instruction tuning of language models. We show that instruction tuning using data selected by our method with a 1% selection ratio often outperforms using the full dataset and beats the baseline selection methods by 1.5 points in F1 score on average. Our code is available at https://github.com/ZifanL/TSDS.	inetuning foundation models [3 ] is the de-facto paradigm for building machine learning applications that focus on specific tasks. Models such as BERT [10] and LLaMA [ 43] are large-scale models pretrained on massive unlabeled data across a wide range of domains. Those models can be specialized to downstream tasks through finetuning. Finetuning can take a variety of forms depending on the target task. For instance, continued pretraining [ 17 ] extends the pretraining stage of a model on a dataset that is more closely related to a target domain. As another setting, instruction tuning [51 ] trains a generative foundation model on instruction-response pairs to improve its performance in responding to task-specific instructions. Finetuning foundation models can lead to significant improvement in downstream tasks, but the effectiveness heavily relies on the right choice of training data [ 17 , 30 , 48 , 47 ]. However, the data repositories that one considers during training of generative models tend to be large—consider for example the use of Common Crawl, which contains 250 billion web pages, or The Pile [14 ]—and hence, it is impractical to manually select the data that are distributed like the use cases in the target task. Therefore, automated task-specific data selection becomes critical. In this paper, we propose TSDS (Task-Specific Data Selection), a framework to select data for taskspecific model finetuning. We consider the scenario of finetuning a foundation model to customize it for a specific task characterized by a few representative examples. The input to our framework is the representative examples and a massive repository of candidate data. Guided by the representative examples, we select training data from the repository for task-specific finetuning. We identify the following requirements for our framework. (Distribution Alignment) First, the distribution of the selected data should match the distribution of the representative examples from the target task. Distribution alignment is essential for a model to learn the target distribution and enable data-efficient finetuning for the target task [ 40 ]. Many works [38 , 17, 2, 50 , 47 ] retrieve candidate examples that are most similar to the representative examples. Such heuristics do not ensure distribution alignment between the selected data and the representative examples. A recent work [ 48 ] selects data by importance resampling to match the target distribution but is limited to an n-gram feature space, which cannot capture high-level semantics. (Diversity) Second, the selected data should be diverse so that the model can learn a wide range of related knowledge rather than overfitting to specific examples. In practice, data repositories created by web crawling may contain a large portion of near-duplicates [13 , 28 ] that can compromise diversity and negatively impact model performance [ 28, 19 ]. For example, a study [ 13 ] on several snapshots of ClueWeb and Common Crawl shows that 14% to 52% of the documents are near-duplicates. Previous works [38, 17 , 2 , 50 , 48 , 47 ] on task-specific data selection overlook near-duplicates, leading to the over-representation of such examples in the selected data. We require our framework to ensure diversity in selection even when a large fraction of the candidate examples are near-duplicates. (Scalability) Finally, the selection algorithm should be efficient, considering the increasing scale of modern data repositories. The high volume of candidate data (e.g., 250 billion pages in Common Crawl) poses a great challenge to efficient selection. Our framework formulates task-specific data selection as an optimization problem that allows a smooth trade-off between two crucial objectives: distribution alignment and diversity. The solution to the optimization problem is a categorical distribution assigned to the candidates which we will sample from. In the optimization objective, we use optimal transport to measure the discrepancy between the distribution assigned to the candidates and the target distribution, encouraging the alignment between them. We show that the optimization problem admits efficient algorithms to compute the optimal solution. In addition, our framework supports distribution alignment in any metric space that supports efficient nearest-neighbor search, including model-agnostic semantic embedding and model-specific features such as gradients. Our contributions: 1) We formulate data selection for task-specific finetuning as an optimization problem based on optimal transport for distribution alignment, with a regularization term that encourages diversity. 2) We make our framework robust to near-duplicates by incorporating kernel density estimation [36 ] into the regularization term. 3) We show the connection between the optimal solution to the optimization problem and nearest neighbor search, which allows us to develop efficient algorithms employing approximate nearest-neighbor search techniques [23]. We conduct extensive experiments to validate the effectiveness of our framework. We focus on natural language processing tasks where foundation models have shown great advancements. We show that our framework beats the state-of-the-art baseline [47 ] by 1.5 points in F1 score on average with a selection ratio of 1% on instruction tuning for two modern large language models on three tasks. In addition, continued pretraining using domain-specific data selected by our framework outperforms the other selection methods by up to 3 F1 points on four classification tasks from various domains. We also demonstrate that our framework is robust to near-duplicates in the data repository, maintaining consistent performance when 1% of the candidate examples are duplicate for up to 1,000 times. Our method is efficient, taking 28 hours to preprocess a corpus of 150M examples and less than 1 hour for each task-specific selection.	https://arxiv.org/pdf/1708.00489	Active Learning for Convolutional Neural Networks: A Core-Set Approach	Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (i.e. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.	Deep convolutional neural networks (CNNs) have shown unprecedented success in many areas of research in computer vision and pattern recognition, such as image classification, object detection, and scene segmentation. Although CNNs are universally successful in many tasks, they have a major drawback; they need a very large amount of labeled data to be able to learn their large number of parameters. More importantly, it is almost always better to have more data since the accuracy of CNNs is often not saturated with increasing dataset size. Hence, there is a constant desire to collect more and more data. Although this a desired behavior from an algorithmic perspective (higher representative power is typically better), labeling a dataset is a time consuming and an expensive task. These practical considerations raise a critical question: "what is the optimal way to choose data points to label such that the highest accuracy can be obtained given a fixed labeling budget." Active learning is one of the common paradigms to address this question. The goal of active learning is to find effective ways to choose data points to label, from a pool of unlabeled data points, in order to maximize the accuracy. Although it is not possible to obtain a universally good active learning strategy (Dasgupta, 2004), there exist many heuristics (Settles, 2010) which have been proven to be effective in practice. Active learning is typically an iterative process in which a model is learned at each iteration and a set of points is chosen to be labelled from a pool of unlabelled points using these aforementioned heuristics. We experiment with many of these heuristics in this paper and find them not effective when applied to CNNs. We argue that the main factor behind this ineffectiveness is the correlation caused via batch acquisition/sampling. In the classical setting, the active learning algorithms typically choose a single point at each iteration; however, this is not feasible for CNNs since i) a single point is likely to have no statistically significant impact on the accuracy due to the local optimization methods, and ii) each iteration requires a full training until convergence which makes it intractable to query labels one-by-one. Hence, it is necessary to query labels for a large subset at each iteration and it results in correlated samples even for moderately small subset sizes. In order to tailor an active learning method for the batch sampling case, we decided to define the active learning as core-set selection problem. Core-set selection problem aims to find a small subset given a large labeled dataset such that a model learned over the small subset is competitive over the whole dataset. Since we have no labels available, we perform the core-set selection without using the labels. In order to attack the unlabeled core-set problem for CNNs, we provide a rigorous bound between an average loss over any given subset of the dataset and the remaining data points via the geometry of the data points. As an active learning algorithm, we try to choose a subset such that this bound is minimized. Moreover, minimization of this bound turns out to be equivalent to the k-Center problem (Wolf, 2011) and we adopt an efficient approximate solution to this combinatorial optimization problem. We further study the behavior of our proposed algorithm empirically for the problem of image classification using three different datasets. Our empirical analysis demonstrates state-of-the-art performance by a large margin.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2410.11303	TSDS: Data Selection for Task-Specific Model Finetuning	Finetuning foundation models for specific tasks is an emerging paradigm in modern machine learning. The efficacy of task-specific finetuning largely depends on the selection of appropriate training data. We present TSDS (Task-Specific Data Selection), a framework to select data for task-specific model finetuning, guided by a small but representative set of examples from the target task. To do so, we formulate data selection for task-specific finetuning as an optimization problem with a distribution alignment loss based on optimal transport to capture the discrepancy between the selected data and the target distribution. In addition, we add a regularizer to encourage the diversity of the selected data and incorporate kernel density estimation into the regularizer to reduce the negative effects of near-duplicates among the candidate data. We connect our optimization problem to nearest neighbor search and design efficient algorithms to compute the optimal solution based on approximate nearest neighbor search techniques. We evaluate our method on data selection for both continued pretraining and instruction tuning of language models. We show that instruction tuning using data selected by our method with a 1% selection ratio often outperforms using the full dataset and beats the baseline selection methods by 1.5 points in F1 score on average. Our code is available at https://github.com/ZifanL/TSDS.	inetuning foundation models [3 ] is the de-facto paradigm for building machine learning applications that focus on specific tasks. Models such as BERT [10] and LLaMA [ 43] are large-scale models pretrained on massive unlabeled data across a wide range of domains. Those models can be specialized to downstream tasks through finetuning. Finetuning can take a variety of forms depending on the target task. For instance, continued pretraining [ 17 ] extends the pretraining stage of a model on a dataset that is more closely related to a target domain. As another setting, instruction tuning [51 ] trains a generative foundation model on instruction-response pairs to improve its performance in responding to task-specific instructions. Finetuning foundation models can lead to significant improvement in downstream tasks, but the effectiveness heavily relies on the right choice of training data [ 17 , 30 , 48 , 47 ]. However, the data repositories that one considers during training of generative models tend to be large—consider for example the use of Common Crawl, which contains 250 billion web pages, or The Pile [14 ]—and hence, it is impractical to manually select the data that are distributed like the use cases in the target task. Therefore, automated task-specific data selection becomes critical. In this paper, we propose TSDS (Task-Specific Data Selection), a framework to select data for taskspecific model finetuning. We consider the scenario of finetuning a foundation model to customize it for a specific task characterized by a few representative examples. The input to our framework is the representative examples and a massive repository of candidate data. Guided by the representative examples, we select training data from the repository for task-specific finetuning. We identify the following requirements for our framework. (Distribution Alignment) First, the distribution of the selected data should match the distribution of the representative examples from the target task. Distribution alignment is essential for a model to learn the target distribution and enable data-efficient finetuning for the target task [ 40 ]. Many works [38 , 17, 2, 50 , 47 ] retrieve candidate examples that are most similar to the representative examples. Such heuristics do not ensure distribution alignment between the selected data and the representative examples. A recent work [ 48 ] selects data by importance resampling to match the target distribution but is limited to an n-gram feature space, which cannot capture high-level semantics. (Diversity) Second, the selected data should be diverse so that the model can learn a wide range of related knowledge rather than overfitting to specific examples. In practice, data repositories created by web crawling may contain a large portion of near-duplicates [13 , 28 ] that can compromise diversity and negatively impact model performance [ 28, 19 ]. For example, a study [ 13 ] on several snapshots of ClueWeb and Common Crawl shows that 14% to 52% of the documents are near-duplicates. Previous works [38, 17 , 2 , 50 , 48 , 47 ] on task-specific data selection overlook near-duplicates, leading to the over-representation of such examples in the selected data. We require our framework to ensure diversity in selection even when a large fraction of the candidate examples are near-duplicates. (Scalability) Finally, the selection algorithm should be efficient, considering the increasing scale of modern data repositories. The high volume of candidate data (e.g., 250 billion pages in Common Crawl) poses a great challenge to efficient selection. Our framework formulates task-specific data selection as an optimization problem that allows a smooth trade-off between two crucial objectives: distribution alignment and diversity. The solution to the optimization problem is a categorical distribution assigned to the candidates which we will sample from. In the optimization objective, we use optimal transport to measure the discrepancy between the distribution assigned to the candidates and the target distribution, encouraging the alignment between them. We show that the optimization problem admits efficient algorithms to compute the optimal solution. In addition, our framework supports distribution alignment in any metric space that supports efficient nearest-neighbor search, including model-agnostic semantic embedding and model-specific features such as gradients. Our contributions: 1) We formulate data selection for task-specific finetuning as an optimization problem based on optimal transport for distribution alignment, with a regularization term that encourages diversity. 2) We make our framework robust to near-duplicates by incorporating kernel density estimation [36 ] into the regularization term. 3) We show the connection between the optimal solution to the optimization problem and nearest neighbor search, which allows us to develop efficient algorithms employing approximate nearest-neighbor search techniques [23]. We conduct extensive experiments to validate the effectiveness of our framework. We focus on natural language processing tasks where foundation models have shown great advancements. We show that our framework beats the state-of-the-art baseline [47 ] by 1.5 points in F1 score on average with a selection ratio of 1% on instruction tuning for two modern large language models on three tasks. In addition, continued pretraining using domain-specific data selected by our framework outperforms the other selection methods by up to 3 F1 points on four classification tasks from various domains. We also demonstrate that our framework is robust to near-duplicates in the data repository, maintaining consistent performance when 1% of the candidate examples are duplicate for up to 1,000 times. Our method is efficient, taking 28 hours to preprocess a corpus of 150M examples and less than 1 hour for each task-specific selection.	https://arxiv.org/pdf/2301.13287	MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning	Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results indicate that MILO can train models 3 x -10x faster and tune hyperparameters 20 x -75x faster than full-dataset training or tuning without compromising performance.	Deep learning has achieved remarkable success in a multitude of machine learning tasks, including natural language processing, computer vision, and speech recognition in recent years. This success is partially due to the availability of massive training datasets and the capacity to train large-scale neural networks. However, training deep models on extensive datasets is computationally demanding, incurring significant financial costs and generating substantial CO2 emissions [63, 59 ]. Bhavya et al. [ 2] overview several research trajectories aimed at enhancing model convergence and reducing training time and costs, including data subset selection, curriculum learning, model architecture improvements, and optimization algorithm enhancements. Our work specifically focuses on selecting useful, generalizable data subsets for efficient deep neural network training. Recent research [66 , 5 , 21 ] suggests that many current training datasets are redundant, indicating that a non-redundant, informative subset could achieve comparable performance to training on the full dataset. To identify such informative samples, metrics such as prediction uncertainty [ 7], prediction flips [ 66 , 72 ], loss [42 ], or gradient/gradient-norm [ 21, 47 , 28, 27 , 29 ] have been applied. These metrics, calculated using either the downstream machine learning model or a lightweight surrogate [ 7], require a fully converged model, a requirement that runs counter to the goal of efficient training. To address this, existing subset selection algorithms [ 47 , 28 , 27, 29, 30 , 54 ] for efficient learning utilize the downstream model during training for heuristic computation and periodically update the subset as the model-dependent metrics for each sample evolve. Drawbacks of Model-Dependent Subset Selection: Despite the theoretical advantages of existing subset selection strategies[ 28 , 27, 47 ], they often fall short in computational efficiency compared to adaptive random subset selection, which selects random subsets periodically. This is primarily because traditional approaches rely on downstream models and typically require the calculation of sample metrics, such as gradients, before each subset selection step. Furthermore, these computationally demanding subset selection steps occur during model training. For instance, Figure 1 compares the convergence rate of the ResNet18 model on the CIFAR100 dataset, in terms of both time and epochs. Here, the model uses 10% subsets chosen every epoch by GradMatchPB [ 27 ], a state-of-the-art data subset selection strategy for efficient training, CraigPB [ 47 ], and Adaptive-Random (where a new 10% subset is randomly selected periodically). The selection of a new subset every epoch is done to demonstrate the maximum performance attainable by GradMatchPB and CraigPB. The results indicate that GradMatchPB achieves faster epoch convergence than both Adaptive-Random and CraigPB when selecting a new subset each epoch. However, due to the necessity for a computationally intensive subset selection step every epoch, both GradMatchPB and CraigPB demonstrate considerable inefficiency in terms of training time. In their respective studies, Killamsetty et al. [27 ] and Mirzasoleiman et al. [47] suggested selecting a new subset every R epochs to enhance training efficiency. However, this comes at the cost of the model’s convergence rate. Lastly, model-dependent subset selection requires a computationally intensive subset selection steps each time a new model is trained. Model-Agnostic Selection: Our primary insight is that a model-agnostic subset selection framework can circumvent the computationally intensive subset selection steps during model training by conducting subset selection in the preprocessing phase. By pre-selecting subsets and storing them as metadata with each dataset, we can train numerous models without incurring additional costs, effectively spreading out the expense of subset selection. In this work, we endeavor to answer the following question: Is it possible to develop a model-agnostic subset selection method that selects new subsets in a minimal amount of time, yet achieves superior model convergence without significant compromise in test accuracy or generalization performance?  MILO Framework: In this study, we introduce MILO, a model-agnostic subset selection framework designed for efficient model training and tuning. MILO employs submodular measures [ 13 , 23 ], which capture higher-order interactions between data samples for subset selection. We utilize pre-trained large language models [ 55 ] and pre-trained vision transformers [25 ] as feature encoders due to their zero-shot feature encoding capabilities. These encoders compute the sample metrics in a nominal amount of time, and this computation is independent of the downstream model, rendering MILO model-agnostic. Figure 3 provides a visual representation of MILO for model training, comprising two steps: a) A pre-processing step that involves the selection of multiple subsets from the training dataset using "Stochastic-Greedy Exploration (SGE)" (refer to Section 3.1.1), and the construction of a probability distribution over the entire dataset for subset sampling through "Weighted Random Exploration (WRE)" (refer to Section 3.1.2); b) A model training scheme that involves training the model on a curriculum of easy-to-hard subsets (refer to Section 3.1.3) using the subsets selected and the probability distribution constructed in the pre-processing step. We also present a class-wise partitioning trick to significantly minimize the memory footprint of MILO (refer to Section 3.2). Effectiveness of MILO : Through extensive experiments on multiple real-world datasets, we empirically demonstrate the effectiveness of the MILO framework for efficient training and hyperparameter tuning. In Figure 2, we provide a summary of the speedup achieved by MILO compared to full data training and tuning, along with the corresponding relative performance. Our results show that MILO can train models 3x to 10x faster and tune hyperparameters 20x to 75x faster, with minimal loss in performance. Furthermore, we consistently outperform the subset selection baselines that were considered. Additionally, MILO exhibits significantly faster initial model convergence and maintains faster convergence throughout training compared to other baselines. These qualities make it highly effective for applications such as hyperparameter tuning and neural architecture search, where the ability to distinguish good models from bad models early in the tuning process is crucial for enhancing efficiency [38, 37]. Related Work: In recent years, data-subset selection strategies have achieved success in various machine learning applications such as speech recognition [ 68 , 67], machine translation [ 32 ], active learning [ 60 , 1 , 33 ], hyper-parameter tuning [ 30 ], continual learning [65 ], domain adaptation [20 ], and computer vision [22]. A recent empirical study [5] demonstrated that datasets frequently contain semantic redundancies. These redundancies can be removed during training without affecting performance. In response, data pruning methods [66 , 51, 62] have developed principled selection criteria for pruning redundant samples before model training, with minimal loss in performance. However, both existing data pruning and compute-efficient learning approaches are model-dependent, resulting in them being computationally expensive. In contrast, our method is model-agnostic and capable of identifying high-quality subsets by intelligent data exploration. Furthermore, our approach can achieve superior model convergence by utilizing an easy-to-hard curriculum, transitioning from representative to diverse subsets during training.	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2410.11303	TSDS: Data Selection for Task-Specific Model Finetuning	Finetuning foundation models for specific tasks is an emerging paradigm in modern machine learning. The efficacy of task-specific finetuning largely depends on the selection of appropriate training data. We present TSDS (Task-Specific Data Selection), a framework to select data for task-specific model finetuning, guided by a small but representative set of examples from the target task. To do so, we formulate data selection for task-specific finetuning as an optimization problem with a distribution alignment loss based on optimal transport to capture the discrepancy between the selected data and the target distribution. In addition, we add a regularizer to encourage the diversity of the selected data and incorporate kernel density estimation into the regularizer to reduce the negative effects of near-duplicates among the candidate data. We connect our optimization problem to nearest neighbor search and design efficient algorithms to compute the optimal solution based on approximate nearest neighbor search techniques. We evaluate our method on data selection for both continued pretraining and instruction tuning of language models. We show that instruction tuning using data selected by our method with a 1% selection ratio often outperforms using the full dataset and beats the baseline selection methods by 1.5 points in F1 score on average. Our code is available at https://github.com/ZifanL/TSDS.	inetuning foundation models [3 ] is the de-facto paradigm for building machine learning applications that focus on specific tasks. Models such as BERT [10] and LLaMA [ 43] are large-scale models pretrained on massive unlabeled data across a wide range of domains. Those models can be specialized to downstream tasks through finetuning. Finetuning can take a variety of forms depending on the target task. For instance, continued pretraining [ 17 ] extends the pretraining stage of a model on a dataset that is more closely related to a target domain. As another setting, instruction tuning [51 ] trains a generative foundation model on instruction-response pairs to improve its performance in responding to task-specific instructions. Finetuning foundation models can lead to significant improvement in downstream tasks, but the effectiveness heavily relies on the right choice of training data [ 17 , 30 , 48 , 47 ]. However, the data repositories that one considers during training of generative models tend to be large—consider for example the use of Common Crawl, which contains 250 billion web pages, or The Pile [14 ]—and hence, it is impractical to manually select the data that are distributed like the use cases in the target task. Therefore, automated task-specific data selection becomes critical. In this paper, we propose TSDS (Task-Specific Data Selection), a framework to select data for taskspecific model finetuning. We consider the scenario of finetuning a foundation model to customize it for a specific task characterized by a few representative examples. The input to our framework is the representative examples and a massive repository of candidate data. Guided by the representative examples, we select training data from the repository for task-specific finetuning. We identify the following requirements for our framework. (Distribution Alignment) First, the distribution of the selected data should match the distribution of the representative examples from the target task. Distribution alignment is essential for a model to learn the target distribution and enable data-efficient finetuning for the target task [ 40 ]. Many works [38 , 17, 2, 50 , 47 ] retrieve candidate examples that are most similar to the representative examples. Such heuristics do not ensure distribution alignment between the selected data and the representative examples. A recent work [ 48 ] selects data by importance resampling to match the target distribution but is limited to an n-gram feature space, which cannot capture high-level semantics. (Diversity) Second, the selected data should be diverse so that the model can learn a wide range of related knowledge rather than overfitting to specific examples. In practice, data repositories created by web crawling may contain a large portion of near-duplicates [13 , 28 ] that can compromise diversity and negatively impact model performance [ 28, 19 ]. For example, a study [ 13 ] on several snapshots of ClueWeb and Common Crawl shows that 14% to 52% of the documents are near-duplicates. Previous works [38, 17 , 2 , 50 , 48 , 47 ] on task-specific data selection overlook near-duplicates, leading to the over-representation of such examples in the selected data. We require our framework to ensure diversity in selection even when a large fraction of the candidate examples are near-duplicates. (Scalability) Finally, the selection algorithm should be efficient, considering the increasing scale of modern data repositories. The high volume of candidate data (e.g., 250 billion pages in Common Crawl) poses a great challenge to efficient selection. Our framework formulates task-specific data selection as an optimization problem that allows a smooth trade-off between two crucial objectives: distribution alignment and diversity. The solution to the optimization problem is a categorical distribution assigned to the candidates which we will sample from. In the optimization objective, we use optimal transport to measure the discrepancy between the distribution assigned to the candidates and the target distribution, encouraging the alignment between them. We show that the optimization problem admits efficient algorithms to compute the optimal solution. In addition, our framework supports distribution alignment in any metric space that supports efficient nearest-neighbor search, including model-agnostic semantic embedding and model-specific features such as gradients. Our contributions: 1) We formulate data selection for task-specific finetuning as an optimization problem based on optimal transport for distribution alignment, with a regularization term that encourages diversity. 2) We make our framework robust to near-duplicates by incorporating kernel density estimation [36 ] into the regularization term. 3) We show the connection between the optimal solution to the optimization problem and nearest neighbor search, which allows us to develop efficient algorithms employing approximate nearest-neighbor search techniques [23]. We conduct extensive experiments to validate the effectiveness of our framework. We focus on natural language processing tasks where foundation models have shown great advancements. We show that our framework beats the state-of-the-art baseline [47 ] by 1.5 points in F1 score on average with a selection ratio of 1% on instruction tuning for two modern large language models on three tasks. In addition, continued pretraining using domain-specific data selected by our framework outperforms the other selection methods by up to 3 F1 points on four classification tasks from various domains. We also demonstrate that our framework is robust to near-duplicates in the data repository, maintaining consistent performance when 1% of the candidate examples are duplicate for up to 1,000 times. Our method is efficient, taking 28 hours to preprocess a corpus of 150M examples and less than 1 hour for each task-specific selection.	https://arxiv.org/pdf/2103.00128	PRISM: A Rich Class of Parameterized Submodular Information Measures for Guided Subset Selection	With ever-increasing dataset sizes, subset selection techniques are becoming increasingly important for a plethora of tasks. It is often necessary to guide the subset selection to achieve certain desiderata, which includes focusing or targeting certain data points, while avoiding others. Examples of such problems include: i) targeted learning, where the goal is to find subsets with rare classes or rare attributes on which the model is underperforming, and ii) guided summarization, where data (e.g., image collection, text, document or video) is summarized for quicker human consumption with specific additional user intent. Motivated by such applications, we present PRISM, a rich class of PaRameterIzed Submodular information Measures. Through novel functions and their parameterizations, PRISM offers a variety of modeling capabilities that enable a trade-off between desired qualities of a subset like diversity or representation and similarity/dissimilarity with a set of data points. We demonstrate how PRISM can be applied to the two real-world problems mentioned above, which require guided subset selection. In doing so, we show that PRISM interestingly generalizes some past work, therein reinforcing its broad utility. Through extensive experiments on diverse datasets, we demonstrate the superiority of PRISM over the state-of-the-art in targeted learning and in guided imagecollection summarization. PRISM is available as a part of the SUBMODLIB (https://github.com/decile-team/submodlib) and TRUST (https://github.com/decile-team/trust) toolkits.	Recent times have seen explosive growth in data across several modalities, including text, images, and videos. This has given rise to the need for finding techniques for selecting effective smaller data subsets with specific characteristics for a variety of down-stream tasks. Often, we would like to guide the data selection to either target or avoid a certain set of data slices. One application is, what we call, targeted learning, where the goal is to select data points similar to data slices on which the model is currently performing poorly. These slices are data points that either belong to rare classes or have common rare attributes (e.g., color, background, etc.). An example of such a scenario is shown in Fig. 1(a), where a self-driving car model struggles in detecting “cars in a dark background“ because of a lack of such images in the training set. The targeted learning problem is to augment the training dataset with more of such rare images, with an aim to improve model performance. Another example is detecting cancers in biomedical imaging datasets, where the number of cancerous images are often a small fraction of the non-cancerous images. Another application comes from the summarization task, where an image collection, a video, or a text document is summarized for quicker human consumption by eliminating redundancy, while preserving the main content. While a number of applications require generic summarization (i.e., simply picking a representative and diverse subset of the massive dataset), it is often important to capture certain user intent in summarization. We call this guided summarization. Examples of guided summarization include: (i) query-focused summarization (Sharghi, Gong, and Shah 2016; Xiao et al. 2020), where a summary similar to a specific query is desired, and (ii) privacy-preserving summarization, where a summary dissimilar to a given private set of data points is desired (say, for privacy issues). See Fig. 1(b) for a pictorial illustration. PRISM Framework: We define PRISM through different instantiations and parameterizations of various submodular information measures (Sec. 2). These allow for modeling a spectrum of semantics required for guided subset selection, like relevance to a query set, irrelevance to a private set, and diversity among selected data points. We study the effect of parameter trade-off among these different semantics and present interesting insights. PRISM for Targeted Learning: We present a novel algorithm (Sec. 3.1, Algo. 1) to apply PRISM for targeted learning, which aims to improve a model’s performance on rare slices of data. Specifically, we show that submodular information measures are very effective in finding the examples from the rare classes in a large unlabeled set (akin to finding a needle in a haystack). On several image classification tasks, PRISM obtains ≈ 20-30% gain in accuracy of rare classes (≈ 12% more than existing approaches) by just adding a few additional labeled points from the rare classes. Furthermore, we show that PRISM is 20× to 50× more label-efficient compared to random sampling, and 2× to 4× more labelefficient compared to existing approaches (see Sec. 4.1). We also show that Algo. 1 generalizes some existing approaches for data subset selection, reinforcing its utility (Sec. 3.3). PRISM for Guided Summarization. We propose a learning framework for guided summarization using PRISM (Sec. 3.2). We show that PRISM offers a unified treatment to the different flavors of guided summarization (query-focused and privacy-preserving) and generalizes some existing approaches to summarization, again reinforcing its utility. We show that it outperforms other existing approaches on a real-world image collections dataset (Sec. 4.2).	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2410.11303	TSDS: Data Selection for Task-Specific Model Finetuning	Finetuning foundation models for specific tasks is an emerging paradigm in modern machine learning. The efficacy of task-specific finetuning largely depends on the selection of appropriate training data. We present TSDS (Task-Specific Data Selection), a framework to select data for task-specific model finetuning, guided by a small but representative set of examples from the target task. To do so, we formulate data selection for task-specific finetuning as an optimization problem with a distribution alignment loss based on optimal transport to capture the discrepancy between the selected data and the target distribution. In addition, we add a regularizer to encourage the diversity of the selected data and incorporate kernel density estimation into the regularizer to reduce the negative effects of near-duplicates among the candidate data. We connect our optimization problem to nearest neighbor search and design efficient algorithms to compute the optimal solution based on approximate nearest neighbor search techniques. We evaluate our method on data selection for both continued pretraining and instruction tuning of language models. We show that instruction tuning using data selected by our method with a 1% selection ratio often outperforms using the full dataset and beats the baseline selection methods by 1.5 points in F1 score on average. Our code is available at https://github.com/ZifanL/TSDS.	inetuning foundation models [3 ] is the de-facto paradigm for building machine learning applications that focus on specific tasks. Models such as BERT [10] and LLaMA [ 43] are large-scale models pretrained on massive unlabeled data across a wide range of domains. Those models can be specialized to downstream tasks through finetuning. Finetuning can take a variety of forms depending on the target task. For instance, continued pretraining [ 17 ] extends the pretraining stage of a model on a dataset that is more closely related to a target domain. As another setting, instruction tuning [51 ] trains a generative foundation model on instruction-response pairs to improve its performance in responding to task-specific instructions. Finetuning foundation models can lead to significant improvement in downstream tasks, but the effectiveness heavily relies on the right choice of training data [ 17 , 30 , 48 , 47 ]. However, the data repositories that one considers during training of generative models tend to be large—consider for example the use of Common Crawl, which contains 250 billion web pages, or The Pile [14 ]—and hence, it is impractical to manually select the data that are distributed like the use cases in the target task. Therefore, automated task-specific data selection becomes critical. In this paper, we propose TSDS (Task-Specific Data Selection), a framework to select data for taskspecific model finetuning. We consider the scenario of finetuning a foundation model to customize it for a specific task characterized by a few representative examples. The input to our framework is the representative examples and a massive repository of candidate data. Guided by the representative examples, we select training data from the repository for task-specific finetuning. We identify the following requirements for our framework. (Distribution Alignment) First, the distribution of the selected data should match the distribution of the representative examples from the target task. Distribution alignment is essential for a model to learn the target distribution and enable data-efficient finetuning for the target task [ 40 ]. Many works [38 , 17, 2, 50 , 47 ] retrieve candidate examples that are most similar to the representative examples. Such heuristics do not ensure distribution alignment between the selected data and the representative examples. A recent work [ 48 ] selects data by importance resampling to match the target distribution but is limited to an n-gram feature space, which cannot capture high-level semantics. (Diversity) Second, the selected data should be diverse so that the model can learn a wide range of related knowledge rather than overfitting to specific examples. In practice, data repositories created by web crawling may contain a large portion of near-duplicates [13 , 28 ] that can compromise diversity and negatively impact model performance [ 28, 19 ]. For example, a study [ 13 ] on several snapshots of ClueWeb and Common Crawl shows that 14% to 52% of the documents are near-duplicates. Previous works [38, 17 , 2 , 50 , 48 , 47 ] on task-specific data selection overlook near-duplicates, leading to the over-representation of such examples in the selected data. We require our framework to ensure diversity in selection even when a large fraction of the candidate examples are near-duplicates. (Scalability) Finally, the selection algorithm should be efficient, considering the increasing scale of modern data repositories. The high volume of candidate data (e.g., 250 billion pages in Common Crawl) poses a great challenge to efficient selection. Our framework formulates task-specific data selection as an optimization problem that allows a smooth trade-off between two crucial objectives: distribution alignment and diversity. The solution to the optimization problem is a categorical distribution assigned to the candidates which we will sample from. In the optimization objective, we use optimal transport to measure the discrepancy between the distribution assigned to the candidates and the target distribution, encouraging the alignment between them. We show that the optimization problem admits efficient algorithms to compute the optimal solution. In addition, our framework supports distribution alignment in any metric space that supports efficient nearest-neighbor search, including model-agnostic semantic embedding and model-specific features such as gradients. Our contributions: 1) We formulate data selection for task-specific finetuning as an optimization problem based on optimal transport for distribution alignment, with a regularization term that encourages diversity. 2) We make our framework robust to near-duplicates by incorporating kernel density estimation [36 ] into the regularization term. 3) We show the connection between the optimal solution to the optimization problem and nearest neighbor search, which allows us to develop efficient algorithms employing approximate nearest-neighbor search techniques [23]. We conduct extensive experiments to validate the effectiveness of our framework. We focus on natural language processing tasks where foundation models have shown great advancements. We show that our framework beats the state-of-the-art baseline [47 ] by 1.5 points in F1 score on average with a selection ratio of 1% on instruction tuning for two modern large language models on three tasks. In addition, continued pretraining using domain-specific data selected by our framework outperforms the other selection methods by up to 3 F1 points on four classification tasks from various domains. We also demonstrate that our framework is robust to near-duplicates in the data repository, maintaining consistent performance when 1% of the candidate examples are duplicate for up to 1,000 times. Our method is efficient, taking 28 hours to preprocess a corpus of 150M examples and less than 1 hour for each task-specific selection.	https://aclanthology.org/2024.findings-naacl.209.pdf	An End-to-End Submodular Framework for Data-Efficient In-Context Learning	Recent advancements in natural language tasks leverage the emergent In-Context Learning (ICL) ability of pretrained Large Language Models (LLMs). ICL enables LLMs to perform new tasks by utilizing a limited number of input-output examples as prompts. While ICL circumvents the costly step of finetuning LLMs, its effectiveness is heavily dependent on the quality and ordering of provided examples (called exemplars). In this work, we propose a two-stage data-efficient framework Div-S3 for exemplar selection for ICL. The first stage focuses on data annotation and employs a poolbased active learning approach to select a set of Diverse and informative exemplars from the target tasks’ unlabeled pool. Given a test input/query, the second stage uses Submodular Span Summarization (S3) to select the most relevant and non-redundant exemplars from the annotated pool of a limited budget. On 7 different NLP datasets and 5 LLMs of varying complexities, we show Div-S3 outperforms (1) existing active learning-based methods for data annotation for ICL and (2) similarity-based methods for test query-specific exemplars retrieval.	Pretrained large language models (LLMs) (Kenton and Toutanova, 2019; Brown et al., 2020; Chowdhery et al., 2022) have become foundational for a wide range of Natural Language Processing (NLP) tasks, demonstrating impressive success across various domains (Bommasani et al., 2021; Bubeck et al., 2023) through in-context learning (ICL) (Dong et al., 2022). ICL enables these pretrained LLMs to perform new tasks by using task-specific prompts containing a limited number of input-output demonstrations (also referred to as shots, exemplars, or prompts) in the natural language format. This approach facilitates deployment across different downstream tasks and reduces the need for labeled downstream training data since ICL does not require any task-specific training. The typical ICL procedure consists of two key components: (1) Exemplar annotation and retrieval (Wu et al., 2022; Köksal et al., 2022; Liu et al., 2022): This step involves annotating and retrieving exemplars that serve as context demonstrations. (2) Prompt template crafting (Sorensen et al., 2022; Deng et al., 2022): this step involves designing a prompt template to wrap these demonstrations in a comprehensible and coherent natural language instruction. Recent studies (Liu et al., 2022; Su et al., 2022; Margatina et al., 2023) show that providing exemplars most relevant to the current input instance is beneficial. Moreover, Zhao et al. (2021), Lu et al. (2022), and Liu et al. (2023) observe that LLMs attend more to the exemplars that are closer in the sequence to the input instance. Therefore, to achieve the best performance of ICL, the selection of exemplars and their ordering in the LLM prompt are crucial. In practice, an extensive collection of unlabeled exemplars is easily available (e.g., posts and discussions on forums like Stack Exchange or user-generated content on social media platforms), but manually annotating all exemplars would be exceptionally costly. To annotate and select the exemplars optimally for a given target task, we follow the two-stage approach shown in Figure 1: (1) Exemplar Annotation: select a subset of exemplars for annotation under a fixed budget (performed only once) and (2) Exemplar Retrieval: identify limited-sized exemplars in an ordering that are most influential for a given input instance from the annotated subset of exemplars. Intuitively, for the first stage, we aim to find the subset with maximal diversity and least redundancy so that, given any input, we can find corresponding labeled exemplars. For the second stage, in addition to the diversity requirement similar to the first stage, we emphasize the relevance of the exemplars to the given input query and order exemplars so that their relevance to the input query decreases as the exemplars are farther away from the input instance. We propose a framework Div-S3 based on submodular optimization that unifies the abovementioned two stages. For Exemplar Annotation, we model the problem as a submodular optimization problem under a cardinality constraint to find as Diverse a subset as possible within a budget. For Exemplar Retrieval, we formalize the problem as a Submodular Span Summarization (S3) problem (Kumari and Bilmes, 2021) with a knapsack constraint, which finds a diverse subset most relevant to the input query under a token length limit. Also, we naturally order the resulting exemplars based on the gains represented by the submodular function. The name of our proposed framework Div-S3 captures the optimization objectives used for both exemplar annotation (Div) and exemplar retrieval (S3) stages. In Fig. 2, we show a sample test query where using Div-S3 for exemplar selection leads to a more diverse and query-relevant exemplar set (more examples provided in Appendix D). Our framework is general, as any submodular function can be plugged into our method. For models beyond LMs, e.g., for text-image multi-modality models, we may use pre-existing submodular functions that are powerful for expressing diversity in the image domain. In addition, we account for relevance, diversity, and ordering for the exemplar retrieval stage, where one or two aspects typically get overlooked by previous methods. Empirically, we evaluate Div-S3 on 7 NLP tasks with 5 LLMs and show significantly improved performance compared to baselines. Our contributions are: 1. We propose an end-to-end framework DivS3 utilizing submodular optimization for performing data-efficient ICL using LLMs. Depending on budget requirements, Div-S3 provides the flexibility to set the budget either in terms of the number of exemplars to be used in the prompt or the LLM’s context window size. 2. We empirically validate the effectiveness of our framework on 7 different NLP tasks and show the transferability of results across LLMs of varying complexities. 3. We thoroughly analyze each component of Div-S3 by (a) studying S3 in a setting with no annotation budget constraint and (b) analyzing the sensitivity of the exemplars selected by Div-S3 to their position in the LLM’s prompt.	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2501.04155	MM-GEN: Enhancing Task Performance Through Targeted Multimodal Data Curation	Vision-language models (VLMs) are highly effective but often underperform on specialized tasks, for example Llava1.5 struggles on chart and diagram understanding, due to scarce task-specific training data. Existing training data, sourced from general-purpose datasets, fails to capture the nuanced details needed for these tasks. We introduce MMGEN, a scalable method that generates task-specific, highquality synthetic text for candidate images by leveraging stronger models. MM-GEN employs a three-stage targeted process: partitioning data into subgroups, generating targeted text based on task descriptions, and filtering out redundant and outlier data. Fine-tuning VLMs with data generated by MM-GEN leads to significant performance gains, including 29% on spatial reasoning and 15% on diagram understanding for Llava-1.5 (7B). Compared to human-curated caption data, MM-GEN achieves up to 1.6× better improvements for the original models, proving its effectiveness in enhancing task-specific VLM performance and bridging the gap between general-purpose datasets and specialized requirements. Code available at https://github.com/sjoshi804/MM-Gen.	While vision-language models (VLMs) demonstrate stateof-the-art performance on several multi-modal tasks [29], they are often on tasks that are simpler in nature [3]. These models still struggle with more complex tasks, e.g., those that require fine-grained understanding of details in images [3, 15, 21, 49]. We posit that the primary limitation for these VLMs is the quality and nature of the training data. VLMs are typically trained on large-scale image-text data scraped from the internet; while the images are rich and informative, the associated text descriptions can (i) have limited relevance to the image [39], or (ii) omit references to several specific details captured in the image [25]. Fig. 1 shows examples of such images and web-scraped captions. While the images are relevant for the tasks of chart understanding, spatial reasoning, and diagram understanding, respectively, the text fails to capture details essential for these tasks. While synthetic caption generation strategies proposed in prior work [25, 39, 57] can create more descriptive text annotations (by referring to more visual details), they are agnostic of the downstream target task. Consequently, they cannot ensure that relevant details are captured in the text annotations. Recently, Shi et al. [48] manually curated a task-specific dataset aimed at the task of multimodal mathematical question-answering by augmenting existing imagetext data with detailed textual annotations, based on their domain expertise, using strong VLMs. While effective, such a curation pipeline involves significant human effort and is not scalable [33, 59]. To address these limitations, we present MM-GEN, a highly general framework for automatically synthesizing taskrelevant text annotations for images by leveraging stronger VLMs (i.e., VLMs that perform well on the specific task) and requiring minimal human effort. MM-GEN takes as input a small set of examples from the target task (henceforth referred to as "reference samples"), a list of image types associated with the task, and a pool of task-relevant candidate images for training. In practice, these inputs can be easily obtained: a small number of reference samples and associated image types can be directly collected from the target task, and a task-relevant image pool can be found via image search with search engines or retrieved from largescale image-caption datasets [5, 46, 47]. Using the reference samples to specify the details of the task to the stronger VLM, MM-GEN generates text-annotations that are taskrelevant for the candidate images. Fig. 1 shows how the text generated by MM-GEN captures task-relevant details. This simple approach is highly effective, resulting in significant improvements across a variety of target tasks. Moreover, human-effort in this process is limited to providing (i) a small set of reference samples for the task, and (ii) a pool of candidate images. To further improve the quality and efficacy of the generated data, MM-GEN introduces a perplexity [4] based data-filtering approach to improve performance on target task using a high-value subset of the synthesized data. The components in MM-GEN are general and applicable to any image-text based target task enabling it to easily generalize across tasks and scale. We evaluate MM-GEN on improving VLMs' (e.g., Llava1.5 7B and 13B parameter versions) performance on finegrained image understanding tasks — chart understanding and reasoning, diagram understanding, and spatial reasoning on maps. The data curated by MM-GEN enables an absolute improvement over Llava-1.5 (7B) of 15%, 14% and 29%, respectively. We also see improvements over the much larger Llava-1.5 (13B) of 13%, , respectively. Moreover, MM-GEN’s filtering strategy helps in shrinking data volumes by up to 50% with no / minimal loss in performance. Empirical results show that models trained via MM-GEN data have a better performance than those trained via generated generic captions. MM-GEN data is also more effective than text annotations generated without task-specific reference examples, showing the importance of a targeted, data-centric approach for describing tasks. Finally, we analyze the effects of key design choices in MMGEN through ablation studies on e.g. size of the reference sample set, generating with / without partitioning, scaling number of in-context samples.	https://arxiv.org/pdf/1708.00489	Active Learning for Convolutional Neural Networks: A Core-Set Approach	Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (i.e. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.	Deep convolutional neural networks (CNNs) have shown unprecedented success in many areas of research in computer vision and pattern recognition, such as image classification, object detection, and scene segmentation. Although CNNs are universally successful in many tasks, they have a major drawback; they need a very large amount of labeled data to be able to learn their large number of parameters. More importantly, it is almost always better to have more data since the accuracy of CNNs is often not saturated with increasing dataset size. Hence, there is a constant desire to collect more and more data. Although this a desired behavior from an algorithmic perspective (higher representative power is typically better), labeling a dataset is a time consuming and an expensive task. These practical considerations raise a critical question: "what is the optimal way to choose data points to label such that the highest accuracy can be obtained given a fixed labeling budget." Active learning is one of the common paradigms to address this question. The goal of active learning is to find effective ways to choose data points to label, from a pool of unlabeled data points, in order to maximize the accuracy. Although it is not possible to obtain a universally good active learning strategy (Dasgupta, 2004), there exist many heuristics (Settles, 2010) which have been proven to be effective in practice. Active learning is typically an iterative process in which a model is learned at each iteration and a set of points is chosen to be labelled from a pool of unlabelled points using these aforementioned heuristics. We experiment with many of these heuristics in this paper and find them not effective when applied to CNNs. We argue that the main factor behind this ineffectiveness is the correlation caused via batch acquisition/sampling. In the classical setting, the active learning algorithms typically choose a single point at each iteration; however, this is not feasible for CNNs since i) a single point is likely to have no statistically significant impact on the accuracy due to the local optimization methods, and ii) each iteration requires a full training until convergence which makes it intractable to query labels one-by-one. Hence, it is necessary to query labels for a large subset at each iteration and it results in correlated samples even for moderately small subset sizes. In order to tailor an active learning method for the batch sampling case, we decided to define the active learning as core-set selection problem. Core-set selection problem aims to find a small subset given a large labeled dataset such that a model learned over the small subset is competitive over the whole dataset. Since we have no labels available, we perform the core-set selection without using the labels. In order to attack the unlabeled core-set problem for CNNs, we provide a rigorous bound between an average loss over any given subset of the dataset and the remaining data points via the geometry of the data points. As an active learning algorithm, we try to choose a subset such that this bound is minimized. Moreover, minimization of this bound turns out to be equivalent to the k-Center problem (Wolf, 2011) and we adopt an efficient approximate solution to this combinatorial optimization problem. We further study the behavior of our proposed algorithm empirically for the problem of image classification using three different datasets. Our empirical analysis demonstrates state-of-the-art performance by a large margin.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2501.04155	MM-GEN: Enhancing Task Performance Through Targeted Multimodal Data Curation	Vision-language models (VLMs) are highly effective but often underperform on specialized tasks, for example Llava1.5 struggles on chart and diagram understanding, due to scarce task-specific training data. Existing training data, sourced from general-purpose datasets, fails to capture the nuanced details needed for these tasks. We introduce MMGEN, a scalable method that generates task-specific, highquality synthetic text for candidate images by leveraging stronger models. MM-GEN employs a three-stage targeted process: partitioning data into subgroups, generating targeted text based on task descriptions, and filtering out redundant and outlier data. Fine-tuning VLMs with data generated by MM-GEN leads to significant performance gains, including 29% on spatial reasoning and 15% on diagram understanding for Llava-1.5 (7B). Compared to human-curated caption data, MM-GEN achieves up to 1.6× better improvements for the original models, proving its effectiveness in enhancing task-specific VLM performance and bridging the gap between general-purpose datasets and specialized requirements. Code available at https://github.com/sjoshi804/MM-Gen.	While vision-language models (VLMs) demonstrate stateof-the-art performance on several multi-modal tasks [29], they are often on tasks that are simpler in nature [3]. These models still struggle with more complex tasks, e.g., those that require fine-grained understanding of details in images [3, 15, 21, 49]. We posit that the primary limitation for these VLMs is the quality and nature of the training data. VLMs are typically trained on large-scale image-text data scraped from the internet; while the images are rich and informative, the associated text descriptions can (i) have limited relevance to the image [39], or (ii) omit references to several specific details captured in the image [25]. Fig. 1 shows examples of such images and web-scraped captions. While the images are relevant for the tasks of chart understanding, spatial reasoning, and diagram understanding, respectively, the text fails to capture details essential for these tasks. While synthetic caption generation strategies proposed in prior work [25, 39, 57] can create more descriptive text annotations (by referring to more visual details), they are agnostic of the downstream target task. Consequently, they cannot ensure that relevant details are captured in the text annotations. Recently, Shi et al. [48] manually curated a task-specific dataset aimed at the task of multimodal mathematical question-answering by augmenting existing imagetext data with detailed textual annotations, based on their domain expertise, using strong VLMs. While effective, such a curation pipeline involves significant human effort and is not scalable [33, 59]. To address these limitations, we present MM-GEN, a highly general framework for automatically synthesizing taskrelevant text annotations for images by leveraging stronger VLMs (i.e., VLMs that perform well on the specific task) and requiring minimal human effort. MM-GEN takes as input a small set of examples from the target task (henceforth referred to as "reference samples"), a list of image types associated with the task, and a pool of task-relevant candidate images for training. In practice, these inputs can be easily obtained: a small number of reference samples and associated image types can be directly collected from the target task, and a task-relevant image pool can be found via image search with search engines or retrieved from largescale image-caption datasets [5, 46, 47]. Using the reference samples to specify the details of the task to the stronger VLM, MM-GEN generates text-annotations that are taskrelevant for the candidate images. Fig. 1 shows how the text generated by MM-GEN captures task-relevant details. This simple approach is highly effective, resulting in significant improvements across a variety of target tasks. Moreover, human-effort in this process is limited to providing (i) a small set of reference samples for the task, and (ii) a pool of candidate images. To further improve the quality and efficacy of the generated data, MM-GEN introduces a perplexity [4] based data-filtering approach to improve performance on target task using a high-value subset of the synthesized data. The components in MM-GEN are general and applicable to any image-text based target task enabling it to easily generalize across tasks and scale. We evaluate MM-GEN on improving VLMs' (e.g., Llava1.5 7B and 13B parameter versions) performance on finegrained image understanding tasks — chart understanding and reasoning, diagram understanding, and spatial reasoning on maps. The data curated by MM-GEN enables an absolute improvement over Llava-1.5 (7B) of 15%, 14% and 29%, respectively. We also see improvements over the much larger Llava-1.5 (13B) of 13%, , respectively. Moreover, MM-GEN’s filtering strategy helps in shrinking data volumes by up to 50% with no / minimal loss in performance. Empirical results show that models trained via MM-GEN data have a better performance than those trained via generated generic captions. MM-GEN data is also more effective than text annotations generated without task-specific reference examples, showing the importance of a targeted, data-centric approach for describing tasks. Finally, we analyze the effects of key design choices in MMGEN through ablation studies on e.g. size of the reference sample set, generating with / without partitioning, scaling number of in-context samples.	https://arxiv.org/pdf/2301.13287	MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning	Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results indicate that MILO can train models 3 x -10x faster and tune hyperparameters 20 x -75x faster than full-dataset training or tuning without compromising performance.	Deep learning has achieved remarkable success in a multitude of machine learning tasks, including natural language processing, computer vision, and speech recognition in recent years. This success is partially due to the availability of massive training datasets and the capacity to train large-scale neural networks. However, training deep models on extensive datasets is computationally demanding, incurring significant financial costs and generating substantial CO2 emissions [63, 59 ]. Bhavya et al. [ 2] overview several research trajectories aimed at enhancing model convergence and reducing training time and costs, including data subset selection, curriculum learning, model architecture improvements, and optimization algorithm enhancements. Our work specifically focuses on selecting useful, generalizable data subsets for efficient deep neural network training. Recent research [66 , 5 , 21 ] suggests that many current training datasets are redundant, indicating that a non-redundant, informative subset could achieve comparable performance to training on the full dataset. To identify such informative samples, metrics such as prediction uncertainty [ 7], prediction flips [ 66 , 72 ], loss [42 ], or gradient/gradient-norm [ 21, 47 , 28, 27 , 29 ] have been applied. These metrics, calculated using either the downstream machine learning model or a lightweight surrogate [ 7], require a fully converged model, a requirement that runs counter to the goal of efficient training. To address this, existing subset selection algorithms [ 47 , 28 , 27, 29, 30 , 54 ] for efficient learning utilize the downstream model during training for heuristic computation and periodically update the subset as the model-dependent metrics for each sample evolve. Drawbacks of Model-Dependent Subset Selection: Despite the theoretical advantages of existing subset selection strategies[ 28 , 27, 47 ], they often fall short in computational efficiency compared to adaptive random subset selection, which selects random subsets periodically. This is primarily because traditional approaches rely on downstream models and typically require the calculation of sample metrics, such as gradients, before each subset selection step. Furthermore, these computationally demanding subset selection steps occur during model training. For instance, Figure 1 compares the convergence rate of the ResNet18 model on the CIFAR100 dataset, in terms of both time and epochs. Here, the model uses 10% subsets chosen every epoch by GradMatchPB [ 27 ], a state-of-the-art data subset selection strategy for efficient training, CraigPB [ 47 ], and Adaptive-Random (where a new 10% subset is randomly selected periodically). The selection of a new subset every epoch is done to demonstrate the maximum performance attainable by GradMatchPB and CraigPB. The results indicate that GradMatchPB achieves faster epoch convergence than both Adaptive-Random and CraigPB when selecting a new subset each epoch. However, due to the necessity for a computationally intensive subset selection step every epoch, both GradMatchPB and CraigPB demonstrate considerable inefficiency in terms of training time. In their respective studies, Killamsetty et al. [27 ] and Mirzasoleiman et al. [47] suggested selecting a new subset every R epochs to enhance training efficiency. However, this comes at the cost of the model’s convergence rate. Lastly, model-dependent subset selection requires a computationally intensive subset selection steps each time a new model is trained. Model-Agnostic Selection: Our primary insight is that a model-agnostic subset selection framework can circumvent the computationally intensive subset selection steps during model training by conducting subset selection in the preprocessing phase. By pre-selecting subsets and storing them as metadata with each dataset, we can train numerous models without incurring additional costs, effectively spreading out the expense of subset selection. In this work, we endeavor to answer the following question: Is it possible to develop a model-agnostic subset selection method that selects new subsets in a minimal amount of time, yet achieves superior model convergence without significant compromise in test accuracy or generalization performance?  MILO Framework: In this study, we introduce MILO, a model-agnostic subset selection framework designed for efficient model training and tuning. MILO employs submodular measures [ 13 , 23 ], which capture higher-order interactions between data samples for subset selection. We utilize pre-trained large language models [ 55 ] and pre-trained vision transformers [25 ] as feature encoders due to their zero-shot feature encoding capabilities. These encoders compute the sample metrics in a nominal amount of time, and this computation is independent of the downstream model, rendering MILO model-agnostic. Figure 3 provides a visual representation of MILO for model training, comprising two steps: a) A pre-processing step that involves the selection of multiple subsets from the training dataset using "Stochastic-Greedy Exploration (SGE)" (refer to Section 3.1.1), and the construction of a probability distribution over the entire dataset for subset sampling through "Weighted Random Exploration (WRE)" (refer to Section 3.1.2); b) A model training scheme that involves training the model on a curriculum of easy-to-hard subsets (refer to Section 3.1.3) using the subsets selected and the probability distribution constructed in the pre-processing step. We also present a class-wise partitioning trick to significantly minimize the memory footprint of MILO (refer to Section 3.2). Effectiveness of MILO : Through extensive experiments on multiple real-world datasets, we empirically demonstrate the effectiveness of the MILO framework for efficient training and hyperparameter tuning. In Figure 2, we provide a summary of the speedup achieved by MILO compared to full data training and tuning, along with the corresponding relative performance. Our results show that MILO can train models 3x to 10x faster and tune hyperparameters 20x to 75x faster, with minimal loss in performance. Furthermore, we consistently outperform the subset selection baselines that were considered. Additionally, MILO exhibits significantly faster initial model convergence and maintains faster convergence throughout training compared to other baselines. These qualities make it highly effective for applications such as hyperparameter tuning and neural architecture search, where the ability to distinguish good models from bad models early in the tuning process is crucial for enhancing efficiency [38, 37]. Related Work: In recent years, data-subset selection strategies have achieved success in various machine learning applications such as speech recognition [ 68 , 67], machine translation [ 32 ], active learning [ 60 , 1 , 33 ], hyper-parameter tuning [ 30 ], continual learning [65 ], domain adaptation [20 ], and computer vision [22]. A recent empirical study [5] demonstrated that datasets frequently contain semantic redundancies. These redundancies can be removed during training without affecting performance. In response, data pruning methods [66 , 51, 62] have developed principled selection criteria for pruning redundant samples before model training, with minimal loss in performance. However, both existing data pruning and compute-efficient learning approaches are model-dependent, resulting in them being computationally expensive. In contrast, our method is model-agnostic and capable of identifying high-quality subsets by intelligent data exploration. Furthermore, our approach can achieve superior model convergence by utilizing an easy-to-hard curriculum, transitioning from representative to diverse subsets during training.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2501.04155	MM-GEN: Enhancing Task Performance Through Targeted Multimodal Data Curation	Vision-language models (VLMs) are highly effective but often underperform on specialized tasks, for example Llava1.5 struggles on chart and diagram understanding, due to scarce task-specific training data. Existing training data, sourced from general-purpose datasets, fails to capture the nuanced details needed for these tasks. We introduce MMGEN, a scalable method that generates task-specific, highquality synthetic text for candidate images by leveraging stronger models. MM-GEN employs a three-stage targeted process: partitioning data into subgroups, generating targeted text based on task descriptions, and filtering out redundant and outlier data. Fine-tuning VLMs with data generated by MM-GEN leads to significant performance gains, including 29% on spatial reasoning and 15% on diagram understanding for Llava-1.5 (7B). Compared to human-curated caption data, MM-GEN achieves up to 1.6× better improvements for the original models, proving its effectiveness in enhancing task-specific VLM performance and bridging the gap between general-purpose datasets and specialized requirements. Code available at https://github.com/sjoshi804/MM-Gen.	While vision-language models (VLMs) demonstrate stateof-the-art performance on several multi-modal tasks [29], they are often on tasks that are simpler in nature [3]. These models still struggle with more complex tasks, e.g., those that require fine-grained understanding of details in images [3, 15, 21, 49]. We posit that the primary limitation for these VLMs is the quality and nature of the training data. VLMs are typically trained on large-scale image-text data scraped from the internet; while the images are rich and informative, the associated text descriptions can (i) have limited relevance to the image [39], or (ii) omit references to several specific details captured in the image [25]. Fig. 1 shows examples of such images and web-scraped captions. While the images are relevant for the tasks of chart understanding, spatial reasoning, and diagram understanding, respectively, the text fails to capture details essential for these tasks. While synthetic caption generation strategies proposed in prior work [25, 39, 57] can create more descriptive text annotations (by referring to more visual details), they are agnostic of the downstream target task. Consequently, they cannot ensure that relevant details are captured in the text annotations. Recently, Shi et al. [48] manually curated a task-specific dataset aimed at the task of multimodal mathematical question-answering by augmenting existing imagetext data with detailed textual annotations, based on their domain expertise, using strong VLMs. While effective, such a curation pipeline involves significant human effort and is not scalable [33, 59]. To address these limitations, we present MM-GEN, a highly general framework for automatically synthesizing taskrelevant text annotations for images by leveraging stronger VLMs (i.e., VLMs that perform well on the specific task) and requiring minimal human effort. MM-GEN takes as input a small set of examples from the target task (henceforth referred to as "reference samples"), a list of image types associated with the task, and a pool of task-relevant candidate images for training. In practice, these inputs can be easily obtained: a small number of reference samples and associated image types can be directly collected from the target task, and a task-relevant image pool can be found via image search with search engines or retrieved from largescale image-caption datasets [5, 46, 47]. Using the reference samples to specify the details of the task to the stronger VLM, MM-GEN generates text-annotations that are taskrelevant for the candidate images. Fig. 1 shows how the text generated by MM-GEN captures task-relevant details. This simple approach is highly effective, resulting in significant improvements across a variety of target tasks. Moreover, human-effort in this process is limited to providing (i) a small set of reference samples for the task, and (ii) a pool of candidate images. To further improve the quality and efficacy of the generated data, MM-GEN introduces a perplexity [4] based data-filtering approach to improve performance on target task using a high-value subset of the synthesized data. The components in MM-GEN are general and applicable to any image-text based target task enabling it to easily generalize across tasks and scale. We evaluate MM-GEN on improving VLMs' (e.g., Llava1.5 7B and 13B parameter versions) performance on finegrained image understanding tasks — chart understanding and reasoning, diagram understanding, and spatial reasoning on maps. The data curated by MM-GEN enables an absolute improvement over Llava-1.5 (7B) of 15%, 14% and 29%, respectively. We also see improvements over the much larger Llava-1.5 (13B) of 13%, , respectively. Moreover, MM-GEN’s filtering strategy helps in shrinking data volumes by up to 50% with no / minimal loss in performance. Empirical results show that models trained via MM-GEN data have a better performance than those trained via generated generic captions. MM-GEN data is also more effective than text annotations generated without task-specific reference examples, showing the importance of a targeted, data-centric approach for describing tasks. Finally, we analyze the effects of key design choices in MMGEN through ablation studies on e.g. size of the reference sample set, generating with / without partitioning, scaling number of in-context samples.	https://arxiv.org/pdf/2103.00128	PRISM: A Rich Class of Parameterized Submodular Information Measures for Guided Subset Selection	With ever-increasing dataset sizes, subset selection techniques are becoming increasingly important for a plethora of tasks. It is often necessary to guide the subset selection to achieve certain desiderata, which includes focusing or targeting certain data points, while avoiding others. Examples of such problems include: i) targeted learning, where the goal is to find subsets with rare classes or rare attributes on which the model is underperforming, and ii) guided summarization, where data (e.g., image collection, text, document or video) is summarized for quicker human consumption with specific additional user intent. Motivated by such applications, we present PRISM, a rich class of PaRameterIzed Submodular information Measures. Through novel functions and their parameterizations, PRISM offers a variety of modeling capabilities that enable a trade-off between desired qualities of a subset like diversity or representation and similarity/dissimilarity with a set of data points. We demonstrate how PRISM can be applied to the two real-world problems mentioned above, which require guided subset selection. In doing so, we show that PRISM interestingly generalizes some past work, therein reinforcing its broad utility. Through extensive experiments on diverse datasets, we demonstrate the superiority of PRISM over the state-of-the-art in targeted learning and in guided imagecollection summarization. PRISM is available as a part of the SUBMODLIB (https://github.com/decile-team/submodlib) and TRUST (https://github.com/decile-team/trust) toolkits.	Recent times have seen explosive growth in data across several modalities, including text, images, and videos. This has given rise to the need for finding techniques for selecting effective smaller data subsets with specific characteristics for a variety of down-stream tasks. Often, we would like to guide the data selection to either target or avoid a certain set of data slices. One application is, what we call, targeted learning, where the goal is to select data points similar to data slices on which the model is currently performing poorly. These slices are data points that either belong to rare classes or have common rare attributes (e.g., color, background, etc.). An example of such a scenario is shown in Fig. 1(a), where a self-driving car model struggles in detecting “cars in a dark background“ because of a lack of such images in the training set. The targeted learning problem is to augment the training dataset with more of such rare images, with an aim to improve model performance. Another example is detecting cancers in biomedical imaging datasets, where the number of cancerous images are often a small fraction of the non-cancerous images. Another application comes from the summarization task, where an image collection, a video, or a text document is summarized for quicker human consumption by eliminating redundancy, while preserving the main content. While a number of applications require generic summarization (i.e., simply picking a representative and diverse subset of the massive dataset), it is often important to capture certain user intent in summarization. We call this guided summarization. Examples of guided summarization include: (i) query-focused summarization (Sharghi, Gong, and Shah 2016; Xiao et al. 2020), where a summary similar to a specific query is desired, and (ii) privacy-preserving summarization, where a summary dissimilar to a given private set of data points is desired (say, for privacy issues). See Fig. 1(b) for a pictorial illustration. PRISM Framework: We define PRISM through different instantiations and parameterizations of various submodular information measures (Sec. 2). These allow for modeling a spectrum of semantics required for guided subset selection, like relevance to a query set, irrelevance to a private set, and diversity among selected data points. We study the effect of parameter trade-off among these different semantics and present interesting insights. PRISM for Targeted Learning: We present a novel algorithm (Sec. 3.1, Algo. 1) to apply PRISM for targeted learning, which aims to improve a model’s performance on rare slices of data. Specifically, we show that submodular information measures are very effective in finding the examples from the rare classes in a large unlabeled set (akin to finding a needle in a haystack). On several image classification tasks, PRISM obtains ≈ 20-30% gain in accuracy of rare classes (≈ 12% more than existing approaches) by just adding a few additional labeled points from the rare classes. Furthermore, we show that PRISM is 20× to 50× more label-efficient compared to random sampling, and 2× to 4× more labelefficient compared to existing approaches (see Sec. 4.1). We also show that Algo. 1 generalizes some existing approaches for data subset selection, reinforcing its utility (Sec. 3.3). PRISM for Guided Summarization. We propose a learning framework for guided summarization using PRISM (Sec. 3.2). We show that PRISM offers a unified treatment to the different flavors of guided summarization (query-focused and privacy-preserving) and generalizes some existing approaches to summarization, again reinforcing its utility. We show that it outperforms other existing approaches on a real-world image collections dataset (Sec. 4.2).	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2501.04155	MM-GEN: Enhancing Task Performance Through Targeted Multimodal Data Curation	Vision-language models (VLMs) are highly effective but often underperform on specialized tasks, for example Llava1.5 struggles on chart and diagram understanding, due to scarce task-specific training data. Existing training data, sourced from general-purpose datasets, fails to capture the nuanced details needed for these tasks. We introduce MMGEN, a scalable method that generates task-specific, highquality synthetic text for candidate images by leveraging stronger models. MM-GEN employs a three-stage targeted process: partitioning data into subgroups, generating targeted text based on task descriptions, and filtering out redundant and outlier data. Fine-tuning VLMs with data generated by MM-GEN leads to significant performance gains, including 29% on spatial reasoning and 15% on diagram understanding for Llava-1.5 (7B). Compared to human-curated caption data, MM-GEN achieves up to 1.6× better improvements for the original models, proving its effectiveness in enhancing task-specific VLM performance and bridging the gap between general-purpose datasets and specialized requirements. Code available at https://github.com/sjoshi804/MM-Gen.	While vision-language models (VLMs) demonstrate stateof-the-art performance on several multi-modal tasks [29], they are often on tasks that are simpler in nature [3]. These models still struggle with more complex tasks, e.g., those that require fine-grained understanding of details in images [3, 15, 21, 49]. We posit that the primary limitation for these VLMs is the quality and nature of the training data. VLMs are typically trained on large-scale image-text data scraped from the internet; while the images are rich and informative, the associated text descriptions can (i) have limited relevance to the image [39], or (ii) omit references to several specific details captured in the image [25]. Fig. 1 shows examples of such images and web-scraped captions. While the images are relevant for the tasks of chart understanding, spatial reasoning, and diagram understanding, respectively, the text fails to capture details essential for these tasks. While synthetic caption generation strategies proposed in prior work [25, 39, 57] can create more descriptive text annotations (by referring to more visual details), they are agnostic of the downstream target task. Consequently, they cannot ensure that relevant details are captured in the text annotations. Recently, Shi et al. [48] manually curated a task-specific dataset aimed at the task of multimodal mathematical question-answering by augmenting existing imagetext data with detailed textual annotations, based on their domain expertise, using strong VLMs. While effective, such a curation pipeline involves significant human effort and is not scalable [33, 59]. To address these limitations, we present MM-GEN, a highly general framework for automatically synthesizing taskrelevant text annotations for images by leveraging stronger VLMs (i.e., VLMs that perform well on the specific task) and requiring minimal human effort. MM-GEN takes as input a small set of examples from the target task (henceforth referred to as "reference samples"), a list of image types associated with the task, and a pool of task-relevant candidate images for training. In practice, these inputs can be easily obtained: a small number of reference samples and associated image types can be directly collected from the target task, and a task-relevant image pool can be found via image search with search engines or retrieved from largescale image-caption datasets [5, 46, 47]. Using the reference samples to specify the details of the task to the stronger VLM, MM-GEN generates text-annotations that are taskrelevant for the candidate images. Fig. 1 shows how the text generated by MM-GEN captures task-relevant details. This simple approach is highly effective, resulting in significant improvements across a variety of target tasks. Moreover, human-effort in this process is limited to providing (i) a small set of reference samples for the task, and (ii) a pool of candidate images. To further improve the quality and efficacy of the generated data, MM-GEN introduces a perplexity [4] based data-filtering approach to improve performance on target task using a high-value subset of the synthesized data. The components in MM-GEN are general and applicable to any image-text based target task enabling it to easily generalize across tasks and scale. We evaluate MM-GEN on improving VLMs' (e.g., Llava1.5 7B and 13B parameter versions) performance on finegrained image understanding tasks — chart understanding and reasoning, diagram understanding, and spatial reasoning on maps. The data curated by MM-GEN enables an absolute improvement over Llava-1.5 (7B) of 15%, 14% and 29%, respectively. We also see improvements over the much larger Llava-1.5 (13B) of 13%, , respectively. Moreover, MM-GEN’s filtering strategy helps in shrinking data volumes by up to 50% with no / minimal loss in performance. Empirical results show that models trained via MM-GEN data have a better performance than those trained via generated generic captions. MM-GEN data is also more effective than text annotations generated without task-specific reference examples, showing the importance of a targeted, data-centric approach for describing tasks. Finally, we analyze the effects of key design choices in MMGEN through ablation studies on e.g. size of the reference sample set, generating with / without partitioning, scaling number of in-context samples.	https://aclanthology.org/2024.findings-naacl.209.pdf	An End-to-End Submodular Framework for Data-Efficient In-Context Learning	Recent advancements in natural language tasks leverage the emergent In-Context Learning (ICL) ability of pretrained Large Language Models (LLMs). ICL enables LLMs to perform new tasks by utilizing a limited number of input-output examples as prompts. While ICL circumvents the costly step of finetuning LLMs, its effectiveness is heavily dependent on the quality and ordering of provided examples (called exemplars). In this work, we propose a two-stage data-efficient framework Div-S3 for exemplar selection for ICL. The first stage focuses on data annotation and employs a poolbased active learning approach to select a set of Diverse and informative exemplars from the target tasks’ unlabeled pool. Given a test input/query, the second stage uses Submodular Span Summarization (S3) to select the most relevant and non-redundant exemplars from the annotated pool of a limited budget. On 7 different NLP datasets and 5 LLMs of varying complexities, we show Div-S3 outperforms (1) existing active learning-based methods for data annotation for ICL and (2) similarity-based methods for test query-specific exemplars retrieval.	Pretrained large language models (LLMs) (Kenton and Toutanova, 2019; Brown et al., 2020; Chowdhery et al., 2022) have become foundational for a wide range of Natural Language Processing (NLP) tasks, demonstrating impressive success across various domains (Bommasani et al., 2021; Bubeck et al., 2023) through in-context learning (ICL) (Dong et al., 2022). ICL enables these pretrained LLMs to perform new tasks by using task-specific prompts containing a limited number of input-output demonstrations (also referred to as shots, exemplars, or prompts) in the natural language format. This approach facilitates deployment across different downstream tasks and reduces the need for labeled downstream training data since ICL does not require any task-specific training. The typical ICL procedure consists of two key components: (1) Exemplar annotation and retrieval (Wu et al., 2022; Köksal et al., 2022; Liu et al., 2022): This step involves annotating and retrieving exemplars that serve as context demonstrations. (2) Prompt template crafting (Sorensen et al., 2022; Deng et al., 2022): this step involves designing a prompt template to wrap these demonstrations in a comprehensible and coherent natural language instruction. Recent studies (Liu et al., 2022; Su et al., 2022; Margatina et al., 2023) show that providing exemplars most relevant to the current input instance is beneficial. Moreover, Zhao et al. (2021), Lu et al. (2022), and Liu et al. (2023) observe that LLMs attend more to the exemplars that are closer in the sequence to the input instance. Therefore, to achieve the best performance of ICL, the selection of exemplars and their ordering in the LLM prompt are crucial. In practice, an extensive collection of unlabeled exemplars is easily available (e.g., posts and discussions on forums like Stack Exchange or user-generated content on social media platforms), but manually annotating all exemplars would be exceptionally costly. To annotate and select the exemplars optimally for a given target task, we follow the two-stage approach shown in Figure 1: (1) Exemplar Annotation: select a subset of exemplars for annotation under a fixed budget (performed only once) and (2) Exemplar Retrieval: identify limited-sized exemplars in an ordering that are most influential for a given input instance from the annotated subset of exemplars. Intuitively, for the first stage, we aim to find the subset with maximal diversity and least redundancy so that, given any input, we can find corresponding labeled exemplars. For the second stage, in addition to the diversity requirement similar to the first stage, we emphasize the relevance of the exemplars to the given input query and order exemplars so that their relevance to the input query decreases as the exemplars are farther away from the input instance. We propose a framework Div-S3 based on submodular optimization that unifies the abovementioned two stages. For Exemplar Annotation, we model the problem as a submodular optimization problem under a cardinality constraint to find as Diverse a subset as possible within a budget. For Exemplar Retrieval, we formalize the problem as a Submodular Span Summarization (S3) problem (Kumari and Bilmes, 2021) with a knapsack constraint, which finds a diverse subset most relevant to the input query under a token length limit. Also, we naturally order the resulting exemplars based on the gains represented by the submodular function. The name of our proposed framework Div-S3 captures the optimization objectives used for both exemplar annotation (Div) and exemplar retrieval (S3) stages. In Fig. 2, we show a sample test query where using Div-S3 for exemplar selection leads to a more diverse and query-relevant exemplar set (more examples provided in Appendix D). Our framework is general, as any submodular function can be plugged into our method. For models beyond LMs, e.g., for text-image multi-modality models, we may use pre-existing submodular functions that are powerful for expressing diversity in the image domain. In addition, we account for relevance, diversity, and ordering for the exemplar retrieval stage, where one or two aspects typically get overlooked by previous methods. Empirically, we evaluate Div-S3 on 7 NLP tasks with 5 LLMs and show significantly improved performance compared to baselines. Our contributions are: 1. We propose an end-to-end framework DivS3 utilizing submodular optimization for performing data-efficient ICL using LLMs. Depending on budget requirements, Div-S3 provides the flexibility to set the budget either in terms of the number of exemplars to be used in the prompt or the LLM’s context window size. 2. We empirically validate the effectiveness of our framework on 7 different NLP tasks and show the transferability of results across LLMs of varying complexities. 3. We thoroughly analyze each component of Div-S3 by (a) studying S3 in a setting with no annotation budget constraint and (b) analyzing the sensitivity of the exemplars selected by Div-S3 to their position in the LLM’s prompt.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/1708.00489	Active Learning for Convolutional Neural Networks: A Core-Set Approach	Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (i.e. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.	Deep convolutional neural networks (CNNs) have shown unprecedented success in many areas of research in computer vision and pattern recognition, such as image classification, object detection, and scene segmentation. Although CNNs are universally successful in many tasks, they have a major drawback; they need a very large amount of labeled data to be able to learn their large number of parameters. More importantly, it is almost always better to have more data since the accuracy of CNNs is often not saturated with increasing dataset size. Hence, there is a constant desire to collect more and more data. Although this a desired behavior from an algorithmic perspective (higher representative power is typically better), labeling a dataset is a time consuming and an expensive task. These practical considerations raise a critical question: "what is the optimal way to choose data points to label such that the highest accuracy can be obtained given a fixed labeling budget." Active learning is one of the common paradigms to address this question. The goal of active learning is to find effective ways to choose data points to label, from a pool of unlabeled data points, in order to maximize the accuracy. Although it is not possible to obtain a universally good active learning strategy (Dasgupta, 2004), there exist many heuristics (Settles, 2010) which have been proven to be effective in practice. Active learning is typically an iterative process in which a model is learned at each iteration and a set of points is chosen to be labelled from a pool of unlabelled points using these aforementioned heuristics. We experiment with many of these heuristics in this paper and find them not effective when applied to CNNs. We argue that the main factor behind this ineffectiveness is the correlation caused via batch acquisition/sampling. In the classical setting, the active learning algorithms typically choose a single point at each iteration; however, this is not feasible for CNNs since i) a single point is likely to have no statistically significant impact on the accuracy due to the local optimization methods, and ii) each iteration requires a full training until convergence which makes it intractable to query labels one-by-one. Hence, it is necessary to query labels for a large subset at each iteration and it results in correlated samples even for moderately small subset sizes. In order to tailor an active learning method for the batch sampling case, we decided to define the active learning as core-set selection problem. Core-set selection problem aims to find a small subset given a large labeled dataset such that a model learned over the small subset is competitive over the whole dataset. Since we have no labels available, we perform the core-set selection without using the labels. In order to attack the unlabeled core-set problem for CNNs, we provide a rigorous bound between an average loss over any given subset of the dataset and the remaining data points via the geometry of the data points. As an active learning algorithm, we try to choose a subset such that this bound is minimized. Moreover, minimization of this bound turns out to be equivalent to the k-Center problem (Wolf, 2011) and we adopt an efficient approximate solution to this combinatorial optimization problem. We further study the behavior of our proposed algorithm empirically for the problem of image classification using three different datasets. Our empirical analysis demonstrates state-of-the-art performance by a large margin.	https://arxiv.org/pdf/2301.13287	MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning	Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results indicate that MILO can train models 3 x -10x faster and tune hyperparameters 20 x -75x faster than full-dataset training or tuning without compromising performance.	Deep learning has achieved remarkable success in a multitude of machine learning tasks, including natural language processing, computer vision, and speech recognition in recent years. This success is partially due to the availability of massive training datasets and the capacity to train large-scale neural networks. However, training deep models on extensive datasets is computationally demanding, incurring significant financial costs and generating substantial CO2 emissions [63, 59 ]. Bhavya et al. [ 2] overview several research trajectories aimed at enhancing model convergence and reducing training time and costs, including data subset selection, curriculum learning, model architecture improvements, and optimization algorithm enhancements. Our work specifically focuses on selecting useful, generalizable data subsets for efficient deep neural network training. Recent research [66 , 5 , 21 ] suggests that many current training datasets are redundant, indicating that a non-redundant, informative subset could achieve comparable performance to training on the full dataset. To identify such informative samples, metrics such as prediction uncertainty [ 7], prediction flips [ 66 , 72 ], loss [42 ], or gradient/gradient-norm [ 21, 47 , 28, 27 , 29 ] have been applied. These metrics, calculated using either the downstream machine learning model or a lightweight surrogate [ 7], require a fully converged model, a requirement that runs counter to the goal of efficient training. To address this, existing subset selection algorithms [ 47 , 28 , 27, 29, 30 , 54 ] for efficient learning utilize the downstream model during training for heuristic computation and periodically update the subset as the model-dependent metrics for each sample evolve. Drawbacks of Model-Dependent Subset Selection: Despite the theoretical advantages of existing subset selection strategies[ 28 , 27, 47 ], they often fall short in computational efficiency compared to adaptive random subset selection, which selects random subsets periodically. This is primarily because traditional approaches rely on downstream models and typically require the calculation of sample metrics, such as gradients, before each subset selection step. Furthermore, these computationally demanding subset selection steps occur during model training. For instance, Figure 1 compares the convergence rate of the ResNet18 model on the CIFAR100 dataset, in terms of both time and epochs. Here, the model uses 10% subsets chosen every epoch by GradMatchPB [ 27 ], a state-of-the-art data subset selection strategy for efficient training, CraigPB [ 47 ], and Adaptive-Random (where a new 10% subset is randomly selected periodically). The selection of a new subset every epoch is done to demonstrate the maximum performance attainable by GradMatchPB and CraigPB. The results indicate that GradMatchPB achieves faster epoch convergence than both Adaptive-Random and CraigPB when selecting a new subset each epoch. However, due to the necessity for a computationally intensive subset selection step every epoch, both GradMatchPB and CraigPB demonstrate considerable inefficiency in terms of training time. In their respective studies, Killamsetty et al. [27 ] and Mirzasoleiman et al. [47] suggested selecting a new subset every R epochs to enhance training efficiency. However, this comes at the cost of the model’s convergence rate. Lastly, model-dependent subset selection requires a computationally intensive subset selection steps each time a new model is trained. Model-Agnostic Selection: Our primary insight is that a model-agnostic subset selection framework can circumvent the computationally intensive subset selection steps during model training by conducting subset selection in the preprocessing phase. By pre-selecting subsets and storing them as metadata with each dataset, we can train numerous models without incurring additional costs, effectively spreading out the expense of subset selection. In this work, we endeavor to answer the following question: Is it possible to develop a model-agnostic subset selection method that selects new subsets in a minimal amount of time, yet achieves superior model convergence without significant compromise in test accuracy or generalization performance?  MILO Framework: In this study, we introduce MILO, a model-agnostic subset selection framework designed for efficient model training and tuning. MILO employs submodular measures [ 13 , 23 ], which capture higher-order interactions between data samples for subset selection. We utilize pre-trained large language models [ 55 ] and pre-trained vision transformers [25 ] as feature encoders due to their zero-shot feature encoding capabilities. These encoders compute the sample metrics in a nominal amount of time, and this computation is independent of the downstream model, rendering MILO model-agnostic. Figure 3 provides a visual representation of MILO for model training, comprising two steps: a) A pre-processing step that involves the selection of multiple subsets from the training dataset using "Stochastic-Greedy Exploration (SGE)" (refer to Section 3.1.1), and the construction of a probability distribution over the entire dataset for subset sampling through "Weighted Random Exploration (WRE)" (refer to Section 3.1.2); b) A model training scheme that involves training the model on a curriculum of easy-to-hard subsets (refer to Section 3.1.3) using the subsets selected and the probability distribution constructed in the pre-processing step. We also present a class-wise partitioning trick to significantly minimize the memory footprint of MILO (refer to Section 3.2). Effectiveness of MILO : Through extensive experiments on multiple real-world datasets, we empirically demonstrate the effectiveness of the MILO framework for efficient training and hyperparameter tuning. In Figure 2, we provide a summary of the speedup achieved by MILO compared to full data training and tuning, along with the corresponding relative performance. Our results show that MILO can train models 3x to 10x faster and tune hyperparameters 20x to 75x faster, with minimal loss in performance. Furthermore, we consistently outperform the subset selection baselines that were considered. Additionally, MILO exhibits significantly faster initial model convergence and maintains faster convergence throughout training compared to other baselines. These qualities make it highly effective for applications such as hyperparameter tuning and neural architecture search, where the ability to distinguish good models from bad models early in the tuning process is crucial for enhancing efficiency [38, 37]. Related Work: In recent years, data-subset selection strategies have achieved success in various machine learning applications such as speech recognition [ 68 , 67], machine translation [ 32 ], active learning [ 60 , 1 , 33 ], hyper-parameter tuning [ 30 ], continual learning [65 ], domain adaptation [20 ], and computer vision [22]. A recent empirical study [5] demonstrated that datasets frequently contain semantic redundancies. These redundancies can be removed during training without affecting performance. In response, data pruning methods [66 , 51, 62] have developed principled selection criteria for pruning redundant samples before model training, with minimal loss in performance. However, both existing data pruning and compute-efficient learning approaches are model-dependent, resulting in them being computationally expensive. In contrast, our method is model-agnostic and capable of identifying high-quality subsets by intelligent data exploration. Furthermore, our approach can achieve superior model convergence by utilizing an easy-to-hard curriculum, transitioning from representative to diverse subsets during training.	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/1708.00489	Active Learning for Convolutional Neural Networks: A Core-Set Approach	Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (i.e. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.	Deep convolutional neural networks (CNNs) have shown unprecedented success in many areas of research in computer vision and pattern recognition, such as image classification, object detection, and scene segmentation. Although CNNs are universally successful in many tasks, they have a major drawback; they need a very large amount of labeled data to be able to learn their large number of parameters. More importantly, it is almost always better to have more data since the accuracy of CNNs is often not saturated with increasing dataset size. Hence, there is a constant desire to collect more and more data. Although this a desired behavior from an algorithmic perspective (higher representative power is typically better), labeling a dataset is a time consuming and an expensive task. These practical considerations raise a critical question: "what is the optimal way to choose data points to label such that the highest accuracy can be obtained given a fixed labeling budget." Active learning is one of the common paradigms to address this question. The goal of active learning is to find effective ways to choose data points to label, from a pool of unlabeled data points, in order to maximize the accuracy. Although it is not possible to obtain a universally good active learning strategy (Dasgupta, 2004), there exist many heuristics (Settles, 2010) which have been proven to be effective in practice. Active learning is typically an iterative process in which a model is learned at each iteration and a set of points is chosen to be labelled from a pool of unlabelled points using these aforementioned heuristics. We experiment with many of these heuristics in this paper and find them not effective when applied to CNNs. We argue that the main factor behind this ineffectiveness is the correlation caused via batch acquisition/sampling. In the classical setting, the active learning algorithms typically choose a single point at each iteration; however, this is not feasible for CNNs since i) a single point is likely to have no statistically significant impact on the accuracy due to the local optimization methods, and ii) each iteration requires a full training until convergence which makes it intractable to query labels one-by-one. Hence, it is necessary to query labels for a large subset at each iteration and it results in correlated samples even for moderately small subset sizes. In order to tailor an active learning method for the batch sampling case, we decided to define the active learning as core-set selection problem. Core-set selection problem aims to find a small subset given a large labeled dataset such that a model learned over the small subset is competitive over the whole dataset. Since we have no labels available, we perform the core-set selection without using the labels. In order to attack the unlabeled core-set problem for CNNs, we provide a rigorous bound between an average loss over any given subset of the dataset and the remaining data points via the geometry of the data points. As an active learning algorithm, we try to choose a subset such that this bound is minimized. Moreover, minimization of this bound turns out to be equivalent to the k-Center problem (Wolf, 2011) and we adopt an efficient approximate solution to this combinatorial optimization problem. We further study the behavior of our proposed algorithm empirically for the problem of image classification using three different datasets. Our empirical analysis demonstrates state-of-the-art performance by a large margin.	https://arxiv.org/pdf/2103.00128	PRISM: A Rich Class of Parameterized Submodular Information Measures for Guided Subset Selection	With ever-increasing dataset sizes, subset selection techniques are becoming increasingly important for a plethora of tasks. It is often necessary to guide the subset selection to achieve certain desiderata, which includes focusing or targeting certain data points, while avoiding others. Examples of such problems include: i) targeted learning, where the goal is to find subsets with rare classes or rare attributes on which the model is underperforming, and ii) guided summarization, where data (e.g., image collection, text, document or video) is summarized for quicker human consumption with specific additional user intent. Motivated by such applications, we present PRISM, a rich class of PaRameterIzed Submodular information Measures. Through novel functions and their parameterizations, PRISM offers a variety of modeling capabilities that enable a trade-off between desired qualities of a subset like diversity or representation and similarity/dissimilarity with a set of data points. We demonstrate how PRISM can be applied to the two real-world problems mentioned above, which require guided subset selection. In doing so, we show that PRISM interestingly generalizes some past work, therein reinforcing its broad utility. Through extensive experiments on diverse datasets, we demonstrate the superiority of PRISM over the state-of-the-art in targeted learning and in guided imagecollection summarization. PRISM is available as a part of the SUBMODLIB (https://github.com/decile-team/submodlib) and TRUST (https://github.com/decile-team/trust) toolkits.	Recent times have seen explosive growth in data across several modalities, including text, images, and videos. This has given rise to the need for finding techniques for selecting effective smaller data subsets with specific characteristics for a variety of down-stream tasks. Often, we would like to guide the data selection to either target or avoid a certain set of data slices. One application is, what we call, targeted learning, where the goal is to select data points similar to data slices on which the model is currently performing poorly. These slices are data points that either belong to rare classes or have common rare attributes (e.g., color, background, etc.). An example of such a scenario is shown in Fig. 1(a), where a self-driving car model struggles in detecting “cars in a dark background“ because of a lack of such images in the training set. The targeted learning problem is to augment the training dataset with more of such rare images, with an aim to improve model performance. Another example is detecting cancers in biomedical imaging datasets, where the number of cancerous images are often a small fraction of the non-cancerous images. Another application comes from the summarization task, where an image collection, a video, or a text document is summarized for quicker human consumption by eliminating redundancy, while preserving the main content. While a number of applications require generic summarization (i.e., simply picking a representative and diverse subset of the massive dataset), it is often important to capture certain user intent in summarization. We call this guided summarization. Examples of guided summarization include: (i) query-focused summarization (Sharghi, Gong, and Shah 2016; Xiao et al. 2020), where a summary similar to a specific query is desired, and (ii) privacy-preserving summarization, where a summary dissimilar to a given private set of data points is desired (say, for privacy issues). See Fig. 1(b) for a pictorial illustration. PRISM Framework: We define PRISM through different instantiations and parameterizations of various submodular information measures (Sec. 2). These allow for modeling a spectrum of semantics required for guided subset selection, like relevance to a query set, irrelevance to a private set, and diversity among selected data points. We study the effect of parameter trade-off among these different semantics and present interesting insights. PRISM for Targeted Learning: We present a novel algorithm (Sec. 3.1, Algo. 1) to apply PRISM for targeted learning, which aims to improve a model’s performance on rare slices of data. Specifically, we show that submodular information measures are very effective in finding the examples from the rare classes in a large unlabeled set (akin to finding a needle in a haystack). On several image classification tasks, PRISM obtains ≈ 20-30% gain in accuracy of rare classes (≈ 12% more than existing approaches) by just adding a few additional labeled points from the rare classes. Furthermore, we show that PRISM is 20× to 50× more label-efficient compared to random sampling, and 2× to 4× more labelefficient compared to existing approaches (see Sec. 4.1). We also show that Algo. 1 generalizes some existing approaches for data subset selection, reinforcing its utility (Sec. 3.3). PRISM for Guided Summarization. We propose a learning framework for guided summarization using PRISM (Sec. 3.2). We show that PRISM offers a unified treatment to the different flavors of guided summarization (query-focused and privacy-preserving) and generalizes some existing approaches to summarization, again reinforcing its utility. We show that it outperforms other existing approaches on a real-world image collections dataset (Sec. 4.2).	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/1708.00489	Active Learning for Convolutional Neural Networks: A Core-Set Approach	Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (i.e. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, i.e. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.	Deep convolutional neural networks (CNNs) have shown unprecedented success in many areas of research in computer vision and pattern recognition, such as image classification, object detection, and scene segmentation. Although CNNs are universally successful in many tasks, they have a major drawback; they need a very large amount of labeled data to be able to learn their large number of parameters. More importantly, it is almost always better to have more data since the accuracy of CNNs is often not saturated with increasing dataset size. Hence, there is a constant desire to collect more and more data. Although this a desired behavior from an algorithmic perspective (higher representative power is typically better), labeling a dataset is a time consuming and an expensive task. These practical considerations raise a critical question: "what is the optimal way to choose data points to label such that the highest accuracy can be obtained given a fixed labeling budget." Active learning is one of the common paradigms to address this question. The goal of active learning is to find effective ways to choose data points to label, from a pool of unlabeled data points, in order to maximize the accuracy. Although it is not possible to obtain a universally good active learning strategy (Dasgupta, 2004), there exist many heuristics (Settles, 2010) which have been proven to be effective in practice. Active learning is typically an iterative process in which a model is learned at each iteration and a set of points is chosen to be labelled from a pool of unlabelled points using these aforementioned heuristics. We experiment with many of these heuristics in this paper and find them not effective when applied to CNNs. We argue that the main factor behind this ineffectiveness is the correlation caused via batch acquisition/sampling. In the classical setting, the active learning algorithms typically choose a single point at each iteration; however, this is not feasible for CNNs since i) a single point is likely to have no statistically significant impact on the accuracy due to the local optimization methods, and ii) each iteration requires a full training until convergence which makes it intractable to query labels one-by-one. Hence, it is necessary to query labels for a large subset at each iteration and it results in correlated samples even for moderately small subset sizes. In order to tailor an active learning method for the batch sampling case, we decided to define the active learning as core-set selection problem. Core-set selection problem aims to find a small subset given a large labeled dataset such that a model learned over the small subset is competitive over the whole dataset. Since we have no labels available, we perform the core-set selection without using the labels. In order to attack the unlabeled core-set problem for CNNs, we provide a rigorous bound between an average loss over any given subset of the dataset and the remaining data points via the geometry of the data points. As an active learning algorithm, we try to choose a subset such that this bound is minimized. Moreover, minimization of this bound turns out to be equivalent to the k-Center problem (Wolf, 2011) and we adopt an efficient approximate solution to this combinatorial optimization problem. We further study the behavior of our proposed algorithm empirically for the problem of image classification using three different datasets. Our empirical analysis demonstrates state-of-the-art performance by a large margin.	https://aclanthology.org/2024.findings-naacl.209.pdf	An End-to-End Submodular Framework for Data-Efficient In-Context Learning	Recent advancements in natural language tasks leverage the emergent In-Context Learning (ICL) ability of pretrained Large Language Models (LLMs). ICL enables LLMs to perform new tasks by utilizing a limited number of input-output examples as prompts. While ICL circumvents the costly step of finetuning LLMs, its effectiveness is heavily dependent on the quality and ordering of provided examples (called exemplars). In this work, we propose a two-stage data-efficient framework Div-S3 for exemplar selection for ICL. The first stage focuses on data annotation and employs a poolbased active learning approach to select a set of Diverse and informative exemplars from the target tasks’ unlabeled pool. Given a test input/query, the second stage uses Submodular Span Summarization (S3) to select the most relevant and non-redundant exemplars from the annotated pool of a limited budget. On 7 different NLP datasets and 5 LLMs of varying complexities, we show Div-S3 outperforms (1) existing active learning-based methods for data annotation for ICL and (2) similarity-based methods for test query-specific exemplars retrieval.	Pretrained large language models (LLMs) (Kenton and Toutanova, 2019; Brown et al., 2020; Chowdhery et al., 2022) have become foundational for a wide range of Natural Language Processing (NLP) tasks, demonstrating impressive success across various domains (Bommasani et al., 2021; Bubeck et al., 2023) through in-context learning (ICL) (Dong et al., 2022). ICL enables these pretrained LLMs to perform new tasks by using task-specific prompts containing a limited number of input-output demonstrations (also referred to as shots, exemplars, or prompts) in the natural language format. This approach facilitates deployment across different downstream tasks and reduces the need for labeled downstream training data since ICL does not require any task-specific training. The typical ICL procedure consists of two key components: (1) Exemplar annotation and retrieval (Wu et al., 2022; Köksal et al., 2022; Liu et al., 2022): This step involves annotating and retrieving exemplars that serve as context demonstrations. (2) Prompt template crafting (Sorensen et al., 2022; Deng et al., 2022): this step involves designing a prompt template to wrap these demonstrations in a comprehensible and coherent natural language instruction. Recent studies (Liu et al., 2022; Su et al., 2022; Margatina et al., 2023) show that providing exemplars most relevant to the current input instance is beneficial. Moreover, Zhao et al. (2021), Lu et al. (2022), and Liu et al. (2023) observe that LLMs attend more to the exemplars that are closer in the sequence to the input instance. Therefore, to achieve the best performance of ICL, the selection of exemplars and their ordering in the LLM prompt are crucial. In practice, an extensive collection of unlabeled exemplars is easily available (e.g., posts and discussions on forums like Stack Exchange or user-generated content on social media platforms), but manually annotating all exemplars would be exceptionally costly. To annotate and select the exemplars optimally for a given target task, we follow the two-stage approach shown in Figure 1: (1) Exemplar Annotation: select a subset of exemplars for annotation under a fixed budget (performed only once) and (2) Exemplar Retrieval: identify limited-sized exemplars in an ordering that are most influential for a given input instance from the annotated subset of exemplars. Intuitively, for the first stage, we aim to find the subset with maximal diversity and least redundancy so that, given any input, we can find corresponding labeled exemplars. For the second stage, in addition to the diversity requirement similar to the first stage, we emphasize the relevance of the exemplars to the given input query and order exemplars so that their relevance to the input query decreases as the exemplars are farther away from the input instance. We propose a framework Div-S3 based on submodular optimization that unifies the abovementioned two stages. For Exemplar Annotation, we model the problem as a submodular optimization problem under a cardinality constraint to find as Diverse a subset as possible within a budget. For Exemplar Retrieval, we formalize the problem as a Submodular Span Summarization (S3) problem (Kumari and Bilmes, 2021) with a knapsack constraint, which finds a diverse subset most relevant to the input query under a token length limit. Also, we naturally order the resulting exemplars based on the gains represented by the submodular function. The name of our proposed framework Div-S3 captures the optimization objectives used for both exemplar annotation (Div) and exemplar retrieval (S3) stages. In Fig. 2, we show a sample test query where using Div-S3 for exemplar selection leads to a more diverse and query-relevant exemplar set (more examples provided in Appendix D). Our framework is general, as any submodular function can be plugged into our method. For models beyond LMs, e.g., for text-image multi-modality models, we may use pre-existing submodular functions that are powerful for expressing diversity in the image domain. In addition, we account for relevance, diversity, and ordering for the exemplar retrieval stage, where one or two aspects typically get overlooked by previous methods. Empirically, we evaluate Div-S3 on 7 NLP tasks with 5 LLMs and show significantly improved performance compared to baselines. Our contributions are: 1. We propose an end-to-end framework DivS3 utilizing submodular optimization for performing data-efficient ICL using LLMs. Depending on budget requirements, Div-S3 provides the flexibility to set the budget either in terms of the number of exemplars to be used in the prompt or the LLM’s context window size. 2. We empirically validate the effectiveness of our framework on 7 different NLP tasks and show the transferability of results across LLMs of varying complexities. 3. We thoroughly analyze each component of Div-S3 by (a) studying S3 in a setting with no annotation budget constraint and (b) analyzing the sensitivity of the exemplars selected by Div-S3 to their position in the LLM’s prompt.	1	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2301.13287	MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning	Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results indicate that MILO can train models 3 x -10x faster and tune hyperparameters 20 x -75x faster than full-dataset training or tuning without compromising performance.	Deep learning has achieved remarkable success in a multitude of machine learning tasks, including natural language processing, computer vision, and speech recognition in recent years. This success is partially due to the availability of massive training datasets and the capacity to train large-scale neural networks. However, training deep models on extensive datasets is computationally demanding, incurring significant financial costs and generating substantial CO2 emissions [63, 59 ]. Bhavya et al. [ 2] overview several research trajectories aimed at enhancing model convergence and reducing training time and costs, including data subset selection, curriculum learning, model architecture improvements, and optimization algorithm enhancements. Our work specifically focuses on selecting useful, generalizable data subsets for efficient deep neural network training. Recent research [66 , 5 , 21 ] suggests that many current training datasets are redundant, indicating that a non-redundant, informative subset could achieve comparable performance to training on the full dataset. To identify such informative samples, metrics such as prediction uncertainty [ 7], prediction flips [ 66 , 72 ], loss [42 ], or gradient/gradient-norm [ 21, 47 , 28, 27 , 29 ] have been applied. These metrics, calculated using either the downstream machine learning model or a lightweight surrogate [ 7], require a fully converged model, a requirement that runs counter to the goal of efficient training. To address this, existing subset selection algorithms [ 47 , 28 , 27, 29, 30 , 54 ] for efficient learning utilize the downstream model during training for heuristic computation and periodically update the subset as the model-dependent metrics for each sample evolve. Drawbacks of Model-Dependent Subset Selection: Despite the theoretical advantages of existing subset selection strategies[ 28 , 27, 47 ], they often fall short in computational efficiency compared to adaptive random subset selection, which selects random subsets periodically. This is primarily because traditional approaches rely on downstream models and typically require the calculation of sample metrics, such as gradients, before each subset selection step. Furthermore, these computationally demanding subset selection steps occur during model training. For instance, Figure 1 compares the convergence rate of the ResNet18 model on the CIFAR100 dataset, in terms of both time and epochs. Here, the model uses 10% subsets chosen every epoch by GradMatchPB [ 27 ], a state-of-the-art data subset selection strategy for efficient training, CraigPB [ 47 ], and Adaptive-Random (where a new 10% subset is randomly selected periodically). The selection of a new subset every epoch is done to demonstrate the maximum performance attainable by GradMatchPB and CraigPB. The results indicate that GradMatchPB achieves faster epoch convergence than both Adaptive-Random and CraigPB when selecting a new subset each epoch. However, due to the necessity for a computationally intensive subset selection step every epoch, both GradMatchPB and CraigPB demonstrate considerable inefficiency in terms of training time. In their respective studies, Killamsetty et al. [27 ] and Mirzasoleiman et al. [47] suggested selecting a new subset every R epochs to enhance training efficiency. However, this comes at the cost of the model’s convergence rate. Lastly, model-dependent subset selection requires a computationally intensive subset selection steps each time a new model is trained. Model-Agnostic Selection: Our primary insight is that a model-agnostic subset selection framework can circumvent the computationally intensive subset selection steps during model training by conducting subset selection in the preprocessing phase. By pre-selecting subsets and storing them as metadata with each dataset, we can train numerous models without incurring additional costs, effectively spreading out the expense of subset selection. In this work, we endeavor to answer the following question: Is it possible to develop a model-agnostic subset selection method that selects new subsets in a minimal amount of time, yet achieves superior model convergence without significant compromise in test accuracy or generalization performance?  MILO Framework: In this study, we introduce MILO, a model-agnostic subset selection framework designed for efficient model training and tuning. MILO employs submodular measures [ 13 , 23 ], which capture higher-order interactions between data samples for subset selection. We utilize pre-trained large language models [ 55 ] and pre-trained vision transformers [25 ] as feature encoders due to their zero-shot feature encoding capabilities. These encoders compute the sample metrics in a nominal amount of time, and this computation is independent of the downstream model, rendering MILO model-agnostic. Figure 3 provides a visual representation of MILO for model training, comprising two steps: a) A pre-processing step that involves the selection of multiple subsets from the training dataset using "Stochastic-Greedy Exploration (SGE)" (refer to Section 3.1.1), and the construction of a probability distribution over the entire dataset for subset sampling through "Weighted Random Exploration (WRE)" (refer to Section 3.1.2); b) A model training scheme that involves training the model on a curriculum of easy-to-hard subsets (refer to Section 3.1.3) using the subsets selected and the probability distribution constructed in the pre-processing step. We also present a class-wise partitioning trick to significantly minimize the memory footprint of MILO (refer to Section 3.2). Effectiveness of MILO : Through extensive experiments on multiple real-world datasets, we empirically demonstrate the effectiveness of the MILO framework for efficient training and hyperparameter tuning. In Figure 2, we provide a summary of the speedup achieved by MILO compared to full data training and tuning, along with the corresponding relative performance. Our results show that MILO can train models 3x to 10x faster and tune hyperparameters 20x to 75x faster, with minimal loss in performance. Furthermore, we consistently outperform the subset selection baselines that were considered. Additionally, MILO exhibits significantly faster initial model convergence and maintains faster convergence throughout training compared to other baselines. These qualities make it highly effective for applications such as hyperparameter tuning and neural architecture search, where the ability to distinguish good models from bad models early in the tuning process is crucial for enhancing efficiency [38, 37]. Related Work: In recent years, data-subset selection strategies have achieved success in various machine learning applications such as speech recognition [ 68 , 67], machine translation [ 32 ], active learning [ 60 , 1 , 33 ], hyper-parameter tuning [ 30 ], continual learning [65 ], domain adaptation [20 ], and computer vision [22]. A recent empirical study [5] demonstrated that datasets frequently contain semantic redundancies. These redundancies can be removed during training without affecting performance. In response, data pruning methods [66 , 51, 62] have developed principled selection criteria for pruning redundant samples before model training, with minimal loss in performance. However, both existing data pruning and compute-efficient learning approaches are model-dependent, resulting in them being computationally expensive. In contrast, our method is model-agnostic and capable of identifying high-quality subsets by intelligent data exploration. Furthermore, our approach can achieve superior model convergence by utilizing an easy-to-hard curriculum, transitioning from representative to diverse subsets during training.	https://arxiv.org/pdf/2103.00128	PRISM: A Rich Class of Parameterized Submodular Information Measures for Guided Subset Selection	With ever-increasing dataset sizes, subset selection techniques are becoming increasingly important for a plethora of tasks. It is often necessary to guide the subset selection to achieve certain desiderata, which includes focusing or targeting certain data points, while avoiding others. Examples of such problems include: i) targeted learning, where the goal is to find subsets with rare classes or rare attributes on which the model is underperforming, and ii) guided summarization, where data (e.g., image collection, text, document or video) is summarized for quicker human consumption with specific additional user intent. Motivated by such applications, we present PRISM, a rich class of PaRameterIzed Submodular information Measures. Through novel functions and their parameterizations, PRISM offers a variety of modeling capabilities that enable a trade-off between desired qualities of a subset like diversity or representation and similarity/dissimilarity with a set of data points. We demonstrate how PRISM can be applied to the two real-world problems mentioned above, which require guided subset selection. In doing so, we show that PRISM interestingly generalizes some past work, therein reinforcing its broad utility. Through extensive experiments on diverse datasets, we demonstrate the superiority of PRISM over the state-of-the-art in targeted learning and in guided imagecollection summarization. PRISM is available as a part of the SUBMODLIB (https://github.com/decile-team/submodlib) and TRUST (https://github.com/decile-team/trust) toolkits.	Recent times have seen explosive growth in data across several modalities, including text, images, and videos. This has given rise to the need for finding techniques for selecting effective smaller data subsets with specific characteristics for a variety of down-stream tasks. Often, we would like to guide the data selection to either target or avoid a certain set of data slices. One application is, what we call, targeted learning, where the goal is to select data points similar to data slices on which the model is currently performing poorly. These slices are data points that either belong to rare classes or have common rare attributes (e.g., color, background, etc.). An example of such a scenario is shown in Fig. 1(a), where a self-driving car model struggles in detecting “cars in a dark background“ because of a lack of such images in the training set. The targeted learning problem is to augment the training dataset with more of such rare images, with an aim to improve model performance. Another example is detecting cancers in biomedical imaging datasets, where the number of cancerous images are often a small fraction of the non-cancerous images. Another application comes from the summarization task, where an image collection, a video, or a text document is summarized for quicker human consumption by eliminating redundancy, while preserving the main content. While a number of applications require generic summarization (i.e., simply picking a representative and diverse subset of the massive dataset), it is often important to capture certain user intent in summarization. We call this guided summarization. Examples of guided summarization include: (i) query-focused summarization (Sharghi, Gong, and Shah 2016; Xiao et al. 2020), where a summary similar to a specific query is desired, and (ii) privacy-preserving summarization, where a summary dissimilar to a given private set of data points is desired (say, for privacy issues). See Fig. 1(b) for a pictorial illustration. PRISM Framework: We define PRISM through different instantiations and parameterizations of various submodular information measures (Sec. 2). These allow for modeling a spectrum of semantics required for guided subset selection, like relevance to a query set, irrelevance to a private set, and diversity among selected data points. We study the effect of parameter trade-off among these different semantics and present interesting insights. PRISM for Targeted Learning: We present a novel algorithm (Sec. 3.1, Algo. 1) to apply PRISM for targeted learning, which aims to improve a model’s performance on rare slices of data. Specifically, we show that submodular information measures are very effective in finding the examples from the rare classes in a large unlabeled set (akin to finding a needle in a haystack). On several image classification tasks, PRISM obtains ≈ 20-30% gain in accuracy of rare classes (≈ 12% more than existing approaches) by just adding a few additional labeled points from the rare classes. Furthermore, we show that PRISM is 20× to 50× more label-efficient compared to random sampling, and 2× to 4× more labelefficient compared to existing approaches (see Sec. 4.1). We also show that Algo. 1 generalizes some existing approaches for data subset selection, reinforcing its utility (Sec. 3.3). PRISM for Guided Summarization. We propose a learning framework for guided summarization using PRISM (Sec. 3.2). We show that PRISM offers a unified treatment to the different flavors of guided summarization (query-focused and privacy-preserving) and generalizes some existing approaches to summarization, again reinforcing its utility. We show that it outperforms other existing approaches on a real-world image collections dataset (Sec. 4.2).	0	1
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2301.13287	MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning	Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results indicate that MILO can train models 3 x -10x faster and tune hyperparameters 20 x -75x faster than full-dataset training or tuning without compromising performance.	Deep learning has achieved remarkable success in a multitude of machine learning tasks, including natural language processing, computer vision, and speech recognition in recent years. This success is partially due to the availability of massive training datasets and the capacity to train large-scale neural networks. However, training deep models on extensive datasets is computationally demanding, incurring significant financial costs and generating substantial CO2 emissions [63, 59 ]. Bhavya et al. [ 2] overview several research trajectories aimed at enhancing model convergence and reducing training time and costs, including data subset selection, curriculum learning, model architecture improvements, and optimization algorithm enhancements. Our work specifically focuses on selecting useful, generalizable data subsets for efficient deep neural network training. Recent research [66 , 5 , 21 ] suggests that many current training datasets are redundant, indicating that a non-redundant, informative subset could achieve comparable performance to training on the full dataset. To identify such informative samples, metrics such as prediction uncertainty [ 7], prediction flips [ 66 , 72 ], loss [42 ], or gradient/gradient-norm [ 21, 47 , 28, 27 , 29 ] have been applied. These metrics, calculated using either the downstream machine learning model or a lightweight surrogate [ 7], require a fully converged model, a requirement that runs counter to the goal of efficient training. To address this, existing subset selection algorithms [ 47 , 28 , 27, 29, 30 , 54 ] for efficient learning utilize the downstream model during training for heuristic computation and periodically update the subset as the model-dependent metrics for each sample evolve. Drawbacks of Model-Dependent Subset Selection: Despite the theoretical advantages of existing subset selection strategies[ 28 , 27, 47 ], they often fall short in computational efficiency compared to adaptive random subset selection, which selects random subsets periodically. This is primarily because traditional approaches rely on downstream models and typically require the calculation of sample metrics, such as gradients, before each subset selection step. Furthermore, these computationally demanding subset selection steps occur during model training. For instance, Figure 1 compares the convergence rate of the ResNet18 model on the CIFAR100 dataset, in terms of both time and epochs. Here, the model uses 10% subsets chosen every epoch by GradMatchPB [ 27 ], a state-of-the-art data subset selection strategy for efficient training, CraigPB [ 47 ], and Adaptive-Random (where a new 10% subset is randomly selected periodically). The selection of a new subset every epoch is done to demonstrate the maximum performance attainable by GradMatchPB and CraigPB. The results indicate that GradMatchPB achieves faster epoch convergence than both Adaptive-Random and CraigPB when selecting a new subset each epoch. However, due to the necessity for a computationally intensive subset selection step every epoch, both GradMatchPB and CraigPB demonstrate considerable inefficiency in terms of training time. In their respective studies, Killamsetty et al. [27 ] and Mirzasoleiman et al. [47] suggested selecting a new subset every R epochs to enhance training efficiency. However, this comes at the cost of the model’s convergence rate. Lastly, model-dependent subset selection requires a computationally intensive subset selection steps each time a new model is trained. Model-Agnostic Selection: Our primary insight is that a model-agnostic subset selection framework can circumvent the computationally intensive subset selection steps during model training by conducting subset selection in the preprocessing phase. By pre-selecting subsets and storing them as metadata with each dataset, we can train numerous models without incurring additional costs, effectively spreading out the expense of subset selection. In this work, we endeavor to answer the following question: Is it possible to develop a model-agnostic subset selection method that selects new subsets in a minimal amount of time, yet achieves superior model convergence without significant compromise in test accuracy or generalization performance?  MILO Framework: In this study, we introduce MILO, a model-agnostic subset selection framework designed for efficient model training and tuning. MILO employs submodular measures [ 13 , 23 ], which capture higher-order interactions between data samples for subset selection. We utilize pre-trained large language models [ 55 ] and pre-trained vision transformers [25 ] as feature encoders due to their zero-shot feature encoding capabilities. These encoders compute the sample metrics in a nominal amount of time, and this computation is independent of the downstream model, rendering MILO model-agnostic. Figure 3 provides a visual representation of MILO for model training, comprising two steps: a) A pre-processing step that involves the selection of multiple subsets from the training dataset using "Stochastic-Greedy Exploration (SGE)" (refer to Section 3.1.1), and the construction of a probability distribution over the entire dataset for subset sampling through "Weighted Random Exploration (WRE)" (refer to Section 3.1.2); b) A model training scheme that involves training the model on a curriculum of easy-to-hard subsets (refer to Section 3.1.3) using the subsets selected and the probability distribution constructed in the pre-processing step. We also present a class-wise partitioning trick to significantly minimize the memory footprint of MILO (refer to Section 3.2). Effectiveness of MILO : Through extensive experiments on multiple real-world datasets, we empirically demonstrate the effectiveness of the MILO framework for efficient training and hyperparameter tuning. In Figure 2, we provide a summary of the speedup achieved by MILO compared to full data training and tuning, along with the corresponding relative performance. Our results show that MILO can train models 3x to 10x faster and tune hyperparameters 20x to 75x faster, with minimal loss in performance. Furthermore, we consistently outperform the subset selection baselines that were considered. Additionally, MILO exhibits significantly faster initial model convergence and maintains faster convergence throughout training compared to other baselines. These qualities make it highly effective for applications such as hyperparameter tuning and neural architecture search, where the ability to distinguish good models from bad models early in the tuning process is crucial for enhancing efficiency [38, 37]. Related Work: In recent years, data-subset selection strategies have achieved success in various machine learning applications such as speech recognition [ 68 , 67], machine translation [ 32 ], active learning [ 60 , 1 , 33 ], hyper-parameter tuning [ 30 ], continual learning [65 ], domain adaptation [20 ], and computer vision [22]. A recent empirical study [5] demonstrated that datasets frequently contain semantic redundancies. These redundancies can be removed during training without affecting performance. In response, data pruning methods [66 , 51, 62] have developed principled selection criteria for pruning redundant samples before model training, with minimal loss in performance. However, both existing data pruning and compute-efficient learning approaches are model-dependent, resulting in them being computationally expensive. In contrast, our method is model-agnostic and capable of identifying high-quality subsets by intelligent data exploration. Furthermore, our approach can achieve superior model convergence by utilizing an easy-to-hard curriculum, transitioning from representative to diverse subsets during training.	https://aclanthology.org/2024.findings-naacl.209.pdf	An End-to-End Submodular Framework for Data-Efficient In-Context Learning	Recent advancements in natural language tasks leverage the emergent In-Context Learning (ICL) ability of pretrained Large Language Models (LLMs). ICL enables LLMs to perform new tasks by utilizing a limited number of input-output examples as prompts. While ICL circumvents the costly step of finetuning LLMs, its effectiveness is heavily dependent on the quality and ordering of provided examples (called exemplars). In this work, we propose a two-stage data-efficient framework Div-S3 for exemplar selection for ICL. The first stage focuses on data annotation and employs a poolbased active learning approach to select a set of Diverse and informative exemplars from the target tasks’ unlabeled pool. Given a test input/query, the second stage uses Submodular Span Summarization (S3) to select the most relevant and non-redundant exemplars from the annotated pool of a limited budget. On 7 different NLP datasets and 5 LLMs of varying complexities, we show Div-S3 outperforms (1) existing active learning-based methods for data annotation for ICL and (2) similarity-based methods for test query-specific exemplars retrieval.	Pretrained large language models (LLMs) (Kenton and Toutanova, 2019; Brown et al., 2020; Chowdhery et al., 2022) have become foundational for a wide range of Natural Language Processing (NLP) tasks, demonstrating impressive success across various domains (Bommasani et al., 2021; Bubeck et al., 2023) through in-context learning (ICL) (Dong et al., 2022). ICL enables these pretrained LLMs to perform new tasks by using task-specific prompts containing a limited number of input-output demonstrations (also referred to as shots, exemplars, or prompts) in the natural language format. This approach facilitates deployment across different downstream tasks and reduces the need for labeled downstream training data since ICL does not require any task-specific training. The typical ICL procedure consists of two key components: (1) Exemplar annotation and retrieval (Wu et al., 2022; Köksal et al., 2022; Liu et al., 2022): This step involves annotating and retrieving exemplars that serve as context demonstrations. (2) Prompt template crafting (Sorensen et al., 2022; Deng et al., 2022): this step involves designing a prompt template to wrap these demonstrations in a comprehensible and coherent natural language instruction. Recent studies (Liu et al., 2022; Su et al., 2022; Margatina et al., 2023) show that providing exemplars most relevant to the current input instance is beneficial. Moreover, Zhao et al. (2021), Lu et al. (2022), and Liu et al. (2023) observe that LLMs attend more to the exemplars that are closer in the sequence to the input instance. Therefore, to achieve the best performance of ICL, the selection of exemplars and their ordering in the LLM prompt are crucial. In practice, an extensive collection of unlabeled exemplars is easily available (e.g., posts and discussions on forums like Stack Exchange or user-generated content on social media platforms), but manually annotating all exemplars would be exceptionally costly. To annotate and select the exemplars optimally for a given target task, we follow the two-stage approach shown in Figure 1: (1) Exemplar Annotation: select a subset of exemplars for annotation under a fixed budget (performed only once) and (2) Exemplar Retrieval: identify limited-sized exemplars in an ordering that are most influential for a given input instance from the annotated subset of exemplars. Intuitively, for the first stage, we aim to find the subset with maximal diversity and least redundancy so that, given any input, we can find corresponding labeled exemplars. For the second stage, in addition to the diversity requirement similar to the first stage, we emphasize the relevance of the exemplars to the given input query and order exemplars so that their relevance to the input query decreases as the exemplars are farther away from the input instance. We propose a framework Div-S3 based on submodular optimization that unifies the abovementioned two stages. For Exemplar Annotation, we model the problem as a submodular optimization problem under a cardinality constraint to find as Diverse a subset as possible within a budget. For Exemplar Retrieval, we formalize the problem as a Submodular Span Summarization (S3) problem (Kumari and Bilmes, 2021) with a knapsack constraint, which finds a diverse subset most relevant to the input query under a token length limit. Also, we naturally order the resulting exemplars based on the gains represented by the submodular function. The name of our proposed framework Div-S3 captures the optimization objectives used for both exemplar annotation (Div) and exemplar retrieval (S3) stages. In Fig. 2, we show a sample test query where using Div-S3 for exemplar selection leads to a more diverse and query-relevant exemplar set (more examples provided in Appendix D). Our framework is general, as any submodular function can be plugged into our method. For models beyond LMs, e.g., for text-image multi-modality models, we may use pre-existing submodular functions that are powerful for expressing diversity in the image domain. In addition, we account for relevance, diversity, and ordering for the exemplar retrieval stage, where one or two aspects typically get overlooked by previous methods. Empirically, we evaluate Div-S3 on 7 NLP tasks with 5 LLMs and show significantly improved performance compared to baselines. Our contributions are: 1. We propose an end-to-end framework DivS3 utilizing submodular optimization for performing data-efficient ICL using LLMs. Depending on budget requirements, Div-S3 provides the flexibility to set the budget either in terms of the number of exemplars to be used in the prompt or the LLM’s context window size. 2. We empirically validate the effectiveness of our framework on 7 different NLP tasks and show the transferability of results across LLMs of varying complexities. 3. We thoroughly analyze each component of Div-S3 by (a) studying S3 in a setting with no annotation budget constraint and (b) analyzing the sensitivity of the exemplars selected by Div-S3 to their position in the LLM’s prompt.	0	0
selecting subsets of data for efficient model fine-tuning	https://arxiv.org/pdf/2103.00128	PRISM: A Rich Class of Parameterized Submodular Information Measures for Guided Subset Selection	With ever-increasing dataset sizes, subset selection techniques are becoming increasingly important for a plethora of tasks. It is often necessary to guide the subset selection to achieve certain desiderata, which includes focusing or targeting certain data points, while avoiding others. Examples of such problems include: i) targeted learning, where the goal is to find subsets with rare classes or rare attributes on which the model is underperforming, and ii) guided summarization, where data (e.g., image collection, text, document or video) is summarized for quicker human consumption with specific additional user intent. Motivated by such applications, we present PRISM, a rich class of PaRameterIzed Submodular information Measures. Through novel functions and their parameterizations, PRISM offers a variety of modeling capabilities that enable a trade-off between desired qualities of a subset like diversity or representation and similarity/dissimilarity with a set of data points. We demonstrate how PRISM can be applied to the two real-world problems mentioned above, which require guided subset selection. In doing so, we show that PRISM interestingly generalizes some past work, therein reinforcing its broad utility. Through extensive experiments on diverse datasets, we demonstrate the superiority of PRISM over the state-of-the-art in targeted learning and in guided imagecollection summarization. PRISM is available as a part of the SUBMODLIB (https://github.com/decile-team/submodlib) and TRUST (https://github.com/decile-team/trust) toolkits.	Recent times have seen explosive growth in data across several modalities, including text, images, and videos. This has given rise to the need for finding techniques for selecting effective smaller data subsets with specific characteristics for a variety of down-stream tasks. Often, we would like to guide the data selection to either target or avoid a certain set of data slices. One application is, what we call, targeted learning, where the goal is to select data points similar to data slices on which the model is currently performing poorly. These slices are data points that either belong to rare classes or have common rare attributes (e.g., color, background, etc.). An example of such a scenario is shown in Fig. 1(a), where a self-driving car model struggles in detecting “cars in a dark background“ because of a lack of such images in the training set. The targeted learning problem is to augment the training dataset with more of such rare images, with an aim to improve model performance. Another example is detecting cancers in biomedical imaging datasets, where the number of cancerous images are often a small fraction of the non-cancerous images. Another application comes from the summarization task, where an image collection, a video, or a text document is summarized for quicker human consumption by eliminating redundancy, while preserving the main content. While a number of applications require generic summarization (i.e., simply picking a representative and diverse subset of the massive dataset), it is often important to capture certain user intent in summarization. We call this guided summarization. Examples of guided summarization include: (i) query-focused summarization (Sharghi, Gong, and Shah 2016; Xiao et al. 2020), where a summary similar to a specific query is desired, and (ii) privacy-preserving summarization, where a summary dissimilar to a given private set of data points is desired (say, for privacy issues). See Fig. 1(b) for a pictorial illustration. PRISM Framework: We define PRISM through different instantiations and parameterizations of various submodular information measures (Sec. 2). These allow for modeling a spectrum of semantics required for guided subset selection, like relevance to a query set, irrelevance to a private set, and diversity among selected data points. We study the effect of parameter trade-off among these different semantics and present interesting insights. PRISM for Targeted Learning: We present a novel algorithm (Sec. 3.1, Algo. 1) to apply PRISM for targeted learning, which aims to improve a model’s performance on rare slices of data. Specifically, we show that submodular information measures are very effective in finding the examples from the rare classes in a large unlabeled set (akin to finding a needle in a haystack). On several image classification tasks, PRISM obtains ≈ 20-30% gain in accuracy of rare classes (≈ 12% more than existing approaches) by just adding a few additional labeled points from the rare classes. Furthermore, we show that PRISM is 20× to 50× more label-efficient compared to random sampling, and 2× to 4× more labelefficient compared to existing approaches (see Sec. 4.1). We also show that Algo. 1 generalizes some existing approaches for data subset selection, reinforcing its utility (Sec. 3.3). PRISM for Guided Summarization. We propose a learning framework for guided summarization using PRISM (Sec. 3.2). We show that PRISM offers a unified treatment to the different flavors of guided summarization (query-focused and privacy-preserving) and generalizes some existing approaches to summarization, again reinforcing its utility. We show that it outperforms other existing approaches on a real-world image collections dataset (Sec. 4.2).	https://aclanthology.org/2024.findings-naacl.209.pdf	An End-to-End Submodular Framework for Data-Efficient In-Context Learning	Recent advancements in natural language tasks leverage the emergent In-Context Learning (ICL) ability of pretrained Large Language Models (LLMs). ICL enables LLMs to perform new tasks by utilizing a limited number of input-output examples as prompts. While ICL circumvents the costly step of finetuning LLMs, its effectiveness is heavily dependent on the quality and ordering of provided examples (called exemplars). In this work, we propose a two-stage data-efficient framework Div-S3 for exemplar selection for ICL. The first stage focuses on data annotation and employs a poolbased active learning approach to select a set of Diverse and informative exemplars from the target tasks’ unlabeled pool. Given a test input/query, the second stage uses Submodular Span Summarization (S3) to select the most relevant and non-redundant exemplars from the annotated pool of a limited budget. On 7 different NLP datasets and 5 LLMs of varying complexities, we show Div-S3 outperforms (1) existing active learning-based methods for data annotation for ICL and (2) similarity-based methods for test query-specific exemplars retrieval.	Pretrained large language models (LLMs) (Kenton and Toutanova, 2019; Brown et al., 2020; Chowdhery et al., 2022) have become foundational for a wide range of Natural Language Processing (NLP) tasks, demonstrating impressive success across various domains (Bommasani et al., 2021; Bubeck et al., 2023) through in-context learning (ICL) (Dong et al., 2022). ICL enables these pretrained LLMs to perform new tasks by using task-specific prompts containing a limited number of input-output demonstrations (also referred to as shots, exemplars, or prompts) in the natural language format. This approach facilitates deployment across different downstream tasks and reduces the need for labeled downstream training data since ICL does not require any task-specific training. The typical ICL procedure consists of two key components: (1) Exemplar annotation and retrieval (Wu et al., 2022; Köksal et al., 2022; Liu et al., 2022): This step involves annotating and retrieving exemplars that serve as context demonstrations. (2) Prompt template crafting (Sorensen et al., 2022; Deng et al., 2022): this step involves designing a prompt template to wrap these demonstrations in a comprehensible and coherent natural language instruction. Recent studies (Liu et al., 2022; Su et al., 2022; Margatina et al., 2023) show that providing exemplars most relevant to the current input instance is beneficial. Moreover, Zhao et al. (2021), Lu et al. (2022), and Liu et al. (2023) observe that LLMs attend more to the exemplars that are closer in the sequence to the input instance. Therefore, to achieve the best performance of ICL, the selection of exemplars and their ordering in the LLM prompt are crucial. In practice, an extensive collection of unlabeled exemplars is easily available (e.g., posts and discussions on forums like Stack Exchange or user-generated content on social media platforms), but manually annotating all exemplars would be exceptionally costly. To annotate and select the exemplars optimally for a given target task, we follow the two-stage approach shown in Figure 1: (1) Exemplar Annotation: select a subset of exemplars for annotation under a fixed budget (performed only once) and (2) Exemplar Retrieval: identify limited-sized exemplars in an ordering that are most influential for a given input instance from the annotated subset of exemplars. Intuitively, for the first stage, we aim to find the subset with maximal diversity and least redundancy so that, given any input, we can find corresponding labeled exemplars. For the second stage, in addition to the diversity requirement similar to the first stage, we emphasize the relevance of the exemplars to the given input query and order exemplars so that their relevance to the input query decreases as the exemplars are farther away from the input instance. We propose a framework Div-S3 based on submodular optimization that unifies the abovementioned two stages. For Exemplar Annotation, we model the problem as a submodular optimization problem under a cardinality constraint to find as Diverse a subset as possible within a budget. For Exemplar Retrieval, we formalize the problem as a Submodular Span Summarization (S3) problem (Kumari and Bilmes, 2021) with a knapsack constraint, which finds a diverse subset most relevant to the input query under a token length limit. Also, we naturally order the resulting exemplars based on the gains represented by the submodular function. The name of our proposed framework Div-S3 captures the optimization objectives used for both exemplar annotation (Div) and exemplar retrieval (S3) stages. In Fig. 2, we show a sample test query where using Div-S3 for exemplar selection leads to a more diverse and query-relevant exemplar set (more examples provided in Appendix D). Our framework is general, as any submodular function can be plugged into our method. For models beyond LMs, e.g., for text-image multi-modality models, we may use pre-existing submodular functions that are powerful for expressing diversity in the image domain. In addition, we account for relevance, diversity, and ordering for the exemplar retrieval stage, where one or two aspects typically get overlooked by previous methods. Empirically, we evaluate Div-S3 on 7 NLP tasks with 5 LLMs and show significantly improved performance compared to baselines. Our contributions are: 1. We propose an end-to-end framework DivS3 utilizing submodular optimization for performing data-efficient ICL using LLMs. Depending on budget requirements, Div-S3 provides the flexibility to set the budget either in terms of the number of exemplars to be used in the prompt or the LLM’s context window size. 2. We empirically validate the effectiveness of our framework on 7 different NLP tasks and show the transferability of results across LLMs of varying complexities. 3. We thoroughly analyze each component of Div-S3 by (a) studying S3 in a setting with no annotation budget constraint and (b) analyzing the sensitivity of the exemplars selected by Div-S3 to their position in the LLM’s prompt.	1	0
wireless sensing of human activity	http://witrack.csail.mit.edu/witrack-paper.pdf	3D Tracking via Body Radio Reflections	This paper introduces WiTrack, a system that tracks the 3D motion of a user from the radio signals reflected off her body. It works even if the person is occluded from the WiTrack device or in a different room. WiTrack does not require the user to carry any wireless device, yet its accuracy exceeds current RF localization systems, which require the user to hold a transceiver. Empirical measurements with a WiTrack prototype show that, on average, it localizes the center of a human body to within a median of 10 to 13 cm in the x and y dimensions, and 21 cm in the z dimension. It also provides coarse tracking of body parts, identifying the direction of a pointing hand with a median of 11.2. WiTrack bridges a gap between RF-based localization systems which locate a user through walls and occlusions, and human-computer interaction systems like Kinect, which can track a user without instrumenting her body, but require the user to stay within the direct line of sight of the device.	Recent years have witnessed a surge in motion tracking and localization systems. Multiple advances have been made both in terms of accuracy and robustness. In particular, RF localization using WiFi and other communication devices has reached sub-meter accuracy and demonstrated its ability to deal with occlusions and non-line of sight scenarios [31, 18]. Yet these systems require the user to carry a wireless device in order to be localized. In contrast, systems like Kinect and depth imaging have revolutionized the field of human-computer interaction by enabling 3D motion tracking without instrumenting the body of the user. However, Kinect and imaging systems require a user to stay within the device’s line-of-sight and cannot track her across rooms. We envision that if an RF system can perform 3D motion tracking without requiring the user to wear a radio, it will motivate the integration of such a technology in systems like Kinect to expand their reach beyond direct line of sight and enable through-wall human-computer interaction. Motivated by this vision, this paper introduces WiTrack, a system that tracks the 3D motion of a user using radio reflections that bounce off her body. It works through walls and occlusions, but does not require the user to carry any wireless device. WiTrack can also provide coarse tracking of a body part. In particular, the user may lift her hand and point at objects in the environment; the device detects the direction of the hand motion, enabling the user to identify objects of interest. WiTrack has one antenna for transmission and three antennas for receiving. At a high level, WiTrack’s motion tracking works as follows. The device transmits a radio signal and uses its reflections to estimate the time it takes the signal to travel from the transmitting antenna to the reflecting object and back to each of the receiving antennas. WiTrack then uses its knowledge of the position of the antennas to create a geometric reference model, which maps the round trip delays observed by the receive antennas to a 3D position of the reflecting body. Transforming this high-level idea into a practical system, however, requires addressing multiple challenges. First, measuring the time of flight is difficult since RF signals travel very fast – at the speed of light. To distinguish between two locations that are closer than one foot apart, one needs to measure differences in reflection time on the order of hundreds of picoseconds, which is quite challenging. To address this problem, we leverage a technique called FMCW (frequency modulated carrier wave) which maps differences in time to shifts in the carrier frequency; such frequency shifts are easy to measure in radio systems by looking at the spectrum of the received signal. A second challenge stems from multipath effects, which create errors in mapping the delay of a reflection to the distance from the target. WiTrack has to deal with two types of multipath effects. Some multipath effects are due to the transmitted signal being reflected off walls and furniture. Others are caused by the signal first reflecting off the human body then reflecting off other objects. This is further complicated by the fact that in non-line-of-sight settings, the strongest signal is not the one directly bouncing off the human body. Rather it is the signal that avoids the occluding object by bouncing off some side walls. WiTrack eliminates reflections from walls and furniture by noting that their distance (and time of flight) does not change over time. Hence, they can be eliminated by subtracting consecutive frames of the signals. Reflections that involve a combination of a human and some static object are more complex and are addressed through filters that account for practical constraints on the continuity of human motion and its speed in indoor settings. We have built a prototype of WiTrack and evaluated it empirically. Since off-the-shelf radios do not perform FMCW, we built an analog FMCW radio frontend, which operates as a daughterboard for the USRP software radio. In our evaluation, we use the VICON motion capture system [6] to report the ground truth location. VICON can achieve sub-centimeter accuracy but requires instrument-ing the human body with infrared markers and positioning an array of infrared cameras on the ceiling. Since VICON cannot operate in non-line-of-sight, the human moves in the VICON room while our device is placed outside the room and tracks the motion across the wall. Our evaluation considers three applications, each of them uses the developed 3D tracking primitive in a different way. In the first application, we consider 3D tracking of human motion through a wall. The objective of such an application is to augment virtual reality and gaming systems to work in non-line-of-sight and across rooms. We compute the tracking error as the difference between the location reported by our device and the actual location of the body center as reported by VICON. Our results show that WiTrack localizes the center of the human body to within 10 to 13 cm in the x and y dimensions, and 21 cm in the z dimension. This high accuracy stems from WiTrack’s ability to eliminate errors due to multipath and the combined performance of FMCW and our geometric mapping algorithm. The results also show that even the 90th percentile of the measurements stays within one foot along the x/y-axis and two feet along the z-axis. In the second application, we consider elderly fall detection. Current solutions to this problem include inertial sensors which old people tend to forget to wear [15], or cameras which infringe on privacy, particularly in bedrooms and bathrooms [20]. In contrast, WiTrack does not require the user to wear any device and protects her privacy much better than a camera. However, simply looking at the change in elevation cannot allow us to distinguish a fall from sitting on the floor. Thus, WiTrack identifies a fall as a fast change in the elevation that reaches the ground level. In a population of 11 users and over 133 experiments, WiTrack distinguishes a fall from standing, walking, sitting on a chair and sitting on the floor with an accuracy of 96.9% (the F-measure is 94.34%). In the third application, we consider a user who desires to control appliances by pointing at them (e.g., the user can turn her monitor on or turn the lights off by simply pointing at these objects.) We consider a gesture in which the user lifts her arm, points at an appliance, and drops her arm. By comparing the position of the arm over time, WiTrack can identify the pointing direction. Our prototype estimates the pointing direction with a median of 11.2 degrees and a 90th percentile of 37.9 degrees. Our results also show that the prototype operates in realtime, and outputs the 3D location within 75 ms from the time the antennas receive the signal. Further, it operates at a fairly low-power, transmitting only 0.75 milliwatts. However, our current prototype can track a single person, and requires the person to move to obtain an initial estimate of his location. Contributions: This paper introduces the first device that can achieve centimeter-scale accuracy in tracking the 3D motion of a human based on radio reflections off her body. The paper presents new algorithms for eliminating errors due to multipath and performing accurate 3D tracking, both of a whole body and a body part. The paper also presents a prototype implementation that includes a lowpower FMCW radio frontend and realtime processing, delivering accurate 3D motion tracking to within a median of 10 to 20 centimeters. Our results demonstrate that WiTrack can expand the space of human-computer interfaces and enable interaction across walls, and occluded spaces. We believe that WiTrack also expands the role that wireless computer networks may play in the future to enable them to provide a variety of services: Communication is definitely a major service, but other services may include motion tracking, through-wall human-computer interaction, and a gesture based interface for controlling appliances and interacting with the environment.	http://witrack.csail.mit.edu/witrack2-paper.pdf	Multi-Person Localization via RF Body Reflections	We have recently witnessed the emergence of RF-based indoor localization systems that can track user motion without requiring the user to hold or wear any device. These systems can localize a user and track his gestures by relying solely on the reflections of wireless signals off his body, and work even if the user is behind a wall or obstruction. However, in order for these systems to become practical, they need to address two main challenges: 1) They need to be able to operate in the presence of more than one user in the environment, and 2) they must be able to localize a user without requiring him to move or change his position. This paper presents WiTrack2.0, a multi-person localization system that operates in multipath-rich indoor environments and pinpoints users’ locations based purely on the reflections of wireless signals off their bodies. WiTrack2.0 can even localize static users, and does so by sensing the minute movements due to their breathing. We built a prototype of WiTrack2.0 and evaluated it in a standard office building. Our results show that it can localize up to five people simultaneously with a median accuracy of 11.7 cm in each of the x/y dimensions. Furthermore, WiTrack2.0 provides coarse tracking of body parts, identifying the direction of a pointing hand with a median error of 12.5◦ , for multiple users in the environment.	Over the past decade, the networking community has made major advances in RF-based indoor localization [5, 34, 21, 26, 38, 17, 21, 11, 22], which led to systems that can localize a wireless device with centimeter-scale accuracy. Recently however the research community has realized that it is possible to localize a user, without requiring him to wear or carry a wireless device [25, 3]. Such a leap from device-based to device-free indoor localization can enable ubiquitous tracking of people and their gestures. For example, it can enable a smart home to continuously localize its occupants and adjust the heating in each room according to the number of people in it. It would also enable a smart home to track our hand gestures so that we may control appliances by pointing at them, or turn the TV with a wave of our arm. Device-free tracking can also be leveraged in many applications where it is either inconvenient or infeasible for the user to hold/wear a device such as in gaming and virtual reality, elderly monitoring, intrusion detection, and search and rescue missions [3]. Past work has taken initial steps towards this vision [3, 18, 7]. However, these proposals have fundamental limitations that render them impractical for natural home environments. Specifically, they either require covering the entire space with a dense, surveyed grid of sensors [18, 7] or they fail in the presence of multiple users in the environment [3]. Additionally, these past proposals are also limited in their ability to detect the presence of users. In particular, they either require the user to continuously move to detect his presence [3], or they need to perform extensive prior calibration or training [18, 7]. In this paper, we introduce WiTrack2.0, a device free localization system that transcends these limitations. Specifically, WiTrack2.0 accurately localizes multiple users in the environment. It does so by disentangling the reflections of wireless signals that bounce off their bodies. Furthermore, it neither requires prior calibration nor that the users move in order to localize them. To achieve its goal, WiTrack2.0 has to deal with multiple challenges. As with traditional device-based localization, the most difficult challenge in indoor environments is the multipath effect [34, 17]. Specifically, wireless signals reflect off all objects in the environment making it hard to associate the incoming signal with a particular location. To overcome this challenge, past work [3] focuses on motion to capture signal reflections that change with time. It then assumes that only one person is present in the environment, and hence all motion can be attributed to him. However, if multiple people move in the environment or if the person is static, then this assumption no longer works. To address this challenge, we observe that the indoor multipath varies significantly when it is measured from different vantage points. Hence, one can address this problem by positioning multiple transmit and receive antennas in the environment, and measuring the time of flight from each of these transmit-receive antenna pairs. However, the signals emitted from the different transmitters will reflect off the bodies of the all the users in the environment, and these reflections interfere with each other leading to wireless collisions. In §5, we show how WiTrack2.0 disentangles these interfering reflected signals to localize multiple users in the presence of heavy indoor multipath. A second challenge that WiTrack2.0 has to address is related to the near-far problem. Specifically, reflections off the nearest person can have much more power than distant reflections, obfuscating the signal from distant people, and preventing their detection or tracking. To address this issue, we introduce Successive Silhouette Cancellation (SSC) an approach to address the nearfar problem, which is inspired by successive interference cancellation. This technique starts by localizing the closest person, then eliminates his impact on the received signal, before proceeding to localize further users (who have weaker reflections). It repeats this process iteratively until it has localized all the users in a scene. Note, however, that each user is not a point reflector; hence, his wireless reflection has a complex structure that must be taken into account, as we describe in §6. A third challenge that our system addresses is related to localizing static users. Specifically, past work that tracks human motion needs to eliminate reflections off static objects by subtracting consecutive measurements. However, this subtraction also results in eliminating the reflections off static users. To enable us to localize static users, we exploit the fact these users still move slightly due to their breathing. However, the breathing motion is fairly slow in comparison to body motion. Specifically, the chest moves by a sub-centimeter distance over a period of few seconds; in contrast, a human would pace indoors at 1 m/s. Hence, WiTrack2.0 processes the reflected signals at multiple time scales that enable it to accurately localize both types of movements as we describe in §7, We have built a prototype of WiTrack2.0, using USRP software radios and an analog FMCW radio. We run experiments both in line-of-sight (LOS) scenarios and nonline-of-sight (NLOS) scenarios, where the device is in a different room and is tracking people’s motion through the wall. Empirical results from over 300 experiments with 11 human subjects show the following:  ment. The paper also presents an evaluation of the system, showing that it can localize moving and static users in line-of-sight and through-wall settings with a median accuracy of 7-18 cm across all of these scenarios.	1	1
deep equilibrium models 	https://arxiv.org/pdf/2006.08591.pdf	Monotone operator equilibrium networks	Implicit-depth models such as Deep Equilibrium Networks have recently been shown to match or exceed the performance of traditional deep networks while being much more memory efficient. However, these models suffer from unstable convergence to a solution and lack guarantees that a solution exists. On the other hand, Neural ODEs, another class of implicit-depth models, do guarantee existence of a unique solution but perform poorly compared with traditional networks. In this paper, we develop a new class of implicit-depth model based on the theory of monotone operators, the Monotone Operator Equilibrium Network (monDEQ). We show the close connection between finding the equilibrium point of an implicit network and solving a form of monotone operator splitting problem, which admits efficient solvers with guaranteed, stable convergence. We then develop a parameterization of the network which ensures that all operators remain monotone, which guarantees the existence of a unique equilibrium point. Finally, we show how to instantiate several versions of these models, and implement the resulting iterative solvers, for structured linear operators such as multi-scale convolutions. The resulting models vastly outperform the Neural ODE-based models while also being more computationally efficient.	Recent work in deep learning has demonstrated the power of implicit-depth networks, models where features are created not by explicitly iterating some number of nonlinear layers, but by finding a solution to some implicitly defined equation. Instances of such models include the Neural ODE [8], which computes hidden layers as the solution to a continuous-time dynamical system, and the Deep Equilibrium (DEQ) Model [5], which finds a fixed point of a nonlinear dynamical system corresponding to an effectively infinite-depth weight-tied network. These models, which trace back to some of the original work on recurrent backpropagation [2, 23], have recently regained attention since they have been shown to match or even exceed to performance of traditional deep networks in domains such as sequence modeling [5]. At the same time, these models show drastically improved memory efficiency over traditional networks since backpropagation is typically done analytically using the implicit function theorem, without needing to store the intermediate hidden layers. However, implict-depth models that perform well require extensive tuning in order to achieve stable convergence to a solution. Obtaining convergence in DEQs requires careful initialization and regularization, which has proven difficult in practice [21]. Moreover, solutions to these models are not guaranteed to exist or be unique, making the output of the models potentially ill-defined. While Neural ODEs [8] do guarantee existence of a unique solution, training remains unstable since the ODE problems can become severely ill-posed [10]. Augmented Neural ODEs [10] improve the stability of Neural ODEs by learning ODEs with simpler flows, but neither model achieves efficient convergence nor performs well on standard benchmarks. Crucial questions remain about how models can have guaranteed, unique solutions, and what algorithms are most efficient at finding them. In this paper, we present a new class of implicit-depth equilibrium model, the Monotone Operator Equilibrium Network (monDEQ), which guarantees stable convergence to a unique fixed point.1 The model is based upon the theory of monotone operators [6, 26], and illustrates a close connection between simple fixed-point iteration in weight-tied networks and the solution to a particular form of monotone operator splitting problem. Using this connection, this paper lays the theoretical and practical foundations for such networks. We show how to parameterize networks in a manner that ensures all operators remain monotone, which establishes the existence and uniqueness of the equilibrium point. We show how to backpropagate through such networks using the implicit function theorem; this leads to a corresponding (linear) operator splitting problem for the backward pass, which also is guaranteed to have a unique solution. We then adapt traditional operator splitting methods, such as forward-backward splitting or Peaceman-Rachford splitting, to naturally derive algorithms for efficiently computing these equilibrium points. Finally, we demonstrate how to practically implement such models and operator splitting methods, in the cases of typical feedforward, fully convolutional, and multi-scale convolutional networks. For convolutional networks, the most efficient fixed-point solution methods require an inversion of the associated linear operator, and we illustrate how to achieve this using the fast Fourier transform. The resulting networks show strong performance on several benchmark tasks, vastly improving upon the accuracy and efficiency of Neural ODEs-based models, the other implicit-depth models where solutions are guaranteed to exist and be unique.	https://arxiv.org/pdf/2006.08656.pdf	Multiscale Deep Equilibrium Models	We propose a new class of implicit networks, the multiscale deep equilibrium model (MDEQ), suited to large-scale and highly hierarchical pattern recognition domains. An MDEQ directly solves for and backpropagates through the equilibrium points of multiple feature resolutions simultaneously, using implicit differentiation to avoid storing intermediate states (and thus requiring only O(1) memory consumption). These simultaneously-learned multi-resolution features allow us to train a single model on a diverse set of tasks and loss functions, such as using a single MDEQ to perform both image classification and semantic segmentation. We illustrate the effectiveness of this approach on two large-scale vision tasks: ImageNet classification and semantic segmentation on high-resolution images from the Cityscapes dataset. In both settings, MDEQs are able to match or exceed the performance of recent competitive computer vision models: the first time such performance and scale have been achieved by an implicit deep learning approach.	State-of-the-art pattern recognition systems in domains such as computer vision and audio processing are almost universally based on multi-layer hierarchical feature extractors [32, 34, 35]. These models are structured in stages: the input is processed via a number of consecutive blocks, each operating at a different resolution [31, 52, 49, 25]. The architectures explicitly express hierarchical structure, with up- and downsampling layers that transition between consecutive blocks operating at different scales. An important motivation for such designs is the prominent multiscale structure and extremely high signal dimensionalities in these domains. A typical image, for instance, contains millions of pixels, which must be processed coherently by the model. An alternative approach to differentiable modeling is exemplified by recent progress on implicit deep networks, such as Neural ODEs (NODEs) [12] and deep equilibrium models (DEQs) [5]. These constructions replace explicit, deeply stacked layers with analytical conditions that the model must satisfy, and are able to simulate models with “infinite” depth within a constant memory footprint. A notable achievement for implicit modeling is its successful application to large-scale sequences in natural language processing [5]. Is implicit deep learning relevant for general pattern recognition tasks? One clear challenge here is that implicit networks do away with flexible “layers” and “stages”. It is therefore not clear whether they can appropriately model multiscale structure, which appears essential to high discriminative power in some domains. This is the challenge that motivates our work. Can implicit models that forego deep sequences of layers and stages attain competitive accuracy in domains characterized by rich multiscale structure, such as computer vision? To address this challenge, we introduce a new class of implicit networks: the multiscale deep equilibrium model (MDEQ). It is inspired by DEQs, which attained high accuracy in sequence modeling [5]. We expand upon the DEQ construction substantially to introduce simultaneous equilibrium modeling of multiple signal resolutions. MDEQ solves for equilibria of multiple resolution streams simultaneously by directly optimizing for stable representations on all feature scales at the same time. Unlike standard explicit deep networks, MDEQ does not process different resolutions in succession, with higher resolutions flowing into lower ones or vice versa. Rather, the different feature scales are maintained side by side in a single “shallow” model that is driven to equilibrium. This design brings two major advantages. First, like the basic DEQ, our model does not require backpropagation through an explicit stack of layers and has an O(1) memory footprint during training. This is especially important as pattern recognition systems are memory-intensive. Second, MDEQ rectifies one of the drawbacks of DEQ by exposing multiple feature scales at equilibrium, thereby providing natural interfaces for auxiliary losses and for compound training procedures such as pretraining (e.g., on ImageNet) and fine-tuning (e.g., on segmentation or detection tasks). Multiscale modeling enables a single MDEQ to simultaneously train for multiple losses defined on potentially very different scales, whose equilibrium features can serve as “heads” for a variety of tasks. We demonstrate the effectiveness of MDEQ via extensive experiments on large-scale image classification and semantic segmentation datasets. Remarkably, this shallow implicit model attains comparable accuracy levels to state-of-the-art deeply-stacked explicit ones. On ImageNet classification, MDEQs outperform baseline ResNets (e.g., ResNet-101) with similar parameter counts, reaching 77.5% top-1 accuracy. On Cityscapes semantic segmentation (dense labeling of 2-megapixel images), identical MDEQs to the ones used for ImageNet experiments match the performance of recent explicit models while consuming much less memory. Our largest MDEQ surpasses 80% mIoU on the Cityscapes validation set, outperforming strong convolutional networks and coming tantalizingly close to the state of the art. This is by far the largest-scale application of implicit deep learning to date and a remarkable result for a class of models that until recently were applied largely to “toy” domains.	0	1
deep equilibrium models 	https://vladlen.info/papers/hyperdeq.pdf	Neural deep equilibrium solvers	A deep equilibrium (DEQ) model abandons traditional depth by solving for the fixed point of a single nonlinear layer fθ. This structure enables decoupling the internal structure of the layer (which controls representational capacity) from how the fixed point is actually computed (which impacts inference-time efficiency), which is usually via classic techniques such as Broyden’s method or Anderson acceleration. In this paper, we show that one can exploit such decoupling and substantially enhance this fixed point computation using a custom neural solver. Specifically, our solver uses a parameterized network to both guess an initial value of the optimization and perform iterative updates, in a method that generalizes a learnable form of Anderson acceleration and can be trained end-to-end in an unsupervised manner. Such a solution is particularly well suited to the implicit model setting, because inference in these models requires repeatedly solving for a fixed point of the same nonlinear layer for different inputs, a task at which our network excels. Our experiments show that these neural equilibrium solvers are fast to train (only taking an extra 0.9-1.1% over the original DEQ’s training time), require few additional parameters (1-3% of the original model size), yet lead to a 2× speedup in DEQ network inference without any degradation in accuracy across numerous domains and tasks.	Recent progress on implicit networks, such as Neural ODEs (NODEs) (Chen et al., 2018b; Dupont et al., 2019; Rubanova et al., 2019; Jia & Benson, 2019; Kelly et al., 2020) and deep equilibrium (DEQ) models (Bai et al., 2019; Winston & Kolter, 2020; Kawaguchi, 2021; Bai et al., 2020; Gilton et al., 2021), has motivated this novel class of networks to the forefront of deep learning research. Instead of stacking a series of operators hierarchically, implicit models define their outputs as solutions to nonlinear dynamical systems. For example, DEQ models (which this paper will focus on) define their outputs as fixed points (a.k.a. equilibria) of a layer fθ and input x; i.e., output z ⋆ = fθ(z ⋆ ,x). Then, in the backward pass, a DEQ implicitly differentiates through the final fixed point z ⋆ (Krantz & Parks, 2012; Bai et al., 2019; Fung et al., 2021), regardless of how forward pass is computed in the first place. Such insulated forward and backward passes enable an equilibrium model to leverage arbitrary black-box solvers to reach the fixed points without storing intermediate activations, thus consuming constant training memory. Recent works have successfully applied the DEQ framework on high-dimensional tasks such as language modeling (Merity et al., 2017) and semantic segmentation (Cordts et al., 2016), with performance competitive with architectures like Transformers (Vaswani et al., 2017; Dai et al., 2019). However, it is also well-known that these implicit models are slow, which is (arguably) their single most limiting drawback compared to traditional feedforward models (Duvenaud et al., 2020; Dupont et al., 2019; Bai et al., 2021). For example, Neural ODEs could take well over 100 forward solver iterations (i.e., evaluations of fθ) even on MNIST classification; DEQs can scale to realistic tasks, but the overhead of fixed-point solvers is magnified by the task scales, rendering the model 3-6× slower than state-of-the-art (SOTA) explicit networks (Vaswani et al., 2017; Wang et al., 2020) at inference. Can we make equilibrium models faster by taking advantage of their implicitness? One benefit of DEQ’s formulation is the fact that they decouple the representational capacity (determined by fθ) and forward computation (controlled by the solver), which is not possible in any explicit model (e.g., ResNet-101 (He et al., 2016)). Hence, given a trained DEQ, one can trade off inference time and the accuracy of the estimated fixed point by simply reducing the number of solver iterations. This yields a speed/accuracy trade-off curve, as shown in Fig. 1. However, this trade-off (i.e., movements along the pareto curves) can be highly risky: as we gradually increase inference speed by compromising the quality of fixed point estimates, model accuracy also degrades drastically. In this work, we show that we can shift the DEQ speed/accuracy trade-off curve by exploiting such decoupling to customize the fixed-point solving. Prior work on equilibrium models relies on classic solvers, which are manually designed and generic (e.g., Broyden’s Method (Broyden, 1965)). We propose a tiny, learnable, and content-aware solver module that is automatically customized to a specific DEQ. Our hypersolver consists of two parts. First, we introduce a learned initializer that estimates a good starting point for the optimization. Second, we introduce a generalized parameterized version of Anderson mixing (Anderson, 1965) that learns the iterative updates as an input-dependent temporal process. Overall, the hypersolver consumes a tiny amount of parameters. Since fθ is frozen when the hypersolver is trained, the training is very fast and does not compromise generalization. Our experiments apply this approach to diverse domains with large datasets: WikiText-103 language modeling (Merity et al., 2017), ImageNet classification (Deng et al., 2009), and Cityscapes segmentation with megapixel images (Cordts et al., 2016). Our results suggest that neural deep equilibrium solvers add little overhead to training (only taking an extra 0.9-1.1% over the original DEQ’s training time), are extremely compact (about 1-3% of the DEQ’s model size), and lead to a consistent and universal 1.6-2× acceleration of inference with no compromise in accuracy. Overall, we believe this paper achieves two major objectives, both vital for the quickly growing community studying implicit models: first, we advance these large-scale implicit models to a much more practical level across architectures (e.g., almost as fast as Transformers); and second, we formally bring up and exploit this valuable notion of how implicit layers decouple representational capacity and forward computation, opening a new door to significantly advancing the agenda of deploying implicit models in practice.	https://arxiv.org/pdf/2006.08656.pdf	Multiscale Deep Equilibrium Models	We propose a new class of implicit networks, the multiscale deep equilibrium model (MDEQ), suited to large-scale and highly hierarchical pattern recognition domains. An MDEQ directly solves for and backpropagates through the equilibrium points of multiple feature resolutions simultaneously, using implicit differentiation to avoid storing intermediate states (and thus requiring only O(1) memory consumption). These simultaneously-learned multi-resolution features allow us to train a single model on a diverse set of tasks and loss functions, such as using a single MDEQ to perform both image classification and semantic segmentation. We illustrate the effectiveness of this approach on two large-scale vision tasks: ImageNet classification and semantic segmentation on high-resolution images from the Cityscapes dataset. In both settings, MDEQs are able to match or exceed the performance of recent competitive computer vision models: the first time such performance and scale have been achieved by an implicit deep learning approach.	State-of-the-art pattern recognition systems in domains such as computer vision and audio processing are almost universally based on multi-layer hierarchical feature extractors [32, 34, 35]. These models are structured in stages: the input is processed via a number of consecutive blocks, each operating at a different resolution [31, 52, 49, 25]. The architectures explicitly express hierarchical structure, with up- and downsampling layers that transition between consecutive blocks operating at different scales. An important motivation for such designs is the prominent multiscale structure and extremely high signal dimensionalities in these domains. A typical image, for instance, contains millions of pixels, which must be processed coherently by the model. An alternative approach to differentiable modeling is exemplified by recent progress on implicit deep networks, such as Neural ODEs (NODEs) [12] and deep equilibrium models (DEQs) [5]. These constructions replace explicit, deeply stacked layers with analytical conditions that the model must satisfy, and are able to simulate models with “infinite” depth within a constant memory footprint. A notable achievement for implicit modeling is its successful application to large-scale sequences in natural language processing [5]. Is implicit deep learning relevant for general pattern recognition tasks? One clear challenge here is that implicit networks do away with flexible “layers” and “stages”. It is therefore not clear whether they can appropriately model multiscale structure, which appears essential to high discriminative power in some domains. This is the challenge that motivates our work. Can implicit models that forego deep sequences of layers and stages attain competitive accuracy in domains characterized by rich multiscale structure, such as computer vision? To address this challenge, we introduce a new class of implicit networks: the multiscale deep equilibrium model (MDEQ). It is inspired by DEQs, which attained high accuracy in sequence modeling [5]. We expand upon the DEQ construction substantially to introduce simultaneous equilibrium modeling of multiple signal resolutions. MDEQ solves for equilibria of multiple resolution streams simultaneously by directly optimizing for stable representations on all feature scales at the same time. Unlike standard explicit deep networks, MDEQ does not process different resolutions in succession, with higher resolutions flowing into lower ones or vice versa. Rather, the different feature scales are maintained side by side in a single “shallow” model that is driven to equilibrium. This design brings two major advantages. First, like the basic DEQ, our model does not require backpropagation through an explicit stack of layers and has an O(1) memory footprint during training. This is especially important as pattern recognition systems are memory-intensive. Second, MDEQ rectifies one of the drawbacks of DEQ by exposing multiple feature scales at equilibrium, thereby providing natural interfaces for auxiliary losses and for compound training procedures such as pretraining (e.g., on ImageNet) and fine-tuning (e.g., on segmentation or detection tasks). Multiscale modeling enables a single MDEQ to simultaneously train for multiple losses defined on potentially very different scales, whose equilibrium features can serve as “heads” for a variety of tasks. We demonstrate the effectiveness of MDEQ via extensive experiments on large-scale image classification and semantic segmentation datasets. Remarkably, this shallow implicit model attains comparable accuracy levels to state-of-the-art deeply-stacked explicit ones. On ImageNet classification, MDEQs outperform baseline ResNets (e.g., ResNet-101) with similar parameter counts, reaching 77.5% top-1 accuracy. On Cityscapes semantic segmentation (dense labeling of 2-megapixel images), identical MDEQs to the ones used for ImageNet experiments match the performance of recent explicit models while consuming much less memory. Our largest MDEQ surpasses 80% mIoU on the Cityscapes validation set, outperforming strong convolutional networks and coming tantalizingly close to the state of the art. This is by far the largest-scale application of implicit deep learning to date and a remarkable result for a class of models that until recently were applied largely to “toy” domains.	0	1
massive MIMO baseband processing	https://arxiv.org/pdf/2407.06755	A 46 Gbps 12 pJ/b Sparsity-Adaptive Beamspace Equalizer for mmWave Massive MIMO in 22FDX	We present a GlobalFoundries 22FDX FD-SOI application-specific integrated circuit (ASIC) of a beamspace equalizer for millimeter-wave (mmWave) massive multiple-input multiple-output (MIMO) systems. The ASIC implements a recently-proposed power-saving technique called sparsity-adaptive equalization (SPADE). SPADE exploits the inherent sparsity of mmWave channels in the beamspace domain to reduce the dynamic power of matrix-vector products by skipping multiplications for which the magnitude of both operands are below pre-defined thresholds. Simulations with realistic mmWave channels show that SPADE incurs less than 0.7dB SNR degradation at 1% target bit error rate compared to antenna-domain equalization. ASIC measurement results demonstrate an equalization throughput of 46Gbps and show that SPADE offers up to 38% power savings compared to antenna-domain equalization. A comparison with state-of-the-art massive MIMO equalizer designs reveals that our ASIC achieves superior normalized energy efficiency.	Fifth generation (5G) and beyond-5G wireless communication systems take advantage of large contiguous portions of the available spectrum at millimeter-wave (mmWave) frequencies to enable wideband communication [1]. Corresponding basestations (BSs) rely on massive multiple-input multiple-output (MIMO) [2], which (i) mitigates the high path loss at mmWave frequencies [3] and (ii) enables multi-user (MU) communication by means of spatial multiplexing. Wideband communication requires high baseband sampling rates and massive MU-MIMO generates high-dimensional data—together, they significantly increase hardware complexity. In this paper, we present a hardware implementation of a technique that reduces the power consumption of data detection. 1) Beamspace Processing: A promising approach to reducing complexity of data detection in all-digital mmWave massive MU-MIMO systems is to exploit the inherent sparsity of mmWave channels [3], [4] in the so-called beamspace. Converting a system from antenna-domain into beamspace is achieved by applying a spatial discrete Fourier transform (DFT) to the signals received at a uniform linear antenna array [5]–[10]. Uplink data detection in beamspace, with the goal of reducing implementation complexity, has been studied recently for mmWave massive MU-MIMO systems, mainly in the context of linear data detectors, as nonlinear methods typically incur higher complexity. Linear data detection consists of two phases: (i) preprocessing, where an equalization matrix is computed based on a channel-matrix estimate and (ii) equalization, where the equalization matrix is multiplied to the received vectors to obtain estimates of the transmitted data symbols. While preprocessing is performed only once per coherence interval, equalization must be performed for each received vector, hence, at much higher rates than preprocessing. In this paper, we focus on reducing the complexity of equalization and assume that preprocessing is performed externally. Existing beamspace data detectors reduce equalization complexity by designing sparse equalization matrices with specific sparsity patterns, thereby reducing the number of multiplications required for equalization. Such sparsity-exploiting beamspace data detectors, however, either incur a notable performance degradation compared to conventional antenna-domain linear minimum mean squared error (LMMSE) equalization, e.g., [5], [6], or require preprocessing algorithms with extremely high computational complexity [7]. In [8], a different approach to reduce complexity by exploiting beamspace sparsity was proposed. The method is referred to as sparsity-adaptive equalization (SPADE) and leverages the fact that the LMMSE equalization matrix is already approximately sparse in beamspace and avoids computing a sparse equalization matrix with a specific sparsity pattern. To reduce equalization complexity, SPADE uses two pre-computed thresholds to skip multiplications whenever the absolute value of both operands are below these thresholds. As shown in [8], SPADE significantly reduces the number of required multiplications, while exhibiting comparable performance to state-of-the-art linear beamspace data detectors [5]–[7]. 2) Contributions: We present the first application specific integrated circuit (ASIC) capable of performing SPADE-based beamspace equalization as well as antenna-domain equalization for a massive MU-MIMO system with 64 BS antennas and up to 16 single-antenna user equipments (UEs). In addition, we demonstrate real-world power savings achieved by SPADEbased beamspace equalization over conventional, antennadomain equalization through extensive ASIC measurements. 3) Notation: Boldface lowercase and uppercase letters represent column vectors and matrices, respectively. For a matrix A, the transpose is AT and Hermitian transpose AH. The mth column of A is am = [A]m, and the entry on the mth row and nth column is Am,n = [A]m,n. For a vector a, the kth entry is ak = [a]k, and the real and imaginary parts are a R and a I , respectively. The ℓ∞- and ℓf∞-norm is ∥a∥∞ ≜ maxk |ak| and ∥a∥f∞ ≜ max{∥a R∥∞, ∥a I ∥∞}, respectively [11]. Bars over variables indicate antenna-domain quantities. Expectation with respect to a random vector a is denoted by Ea[·].	https://arxiv.org/pdf/2107.11073	Resolution-Adaptive All-Digital Spatial Equalization for mmWave Massive MU-MIMO	All-digital basestation (BS) architectures for millimeter-wave (mmWave) massive multi-user multiple-input multiple-output (MU-MIMO), which equip each radio-frequency chain with dedicated data converters, have advantages in spectral efficiency, flexibility, and baseband-processing simplicity over hybrid analog-digital solutions. For all-digital architectures to be competitive with hybrid solutions in terms of power consumption, novel signal-processing methods and baseband architectures are necessary. In this paper, we demonstrate that adapting the resolution of the analog-to-digital converters (ADCs) and spatial equalizer of an all-digital system to the communication scenario (e.g., the number of users, modulation scheme, and propagation conditions) enables orders-of-magnitude power savings for realistic mmWave channels. For example, for a 256-BS-antenna 16-user system supporting 1 GHz bandwidth, a traditional baseline architecture designed for a 64-user worst-case scenario would consume 23 W in 28 nm CMOS for the ADC array and the spatial equalizer, whereas a resolution-adaptive architecture is able to reduce the power consumption by 6.7x.	Millimeter-wave (mmWave) communication [1], [2] offers vast amounts of unused spectrum and is considered a key technology component of fifth-generation (5G) and beyond 5G wireless systems. A major challenge of mmWave communication is the high path loss [3], which can be mitigated with massive multiple-input multiple-output (MIMO) [4]. Massive multi-user (MU) MIMO is able to compensate for the high path loss via fine-grained beamforming while supporting concurrent communication with multiple user equipments (UEs) in the same frequency band. A practical realization of mmWave massive MU-MIMO basestations (BSs), however, faces serious implementation challenges that are caused by the large number of BS antennas and the large communication bandwidth. Two prominent solutions that lower the cost and power consumption of mmWave massive MU-MIMO systems are (i) hybrid digital-analog architectures [5], [6] in which the number of radio-frequency (RF) chains is less than the number of antennas and (ii) all-digital architectures that rely on nonlinear RF chains and low-resolution data converters [7]–[9]. In this paper, we focus on all-digital architectures that are able to exploit the full potential of massive MU-MIMO and offer higher spectral efficiency with comparable power consumption to hybrid architectures [10], [11]. However, in order to keep the system costs and power consumption of all-digital architectures within practical limits, it is indispensable to rely on lowresolution data converters [12] as well as on low-resolution digital baseband processing methods [13]. A. Contributions In this paper, we show that for all-digital massive MUMIMO systems with B BS antennas and U UEs, the system’s load factor defined as β , U/B, as well as other specifics of the communication system (e.g., the modulation and channel conditions) do not only determine the spectral efficiency [14] and robustness against RF impairments [12], [15], [16], but also open up new means to optimize the power consumption of alldigital spatial equalization architectures. More specifically, if the number of active UEs is low, then it is possible to reduce the resolution of the analog-to-digital converters (ADCs) and the resolution of finite-alphabet equalizers, as well as the number of active BS antennas, without noticeably degrading the system’s error-rate performance. We demonstrate that such resolutionadaptive all-digital massive MU-MIMO BS architectures are key to enabling up to two orders-of-magnitude power savings in the ADC array and the all-digital spatial equalizer. Previous work [17]–[19] focused on adapting the resolution of each individual ADC to the channel state. In contrast, we (i) adapt the resolutions of both the ADC array and the spatial equalizer to the instantaneous communication scenario and (ii) study their individual impact on power consumption. B. Notation Boldface uppercase and lowercase letters represent matrices and column vectors, respectively. For a matrix A, the conjugate transpose is AH, the kth column is ak, and the entry on the kth row and `th column is Ak,`. The N × N identity matrix is IN . For a vector a, the kth entry is ak, the `2-norm is kak2, the real part is <{a} and the imaginary parts is ={a}. The `∞-norm and `f∞-norm are defined as kak∞ , maxk |ak| and kakf∞ , max{k<{a}k∞, k={a}k∞}. Expectation with respect to the random vector x is denoted by Ex[·].	0	1
massive MIMO baseband processing	https://arxiv.org/pdf/2009.02747	Finite-Alphabet MMSE Equalization for All-Digital Massive MU-MIMO mmWave Communication	We propose finite-alphabet equalization, a new paradigm that restricts the entries of the spatial equalization matrix to low-resolution numbers, enabling high-throughput, low-power, and low-cost hardware equalizers. To minimize the performance loss of this paradigm, we introduce FAME, short for finite-alphabet minimum mean-square error (MMSE) equalization, which is able to significantly outperform a naive quantization of the linear MMSE matrix. We develop efficient algorithms to approximately solve the NP-hard FAME problem and showcase that near-optimal performance can be achieved with equalization coefficients quantized to only 1-3 bits for massive multi-user multiple-input multiple-output (MU-MIMO) millimeter-wave (mmWave) systems. We provide very-large scale integration (VLSI) results that demonstrate a reduction in equalization power and area by at least a factor of 3.9x and 5.8x, respectively.	Future wireless systems are expected to deliver even higher data-rates within the already crowded frequency spectrum. Emerging technologies, such as millimeter-wave (mmWave) communication [1], [2] and massive multi-user multiple-input multiple-output (MU-MIMO) [3], [4], have risen as promising candidates to provide such high datarates. The abundance of available bandwidth at mmWave frequencies, combined with the fine-grained beamforming capabilities provided by massive MU-MIMO, enables highthroughput communication to multiple user equipments (UEs) in the same time-frequency resource. However, the presence of hundreds of antennas at the basestation (BS), each receiving a wideband signal, necessitates sophisticated radio frequency (RF) and digital baseband processing circuitry. As a result, circuit power consumption and system costs may increase significantly, which may hamper the success of this technology. To reduce power consumption, the literature has largely focused on multi-antenna mmWave architectures that rely on hybrid analog-digital solutions [5]–[7]. Albeit energy efficient, such architectures have limited multiplexing capabilities as they are only capable of simultaneously combining signals coming from a restricted number of directions [7]–[10]; this key limitation may result in a reduced spectral efficiency. An emerging alternative is the use of all-digital BS architectures [11]–[13]. While it is commonly believed that all-digital BS designs would be energy inefficient, it has been shown recently that the power consumption of the RF and data-conversion elements in an alldigital BS is comparable to that of hybrid solutions, provided that the resolution of the data converters at the BS is suitably reduced [10], [12]. The power consumption and system costs of baseband processing for all-digital BS architectures is, however, largely unexplored. A. The Case for Efficient Spatial Equalization Spatial equalization in the uplink (UEs transmit to BS) is among the most power- and throughput-critical tasks in alldigital BS architectures. The purpose of spatial equalization is to collect the signals from all U UEs at the B BS antennas, while suppressing inter-UE interference. Mathematically, spatial equalization amounts to one or multiple U × B matrix-vector multiplications involving a U × B equalization matrix and the B-dimensional received vector. These multiplications need to be performed on a per-baseband-sample basis (at the sample rate of the analog-to-digital converters). Unfortunately, for a BS with B = 256 antennas serving U = 16 UEs, a conventional matrix-vector-product circuit operating at 2 G vectors/s consumes over 28 W and occupies more than 128 mm2 when implemented in 28 nm CMOS (see Section V for the details). If more BS antennas and/or more UEs are considered, circuit power and area increase even further. Clearly, more efficient spatial equalization circuitry is necessary for all-digital BS architectures operating at mmWave frequencies in order to minimize power consumption and silicon area (which translate to system costs), while achieving high spectral efficiency. The matrix-vector products required for spatial equalization involve multiplications and additions, where the hardware multipliers dominate power and area. The area and delay of a hardware multiplier scales with O(mn) and O(log(max{m, n})), respectively, where m and n are the number of bits of each operand [14]. As a consequence, circuit area, delay, and power consumption (which is roughly proportional to circuit area) of a matrix-vector-product engine can be minimized by using a low number of bits to represent both operands. The literature has extensively explored the efficacy of lowresolution data converters at the BS antennas of massive MUMIMO systems [7], [10]–[12], [15]–[17]. Depending on the scenario, 3 to 8 bits were shown to achieve near-optimal spectral efficiency [10], [12], [15]–[17]. Such methods reduce the precision of one of the operands (i.e., that of the received vector) in a matrix-vector product. However, the coefficients of the equalization matrix (the other operand) are typically left at relatively high precision, e.g., 10 to 12 bits [18], [19]. B. Contributions To reduce power consumption and implementation costs of spatial equalization, we propose to coarsely quantize the coefficients of spatial equalization matrices, a paradigm that we call finite-alphabet equalization. We emphasize that, in contrast to approaches that use low-resolution analog-to-digital converters (ADCs) to quantize the received vector to be equalized [7], [10]–[12], [15]–[17], finite-alphabet equalization intends to coarsely quantize the entries of the spatial equalization matrix. Although a straightforward concept, it turns out that obtaining low-resolution finite-alphabet equalization matrices that achieve high spectral efficiency is a hard problem. Figure 1 illustrates this claim for a case where the coefficients of a spatial equalization matrix are quantized using 1 bit per real and imaginary part. A naïve quantization of the linear minimum mean-square error (L-MMSE) equalization matrix to 1-bit leads to a finite-alphabet L-MMSE (FL-MMSE) equalizer, which, as we can see from Figure 1, suffers from a high error floor. To combat this problem, we propose finite-alphabet minimum mean-square error equalization (FAME), which leads to an NPhard optimization problem that can be solved approximately (and efficiently) using forward-backward splitting (FBS). We refer to the resulting method as FAME-FBS. As shown in Figure 1, using FAME-FBS results in a substantially improved error rate compared to FL-MMSE equalization. The main contributions of this paper are summarized as follows. We propose a specific finite-alphabet equalizationmatrix structure that enables one to reduce the complexity of a U × B matrix-vector product by using U × B low-resolution coefficients, while still being able to deliver a performance similar to conventional, high-resolution spatial equalization matrices. We derive the so-called FAME problem, whose solution leads to finite-alphabet equalization matrices that minimize the post-equalization mean-square error (MSE). We propose a range of algorithms that approximate the NP-hard FAME problem—some of these algorithms achieve excellent performance even for 1-bit resolution; some require very low complexity. We present simulation results for line-ofsight (LoS) and non-LoS mmWave channel models, which demonstrate the efficacy of FAME in terms of error-vector magnitude (EVM), beamforming capabilities, and uncoded bit error-rate (BER). Finally, we implement reference finitealphabet equalization circuits for different number of bits in 28 nm CMOS to demonstrate the effectiveness of FAME in practice. C. Notation Matrices and column vectors are represented with uppercase and lowercase boldface letters, respectively. The Hermitian transpose and the Frobenius norm of a matrix A are denoted by AH and kAkF , respectively. The real part of a complexvalued matrix A is <{A} and the imaginary part is ={A}. The M×M identity matrix is denoted by IM. The kth entry and the `2-norm of a vector a are ak and kak2 , respectively; the entrywise complex conjugate is denoted by a ∗ . The kth standard basis vector is represented by ek. The signum function sgn(·) is defined as sgn(a) = +1 for a ≥ 0 and sgn(a) = −1 for a < 0 and is applied entry-wise to vectors. We use Ex[·] to denote expectation with respect to the random vector x. The set S+ contains all positive semidefinite matrices, and the set R+ contains all the non-negative real numbers. D. Paper Outline The rest of the paper is organized as follows. Section II introduces the system model and summarizes the basics of L-MMSE equalization. Section III proposes the FAME problem and presents numerical experiments. Section IV develops lowcomplexity algorithms that approximate the NP-hard FAME problem. Section V shows hardware implementation results. We conclude in Section VI. Proofs and complexity counts are relegated to the appendices. 	https://arxiv.org/pdf/2107.11073	Resolution-Adaptive All-Digital Spatial Equalization for mmWave Massive MU-MIMO	All-digital basestation (BS) architectures for millimeter-wave (mmWave) massive multi-user multiple-input multiple-output (MU-MIMO), which equip each radio-frequency chain with dedicated data converters, have advantages in spectral efficiency, flexibility, and baseband-processing simplicity over hybrid analog-digital solutions. For all-digital architectures to be competitive with hybrid solutions in terms of power consumption, novel signal-processing methods and baseband architectures are necessary. In this paper, we demonstrate that adapting the resolution of the analog-to-digital converters (ADCs) and spatial equalizer of an all-digital system to the communication scenario (e.g., the number of users, modulation scheme, and propagation conditions) enables orders-of-magnitude power savings for realistic mmWave channels. For example, for a 256-BS-antenna 16-user system supporting 1 GHz bandwidth, a traditional baseline architecture designed for a 64-user worst-case scenario would consume 23 W in 28 nm CMOS for the ADC array and the spatial equalizer, whereas a resolution-adaptive architecture is able to reduce the power consumption by 6.7x.	Millimeter-wave (mmWave) communication [1], [2] offers vast amounts of unused spectrum and is considered a key technology component of fifth-generation (5G) and beyond 5G wireless systems. A major challenge of mmWave communication is the high path loss [3], which can be mitigated with massive multiple-input multiple-output (MIMO) [4]. Massive multi-user (MU) MIMO is able to compensate for the high path loss via fine-grained beamforming while supporting concurrent communication with multiple user equipments (UEs) in the same frequency band. A practical realization of mmWave massive MU-MIMO basestations (BSs), however, faces serious implementation challenges that are caused by the large number of BS antennas and the large communication bandwidth. Two prominent solutions that lower the cost and power consumption of mmWave massive MU-MIMO systems are (i) hybrid digital-analog architectures [5], [6] in which the number of radio-frequency (RF) chains is less than the number of antennas and (ii) all-digital architectures that rely on nonlinear RF chains and low-resolution data converters [7]–[9]. In this paper, we focus on all-digital architectures that are able to exploit the full potential of massive MU-MIMO and offer higher spectral efficiency with comparable power consumption to hybrid architectures [10], [11]. However, in order to keep the system costs and power consumption of all-digital architectures within practical limits, it is indispensable to rely on lowresolution data converters [12] as well as on low-resolution digital baseband processing methods [13]. A. Contributions In this paper, we show that for all-digital massive MUMIMO systems with B BS antennas and U UEs, the system’s load factor defined as β , U/B, as well as other specifics of the communication system (e.g., the modulation and channel conditions) do not only determine the spectral efficiency [14] and robustness against RF impairments [12], [15], [16], but also open up new means to optimize the power consumption of alldigital spatial equalization architectures. More specifically, if the number of active UEs is low, then it is possible to reduce the resolution of the analog-to-digital converters (ADCs) and the resolution of finite-alphabet equalizers, as well as the number of active BS antennas, without noticeably degrading the system’s error-rate performance. We demonstrate that such resolutionadaptive all-digital massive MU-MIMO BS architectures are key to enabling up to two orders-of-magnitude power savings in the ADC array and the all-digital spatial equalizer. Previous work [17]–[19] focused on adapting the resolution of each individual ADC to the channel state. In contrast, we (i) adapt the resolutions of both the ADC array and the spatial equalizer to the instantaneous communication scenario and (ii) study their individual impact on power consumption. B. Notation Boldface uppercase and lowercase letters represent matrices and column vectors, respectively. For a matrix A, the conjugate transpose is AH, the kth column is ak, and the entry on the kth row and `th column is Ak,`. The N × N identity matrix is IN . For a vector a, the kth entry is ak, the `2-norm is kak2, the real part is <{a} and the imaginary parts is ={a}. The `∞-norm and `f∞-norm are defined as kak∞ , maxk |ak| and kakf∞ , max{k<{a}k∞, k={a}k∞}. Expectation with respect to the random vector x is denoted by Ex[·].	1	1
massive MIMO baseband processing	https://wcsl.ece.ucsb.edu/sites/default/files/publications/beamspace_local_mmse_spawc.pdf	Beamspace Local LMMSE: An Efficient Digital Backend for mmWave Massive MIMO	We explore an all-digital architecture for a mmWave massive MIMO cellular uplink in which the number of users scales with the number of antenna elements at the base station. We consider the design of multiuser detection strategies after a spatial DFT, which concentrates the energy of each user onto a few DFT bins in “beamspace.” In this paper, we propose and investigate a local LMMSE receiver that exploits this property, using a small window in beamspace to demodulate each user. The proposed architecture is computationally efficient: the required window size depends on load factor (the number of users divided by the number of antenna elements) and does not scale with the number of elements. We also show that adaptive implementations of such local LMMSE receivers naturally extend to provide implicit channel estimation.	All-digital architectures enable taking full advantage of the large number of antennas that can be integrated in mmWave transceivers, with fully flexible beamforming that enables the number of simultaneous users K sharing the band to scale with the number of antennas N, with scaling ratio, or load factor, β = K N . Standard criteria for beamforming include spatial matched filtering (MF), as well as linear interference suppression using the zero forcing (ZF) or linear minimum mean square error (LMMSE) criteria. Fig. 1(a) depicts the raw bit error rate (BER) achieved by 95% of the mobiles for the picocellular uplink considered in this paper. Clearly, interference suppression becomes necessary for moderate load factors (e.g., β > 1/16), where MF performance is far inferior to that of LMMSE, with the gap persisting even if power control is employed, as shown in Fig. 1(b). However, the computational complexity of LMMSE detection becomes prohibitive for large K and N. Recent efforts at complexity reduction, for both uplink and downlink, include two-stage beamforming strategies [1]– [4]. In [1], a statistical outer beamformer based on grouping mobiles based on similar correlation matrices reduces the effective spatial dimension of the equivalent channels [1], [2]. This is followed by an inner beamformer that suppresses both intra- and inter-group interference, resulting in significant reduction in computation [3], [4]. In the present paper, we propose a Beamspace Local LMMSE algorithm which leverages the sparsity of the spatial channel in mmWave bands. A spatial discrete Fourier transform (DFT) is employed to concentrate the energy of each mobile into a smaller number of DFT bins, i.e., in “beamspace.” We show that performance close to that of standard LMMSE can be obtained by a local LMMSE detector operating on a beamspace window of a size that does not scale with N. We provide analytical rules of thumb for choosing window size as a function of load factor β and target outage rate. We also show how our architecture provides a lowcomplexity solution for implicit channel estimation via an efficient adaptive implementation.	https://arxiv.org/pdf/2410.13838	A 1.2 mm2 416 mW 1.44 M mat/s 64×16 Matrix Preprocessing ASIC for Massive MIMO in 22FDX	Massive multiuser (MU) multiple-input multipleoutput (MIMO) enables concurrent transmission of multiple users to a multi-antenna basestation (BS). To detect the users’ data using linear equalization, the BS must perform preprocessing, which requires, among other tasks, the inversion of a matrix whose dimension equals the number of user data streams. Explicit inversion of large matrices is notoriously difficult to implement due to high complexity, stringent data dependencies that lead to high latency, and high numerical precision requirements. We propose a novel preprocessing architecture based on the block-LDL matrix factorization, which improves parallelism and, hence, reduces latency. We demonstrate the effectiveness of our architecture through (i) massive MU-MIMO system simulations with mmWave channel vectors and (ii) measurements of a 22FDX ASIC, which is, to our knowledge, the first fabricated preprocessing engine for massive MU-MIMO with 64 BS antennas and 16 single-antenna users. Our ASIC reaches a clock frequency of 870 MHz while consuming 416 mW. At its peak throughput, the ASIC preprocesses 1.44 M 64 × 16 matrices per second at a latency of only 0.7 µs.	Modern wireless communication systems leverage massive multiple-input multiple-output (MIMO) to enable multiuser (MU) communication at high data rates [1]. To enable efficient hardware implementation of data detection at the basestation (BS), one typically resorts to linear methods, such as linear minimum mean square error (LMMSE)-based equalization [2]. The complexity of such approaches, however, grows quickly for systems that must support a large number of simultaneouslytransmitting users. In particular, the complexity of preprocessing, which computes the LMMSE filter matrix every time the channel changes, grows cubically in the number of users for all methods that have been implemented in hardware. Besides minimizing complexity, the preprocessing latency must be kept at a minimum to adhere to the stringent latency constraints of modern wireless systems. While approximate preprocessing methods that scale only quadratically in the number of users have been proposed [3], they only perform well (i) if the number of BS antennas is substantially larger than the number of users and (ii) the users’ channels are sufficiently distinct. Therefore, efficient and also exact matrix preprocessing algorithms and hardware implementations are crucial to meeting the latency and quality constraints of massive MU-MIMO systems. A. Contributions We propose the first fabricated ASIC of an exact matrix preprocessing engine for LMMSE-based data detection in massive MU-MIMO systems with 64 BS antennas and 16 single-antenna users. Our architecture carries out the following steps: (i) Gram-matrix computation, (ii) block-LDL (BLDL) matrix factorization, and (iii) backward substitution. To reduce complexity, we utilize the method from [4] to skip the otherwise necessary forward-substitution step. In contrast to other matrixfactorization methods, our BLDL-based architecture processes more data items in parallel, which reduces latency. To reduce silicon area, steps (i) and (ii) share the same hardware resources. A comparison with existing designs reveals that our fabricated and measured ASIC outperforms other designs in terms of throughput, area, latency, and/or error-rate performance. B. Relevant Prior Work A variety of algorithms for explicit matrix inversion exist, such as methods based on the Cholesky, LU, LDL, and QR matrix factorizations [5]. Several hardware architectures for preprocessing and LMMSE-based data detection in small-scale MIMO systems have been proposed: Reference [6] implements matrix inversion using rank-1 updates; references [7] and [8] perform an LU and LDL factorization, respectively, followed by forward and backward substitution; and references [9], [10] perform a QR factorization followed by inversion of the triangular matrix. For massive MU-MIMO systems, reference [11] provides synthesis results of a 128 × 16 preprocessing engine that uses the Cholesky decomposition followed by forward and backward substitution. References [12], [13] implement an approximate matrix inversion based on the Neumann series, which reduces complexity but sacrifices error-rate performance. In contrast, we propose an exact 64 × 16 BLDL-based matrixpreprocessing engine that avoids backward substitution, which reduces latency and complexity. Furthermore, we provide measurement results of a fabricated ASIC in 22FDX. C. Notation Boldface lowercase and uppercase letters represent column vectors and matrices, respectively. For a matrix G partitioned into 2 × 2 blocks, Gij ∈ C 2×2 is the submatrix formed by the elements of G consisting of the rows (2(i−1)+1 : 2(i−1)+2) and columns (2(j − 1) + 1 : 2(j − 1) + 2). The Hermitian transpose of G is GH, and the entry on the mth row and nth column is gmn. Complex conjugation is indicated by the superscript ∗ . The N × N identity matrix is IN .	0	0
massive MIMO baseband processing	https://wcsl.ece.ucsb.edu/sites/default/files/publications/beamspace_local_mmse_spawc.pdf	Beamspace Local LMMSE: An Efficient Digital Backend for mmWave Massive MIMO	We explore an all-digital architecture for a mmWave massive MIMO cellular uplink in which the number of users scales with the number of antenna elements at the base station. We consider the design of multiuser detection strategies after a spatial DFT, which concentrates the energy of each user onto a few DFT bins in “beamspace.” In this paper, we propose and investigate a local LMMSE receiver that exploits this property, using a small window in beamspace to demodulate each user. The proposed architecture is computationally efficient: the required window size depends on load factor (the number of users divided by the number of antenna elements) and does not scale with the number of elements. We also show that adaptive implementations of such local LMMSE receivers naturally extend to provide implicit channel estimation.	All-digital architectures enable taking full advantage of the large number of antennas that can be integrated in mmWave transceivers, with fully flexible beamforming that enables the number of simultaneous users K sharing the band to scale with the number of antennas N, with scaling ratio, or load factor, β = K N . Standard criteria for beamforming include spatial matched filtering (MF), as well as linear interference suppression using the zero forcing (ZF) or linear minimum mean square error (LMMSE) criteria. Fig. 1(a) depicts the raw bit error rate (BER) achieved by 95% of the mobiles for the picocellular uplink considered in this paper. Clearly, interference suppression becomes necessary for moderate load factors (e.g., β > 1/16), where MF performance is far inferior to that of LMMSE, with the gap persisting even if power control is employed, as shown in Fig. 1(b). However, the computational complexity of LMMSE detection becomes prohibitive for large K and N. Recent efforts at complexity reduction, for both uplink and downlink, include two-stage beamforming strategies [1]– [4]. In [1], a statistical outer beamformer based on grouping mobiles based on similar correlation matrices reduces the effective spatial dimension of the equivalent channels [1], [2]. This is followed by an inner beamformer that suppresses both intra- and inter-group interference, resulting in significant reduction in computation [3], [4]. In the present paper, we propose a Beamspace Local LMMSE algorithm which leverages the sparsity of the spatial channel in mmWave bands. A spatial discrete Fourier transform (DFT) is employed to concentrate the energy of each mobile into a smaller number of DFT bins, i.e., in “beamspace.” We show that performance close to that of standard LMMSE can be obtained by a local LMMSE detector operating on a beamspace window of a size that does not scale with N. We provide analytical rules of thumb for choosing window size as a function of load factor β and target outage rate. We also show how our architecture provides a lowcomplexity solution for implicit channel estimation via an efficient adaptive implementation.	https://arxiv.org/pdf/1910.00756	Beamspace Channel Estimation for Massive MIMO mmWave Systems: Algorithm and VLSI Design	Millimeter-wave (mmWave) communication in combination with massive multiuser multiple-input multiple-output (MU-MIMO) enables high-bandwidth data transmission to multiple users in the same time-frequency resource. The strong path loss of wave propagation at such high frequencies necessitates accurate channel state information to ensure reliable data transmission. We propose a novel channel estimation algorithm called BEAmspace CHannel EStimation (BEACHES), which leverages the fact that wave propagation at mmWave frequencies is predominantly directional. BEACHES adaptively denoises the channel vectors in the beamspace domain using an adaptive shrinkage procedure that relies on Stein’s unbiased risk estimator (SURE). Simulation results for line-of-sight (LoS) and non-LoS mmWave channels reveal that BEACHES performs on par with state-ofthe-art channel estimation methods while requiring orders-ofmagnitude lower complexity. To demonstrate the effectiveness of BEACHES in practice, we develop a very large-scale integration (VLSI) architecture and provide field-programmable gate array (FPGA) implementation results. Our results show that adaptive channel denoising can be performed at high throughput and in a hardware-friendly manner for massive MU-MIMO mmWave systems with hundreds of antennas. 	Millimeter-wave (mmWave) communication [2], [3] and massive multiuser (MU) multiple-input multiple-output (MIMO) [4], [5] are expected to be core technologies of nextgeneration wireless communication systems. By combining both of these technologies, one can achieve unprecedentedly high-bandwidth data transmission to multiple user equipments (UEs) in the same time-frequency resource via fine-grained beamforming. The strong path loss of wave propagation at mmWave frequencies necessitates the infrastructure basestations (BSs) to acquire accurate channel state information (CSI) in order to perform data detection in the uplink (UEs transmit to BS) and MU precoding in the downlink (BS transmits to UEs) [6], [7]. To optimally determine the beamforming weights, accurate CSI is not only of paramount importance for hybrid analog-digital BS architectures [8]–[10] but also for emerging all-digital BS architectures [11], [12]. In addition, the trend towards BS architectures with low-precision data converters to reduce power consumption, interconnect bandwidth, and system costs [13]–[15] requires novel algorithms and hardware designs that denoise the estimated channel vectors. A. Sparsity-Based Channel Estimation Fortunately, wave propagation at mmWave frequencies is predominantly directional and real-world channels typically comprise only a small number of strong propagation paths, such as a line-of-sight (LoS) component and a few first-order reflections [16]. These properties enable the design of sparsityexploiting CSI estimation algorithms that effectively suppress channel estimation errors [17]–[20]. Compressive sensing (CS)- based methods have been proposed for mmWave channel estimation in [21], [22], including methods that rely upon orthogonal matching pursuit (OMP) [22]–[24]. The majority of such methods uses a discretization procedure of the number of propagation paths that can be resolved in the beamspace (or angular) domain [25], which results in a problem widely known as basis mismatch [26]. To avoid the basis mismatch problem, sparse channel estimation for mmWave channels can, for example, be accomplished with atomic norm minimization (ANM) [27], [28] or Newtonized OMP [29]. ANM estimates a discrete set of propagation paths off-the-grid by solving a semidefinite program (SDP). Newtonized OMP (NOMP) is a more efficient alternative to ANM and iteratively refines the incident angles of the dominant propagation paths off-the-grid with a complexity only slightly higher than that of conventional OMP. Although both of these methods do not suffer from the basis mismatch problem and exhibit excellent denoising performance, they entail high computational complexity. Hence, from a hardware-implementation perspective, such methods are less attractive, especially in massive MU-MIMO systems where the complexity is dominated by the large number of BS antennas. In addition, the performance of both of these methods strongly depends on algorithm parameters that need to be tuned for the given propagation conditions. Another strain of sparsity-exploiting channel-estimation methods build upon approximate message passing (AMP) [30], [31]. While such methods promise high estimation accuracy, they suffer from a number of drawbacks when implemented in VLSI. AMP-based methods require at least two matrix-vector multiplications in each iteration, whose dimension scales with the number of BS antennas, the number of UEs, and the pilot sequence length. In addition, each iteration requires multiple divisions and other nonlinear functions (such as exponentials and Q-functions). As shown in [32], the presence of such nonlinear functions in AMP-based algorithms causes finiteprecision issues when implemented with fixed-point arithmetic. Sparsity has been exploited in many other applications in communication systems, including beam selection in mmWave systems [33], channel estimation for angle-division multiple access [34], and sparse signal recovery via compressive sensing [35]. Even though these results are not directly related to channel estimation in mmWave systems, the proposed adaptive denoising approach might find use in such applications. B. Contributions In order to perform denoising-based channel estimation in real-world systems, we propose a low-complexity and adaptive channel estimation algorithm for massive MU-MIMO mmWave systems that can be implemented efficiently in VLSI. Our main contributions are summarized as follows: • We propose a novel channel estimation algorithm that relies on Stein’s unbiased risk estimator (SURE), which we call BEAmspace CHannel EStimation (BEACHES). BEACHES exploits sparsity of mmWave channels in the beamspace domain and adaptively denoises the channel vectors at a fixed computational complexity that scales with O(B log(B)), where B is the number of BS antennas. • We prove that BEACHES minimizes the mean-square error (MSE) between the noiseless and denoised channel vector in the large-antenna limit, i.e., when B → ∞, without requiring tedious parameter tuning. • We evaluate the efficacy of BEACHES for LoS and non-LoS mmWave channel models and show that it performs on par with state-of-the-art channel estimation algorithms in terms of uncoded bit error-rate, but at ordersof-magnitude lower computational complexity. • We develop a very large-scale integration (VLSI) architecture and present corresponding field-programmable gate array (FPGA) implementation results, which demonstrate that BEACHES enables high-throughput channel estimation in a hardware-efficient manner. C. Notation Lowercase and uppercase boldface letters designate column vectors and matrices, respectively. For a vector a, the kth entry is denoted by [a]k = ak; the real and imaginary parts are indicated with [a]R = aR and [a]I = aI, respectively. The `1-norm and `2-norm of a vector a is kak1 and kak2, respectively. For a matrix A, we define its transpose and conjugate transpose as AT and AH, respectively. The N × M all-zeros, N ×N identity, and N ×N discrete Fourier transform (DFT) matrices are 0N×M, IN , and F, respectively; the DFT matrix is normalized so that FFH = IN . Vectors in the DFT domain are designated with a hat as in aˆ = Fa. A proper complex-valued Gaussian vector a with mean vector m and covariance matrix K is written as a ∼ CN (m, K) and its probability density function (PDF) as f CN (a; m, K). A real-valued Gaussian vector a with mean vector m and covariance matrix K is written as a ∼ N (m, K) and its PDF as f N (a; m, K). The expectation operator is E[·]. Optimal values are designated with the superscript ? . D. Paper Outline The rest of the paper is organized as follows. Section II introduces the system model and outlines the concept of denoising-based beamspace channel estimation. Section III details the BEACHES algorithm and presents the simulation results. Section IV proposes a VLSI architecture and provides FPGA implementation results. We conclude in Section V. All proofs are relegated to the appendices.	1	0
massive MIMO baseband processing	https://arxiv.org/pdf/2211.03624	Extremely-Fast, Energy-Efficient Massive MIMO Precoding with Analog RRAM Matrix Computing	Signal processing in wireless communications, such as precoding, detection, and channel estimation, are basically about solving inverse matrix problems, which, however, are slow and inefficient in conventional digital computers, thus requiring a radical paradigm shift to achieve fast, real-time solutions. Here, for the first time, we apply the emerging analog matrix computing (AMC) to the linear precoding of massive MIMO. The real-valued AMC concept is extended to process complex-valued signals. In order to adapt the MIMO channel models to RRAM conductance mapping, a new matrix inversion circuit is developed. In addition, fully analog dataflow and optimized operational amplifiers are designed to support AMC precoding implementation. Simulation results show that the zero-forcing precoding is solved within 20 ns for a 16x128 MIMO system, which is two orders of magnitude faster than the conventional digital approach. Meanwhile, the energy efficiency is improved by 50x.	Massive multiple input multiple output (MIMO) is an enabling technology of the 5th generation (5G) mobile network. Compared with the conventional MIMO used in the earlier generations, such as 4×4 or 8×8 antenna configurations in LTE-advanced 4G, the number of antennas in massive MIMO is radically increased [1]. As a result, the spectral capacity, data rate, and energy efficiency of wireless communications are significantly improved [2], [3]. Looking forward to the upcoming 6G, the ultra-massive MIMO is expected to be equipped with >1000 antennas for further performance improvements [4]. To deliver the desired throughput and efficiency of massive MIMO, precoding is usually used to minimize the interference between individual user channels. Compared with the complicated, expensive dirty-paper coding method, zeroforcing (ZF) linear precoding is advantageous in terms of computing resource demands, while achieving near optimal performance [5]. ZF precoding is essentially about solving a system of linear equations (or a generalized inverse matrix problem) in the complex number domain, which features a cubic computational complexity and thus imposing a heavy workload to the baseband processors. To accelerate ZF precoding in the conventional digital paradigm, various algorithmic optimizations have been proposed, such as QR decomposition [6], Gauss-Jordan elimination [7], and Neumann series [8]-[10]. However, the optimization is fundamentally limited by the binary representation, logic gates, and sequential processing, which prevent the reduction of the order of digital algorithm complexity. In addition, digital computers adopt the von Neumann architecture, where the communications between the memory and the processor cause extra costs of time and energy that are aggravated in data-intensive applications [11]. Recently, analog matrix computing (AMC) with resistive memory devices has been developed for fast, efficient matrix computations [12]. A device prototype is the resistive randomaccess memory (RRAM), which features simple device structure and good analog conductance capability. A crosspoint RRAM array represent naturally a physical analog matrix, based on which the basic matrix operations can be realized, including matrix multiplication [13], matrix inversion and eigenvector [14], and generalized inverse [15]. Due to the high spatial parallelism of the crosspoint array, the time complexity of AMC is dramatically reduced, literally reaching O(1) [16]- [18]. As the computation is carried out in situ in the memory array (and the peripheral circuits), this approach realizes inmemory computing to save communication costs. In this work, for the first time, we apply the AMC circuits to the precoding of massive MIMO. We designed an AMC-based linear precoding architecture, by developing a fully analog dataflow including the matrix multiplication and inversion circuits. The complex-valued matrix/vector data can be easily accommodated in the AMC circuits for fast computations. According to the parameter distribution of the MIMO channel model, the matrix inversion circuit is modified to take full advantage of the RRAM conductance range. Based on the RRAM model and the optimized operational amplifiers, ZF precoding with the AMC architecture is completed within only 20 ns, demonstrating significant advantages of computing speed and energy efficiency over traditional digital approaches. The bit error rate is also analyzed to validate the adequacy of AMC for reliable wireless communications.	https://arxiv.org/pdf/1907.08641	PPAC: A Versatile In-Memory Accelerator for Matrix-Vector-Product-Like Operations	Processing in memory (PIM) moves computation into memories with the goal of improving throughput and energy-efficiency compared to traditional von Neumann-based architectures. Most existing PIM architectures are either general-purpose but only support atomistic operations, or are specialized to accelerate a single task. We propose the Parallel Processor in Associative Content-addressable memory (PPAC), a novel in-memory accelerator that supports a range of matrix-vector-product (MVP)-like operations that find use in traditional and emerging applications. PPAC is, for example, able to accelerate low-precision neural networks, exact/approximate hash lookups, cryptography, and forward error correction. The fully-digital nature of PPAC enables its implementation with standard-cell-based CMOS, which facilitates automated design and portability among technology nodes. To demonstrate the efficacy of PPAC, we provide post-layout implementation results in 28nm CMOS for different array sizes. A comparison with recent digital and mixed-signal PIM accelerators reveals that PPAC is competitive in terms of throughput and energy-efficiency, while accelerating a wide range of applications and simplifying development.	Traditional von Neumann-based architectures have taken a variety of forms that trade-off flexibility with hardware efficiency. Central processing units (CPUs) are able to compute any given task that can be expressed as a computer program. In contrast, application-specific integrated circuits (ASICs) are specialized to accelerate a single task but achieve (often significantly) higher throughputs and superior energy-efficiency. In between reside graphics processing units (GPUs) and fieldprogrammable gate arrays (FPGAs), that are more specialized than CPUs, but typically offer higher throughput and energyefficiency for the supported tasks. The ever-growing gap between computing performance and memory access times has lead today’s von Neumann-based computing systems to hit a socalled “memory wall” [1], which describes the phenomenon that most of a system’s bandwidth, energy, and time is consumed by memory operations. This problem is further aggravated with the rise of applications, such as machine learning, data mining, or 5G wireless systems, where massive amounts of data need to be processed at high rates and in an energy-efficient way. A. Processing In Memory Processing in memory (PIM) is an emerging computing paradigm that promises to tear down the memory wall [2]. Put simply, PIM brings computation closer to the memories, with the objective of reducing the time and energy of memory accesses, which ultimately increases the circuit’s overall efficiency (see Fig. 1 for an illustration). The application of PIM to general-purpose processors has been explored recently in [3]– [5]. While such PIM-aided CPUs enable improved throughput and energy-efficiency for certain memory-intensive workloads, the supported PIM operations are typically limited to atomistic operations (such as bit-wise AND/NOR). As a consequence, executing even slightly more complex operations (such as multibit additions or multiplications) requires a repeated use of the supported PIM operations; this prevents such architectures from reaching the throughput and energy-efficiency required in many of today’s applications. Hence, a number of PIM-based ASICs have been explored recently in [6]–[10]. Such solutions generally excel in throughput and energy-efficiency, but have limited applicability, often accelerating a single task only. For example, the PIM-ASIC in [6] is designed to accelerate neural network inference using mixed-signal techniques, but suffers from effects caused by noise and process variation; this prevents its use in applications in which the least significant bit must be computed accurately (e.g., in cryptography, forward error B. Contributions While a range of PIM-based ASICs and CPUs have been proposed in recent years, to the best of our knowledge, no PIM-based solutions exist that simultaneously offer high flexibility and high efficiency. To fill in this void in the tradeoff space with PIM-based hardware solutions (see Fig. 1), we propose a novel, versatile in-memory processor called Parallel Processor in Associative Content-addressable memory (PPAC), which supports a range of matrix-vector-product (MVP)-like operations. PPAC is designed entirely in digital standardcell-based CMOS, accelerates some of the key operations in a wide range of traditional and emerging applications, and achieves high throughput and energy-efficiency for the supported tasks. The proposed architecture consists of a twodimensional array of latch-based bit-cells that support two types of binary-valued operations; each row of the PPAC array is equipped with a row arithmetic-logic unit (ALU) that supports a variety of tasks, including content-addressable memory (CAM) functionality, Hamming-distance calculation, one- and multibit MVPs, Galois field of two elements GF(2) MVPs, and programmable logic array (PLA) functionality. We provide postlayout implementation results in a 28 nm CMOS technology and compare the area, throughput, and energy-efficiency to that of recent related accelerators. C. Paper Outline The rest of the paper is organized as follows. In Section II, we describe the operating principle and architecture of PPAC. In Section III, we detail all operation modes and outline potential use cases. In Section IV, we present post-layout implementation results and compare PPAC to related accelerator designs. We conclude in Section V. 	0	0
massive MIMO baseband processing	https://arxiv.org/pdf/2407.06755	A 46 Gbps 12 pJ/b Sparsity-Adaptive Beamspace Equalizer for mmWave Massive MIMO in 22FDX	We present a GlobalFoundries 22FDX FD-SOI application-specific integrated circuit (ASIC) of a beamspace equalizer for millimeter-wave (mmWave) massive multiple-input multiple-output (MIMO) systems. The ASIC implements a recently-proposed power-saving technique called sparsity-adaptive equalization (SPADE). SPADE exploits the inherent sparsity of mmWave channels in the beamspace domain to reduce the dynamic power of matrix-vector products by skipping multiplications for which the magnitude of both operands are below pre-defined thresholds. Simulations with realistic mmWave channels show that SPADE incurs less than 0.7dB SNR degradation at 1% target bit error rate compared to antenna-domain equalization. ASIC measurement results demonstrate an equalization throughput of 46Gbps and show that SPADE offers up to 38% power savings compared to antenna-domain equalization. A comparison with state-of-the-art massive MIMO equalizer designs reveals that our ASIC achieves superior normalized energy efficiency.	Fifth generation (5G) and beyond-5G wireless communication systems take advantage of large contiguous portions of the available spectrum at millimeter-wave (mmWave) frequencies to enable wideband communication [1]. Corresponding basestations (BSs) rely on massive multiple-input multiple-output (MIMO) [2], which (i) mitigates the high path loss at mmWave frequencies [3] and (ii) enables multi-user (MU) communication by means of spatial multiplexing. Wideband communication requires high baseband sampling rates and massive MU-MIMO generates high-dimensional data—together, they significantly increase hardware complexity. In this paper, we present a hardware implementation of a technique that reduces the power consumption of data detection. 1) Beamspace Processing: A promising approach to reducing complexity of data detection in all-digital mmWave massive MU-MIMO systems is to exploit the inherent sparsity of mmWave channels [3], [4] in the so-called beamspace. Converting a system from antenna-domain into beamspace is achieved by applying a spatial discrete Fourier transform (DFT) to the signals received at a uniform linear antenna array [5]–[10]. Uplink data detection in beamspace, with the goal of reducing implementation complexity, has been studied recently for mmWave massive MU-MIMO systems, mainly in the context of linear data detectors, as nonlinear methods typically incur higher complexity. Linear data detection consists of two phases: (i) preprocessing, where an equalization matrix is computed based on a channel-matrix estimate and (ii) equalization, where the equalization matrix is multiplied to the received vectors to obtain estimates of the transmitted data symbols. While preprocessing is performed only once per coherence interval, equalization must be performed for each received vector, hence, at much higher rates than preprocessing. In this paper, we focus on reducing the complexity of equalization and assume that preprocessing is performed externally. Existing beamspace data detectors reduce equalization complexity by designing sparse equalization matrices with specific sparsity patterns, thereby reducing the number of multiplications required for equalization. Such sparsity-exploiting beamspace data detectors, however, either incur a notable performance degradation compared to conventional antenna-domain linear minimum mean squared error (LMMSE) equalization, e.g., [5], [6], or require preprocessing algorithms with extremely high computational complexity [7]. In [8], a different approach to reduce complexity by exploiting beamspace sparsity was proposed. The method is referred to as sparsity-adaptive equalization (SPADE) and leverages the fact that the LMMSE equalization matrix is already approximately sparse in beamspace and avoids computing a sparse equalization matrix with a specific sparsity pattern. To reduce equalization complexity, SPADE uses two pre-computed thresholds to skip multiplications whenever the absolute value of both operands are below these thresholds. As shown in [8], SPADE significantly reduces the number of required multiplications, while exhibiting comparable performance to state-of-the-art linear beamspace data detectors [5]–[7]. 2) Contributions: We present the first application specific integrated circuit (ASIC) capable of performing SPADE-based beamspace equalization as well as antenna-domain equalization for a massive MU-MIMO system with 64 BS antennas and up to 16 single-antenna user equipments (UEs). In addition, we demonstrate real-world power savings achieved by SPADEbased beamspace equalization over conventional, antennadomain equalization through extensive ASIC measurements. 3) Notation: Boldface lowercase and uppercase letters represent column vectors and matrices, respectively. For a matrix A, the transpose is AT and Hermitian transpose AH. The mth column of A is am = [A]m, and the entry on the mth row and nth column is Am,n = [A]m,n. For a vector a, the kth entry is ak = [a]k, and the real and imaginary parts are a R and a I , respectively. The ℓ∞- and ℓf∞-norm is ∥a∥∞ ≜ maxk |ak| and ∥a∥f∞ ≜ max{∥a R∥∞, ∥a I ∥∞}, respectively [11]. Bars over variables indicate antenna-domain quantities. Expectation with respect to a random vector a is denoted by Ea[·].	https://arxiv.org/pdf/1910.00756	Beamspace Channel Estimation for Massive MIMO mmWave Systems: Algorithm and VLSI Design	Millimeter-wave (mmWave) communication in combination with massive multiuser multiple-input multiple-output (MU-MIMO) enables high-bandwidth data transmission to multiple users in the same time-frequency resource. The strong path loss of wave propagation at such high frequencies necessitates accurate channel state information to ensure reliable data transmission. We propose a novel channel estimation algorithm called BEAmspace CHannel EStimation (BEACHES), which leverages the fact that wave propagation at mmWave frequencies is predominantly directional. BEACHES adaptively denoises the channel vectors in the beamspace domain using an adaptive shrinkage procedure that relies on Stein’s unbiased risk estimator (SURE). Simulation results for line-of-sight (LoS) and non-LoS mmWave channels reveal that BEACHES performs on par with state-ofthe-art channel estimation methods while requiring orders-ofmagnitude lower complexity. To demonstrate the effectiveness of BEACHES in practice, we develop a very large-scale integration (VLSI) architecture and provide field-programmable gate array (FPGA) implementation results. Our results show that adaptive channel denoising can be performed at high throughput and in a hardware-friendly manner for massive MU-MIMO mmWave systems with hundreds of antennas. 	Millimeter-wave (mmWave) communication [2], [3] and massive multiuser (MU) multiple-input multiple-output (MIMO) [4], [5] are expected to be core technologies of nextgeneration wireless communication systems. By combining both of these technologies, one can achieve unprecedentedly high-bandwidth data transmission to multiple user equipments (UEs) in the same time-frequency resource via fine-grained beamforming. The strong path loss of wave propagation at mmWave frequencies necessitates the infrastructure basestations (BSs) to acquire accurate channel state information (CSI) in order to perform data detection in the uplink (UEs transmit to BS) and MU precoding in the downlink (BS transmits to UEs) [6], [7]. To optimally determine the beamforming weights, accurate CSI is not only of paramount importance for hybrid analog-digital BS architectures [8]–[10] but also for emerging all-digital BS architectures [11], [12]. In addition, the trend towards BS architectures with low-precision data converters to reduce power consumption, interconnect bandwidth, and system costs [13]–[15] requires novel algorithms and hardware designs that denoise the estimated channel vectors. A. Sparsity-Based Channel Estimation Fortunately, wave propagation at mmWave frequencies is predominantly directional and real-world channels typically comprise only a small number of strong propagation paths, such as a line-of-sight (LoS) component and a few first-order reflections [16]. These properties enable the design of sparsityexploiting CSI estimation algorithms that effectively suppress channel estimation errors [17]–[20]. Compressive sensing (CS)- based methods have been proposed for mmWave channel estimation in [21], [22], including methods that rely upon orthogonal matching pursuit (OMP) [22]–[24]. The majority of such methods uses a discretization procedure of the number of propagation paths that can be resolved in the beamspace (or angular) domain [25], which results in a problem widely known as basis mismatch [26]. To avoid the basis mismatch problem, sparse channel estimation for mmWave channels can, for example, be accomplished with atomic norm minimization (ANM) [27], [28] or Newtonized OMP [29]. ANM estimates a discrete set of propagation paths off-the-grid by solving a semidefinite program (SDP). Newtonized OMP (NOMP) is a more efficient alternative to ANM and iteratively refines the incident angles of the dominant propagation paths off-the-grid with a complexity only slightly higher than that of conventional OMP. Although both of these methods do not suffer from the basis mismatch problem and exhibit excellent denoising performance, they entail high computational complexity. Hence, from a hardware-implementation perspective, such methods are less attractive, especially in massive MU-MIMO systems where the complexity is dominated by the large number of BS antennas. In addition, the performance of both of these methods strongly depends on algorithm parameters that need to be tuned for the given propagation conditions. Another strain of sparsity-exploiting channel-estimation methods build upon approximate message passing (AMP) [30], [31]. While such methods promise high estimation accuracy, they suffer from a number of drawbacks when implemented in VLSI. AMP-based methods require at least two matrix-vector multiplications in each iteration, whose dimension scales with the number of BS antennas, the number of UEs, and the pilot sequence length. In addition, each iteration requires multiple divisions and other nonlinear functions (such as exponentials and Q-functions). As shown in [32], the presence of such nonlinear functions in AMP-based algorithms causes finiteprecision issues when implemented with fixed-point arithmetic. Sparsity has been exploited in many other applications in communication systems, including beam selection in mmWave systems [33], channel estimation for angle-division multiple access [34], and sparse signal recovery via compressive sensing [35]. Even though these results are not directly related to channel estimation in mmWave systems, the proposed adaptive denoising approach might find use in such applications. B. Contributions In order to perform denoising-based channel estimation in real-world systems, we propose a low-complexity and adaptive channel estimation algorithm for massive MU-MIMO mmWave systems that can be implemented efficiently in VLSI. Our main contributions are summarized as follows: • We propose a novel channel estimation algorithm that relies on Stein’s unbiased risk estimator (SURE), which we call BEAmspace CHannel EStimation (BEACHES). BEACHES exploits sparsity of mmWave channels in the beamspace domain and adaptively denoises the channel vectors at a fixed computational complexity that scales with O(B log(B)), where B is the number of BS antennas. • We prove that BEACHES minimizes the mean-square error (MSE) between the noiseless and denoised channel vector in the large-antenna limit, i.e., when B → ∞, without requiring tedious parameter tuning. • We evaluate the efficacy of BEACHES for LoS and non-LoS mmWave channel models and show that it performs on par with state-of-the-art channel estimation algorithms in terms of uncoded bit error-rate, but at ordersof-magnitude lower computational complexity. • We develop a very large-scale integration (VLSI) architecture and present corresponding field-programmable gate array (FPGA) implementation results, which demonstrate that BEACHES enables high-throughput channel estimation in a hardware-efficient manner. C. Notation Lowercase and uppercase boldface letters designate column vectors and matrices, respectively. For a vector a, the kth entry is denoted by [a]k = ak; the real and imaginary parts are indicated with [a]R = aR and [a]I = aI, respectively. The `1-norm and `2-norm of a vector a is kak1 and kak2, respectively. For a matrix A, we define its transpose and conjugate transpose as AT and AH, respectively. The N × M all-zeros, N ×N identity, and N ×N discrete Fourier transform (DFT) matrices are 0N×M, IN , and F, respectively; the DFT matrix is normalized so that FFH = IN . Vectors in the DFT domain are designated with a hat as in aˆ = Fa. A proper complex-valued Gaussian vector a with mean vector m and covariance matrix K is written as a ∼ CN (m, K) and its probability density function (PDF) as f CN (a; m, K). A real-valued Gaussian vector a with mean vector m and covariance matrix K is written as a ∼ N (m, K) and its PDF as f N (a; m, K). The expectation operator is E[·]. Optimal values are designated with the superscript ? . D. Paper Outline The rest of the paper is organized as follows. Section II introduces the system model and outlines the concept of denoising-based beamspace channel estimation. Section III details the BEACHES algorithm and presents the simulation results. Section IV proposes a VLSI architecture and provides FPGA implementation results. We conclude in Section V. All proofs are relegated to the appendices.	1	0
massive MIMO baseband processing	https://wcsl.ece.ucsb.edu/sites/default/files/publications/beamspace_local_mmse_spawc.pdf	Beamspace Local LMMSE: An Efficient Digital Backend for mmWave Massive MIMO	We explore an all-digital architecture for a mmWave massive MIMO cellular uplink in which the number of users scales with the number of antenna elements at the base station. We consider the design of multiuser detection strategies after a spatial DFT, which concentrates the energy of each user onto a few DFT bins in “beamspace.” In this paper, we propose and investigate a local LMMSE receiver that exploits this property, using a small window in beamspace to demodulate each user. The proposed architecture is computationally efficient: the required window size depends on load factor (the number of users divided by the number of antenna elements) and does not scale with the number of elements. We also show that adaptive implementations of such local LMMSE receivers naturally extend to provide implicit channel estimation.	All-digital architectures enable taking full advantage of the large number of antennas that can be integrated in mmWave transceivers, with fully flexible beamforming that enables the number of simultaneous users K sharing the band to scale with the number of antennas N, with scaling ratio, or load factor, β = K N . Standard criteria for beamforming include spatial matched filtering (MF), as well as linear interference suppression using the zero forcing (ZF) or linear minimum mean square error (LMMSE) criteria. Fig. 1(a) depicts the raw bit error rate (BER) achieved by 95% of the mobiles for the picocellular uplink considered in this paper. Clearly, interference suppression becomes necessary for moderate load factors (e.g., β > 1/16), where MF performance is far inferior to that of LMMSE, with the gap persisting even if power control is employed, as shown in Fig. 1(b). However, the computational complexity of LMMSE detection becomes prohibitive for large K and N. Recent efforts at complexity reduction, for both uplink and downlink, include two-stage beamforming strategies [1]– [4]. In [1], a statistical outer beamformer based on grouping mobiles based on similar correlation matrices reduces the effective spatial dimension of the equivalent channels [1], [2]. This is followed by an inner beamformer that suppresses both intra- and inter-group interference, resulting in significant reduction in computation [3], [4]. In the present paper, we propose a Beamspace Local LMMSE algorithm which leverages the sparsity of the spatial channel in mmWave bands. A spatial discrete Fourier transform (DFT) is employed to concentrate the energy of each mobile into a smaller number of DFT bins, i.e., in “beamspace.” We show that performance close to that of standard LMMSE can be obtained by a local LMMSE detector operating on a beamspace window of a size that does not scale with N. We provide analytical rules of thumb for choosing window size as a function of load factor β and target outage rate. We also show how our architecture provides a lowcomplexity solution for implicit channel estimation via an efficient adaptive implementation.	https://arxiv.org/pdf/2407.06755	A 46 Gbps 12 pJ/b Sparsity-Adaptive Beamspace Equalizer for mmWave Massive MIMO in 22FDX	We present a GlobalFoundries 22FDX FD-SOI application-specific integrated circuit (ASIC) of a beamspace equalizer for millimeter-wave (mmWave) massive multiple-input multiple-output (MIMO) systems. The ASIC implements a recently-proposed power-saving technique called sparsity-adaptive equalization (SPADE). SPADE exploits the inherent sparsity of mmWave channels in the beamspace domain to reduce the dynamic power of matrix-vector products by skipping multiplications for which the magnitude of both operands are below pre-defined thresholds. Simulations with realistic mmWave channels show that SPADE incurs less than 0.7dB SNR degradation at 1% target bit error rate compared to antenna-domain equalization. ASIC measurement results demonstrate an equalization throughput of 46Gbps and show that SPADE offers up to 38% power savings compared to antenna-domain equalization. A comparison with state-of-the-art massive MIMO equalizer designs reveals that our ASIC achieves superior normalized energy efficiency.	Fifth generation (5G) and beyond-5G wireless communication systems take advantage of large contiguous portions of the available spectrum at millimeter-wave (mmWave) frequencies to enable wideband communication [1]. Corresponding basestations (BSs) rely on massive multiple-input multiple-output (MIMO) [2], which (i) mitigates the high path loss at mmWave frequencies [3] and (ii) enables multi-user (MU) communication by means of spatial multiplexing. Wideband communication requires high baseband sampling rates and massive MU-MIMO generates high-dimensional data—together, they significantly increase hardware complexity. In this paper, we present a hardware implementation of a technique that reduces the power consumption of data detection. 1) Beamspace Processing: A promising approach to reducing complexity of data detection in all-digital mmWave massive MU-MIMO systems is to exploit the inherent sparsity of mmWave channels [3], [4] in the so-called beamspace. Converting a system from antenna-domain into beamspace is achieved by applying a spatial discrete Fourier transform (DFT) to the signals received at a uniform linear antenna array [5]–[10]. Uplink data detection in beamspace, with the goal of reducing implementation complexity, has been studied recently for mmWave massive MU-MIMO systems, mainly in the context of linear data detectors, as nonlinear methods typically incur higher complexity. Linear data detection consists of two phases: (i) preprocessing, where an equalization matrix is computed based on a channel-matrix estimate and (ii) equalization, where the equalization matrix is multiplied to the received vectors to obtain estimates of the transmitted data symbols. While preprocessing is performed only once per coherence interval, equalization must be performed for each received vector, hence, at much higher rates than preprocessing. In this paper, we focus on reducing the complexity of equalization and assume that preprocessing is performed externally. Existing beamspace data detectors reduce equalization complexity by designing sparse equalization matrices with specific sparsity patterns, thereby reducing the number of multiplications required for equalization. Such sparsity-exploiting beamspace data detectors, however, either incur a notable performance degradation compared to conventional antenna-domain linear minimum mean squared error (LMMSE) equalization, e.g., [5], [6], or require preprocessing algorithms with extremely high computational complexity [7]. In [8], a different approach to reduce complexity by exploiting beamspace sparsity was proposed. The method is referred to as sparsity-adaptive equalization (SPADE) and leverages the fact that the LMMSE equalization matrix is already approximately sparse in beamspace and avoids computing a sparse equalization matrix with a specific sparsity pattern. To reduce equalization complexity, SPADE uses two pre-computed thresholds to skip multiplications whenever the absolute value of both operands are below these thresholds. As shown in [8], SPADE significantly reduces the number of required multiplications, while exhibiting comparable performance to state-of-the-art linear beamspace data detectors [5]–[7]. 2) Contributions: We present the first application specific integrated circuit (ASIC) capable of performing SPADE-based beamspace equalization as well as antenna-domain equalization for a massive MU-MIMO system with 64 BS antennas and up to 16 single-antenna user equipments (UEs). In addition, we demonstrate real-world power savings achieved by SPADEbased beamspace equalization over conventional, antennadomain equalization through extensive ASIC measurements. 3) Notation: Boldface lowercase and uppercase letters represent column vectors and matrices, respectively. For a matrix A, the transpose is AT and Hermitian transpose AH. The mth column of A is am = [A]m, and the entry on the mth row and nth column is Am,n = [A]m,n. For a vector a, the kth entry is ak = [a]k, and the real and imaginary parts are a R and a I , respectively. The ℓ∞- and ℓf∞-norm is ∥a∥∞ ≜ maxk |ak| and ∥a∥f∞ ≜ max{∥a R∥∞, ∥a I ∥∞}, respectively [11]. Bars over variables indicate antenna-domain quantities. Expectation with respect to a random vector a is denoted by Ea[·].	0	1
fundamental limits of in-memory computing architectures	https://shanbhag.ece.illinois.edu/publications/IMC-Precision-ICCAD-2020.pdf	Fundamental Limits on the Precision of In-memory Architectures	This paper obtains the fundamental limits on the computational precision of in-memory computing architectures (IMCs). Various compute SNR metrics for IMCs are defined and their interrelationships analyzed to show that the accuracy of IMCs is fundamentally limited by the compute SNR (SNRa) of its analog core, and that activation, weight and output precision needs to be assigned appropriately for the final output SNR SNRT → SNRa. The minimum precision criterion (MPC) is proposed to minimize the output and hence the column analog-to-digital converter (ADC) precision. The charge summing (QS) compute model and its associated IMC QSArch are studied to obtain analytical models for its compute SNR, minimum ADC precision, energy and latency. Compute SNR models of QS-Arch are validated via Monte Carlo simulations in a 65 nm CMOS process. Employing these models, upper bounds on SNRa of a QS-Arch-based IMC employing a 512 row SRAM array are obtained and it is shown that QS-Arch’s energy cost reduces by 3.3× for every 6 dB drop in SNRa, and that the maximum achievable SNRa reduces with technology scaling while the energy cost at the same SNRa increases. These models also indicate the existence of an upper bound on the dot product dimension 𝑁 due to voltage headroom clipping, and this bound can be doubled for every 3 dB drop in SNRa.	In-memory computing (IMC) [13, 19, 28, 34] has emerged as an attractive alternative to conventional von Neumann (digital) architectures for addressing the energy and latency cost of memory accesses in data-centric machine learning workloads. IMCs embed analog mixed-signal computations in close proximity to the bit-cell array (BCA) in order to execute machine learning computations such as matrix-vector multiply (MVM) and dot products (DPs) as an intrinsic part of the read cycle and thereby avoid the need to access raw data. IMCs exhibit a fundamental trade-off between its energy-delay product (EDP) and the accuracy or signal-to-noise ratio (SNR) of its analog computations. This trade-off arises due to constraints on the maximum bit-line (BL) voltage discharge and due to process variations, specifically spatial variations in the threshold voltage 𝑉t , which limit the dynamic range and the SNR. Additionally, IMCs also exhibit noise due to the quantization of its input activation and weight parameters and due to the column analog-to-digital converters (ADCs). Henceforth, we use "compute SNR" to refer to the computational precision/accuracy of an IMC, and "precision" to the number of bits assigned to various signals. Today, a large number of IMC prototype ICs have been demonstrated [1, 3, 4, 7, 12, 15–17, 31–33, 36, 38, 40]. While these IMCs have shown impressive reductions in the EDP over a von Neumann equivalent with minimal loss in inference accuracy, it is not clear that these gains are sustainable for larger problem sizes across data sets and inference tasks. Unlike digital architectures whose compute SNR can be made arbitrarily high by assigning sufficiently high precision to various signals, IMCs need to contend with both quantization noise as well as analog non-idealities. Therefore, IMCs will have intrinsic limits on their compute SNR. Since the compute SNR trades-off with energy and delay, it raises the following question: What are the fundamental limits on the achievable computational precision of IMCs? Answering this question is made challenging due to the rich design space occupied by IMCs encompassing a huge diversity of available memory devices, bitcell circuit topologies, circuit and architectural design methods. Today’s IMCs tend to employ ad-hoc approaches to assign input and ADC precisions or tend to overprovision its analog SNR in order to emulate the determinism of digital computations. An analytical understanding of the relationship between precision, compute SNR, energy, and delay in IMCs, is presently missing. This paper attempts to fill this gap by: 1) defining compute SNR metrics for IMCs, 2) developing a systematic methodology to obtain a minimum precision assignment for activations, weights and outputs of fixed-point DPs realized on IMCs to meet network accuracy requirements, and 3) employing this methodology to obtain the limits on achievable compute SNR of a commonly employed IMC topology, and quantify it energy vs. accuracy trade-off.	https://shanbhag.ece.illinois.edu/publications/ISCAS-2022-Crossbar-Limits.pdf	Fundamental Limits on the Computational Accuracy of Resistive Crossbar-based In-memory Architectures	In-memory computing (IMC) architectures exhibit an intrinsic trade-off between computational accuracy and energy efficiency. This paper determines the fundamental limits on the compute SNR of MRAM-, ReRAM-, and FeFET-based crossbars by employing statistical signal and noise models. For a specific dot-product dimension N, the maximum compute SNR (SNRmax) is shown to occur at an optimum value of sensing resistance R⇤ s where clipping and quantization noise contributions from the analog-to-digital converter (ADC) are balanced out. SNRmax can be further improved by choosing devices with higher resistive contrast Roff/Ron, e.g., FeFET, but only until it attains a value in the range 12-15. Beyond this point, mismatch in the input digital-to-analog converters (DACs) and bitcell variations begin to dominate the compute SNR. Finally, by mapping a ResNet20 (CIFAR-10) network onto resistive crossbars, it is shown that the array-level compute SNR maximizing circuit parameters also maximizes the network-level accuracy.	In-memory computing architectures (IMCs) have emerged as an attractive computational platform for machine learning (ML) workloads due to their ability to overcome the high energy and latency costs associated with data movement inherent in such workloads. Emerging embedded non-volatile memory (eNVM) technologies such as ReRAM, MRAM, and FeFET, are deemed attractive for IMCs [1]–[16] because of their nonvolatility and high storage density compared to SRAM. However, these IMCs usually trade-off computational accuracy in order to attain energy efficiency. This trade-off is intrinsic to all IMCs due to their heavy reliance on analog computations – a trade-off that is not well-understood today. Though some work has been done for SRAM-based IMCs [17], [18] it is not clear what the limits on computational accuracy of resistive IMCs are. Unlike digital architectures where accuracy can be conveniently enhanced by assigning more precision to the computation, there are fundamental limits to the computational accuracy of resistive IMCs imposed by various analog non-idealities [6], [11], [15], [16] such as the resistive contrast (Roff/Ron) of the device type, the input resistance of the read-out circuit (sensing resistance Rs), resistive parasitics, readout noise, and the interplay between these noise sources. Developing a comprehensive understanding of the limits on computational accuracy of resistive IMCs is critical in order to be able to scale-up today’s single-bank macro-level designs to multi-bank system architectures. Not surprisingly, multiple approaches to improve the computational accuracy of resistive IMCs have been proposed [19]–[28]. However, these tend to be either empirical design approaches or are simulation-based and therefore are unable to pinpoint the precise limits on accuracy or the key contributors to such limits. In this paper, we develop an analytical framework for obtaining the fundamental limits on the computational accuracy of ReRAM, MRAM, and FeFET crossbars. We derive expressions for the signal-to-noise ratio (SNR) of array-level computation (compute SNR) based on the circuit architecture and noise parameters such as the input digital-to-analog converter (DAC) mismatch, bitcell conductance variations, output clipping noise, and analog-to-digital converter (ADC) quantization noise. We validate these expressions in a commercial 22 nm node, and employ them to obtain limits on the maximum achievable compute SNR as a function of the sensing resistance Rs, dot-product dimension N, ADC precision, and the resistive contrast Roff/Ron of the device. Finally, we map a ResNet-20 (CIFAR-10) network on to resistive crossbars and demonstrate that the circuit parameter values that maximize the array-level compute SNR also maximize the networklevel accuracy. This result enables one to design multi-bank IMC system architectures that can realize deep nets with the maximum achievable network accuracy without relying on tedious simulation-based ad-hoc methodologies. 	1	1
benchmarking of in-memory computing architectures	https://shanbhag.ece.illinois.edu/publications/CICC-2022-Invited-Paper.pdf	Comprehending In-memory Computing Trends via Proper Benchmarking	Since its inception in 2014 [1], the modern version of in-memory computing (IMC) has become an active area of research in integrated circuit design globally for realizing artificial intelligence and machine learning workloads. Since 2018, > 40 IMC-related papers have been published in top circuit design conferences demonstrating significant reductions (>20X) in energy over their digital counterparts especially at the bank-level. Today, bank-level IMC designs have matured but it is not clear what the limiting factors are. This lack of clarity is due to multiple reasons including: 1) the conceptual complexity of IMCs due to its full-stack (devices-to-systems) nature, 2) the presence of a fundamental energy-efficiency vs. compute SNR trade-off due to its analog computations, and 3) the statistical nature of machine learning workloads. The absence of a rigorous benchmarking methodology for IMCs - a problem facing machine learning ICs in general [2] - further obfuscates the underlying trade-offs. As a result, it has become difficult to evaluate the novelty of IMC-related ideas being proposed and therefore gauge the true progress in this exciting field.	Since its inception in 2014 [1], the modern version of in-memory computing (IMC) has become an active area of research in integrated circuit design globally for realizing artificial intelligence and machine learning workloads. Since 2018, > 40 IMC-related papers have been published in top circuit design conferences demonstrating significant reductions (>20X) in energy over their digital counterparts especially at the bank-level. Today, bank-level IMC designs have matured but it is not clear what the limiting factors are. This lack of clarity is due to multiple reasons including: 1) the conceptual complexity of IMCs due to its full-stack (devices-tosystems) nature, 2) the presence of a fundamental energy-efficiency vs. compute SNR trade-off due to its analog computations, and 3) the statistical nature of machine learning workloads. The absence of a rigorous benchmarking methodology for IMCs – a problem facing machine learning ICs in general [2] – further obfuscates the underlying trade-offs. As a result, it has become difficult to evaluate the novelty of IMC-related ideas being proposed and therefore gauge the true progress in this exciting field. At their core, IMCs are decision(inference)-making machines. Ideally, one should benchmark IMCs using system-level metrics such as energy-per-inference (decision energy), latency-perinference (decision latency), inference throughput, and inference accuracy. Furthermore, since energy, latency, and accuracy tradeoff with each other in all decision-making systems, IMC designers need to quantify this trade-off. To do so, requires one to map complete deep nets onto IMCs accounting for all associated overheads when quantifying these metrics. However, much of the reported IMC metrics today are at the bank-level. Therefore, in this paper, we will focus primarily on bank-level benchmarking of IMCs. Specifically, we propose a rigorous benchmarking methodology for IMCs, and employ it to analyze data collected from 50+ publications spread across CICC, VLSI and ISSCC since 2018 to explain trends and identify challenges in IMC design. Though much of the discussion is on SRAM-based IMCs [3]-[30], comparisons are drawn with eNVM-based IMCs [31]-[42] and with recent digital accelerators [45]-[57].	https://arxiv.org/pdf/2305.18335	Benchmarking and modeling of analog and digital SRAM in-memory computing architectures	In-memory-computing is emerging as an efficient hardware paradigm for deep neural network accelerators at the edge, enabling to break the memory wall and exploit massive computational parallelism. Two design models have surged: analog in-memory-computing (AIMC) and digital in-memory-computing (DIMC), offering a different design space in terms of accuracy, efficiency and dataflow flexibility. This paper targets the fair comparison and benchmarking of both approaches to guide future designs, through a.) an overview of published architectures; b.) an analytical cost model for energy and throughput; c.) scheduling of workloads on a variety of modeled IMC architectures for end-to-end network efficiency analysis, offering valuable workload-hardware co-design insights.	The need for executing AI workloads on edge devices, characterized by limited power budgets and area constraints, results in large research interest in custom hardware accelerator design, with large advancements in a short time frame [1]. As these workloads require a significant amount of data movement between the processor and the memory, traditional Von Neumann architectures have proven unsuitable for the application [2]. To handle this data transfer bottleneck, a functional approach adopted from the early published designs is to parallelize the computing units in space, thus allowing for a higher degree of spatial multi-cast reuse of the operands as they are fetched from memory. Conventional accelerators [3] thus are composed of a 2D array of processing elements (PE), each of which contains a multiply-accumulate (MAC) unit (or vector) and small register files to store operands. These operands are fetched from larger memories outside the PE array and distributed in space across the processing elements, according to the spatial parallelization scheme adopted for the workload (also called ”spatial unrolling”) [4]. Yet, memory interfacing remains a performance-limiting aspect of modern neural accelerators. In recent years, in-memory computing (IMC) has therefore emerged as a promising alternative to PE-based accelerators, by performing the MAC operations near/in the memory cells directly. This allows to greatly reduce access overheads and enables massive parallelization opportunities, with potential orders of magnitude improvements in energy efficiency and throughput [5]. Most recent IMC designs published in the literature are focused on analog IMC (AIMC), where the computation is carried out in the analog domain. While this approach ensures extreme energy efficiencies and massive parallelization, the analog nature of the computation and the presence of intrinsic circuit noise and mismatches compromises the output accuracy. Furthermore, the rigid structure of the computation’s dataflow limits the spatial mapping possibilities. To avoid the hurdles of AIMC, digital in memory computing (DIMC) is lately gaining more interest as a valid alternative, thanks to its noise-free computation and more flexible spatial mapping possibilities, trading off added flexibility and accurate computation for less energy efficiency. These new opportunities stemming from AIMC and DIMC resulted in many recent implementations and publications in the literature. However, these works vary strongly in terms of hardware architecture, array dimensions, and silicon technology. This makes it difficult to grasp their relative strengths, trends and future directions. While several works assess and discuss IMC trends qualitatively [5]–[12], only few aim at quantitatively modeling or benchmarking architectural strategies. The latter, moreover, primarily focus on AIMC designs [5], [13]–[19], while there is a lack of DIMC modeling efforts. Similarly, for mapping space explorations, most of the focus has been dedicated to AIMC designs, while lacking DIMC assessment [16], [17], [20], [21]. To this purpose, this paper aims at: • Providing a comprehensive assessment of recently published AIMC and DIMC chips, in order to understand through benchmarking the capabilities and limitations of the two design paradigms. • Provide a unified analytical model for IMC architectures, validated against numerous design points from literature. • Integrate the model into ZigZag [4], a design space exploration (DSE) framework for hardware accelerators. This allows to compare different AIMC / DIMC hardware design points, data flows and mappings in terms of energy and throughput efficiency, towards insights on optimal design points for targeted tinyMLperf workloads [22].	0	1
in-memory computing processors	https://shanbhag.ece.illinois.edu/publications/MICRO_Mingu_2019.pdf	An Energy-efficient Programmable Mixed-Signal Accelerator for Machine-Learning Algorithms	We propose PROMISE, the first end-to-end design of a PROgrammable MIxed-Signal accElerator from Instruction Set Architecture (ISA) to high-level language compiler for acceleration of diverse machine learning (ML) algorithms. We take advantage of the superior energy efficiency from analog/mixed-signal processing by exploiting the inherent error tolerance of ML algorithms. We first propose an ISA to support operations pervasive in ML algorithms with a PROMISE architecture based on silicon-validated components. Second, we develop a compiler that can take a high-level programming language (Julia) and generate PROMISE code with an IR design. Third, we show how the compiler can map an application-level error tolerance specification for neural networks down to low-level hardware parameters to minimize energy consumption. PROMISE achieves 2.3× delay and 4.5× energy savings, and 14% additional energy savings with compiler optimization, on average for diverse ML algorithms as compared to digital ASICs.	Emerging applications such as in health care, surveillance/monitoring and others leverage the decision making capability based on machine learning (ML) algorithms. Those algorithms have demanded high computing capability to process large volume of data with limited energy budget. In order to improve energy efficiency, researchers have proposed analog/mixed-signal accelerators [1], [2]. These rely on small-signal computation which is less precise but more energy efficient than traditional large-signal computation in the digital domain (See Fig. 1). Therefore, they are suitable for ML inference where the application itself is tolerant to such imprecision. However, these accelerators lack a programmable architecture, instruction sets, or compiler support necessary for supporting high-level programming languages. Moreover, the small-signal computations create energy vs. accuracy trade-offs that must be controlled from the application level in order to ensure that accuracy goals are met. Designing hardware architecture and instruction set support to expose the available hardware knobs to software in a controllable way require careful hardware, ISA and software design. Tackling these challenges, we propose PROMISE, the first end-to-end design of a PROgrammable MIxed-Signal accElerator from Instruction Set Architecture (ISA) to highlevel language compiler for acceleration of diverse ML algorithms. This article makes the following major contributions: • We propose an ISA with a PROMISE architecture built with silicon-validated components. • We develop a compiler that takes a high-level programming language (Julia) and generate PROMISE code. • The compiler maps an application-level error tolerance specification down to low-level hardware parameters (swing voltages ∆VBL) to minimize energy consumption. • In this extended article, we show how to maximize the throughput of deep neural network (DNN) by optimally mapping the algorithm to a multi-bank structure. • We also extend our results from [3] to enable the convolutional neural network (CNN) based on PROMISE ISA and compiler infrastructure. • Analog kernel data reuse is enabled by employing a charge-recycling mixed-signal multiplier [4]. 	https://www.princeton.edu/~nverma/VermaLabSite/Publications/2021/JiaOzatayTangValaviPathakLeeVerma_JSSC2021.pdf	Scalable and Programmable Neural Network Inference Accelerator Based on In-Memory Computing	This work demonstrates a programmable inmemory-computing (IMC) inference accelerator for scalable execution of neural network (NN) models, leveraging a highsignal-to-noise ratio (SNR) capacitor-based analog technology. IMC accelerates computations and reduces memory accessing for matrix-vector multiplies (MVMs), which dominate in NNs. The accelerator architecture focuses on scalable execution, addressing the overheads of state swapping and the challenges of maintaining high utilization across highly dense and parallel hardware. The architecture is based on a configurable on-chip network (OCN) and scalable array of cores, which integrate mixed-signal IMC with programmable near-memory single-instruction multipledata (SIMD) digital computing, configurable buffering, and programmable control. The cores enable flexible NN execution mappings that exploit data- and pipeline-parallelism to address utilization and efficiency across models. A prototype is presented, incorporating a 4 × 4 array of cores demonstrated in 16 nm CMOS, achieving peak multiply-accumulate (MAC)- level throughput of 3 TOPS and peak MAC-level energy efficiency of 30 TOPS/W, both for 8-b operations. The measured results shows high accuracy of the analog computations, matching bit-true simulations. This enables the abstractions required for robust and scalable architectural and software integration. Developed software libraries and NN-mapping tools are used to demonstrate CIFAR-10 and ImageNet classification, with an 11-layer CNN and ResNet-50, respectively, achieving accuracy, throughput, and energy efficiency of 91.51% and 73.33%, 7815 and 581 image/s, 51.5 k and 3.0 k image/s/W, with 4-b weights and activations.	Deep learning based on neural networks (NNs) has enabled breakthroughs in a broad range of artificialintelligence (AI) tasks, such as vision, language processing, and molecular discovery/diagnosis [1]–[9]. The increasing scale and computational complexity of NN models has pushed computing systems to their limits of energy efficiency and performance, especially in power/energy/space-constrained edge applications. This has motivated algorithmic and software solutions (e.g., model quantization/compression [10]–[14] and compiler optimizations [15], [16]), as well as hardware solutions in the form of accelerators [17]–[21]. In terms of accelerators, a key focus has been on matrix-vector multiplies (MVMs), which dominate NN inference computations, with data movement/accessing being a critical concern due to the high-dimensionality MVMs typically involved [22]. This has led to spatial architectures (e.g., systolic arrays), employing a 2-D structure of processing engines (PEs) providing storage and compute, to exploit the 2-D data reuse. In-memory computing (IMC) is an emerging approach, aimed at taking the spatial architecture to an extreme, with high-density memory bit cells serving as the PEs. Recent implementations show that IMC can simultaneously achieve 10 × higher energy efficiency and compute density for MVMs compared to standard digital accelerators [23]–[29]. However, addressing full NN workloads requires incorporating MVMs into programmable architectures and to which the workloads can be scalably mapped, while preserving the efficiency and throughput benefits. Recent work [30] has begun to explore programmability, through a heterogeneous CPU-centric architecture. This provided a platform for building-up a software stack to deploy workloads, but without architectural optimizations to address efficiency and performance with scalable execution. This work focuses on such architectural design, together with the workload-mapping techniques, to maximize hardware utilization through flexible parallelism optimized for IMC. Architecture-software codesign is employed to address the drastically different physical attributes of IMC, compared to digital accelerators. The primary contributions of this work are as follows. 1. We analyze the challenges for scalable NN mapping to IMC introduced by its physical attributes, namely high compute efficiency/density and large state-swapping overheads, and outline flexible mapping approaches for exploiting parallelism while mitigating associated overheads for IMC executions. 2. We propose a programmable array-of-cores IMC architecture designed to support flexible parallelism for optimally mapping different NNs and to enable architectural and execution scale-up with minimal overheads. 3. We demonstrate the programmable NN inference accelerator architecture in 16 nm CMOS, including a scalable on-chip network (OCN) and 4 × 4 array of IMC-based cores with integrated digital near-memory SIMD engines, reconfigurable buffering for flexible dataflow, and localized control. In addition, accompanying software is developed for the prototype chip, including: 1) NN training libraries incorporated in standard deep-learning design frameworks (PyTorch, TensorFlow) to quantize NN models for execution on the chip and 2) a prototype NN mapping toolchain to deploy NNs onto the architecture. Multiple representative NN benchmarks executing on the chip are demonstrated, achieving accuracies equivalent to ideal digital computation. The remainder of this article is organized as follows. Section II provides background on IMC technology and its challenges for efficient NN mapping. Section III provides an architectural overview of the NN inference accelerator, describing the key building blocks of the array-based architecture. Section IV describes the microarchitectural design within the IMC-based cores. Section V provides mapping illustrations for supporting different NN dataflows, and analyzes the mapping approaches and architectural supports. Section VI presents the prototype measurements, software toolkits, as well as NN demonstrations. Finally, Section VII concludes.	0	1
dual catalytic probes	https://pkargupta.github.io/tod_dataset/herrmann-et-al-2024-enhanced-catalytic-probe-design-for-mapping-radical-density-in-the-plasma-afterglow.pdf	Enhanced Catalytic Probe Design for Mapping Radical Density in the Plasma Afterglow	The electrification of chemical processes using plasma generates an increasing demand for sensors, monitoring concentrations of plasma-activated species such as radicals. Radical probes are a low-cost in situ method for spatially resolved quantification of the radical density in a plasma afterglow using the heat from the exothermal recombination of radicals on a catalytic surface. However, distinguishing recombination heating from other heat fluxes in the system is challenging. In this study, we present a heat flux analysis based on probe measurements inside the reactor, with simultaneous IR imaging monitoring of the temperature of the reactor wall. The impact of radiation heat on a single thermocouple as well as the advantage of a dual thermocouple setup (using a catalytic unit together with a reference thermocouple) is shown. We add a heat sink with a monitored temperature to the dual thermocouple setup, allowing the determination of conductive and radiative heat fluxes. The heat sink gives more information on the measurement and reduces ambiguities in the evaluation used by others. The probe was tested by mapping N atom densities throughout the plasma afterglow of our reactor, enabling evaluation of the recombination kinetics of the radicals in the gas phase. Three-body recombination was shown to be the main pathway of recombination, with a recombination rate of krec = (2.0 ± 0.9)·10–44 m6/s, which is in line with the known literature findings, demonstrating that the measured species are N radicals and the probe did not influence the plasma or recombination reactions in the afterglow	The usage of plasma can be a powerful tool in power-to-X transformation, transforming electricity from renewable sources to chemical energy. Ignition of plasma involves applying an external electric field to gas, causing electrons to accelerate and transfer their newly gained energy to the gas molecules through collisions, creating highly reactive species. Examples of applications are energy storage through the production of synthetic fuels, CO2 capture, and the electrification of chemical processes such as nitrogen fixation. (1−4) The common feature of all applications is the need for sensors capable of in situ measurement of the concentration of active species for process control and enhancement.One of the activated species are radicals. These are atoms or molecules formed through dissociation of (larger) molecules. They are highly reactive due to having free valence electrons. A low-cost, in situ method for measuring radical flux densities in the afterglow of plasma uses radical probes.Radical probes are based on a calorimetric principle: they measure the heat produced from the recombination of radicals on a surface with a known recombination probability. This principle was used as early as 1924 by Bonhoeffer who coated the bulb of a thermometer with various catalysts to study hydrogen radical recombination. (5) The heat produced at the surface due to the recombination reactions is related to the number of atoms recombining, as well as the energy released per recombination reaction, which is related to the dissociation energy of the molecule (WD). The power of heating from recombination on the catalytic surface is calculated according to (6)𝑃r=𝑗a𝛾𝑊D2𝐴(1)with the recombination coefficient of atoms on the surface γ, the surface area A of the catalyst, and the atomic flux to the surface 𝑗a=14𝑛𝑣 containing the atomic density n and the thermal velocity v. If the recombination at the probe surface is too high, it can deplete atoms close to the surface, leading to a lower atomic flux. The amount of flux lost at a surface can be calculated according to (7) 𝑗loss=14𝑛𝑣𝛽1−𝛽, with β being the surface reaction probability consisting of sticking and recombination coefficient. The size of the probe also plays a role in the total depletion of atoms, so it is generally suggested to keep the probes small. (8,9) In case of no depletion, the atomic flux at the catalyst can then be calculated according to𝑛𝑣=8·𝑃r𝛾𝑊D𝐴(2)While probes using heat flux sensors have been proposed, (10,11) sensors measuring the temperature of a catalyst over time are more commonly used. The main designs are thermocouple catalytic probes (9,12−16) and fiber optical catalytic probes. (6,17,18) Initially, the thermocouple catalytic probes were used for studying the recombination coefficients of surfaces in contact with radicals. (19,20) Later, they were further developed for the detection of atomic density. (9,12−16,21) The development of fiber optical catalytic probes, where the temperature of the catalyst is measured optically from its emitted heat, was introduced by Babič et al. (17) All systems have in common that heating from recombination reactions must be distinguished from other heat fluxes in the system to gain a correct value for the radical flux.The heat fluxes affecting the probes are schematically shown in Figure 1, with the example of a single thermocouple (with a catalytic coating at the tip) in a flow reactor. Plasma interactions (including recombination), convection heat exchange between the gas and the probe, conduction cooling along the probe, and radiation heat exchange with the plasma and rector walls are expected to impact the measured temperature of the catalytic probe.Figure 1Figure 1. Heat fluxes on a thermocouple in a plasma flow reactor.The measured temperature of a thermocouple over time, including the impact of the various heat fluxes, can be described according to𝜌𝑉𝑐pd𝑇d𝑡=−𝑆·𝜒𝐿(𝑇−𝑇0)+ℎ·𝐴c·(𝑇−𝑇gas)−𝜎𝐴r(𝑇4−𝑇4quartz)+𝑃plasma+𝑃r(3)where ρVcp is the characteristics of the thermocouple (density, volume, and heat capacity respectively), T denotes the measured temperature of the thermocouple, and t denotes the time. The first term on the right-hand side of the eq 3 describes heat transfer due to thermal conductivity along the thermocouple, with S being the cross-sectional area of the thermocouple, χ its thermal conductivity, L the length over which there is a temperature gradient, and T0 the temperature at the end of this gradient (e.g., at the cold side of the thermocouple where it leaves the reactor). The second term denotes the convective heat transfer, describing the energy transferred at the interface between the gas and the thermocouples, with h being the heat transfer coefficient and Ac being the surface area of the thermocouple maintained at different temperatures from the flowing gas. The third term represents heat transfer through thermal radiation, with σ being the Stefan–Boltzmann constant and Ar being the surface area of the probe with a different temperature than the surrounding quartz glass tube, which has Tquartz as the temperature. Pplasma represents an accumulation of heating processes from the plasma on the thermocouple, while Pr specifies recombination heating on the catalyst.Various ways of distinguishing the different heat fluxes are described in the literature, from purely analytical to mechanical. The analytical approach is based on the evaluations of cooling measurements for estimating heat fluxes. Here, single catalytic probes are used (fiber optic catalytic probes or thermocouples). (6,14,16,17,22) The analysis is based on two assumptions taken at thermal equilibrium: (i) The only source of heating the catalytic probe comes from recombination and (ii) the heating of the probe is equal to the thermal losses Pc due to convection, conduction, or radiation; thus, Pr = Pc. (6,18) The combination of cooling processes Pc is simplified to𝑃c=𝑚𝑐pd𝑇d𝑡(4)with mcp being the thermocouple’s characteristics (mass and heat capacity respectively of the part with thermal gradient). d𝑇d𝑡 is obtained by fitting the temperature over time in the initial cooling phase after the plasma is switched off. Then, nv becomes (18)𝑛𝑣=8𝑚𝑐p𝛾𝑊D𝐴d𝑇d𝑡(5)Disadvantages of this evaluation are the uncertainty of the length of the temperature gradient, translating to uncertainties in the value for m and the variation in cooling time scales of the different cooling processes giving varying results for d𝑇d𝑡 for different choices of cooling time in the fitting.To avoid ambiguities of the fitting approaches by gaining more information on the heat fluxes, dual probe approaches have been suggested. (9,12,15) A catalytic probe is paired with an identical reference probe without catalyst coating, measuring the temperature simultaneously. Carruth et al. (12) proposed a dual probe design using two encased thermocouples. The reference side had a resistive heater next to the thermocouple inside the encasing, bringing the reference side to the same temperature as the catalyst side. With both sides at the same temperature, the heat losses are the same, and the information on their magnitude is contained in the power needed to heat the reference side. A disadvantage of this design is the rather large size of the probes. Qerimi et al. (9) proposed a dual probe approach with two simple thermocouples. In their evaluation, they assumed the only relevant thermal loss process to be thermal conduction, leading to the evaluation function (9)𝑛=8𝑆𝜒𝑣𝛾𝑊D𝐴𝐿𝑇cat−𝑇ref𝛾cat−𝛾ref(6)with S being the cross-section of the thermocouples, χ the thermal conductivity, Tcat and Tref the measured temperatures of the catalytic and the reference probe, respectively. γcat and γref are the corresponding recombination coefficients. L denotes the thermal gradient length of the probe, showing that this approach still has the disadvantage of having to know the length of the temperature gradient.In this paper, we propose a new probe design, reducing uncertainties in the current evaluations such as the length of the thermal gradient L, probe properties, and the impact of heat radiation. Measurements shown here use the example of Nitrogen plasma, forming N atoms as radicals. However, the probe design is transferable to other systems. We present a heat flux analysis of the system, comparing measurements of the probe with infrared imaging of the reactor walls. The probe is based on three thermocouples, adding a heat sink monitored by a thermocouple to a dual thermocouple setup. This facilitates good control of L and helps us gain more information on the various heat fluxes. This setup facilitates the estimation of the radiation heat flow without knowing the reactor wall temperature. Furthermore, we mount the probe on an adjustable feedthrough making it possible to map the density throughout the reactor, gaining insights into radical recombination kinetics at varying pressure and flow rate settings.	https://research.tue.nl/files/293321476/Wang_2022_Plasma_Sources_Sci._Technol._31_085011.pdf	Application of a dual-thermopile radical probe to expanding hydrogen plasmas	We comparethe performanceof a hydrogenradical probe to historic data determined via two-photon absorption laser induced fluorescence using a comparable cascaded arc source under similar operating conditions. This probe has dual heat flux sensors (DHFS) each coated with materials with different catalytic properties for hydrogen atoms. In the ideal situation, the hydrogen radical flux can be deduced based on the difference between the heat loads measured by these two sensors. The influence of DHFS temperature on the performance was also assessed. The experimental results showed measurement errors of <10% could be obtained regardless of the probe temperature during plasma exposures. To convert heat fluxes into atomic fluxes, we calibrated the difference of the recombination coefficients using a vacuum ultraviolet absorption technique, which is more reliable than modeled values based on assumptions or scattered values reported in literature. As a result, we measured the hydrogen plasma and radical parameters at various settings using both a double Langmuir probe and the DHFS. The typical atom flux in the 1022 m−2s−1 range was in good agreement with those obtained using optical techniques. We also observed that the ion and atom fluxes are both sensitive to the background gas pressure. These findings validate application of the DHFS to the cascaded arc source, and could pave the way for optimization of the source performance in the plasma material processing experiments	Hydrogen plasmas have attracted great attention in both the academic and industrial fields due to the influence of plasmas on materials. In a fusion reactor, ions in hydrogenisotope plasmas could penetrate through the material surface, leading to hydrogen retention and blister formation [1]. In the semiconductor industry, radicals in hydrogen plasmas are used in plasma-enhanced atomic layer deposition to achieve high and selective reactivity [2]. Moreover, in extreme ultraviolet (EUV) lithography machines, synergy between hydrogen ions and radicals accelerates tin removal from EUV mirrors [3, 4], whereas hydrogen radicals could also lead to cracks on tin-deposited mirrors [5] and the detailed mechanism yet needs to be investigated. To study plasma-material processing, researchers have deployed cascaded arc sources to generate high-flux hydrogen plasmas which accelerate reactions of interest, such as deposition of hydrogenated amorphous silicon [6, 7]. This type of thermal plasma source produces a expanding plasma jet with a hydrogen atom flux of typically 1022–1023 m−2s−1 at the material surface [8, 9]. Applications of this plasma source has been successfully demonstrated for fast deposition or etching processes using hydrogen or hydrogen–argon mixture plasmas [7, 10]. The fundamentals of the cascaded arc source also have been extensively investigated in order to understand its behavior and expand its applications [11, 12]. Avariety of plasma diagnosticshave been implementedfor cascaded arc sources to obtain H2 or H2–Ar plasma parameters, such as electron densities and radical densities, which are relevant to material processing. For example, electron densities are important for plasma-facing materials in fusion reactors [1], while both electron and radical densities need to be considered for synergistic etching of hydrogenated carbon [10]. Measurements of electron temperatures and densities have been achieved using Langmuir probes, optical emission spectroscopy (OES), and Thomson scattering (TS) [13,14].Asfortheradicaldensity,techniquessuchaselectronbeam induced fluorescence and two-photon absorption laser induced fluorescence (TALIF) have been successfully realized [8, 9, 15]. Among these diagnostics, optical techniques, such as TS and TALIF, are generally considered more reliable than non-optical ones due to their non-intrusive characters [16]. However, these techniques are often complicated and expensive, and they require optical specialists to build and maintain the setups [17]. Fortunately, as an alternative to TS, Langmuir probes are cheap and relatively simple to build, and its outputs can be comparable with TS for plasmas generated by cascaded arc sources [15]. On the other hand, so far no good alternative to TALIF has been demonstrated for radical density measurements for cascaded arc sources, which operate at relatively high pressures and produce low temperature plasmas <5 eV. For other types of plasma source, a relatively simple method like actinometry-basedOES can be considered as an alternative [18], but it is expected to generate results with large errors for cascaded arc sources, due to the high gas pressure of 25–85 Pa and the low electron temperature of 0.3 eV. These two parameters respectively lead to serious quenching processes and inaccurate estimation of the electron impact excitation from the ground state [18, 19]. A catalyst-based radical probe is another alternative to TALIFforhydrogenradicaldensitymeasurements.Thedesign of a catalytic probe essentially consists of an actively heated thermocouple coated with a specific metal such as Pt [20, 21]. Whentheprobeis exposedto hydrogenatom flux in a plasma, the recombination of hydrogen radicals at the metal surface generates a heat load to the probe. The probe is typically heated to a constanttemperatureat knownheatingpowers,and the hydrogen radical flux can be estimated by measuring the probe temperature and solving the heat balance equation for the probe [21]. However, this method might give large errors due to large background heat loads compared with the recombinationheatload[22].Thisproblemmaybecomeveryserious for a cascaded arc source, which is known to produce a range of heat loads, such as through gas and light, in addition to the plasma one [23, 24]. To solve this problem, dual-sensor catalytic probes have been recently designed and demonstrated for high hydrogen radical fluxes [22, 25]. Instead of only one thermocouple, this type of catalytic probe consists of two heat sensors coated with two different materials, one of which promotes recombinationof hydrogenradicals while the other onedoes not. If these two heat sensors receive the same background heat loads from the plasma, it is possible to extract the recombination heat load by differentiating the individual heat loads of two sensors [22, 25]. Therefore, this type of radical probe may become a relatively simple alternative to TALIF to measurethehydrogenradicaldensityforcascadedarcsources. However, up till now direct validation against TALIF results have not been gathered. In the following sections, we will present our work to validate the measurements of hydrogen radical flux using a dualthermopile radical probe for a cascaded arc source. This will be the first application of a catalyst-based radical probe to this thermal expanding plasma source. We also calibrated the difference of recombination coefficients using a vacuum ultraviolet (VUV) absorption technique to convert measured heat f luxes into hydrogen radical fluxes. This way we can compare our results with those obtained using TALIF in literature. In addition,we will showtheplasmaparametersatvarioussource settings to assess the source performance. These studies indicate thatthe dual-thermopileradicalprobecouldbeconsidered agoodalternativetoTALIFforhydrogenradicalfluxmeasurement. We could use it to characterize hydrogen plasmas from the cascaded arc source in future experiments, for example, to study the effect of hydrogen radicals on blistering or cracking of EUVmirrors [26, 27]	1	0
oxygen catalytic probes	https://pkargupta.github.io/tod_dataset/oxygenatomdensity.pdf	Oxygen atom density in a large reactor powered by four inductively coupled plasma sources	Gradients in O-atom density in metallic plasma reactor useful for rapid surface activation of 3D polymer products have been measured. The reactor of a volume of 150 l was equipped with four inductively coupled plasma sources (ICPS) of (predominantly) oxygen atoms, parallelly coupled with a radio-frequency generator of adjustable power up to 5 kW at 13.56 MHz. Molecular oxygen was continuously introduced into the ICP sources, where it was dissociated upon plasma conditions. The constant pumping of the plasma reactor enabled the effective transfer of O-atoms from the ICP sources, so that their density remained high in the entire chamber even far from plasma sources. The O-atom density was measured across the reactor with a movable catalytic probe calibrated for oxygen. The sophisticated immersible design enabled the O-atom density to exceed 1021 m−3 in the major chamber volume at the pressure of 20 Pa. At this pressure, a uniform plasma in the H-mode was sustained at the total real RF power of 1800 W. Significant gradients in the O-atom density were only detected next to ICP sources' exhausts and in the main chamber's metallic side tubes. Such uniform distribution of O-atoms in a large reactor is advantageous for rapid surface activation of 3D polymer products.	Oxygen-plasma treatment of polymers has been a subject of numerous scientific studies due to its applicability on an industrial scale [1,2]. Such plasma is a source of charged particles, neutral reactive species, and vacuum ultraviolet (VUV) radiation [3,4]. Non-equilibrium oxygen plasma can be sustained at low pressures (often between 10 and 100 Pa) or atmospheric pressure [5,6]. Atmospheric-pressure plasmas are characterized by extensive gradients in reactive species [7,8], so they are not very useful for treating 3D objects but perform well in cases of 1D or 2D objects such as wires, foils of fabrics. Such atmospheric-pressure plasmas indeed enable treatment in continuous mode [9,10], which is preferred in the industry [10,11]. However, large gradients in plasma parameters remain an obstacle for extensive application of these plasmas for treating 3D objects, where a robot is necessary to move a plasma jet over the surface. On the other hand, low-pressure plasmas are convenient low power density sources (power per unit volume) of neutral reactive species and thus suitable for treating objects of almost arbitrary shape [12,13]. An appropriate gaseous discharge sustains plasma, and the neutral reactive species diffuse inside the reactor, so their density may be significant even far away from plasma sources [1]. Still, gradients in O-atom density are likely to occur even in low-pressure oxygen reactors [14]. A polymer material's surface finish depends on the type of polymer and fluxes of different reactive species onto the polymer surface. The oxygen-plasma treatment usually leads to two effects: surface functionalization with polar functional groups and etching. An appropriate surface finish is achieved by optimizing fluences of different plasma species onto a polymer surface. Functionalization and etching kinetics are still a subject of extensive research. According to a recent theory [15], which has been confirmed by carefully designed experiments [16], surface functionalization with oxygen functional groups occurs in several steps. First, the hydrogen atoms on the polymer surface are substituted with hydroxyl radicals. The surface saturation with the hydroxyl groups is accomplished at the O-atom fluence as low as about 1019 m−2. Second, epoxy bonds are formed, followed by breaking bonds between the carbon atoms in the polymer chain. Such effects have been observed at the O-atom fluence of several 1020 m−2. The bond scission enables the formation of various functional groups and oxygen atoms' diffusion into the sub-surface film. The surface is finally saturated with polar functional groups after being treated with an O-atom fluence of about 1023 m−2. Etching is observed simultaneously with functionalization with carboxyl or ester groups [16]. The polymers' etching rate upon treatment with oxygen atoms only may be lower than 0.1 nm/s [17]. The etching rate is usually between 1 and 10 nm/s for many polymers when exposed to weakly ionized oxygen plasma rich in oxygen atoms at room temperature [18,19]. Highly ionized oxygen plasma is not very appropriate for surface functionalization since the etching prevails and the surface wettability remains inadequate even at prolonged treatment times [20,21]. Therefore, the best conditions for surface activation of polymers are at relatively high O-atom density and low density of charged particles. The requirements are contradictory, so the careful design of a plasma reactor is necessary to meet these criteria. The O-atom density in a plasma reactor depends on the dissociation and recombination rates. The most intensive production of oxygen atoms is in moderately ionized gaseous plasma, in particular oxygen plasma sustained by electrodeless discharges, for example, inductively coupled RF plasma [22] and microwave plasma [23]. Gas-phase recombination is unlikely to occur at pressures below about 100 Pa since a three-body collision is required. The primary loss mechanism at low pressures is heterogeneous surface recombination. The probability depends on the recombination coefficient, which depends on the type of material facing plasma and its morphology. For example, at room temperature, the recombination coefficient for oxygen atoms on stainless steel is 0.07 [24]; on glass surfaces, it is as low as 10−3 [25], while on extremely porous nanostructured materials, it may come close to 1 [26]. Values of O-atom loss coefficients on many polymers are a few 10−3, so the polymer products inside a metallic vacuum chamber do not present a significant drain of O-atoms [27]. Stainless steel is the material of choice for the inside part of an industrial-scale low-pressure plasma reactor. However, in such reactors, it is difficult to obtain a large density of O-atoms, which is required for rapid treatment of polymeric products. Hence, using an electrodeless discharge atom source in metallic chambers is impractical. Further, a moderately large coefficient for heterogeneous surface recombination of O-atoms to parent molecules on stainless steel surfaces prevents large O-atom density at reasonable discharge power. High-power reactors are not suitable for surface activation of polymers with oxygen plasma since the surface functional groups are unstable at elevated temperatures. The limitations of the stainless-steel reactors may be avoided by immersing electrodeless plasma sources inside the vacuum chamber, as explained in this paper.	https://pkargupta.github.io/tod_dataset/atomic_oxygen_concentration.pdf	Atomic Oxygen Concentration in a Flowing Post-Discharge Reactor	Concentration of neutral oxygen atoms in the flowing post-discharge of a pure oxygen microwave discharge at different experimental conditions was determined with a nickel catalytic probe. The post-discharge reactor was setup for metal surface cleaning. It worked at the pressure between 20 and 100 Pa and at output power of the microwave plasma generator between 80 and 150 W. At those experimental conditions the O-atom density was found to be of the order of 10 21 m−3. It increased both with increasing pressure and microwave power. The degree of dissociation of oxygen molecules, on the other hand, decreased with increasing pressure	In the past decade, technologies based on application of oxygen plasma have been successfully applied to different branches of science and industry. The technologies include the an-isotropic plasma etching, plasma drilling, plasma oxidation, plasma cleaning, plasma ashing and plasma surface activation.(1−6) Different technologies require application of plasma with different parameters. For an-isotropic etching and drilling, a plasma with a high degree of ionization is needed, while for some other technologies, an oxygen plasma with a low density of charged particles performs better. For very delicate treatments of samples, for instance during plasma ashing and selective plasma etching, a state of a gas with a negligible concentration of charged particles should be used. In such cases, it is much better to treat samples in post-discharges rather than in plasmas themselves. In systems used for delicate plasma treatments, the most important parameter isthe density of neutral oxygen atoms. It can be measured by different means including laser spectroscopy (LIF),(7,8) NO titration(9) and catalytic probes.(10,11) The latter technique was applied to determine the O concentration in the post-discharge reactor for the experimental study of the role neutral oxygen atoms play in low temperature degreasing.	1	0
catalytic probes	https://pkargupta.github.io/tod_dataset/excited_species.pdf	Excited species in H2, N2, O2 microwave flowing discharges and post-discharges	The density of O and H atoms have been measured by emission spectroscopy and by catalytic probes in a flowing post-discharge reactor. In the post-discharge reactor at 1 torr and a time of 2–3×10−2 s after a 2.45-GHz Ar10%O2 discharge (quartz tube of diameter 6 mm) at 60 W, approximately the same values of 1(±0.3)×1015 cm−3 oxygen atom densities were obtained by NO titration and by a Ni catalytic probe. In ArH2 post-discharge, the Ni catalytic probe gave a H-atom density of (1–1.2)×1015 cm−3 for an Ar4%H2 gas mixture at 3 torr and a time less than 10−3 s after a microwave discharge of 50 W.	Reactive plasmas in H2, N2 and O2 molecular gases are extensively studied for surface treatments, such as surface cleaning and nitriding. The present analysed process is a flowing post-discharge at low gas temperature without the interaction of ions and electrons with the treated surfaces. The production of H, N and O active atoms has been previously determined in such flowing microwave post-discharges [1]. In previous work, the plasma nitriding of iron surfaces has been analysed in ArN2H2 post-discharge. More recently [2], the cleaning of oil contaminated metal surfaces (steel and aluminium) has been successfully performed in an ArO2 flowing microwave post-discharge. In the present study, catalytic probes [3], [4] have been set up in the post-discharge reactor to determine the absolute densities of H and O atoms. Variations of atom densities in ArH2 and ArO2 gas mixtures have been measured vs. the microwave power and the H2 and O2 percentages into Ar. A comparison is first given between the measurements of O-atom densities by NO titration and by catalytic probes in the post-discharge. Within the ArH2 microwave discharges, the H-atom relative densities are obtained by emission spectroscopy with interpretation from the relevant kinetics reactions. Such relative densities in the discharge are compared with the quantitative results obtained by the catalytic probe in the early post-discharge.	https://pkargupta.github.io/tod_dataset/comparisonofspatial.pdf	Comparison of spatial distributions of atomic oxygen and hydrogen in ICP by means of catalytic probes and actinometry	Low pressure inductively coupled plasma was investigated with a combination of two diagnostic techniques: catalytic probes and optical emission spectroscopy (actinometry). Three working gases were used: oxygen, oxygen–argon mixture and hydrogen. Concentrations of oxygen and hydrogen atoms were determined in a wide range of plasma parameters, with pressure ranging from 10 Pa to 70 Pa, applied power in the range from 50 to 250 W and at many different positions throughout the system (including the coil and afterglow in sample chamber). Concentrations of oxygen atoms were measured to be 1.3 × 1015 cm−3 in the middle of the coil and 4 × 1012 cm−3 in the regions of the chamber furthest from the coil. These spatially resolved concentrations are very important in plasma processing of materials, because radical concentrations over the sample determine plasma–material interactions. This work demonstrates that a relatively simple detection system can be practical and sufficiently successful in many plasma applications. Introduction	Deeper understanding of plasma properties and processes is important for numerous applications, ranging from biology and medicine [1] to materials processing [2], [3]. Even though the current focus in biomedical applications is on atmospheric plasma processing [4], low pressure plasma processing is still quite challenging [5], [6], [7]. Insight into the plasma processes is enabled by a wide range of different plasma diagnostic techniques; sometimes just a basic investigation of main characteristics (composition, temperatures, pressures) is enough, but for a complete understanding experimental results have to be combined with modelling and simulations. The most widely used methods include emission [8], [9] and absorption spectroscopy [10], [11], Langmuir probes [12], [13], hairpin probes [14], catalytic probes [15] and other active probes (probes that are immersed into plasma [16]). Most often their combination is sought, where complementarity of their characteristics can be utilized. Even though this direction is generally the best path of research, it can sometimes lead to addition of numerous diagnostics which are not always necessary for every experiment and it will only drive the costs up. In this work we are combining two low-cost detection techniques: actinometry (using low cost CCD spectrometer with a quartz rod) and catalytic probes. Results obtained previously with catalytic probes have been compared with TALIF [17] (for nitrogen), NO-titration [18] and actinometry [19] (only for hydrogen). In this work results obtained by catalytic probes and actinometry are spatially resolved and include oxygen plasma for the first time. Oxygen and hydrogen neutral atom concentrations were measured at various pressures and applied powers of the RF inductively coupled low pressure (RF ICP) system. We show that this combination of diagnostic methods can be utilized for a detailed description of the whole plasma system, because oxygen and hydrogen neutral atom concentrations vary significantly throughout the system and their exact values are important for materials processing in RF ICP.	0	0
hall thruster erosion	https://pkargupta.github.io/tod_dataset/in_situ_erosion.pdf	In situ erosion measurement tools for electric propulsion thrusters: triangular laser head and telemicroscope	The lifetime of electric propulsion (EP) thrusters depends particularly on the erosion characteristics of operation relevant components, for instance, the grid hole erosion of gridded ion thrusters or the channel wall erosion of Hall effect thrusters. Here two tools for in situ erosion measurements are presented, a triangular laser head for surface profiling and a telemicroscope for high-resolution optical imaging. Both can give access to radial and axial erosion parameters. The measurements can be done in situ without the need for breaking the vacuum and dismounting the thruster, which reduces thruster testing time considerably. In situ measurements can also help to ensure reproducibility of thruster performance conditions and can improve statistics of thruster characterization. The present work describes the fundamentals of both techniques in detail, selected experimental setups are presented, their performance is characterized and critically evaluated. The capabilities and limitations related to erosion measurements of EP thrusters are, exemplary, demonstrated for a gridded ion thruster RIT-22 and a Hall effect thruster SPT-100D.	Electric propulsion (EP) thrusters have to undergo extensive testing before flying into space. Predominantly, ion beam and plasma parameters are of interest, because they determine the primary function of the thruster: producing thrust by acceleration of charged particles. However, secondary parameters are also of interest. For instance, the erosion of operation relevant components must be measured and minimized in order to maximize thruster lifetime. Examples are the grid hole erosion of gridded ion thrusters or the channel wall erosion of Hall effect thrusters. Erosion measurements can be done by profilometry using mechanical or optical (laser-based) profilers, or imaging techniques, such as white-light interferometry or optical microscopy using high-resolution tabletop microscopes or telemicroscopes.Typically, erosion measurements are done outside of the vacuum chamber. Here the thruster needs to be removed from the test stand and to be taken apart. If it was possible to do the measurements in situ, i.e. inside the vacuum chamber without breaking the vacuum, venting and evacuating of the vacuum chamber can be avoided. This can save several days per thruster characterization step and, hence, reduces costs. Furthermore, if the thruster does not need to be removed or taking apart, reproducibility of the thruster performance conditions is ensured. Finally, in situ measurements allow to increase the number of experiments, which gives a statistically more reliable thruster performance characterization. In situ erosion measurements of different EP thrusters were reported using a triangular laser head (TLH, [1, 2]) or telemicroscopes [1–4]. Yuan et al. reported on the use of a telemicroscope for measuring in situ the grid deformation of a gridded ion thruster by videometrics [5]. Measurements of coordinates of the erosion profile points using a microscope provide not only quantitative data about erosion profile as a whole. The use of microscope also allows investigating microstructure of the erosion profile surface [6]. Recently, we were reporting on the advanced electric propulsion diagnostic (AEPD) platform [1, 2, 7], which consists of a modular, high-precision, five-axis positioning system and several diagnostic tools for ion beam and plasma characterization (Faraday probe, retarding potential analyzer, ExB probe, and active thermal probe), optical inspection (TLH, telemicroscope), and thermal characterization (pyrometer, thermocamera). All measurements can be done in situ, if needed all of them with a firing thruster.Here, the fundamentals of TLH and telemicroscope measurements are described in detail. Selected experimental setups are presented, their performance is characterized and critically evaluated. The capabilities of the diagnostic tools are demonstrated, exemplary, for two EP thrusters: a gridded ion thruster RIT-22 [8, 9] and a Hall effect thruster SPT-100D [10].	https://ntrs.nasa.gov/api/citations/20180005344/downloads/20180005344.pdf	In-situ Diagnostic for Assessing Hall Thruster Wear 	The design of a new diagnostic to measure the net erosion of Hall thruster surfaces is presented.  This diagnostic consists of a pair of optical non-contact profilometer pens mounted to a set of motion stages, which can interrogate the surface features of multiple components of interest including the hollow cathode assembly, magnet front pole covers, and discharge channel. By comparing scans of these surfaces to reference features, estimates of the component erosion rates can be acquired throughout long-duration lifetime tests without venting and removing the thruster from the vacuum facility for external profilometry. This work presents a detailed overview of the diagnostic design including the precision positioning system. In addition, preliminary data are shown which verify diagnostic operation and establish a baseline that will be used to track the erosion of the Hall Effect Rocket with Magnetic Shielding (HERMeS) Technology Demonstration Unit 3 (TDU-3) during an ongoing long-duration wear test	NASA continues to evolve a human exploration approach for beyond low-Earth orbit and to do so, where practical, in a manner involving international, academic, and industry partners [1]. NASA publicly presented a reference exploration concept at the Human Exploration and Operations Mission Directorate (HEOMD) Committee of the NASA Advisory Council meeting on March 28, 2017 [2]. This approach is based on an evolutionary human exploration architecture, expanding into the solar system with cis-lunar flight testing and validation of exploration capabilities before crewed missions beyond the earth-moon system and eventual crewed Mars missions. One of the key objectives is to achieve human exploration of Mars and beyond through the prioritization of those technologies and capabilities best suited for such a mission in accordance with the stepping stone approach to exploration [2]. High-power solar electric propulsion (SEP) is one of those key technologies that has been prioritized because of its significant exploration benefits. Specifically, for missions beyond low Earth orbit, spacecraft size and mass can be dominated by onboard chemical propulsion systems and propellants that may constitute more than 50 percent of spacecraft mass. This impact can be substantially reduced through the utilization of SEP due to its substantially higher specific impulse. Studies performed for NASA’s HEOMD and Science Mission Directorate have demonstrated that a 40-kW-class SEP capability can be enabling for both near term and future architectures and science missions [3]. In addition, a high-power, 40 kW-class Hall thruster propulsion system provides significant capability and represents, along with flexible blanket solar array technology, a readily scalable technology with a clear path to much higher power systems. Accordingly, since 2012, NASA has been developing a 14-kW Hall thruster electric propulsion string that can serve as the building block for realizing a 40-kW-class SEP capability. The 14-kW Hall thruster system development, led by the NASA Glenn Research Center (GRC) and the Jet Propulsion Laboratory (JPL), began with maturation of the Hall Effect Rocket with Magnetic Shielding (HERMeS) and power processing unit.  The technology development work has transitioned to Aerojet Rocketdyne via a competitive procurement selection for the Advanced Electric Propulsion System (AEPS) contract. The AEPS contract includes the development, qualification, and multiple flight 14-kW electric propulsion string deliveries. The AEPS SEP string consists of the Hall thruster, power processing unit (including digital control and interface functionality), xenon flow controller, and associated intra-string harnesses. NASA continues to support the AEPS development leveraging in-house expertise, plasma modeling capability, and world-class test facilities. NASA also executes AEPS and mission risk reduction activities to support the AEPS development and mission application. As part of this effort, NASA has conducted two wear test campaigns to identify erosion phenomena and the accompanying failure modes as well as to validate service-life models for magnetically-shielded thrusters. These campaigns utilized two different technology demonstration unit (TDU) thrusters with similar designs. The first began in 2016 and accumulated approximately 1700 h of operation using the TDU-1 thruster [4]. The second was performed in 2017 with the TDU-1 and TDU-3 thrusters and consisted of seven short duration (~200 h) segments, each testing a different thruster configuration or operating condition [5]. This test is referred to as the Short Duration Wear Tests (SDWT) throughout this paper.  A new in-situ wear diagnostic was implemented as part of a third wear test (named the TDU-3 Long Duration Wear Test or TDU-3 LDWT), which began in October 2017. This paper presents the detailed design and implementation of this diagnostic as well as preliminary data showing how it might provide additional insight into how erosion changes as a function of operational time and operating condition	0	0
hall thruster erosion	https://deepblue.lib.umich.edu/bitstream/handle/2027.42/76234/AIAA-2006-4657-608.pdf?sequence=1	An Investigation of Factors Involved in Hall Thruster Wall Erosion Modeling	A hydrodynamic description of the plasma flow within a SPT-100 type Hall thruster is used to study the erosion of the channel walls over time. The simulation of thousands of hours of erosion is performed on the order of tens of minutes of computation time. The calculated erosion matches satisfactorily against experimental data, especially the erosion at the exit plane. The effects of scattering collisions, ion velocity distributions, sputtering threshold energy, sputter yield curve fits, and potential drop within the thruster on the erosion are examined. While all of the above do affect erosion rates, the semi-empirical sputter yield curve fits, especially at the low ion energies prevalent in Hall thrusters, play the largest role in influencing the simulated erosion of the channel walls.	As lifetime requirements desired for Hall thruster operations increase, there is a greater need to be able to predict and analyze the wear of the thruster over time. The main mechanism of Hall thruster failure is the erosion of the acceleration channel walls to the point where the magnetic coils are exposed to the plasma flow. Experimental testing to determine the rate and extent of wall erosion is expensive and time-consuming. Thus, capturing the erosion process through computational simulations is a useful means of predicting lifetime for design and analysis purposes alongside experimentally obtained data. Interest in Hall thruster erosion, including the development of Hall thruster erosion models, has been increasing recently.¹–³ However, there are still aspects of the erosion process that remain unsatisfactorily modeled. In our previous work, we developed a hydrodynamic-based erosion model for Hall thrusters.⁴,⁵ However, that model was unable to capture the low erosion rates registered at later portions of the thruster life after around 1000 hours and instead calculated zero erosion.The work presented in this paper uses the same sheath and plasma model, though with a new numerical scheme, with investigations on various aspects of the plasma flow and sputter model that are thought to perhaps contribute to the erosion process. It is thought that perhaps a secondary mechanism for erosion may dominate at the later times.⁶ Thus, the contribution to erosion from scattering collisions is examined. Also, the sputtering portion of the model is further investigated with the incorporation of ion velocity distributions instead of assuming monoenergetic streams.The extent of the influence of different empirical sputter yield fits on the erosion rates is also observed. Finally, a simple parametric study of the effects of changing the plasma parameters is performed. The numerical scheme for the hydrodynamic model is different than the one we have used before and is described in the next section. The results of the various test cases are compared to each other and experimental data. Discussion of the results and areas of further work are given later in the paper.	https://electricrocket.org/IEPC/IEPC-2007-151.pdf	Investigation of the SPT operation and discharge chamber wall erosion rate under  increased discharge voltages 	 New results of the thruster operation specifics study under operation modes with increased discharge voltages and moderate discharge power are represented in the given paper. Particularly  results of the following studies are considered: - characterization of the standard PPS1350 model under different operation modes including that ones with increased discharge voltages to clarify reasons of the performance degradation with increase of the discharge voltage and simultaneous reduction of the mass flow rate; -The SPT-100 type laboratory model erosion test and local plasma parameter measurements along its external discharge chamber wall by the near wall probes. Obtained results are represented in the given paper. 	Nowadays it is reasonable to develop SPT with increased specific impulse. It is  known  also that the most simple way to increase specific impulse is to increase discharge voltage and to reduce mass flow rate in order to maintain moderate discharge power density under fixed thruster sizes. Therefore it is interesting to study thruster operation under such operation modes and some studies were already done earlier. 1-6  Obtained results show that under the mentioned operation modes and fixed power thrust efficiency is typically reduced relative to the possible one with increase of the discharge voltage. “The possible one” means the level which is demonstrated under increased discharge voltages and moderate mass flow rate, if one neglect the necessity to limit discharge power in order to obtain large enough life time. And it is important to clarify the reasons of the mentioned trend. Taking this into account the following studies were made: - characterization of the standard PPS1350 model under different operation modes with increased discharge voltages including the registration of its thermal state and measurements of the current of ions leaving thruster -the SPT-100 type laboratory model performance and erosion tests combined with the local plasma parameter measurements along its external discharge chamber wall by the near wall probes;   These studies were fulfilled within the frames of the CNES-INTAS projects ##03-53- 3358, 06-1000024-8851 and internal Russian projects including the RFBR one #05-08-011411 and the state purchase order 02.445.11.7323. Obtained results are represented below	0	0
enabling large language model reasoning via prompting	https://arxiv.org/pdf/2305.10601	Tree of Thoughts: Deliberate Problem Solving with Large Language Models	Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, “Tree of Thoughts” (ToT), which generalizes over the popular “Chain of Thought” approach to prompting language models, and enables exploration over coherent units of text (“thoughts”) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.	Originally designed to generate text, scaled-up versions of language models (LMs) such as GPT [ 25 , 26 , 1, 23] and PaLM [5] have been shown to be increasingly capable of performing an ever wider range of tasks requiring mathematical, symbolic, commonsense, and knowledge reasoning. It is perhaps surprising that underlying all this progress is still the original autoregressive mechanism for generating text, which makes token-level decisions one by one and in a left-to-right fashion. Is such a simple mechanism sufficient for a LM to be built toward a general problem solver? If not, what problems would challenge the current paradigm, and what should be alternative mechanisms? The literature on human cognition provides some clues to answer these questions. Research on “dual process” models suggests that people have two modes in which they engage with decisions – a fast, automatic, unconscious mode (“System 1”) and a slow, deliberate, conscious mode (“System 2”) [ 30 , 31, 16 , 15 ]. These two modes have previously been connected to a variety of mathematical models used in machine learning. For example, research on reinforcement learning in humans and other animals has explored the circumstances under which they engage in associative “model free” learning or more deliberative “model based” planning [7 ]. The simple associative token-level choices of LMs are also reminiscent of “System 1”, and thus might benefit from augmentation by a more deliberate “System 2” planning process that (1) maintains and explores diverse alternatives for current choices instead of just picking one, and (2) evaluates its current status and actively looks ahead or backtracks to make more global decisions. To design such a planning process, we return to the origins of artificial intelligence (and cognitive science), drawing inspiration from the planning processes explored by Newell, Shaw, and Simon starting in the 1950s [ 21, 22 ]. Newell and colleagues characterized problem solving [21 ] as search through a combinatorial problem space, represented as a tree. We thus propose the Tree of Thoughts (ToT) framework for general problem solving with language models. As Figure 1 illustrates, while existing methods (detailed below) sample continuous language sequences for problem solving, ToT actively maintains a tree of thoughts, where each thought is a coherent language sequence that serves as an intermediate step toward problem solving (Table 1). Such a high-level semantic unit allows the LM to self-evaluate the progress different intermediate thoughts make towards solving the problem through a deliberate reasoning process that is also instantiated in language (Figures 2,4,6). This implementation of search heuristics via LM self-evaluation and deliberation is novel, as previous search heuristics are either programmed or learned. Finally, we combine this language-based capability to generate and evaluate diverse thoughts with search algorithms, such as breadth-first search (BFS) or depth-first search (DFS), which allow systematic exploration of the tree of thoughts with lookahead and backtracking. Empirically, we propose three new problems that challenge existing LM inference methods even with the state-of-the-art language model, GPT-4 [ 23 ]: Game of 24, Creative Writing, and Crosswords (Table 1). These tasks require deductive, mathematical, commonsense, lexical reasoning abilities, and a way to incorporate systematic planning or search. We show ToT obtains superior results on all three tasks by being general and flexible enough to support different levels of thoughts, different ways to generate and evaluate thoughts, and different search algorithms that adapt to the nature of different problems. We also analyze how such choices affect model performances via systematic ablations and discuss future directions to better train and use LMs.	https://arxiv.org/pdf/2201.11903	Chain-of-Thought Prompting Elicits Reasoning in Large Language Models	We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.	The NLP landscape has recently been revolutionized by language models (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020, inter alia). Scaling up the size of language models has been shown to confer a range of benefits, such as improved performance and sample efficiency (Kaplan et al., 2020; Brown et al., 2020, inter alia). However, scaling up model size alone has not proved sufficient for achieving high performance on challenging tasks such as arithmetic, commonsense, and symbolic reasoning (Rae et al., 2021). This work explores how the reasoning ability of large language models can be unlocked by a simple method motivated by two ideas. First, techniques for arithmetic reasoning can benefit from generating natural language rationales that lead to the final answer. Prior work has given models the ability to generate natural language intermediate steps by training from scratch (Ling et al., 2017) or finetuning a pretrained model (Cobbe et al., 2021), in addition to neuro-symbolic methods that use formal languages instead of natural language (Roy and Roth, 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Second, large language models offer the exciting prospect of in-context few-shot learning via prompting. That is, instead of finetuning a separate language model checkpoint for each new task, one can simply “prompt” the model with a few input–output exemplars demonstrating the task. Remarkably, this has been successful for a range of simple question-answering tasks (Brown et al., 2020). Both of the above ideas, however, have key limitations. For rationale-augmented training and finetuning methods, it is costly to create a large set of high quality rationales, which is much more complicated than simple input–output pairs used in normal machine learning. For the traditional fewshot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning abilities, and often does not improve substantially with increasing language model scale (Rae et al., 2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations. Specifically, we explore the ability of language models to perform few-shot prompting for reasoning tasks, given a prompt that consists of triples: 〈input, chain of thought, output〉. A chain of thought is a series of intermediate natural language reasoning steps that lead to the final output, and we refer to this approach as chain-of-thought prompting. An example prompt is shown in Figure 1. We present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks, showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree. Figure 2 illustrates one such result—on the GSM8K benchmark of math word problems (Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting by a large margin and achieves new state-of-the-art performance. A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of generality. This work underscores how large language models can learn via a few examples with natural language data about the task (c.f. automatically learning the patterns underlying inputs and outputs via a large training dataset).	0	1
using preferences to train language models for better reasoning	https://arxiv.org/pdf/2404.02078	Advancing LLM Reasoning Generalists with Preference Trees	We introduce EURUS, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, EURUS models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, EURUS-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of EURUS can be primarily attributed to ULTRAINTERACT, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. ULTRAINTERACT can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting of (1) reasoning chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning. ULTRAINTERACT allows us to conduct an in-depth exploration of preference learning for reasoning tasks. Our investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks compared to their effectiveness in general conversations. Inspired by this, we derive a novel reward modeling objective which, together with ULTRAINTERACT, leads to a strong reward model.	Current alignment techniques have significantly advanced the development of open-source large language models (LLMs) that effectively meet user expectations and align with human values (Touvron et al., 2023; Tunstall et al., 2023). On complex reasoning, success has been achieved by specializing models for specific capabilities, such as coding (Wei et al., 2023; Guo et al., 2024a; Zheng et al., 2024) and solving math problems (Fu et al., 2023; Yue et al., 2023; Luo et al., 2023a; Toshniwal et al., 2024). However, these models still fall short, by large margins, of the most advanced proprietary models in their all-around capabilities to tackle a diverse range of challenging problems. We conjecture that this performance gap can be primarily attributed to (1) the lack of high-quality alignment data and (2) the underexploration of preference learning techniques for improving models' complex reasoning capabilities. In this paper, we take strides towards bridging this gap by addressing both factors and developing EURUS. EURUS consists of a suite of LLMs finetuned from Mistral-7B (Jiang et al., 2023a) and CodeLLaMA-70B (Roziere et al., 2023). Across a diverse set of complex reasoning benchmarks that are mostly out-of-distribution (OOD), EURUS achieves state-of-the-art overall performance among all open-source models. In particular, EURUS excels in solving challenging problems that often require sophisticated planning, reasoning, tool integration, and the ability to interact with and learn from the environment and users. As shown in Figure 1, on university-level STEM questions TheoremQA (Chen et al., 2023) and competition-level coding problems LeetCode Contest (Guo et al., 2024a), EURUS-70B significantly outperforms all open-source models, achieving comparable performance to GPT-3.5 Turbo. EURUS models are trained on ULTRAINTERACT, our newly-curated, large-scale, and high-quality alignment data specifically designed to improve LLMs' reasoning capabilities. ULTRAINTERACT consists of a diverse set of instructions spanning math, coding, and logical reasoning problems from 12 established datasets. For each instruction, ULTRAINTERACT collects a preference tree that includes: (1) Diverse planning strategies in a unified pattern, such as sequential processing (Wei et al., 2022) and tool creation (Qian et al., 2023), followed by executing step-by-step actions formatted in either text or code, to provide divserse reasoning trajectories. (2) Multi-turn interaction trajectories with the environment and the critique, to improve models' capabilities to learn from feedback and correct previous errors (Wang et al., 2023b). (3) Paired correct and incorrect actions organized in tree structures, to facilitate preference learning. In total, ULTRAINTERACT contains 86K instructions and 220K action pairs, where each pair consists of an instruction, a correct response, and an incorrect one. Conceptually, ULTRAINTERACT's data resemble imbalanced binary trees as shown in Figure 2. ULTRAINTERACT can be used in both supervised fine-tuning and preference learning. Our experiments show that, using ULTRAINTERACT along with established datasets in instruction fine-tuning already achieves strong performance. ULTRAINTERACT further facilitates preference learning for reasoning tasks, improving the performance even further with KTO (Ethayarajh et al., 2024) and NCA (Chen et al., 2024a). Surprisingly, applied to an instruction finetuned EURUS model, DPO (Rafailov et al., 2023) hurts the performance. Through careful analysis, we provide evidence that the performance in reasoning correlates with the value of rewards of chosen data—a higher final reward often indicates a better reasoning capability. Besides, our investigation suggests that DPO may be less suitable for reasoning tasks than KTO and NCA. Inspired by this fresh finding, we devise a new objective for reward modeling to augment the Bradley-Terry objective (Bradley & Terry, 1952), explicitly encouraging training to increase the absolute rewards of chosen solution and decrease those of rejected data. Furthermore, ULTRAINTERACT leads to our reward model EURUS-RM-7B, which achieves a better correlation with human annotators than all existing models on AutoJ (Li et al., 2023a) and MT-Bench (Zheng et al., 2023), including GPT-4 (OpenAI, 2023). EURUS-RM-7B demonstrates especially strong preference modeling performance on reasoning tasks. Checkpoints of our EURUS models, accompanying ULTRAINTERACT alignment data to reproduce this research, will be publicly available.	https://arxiv.org/pdf/2406.09136	Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs	The recent development of chain-of-thought (CoT) decoding has enabled large language models (LLMs) to generate explicit logical reasoning paths for complex problem-solving. However, research indicates that these paths are not always deliberate and optimal. The tree-of-thought (ToT) method employs tree-searching to extensively explore the reasoning space and find better reasoning paths that CoT decoding might overlook. This deliberation, however, comes at the cost of significantly increased inference complexity. In this work, we demonstrate that fine-tuning LLMs leveraging the search tree constructed by ToT allows CoT to achieve similar or better performance, thereby avoiding the substantial inference burden. This is achieved through Chain of Preference Optimization (CPO), where LLMs are fine-tuned to align each step of the CoT reasoning paths with those of ToT using the inherent preference information in the tree-search process. Extensive experimental results show that CPO significantly improves LLM performance in solving a variety of complex problems, including question answering, fact verification, and arithmetic reasoning, demonstrating its effectiveness. Our code is available at https://github.com/sail-sg/CPO.	Recent advances in large language models (LLMs) have shown that constructing reasoning chains is critical to improving their problem-solving capabilities [1 , 2, 3, 4 , 5 , 6, 7]. A representative method is chain-of-thought (CoT) [1], which prompts LLMs to generate intermediate reasoning steps, i.e., thoughts, thereby constructing explicit reasoning paths (as depicted in Figure 1(a)). While straightforward and intuitive, recent research observes that CoT can often overlook optimal reasoning paths and exhibit an unconscious style of answering due to its single-path focus [ 8, 9]. To foster a more deliberate and conscious reasoning style, Yao et al. [8] propose tree-of-thought (ToT), which generates multiple branching thoughts at each step of the reasoning process and conducts selfevaluation for pruning and planning to search for reasoning paths (as shown in Figure 1(b)). However, despite improving reasoning quality, ToT significantly increases computational complexity, which limits its practical application. This raises the question: Can the strategic depth of ToT be integrated into CoT to enhance its effectiveness while maintaining efficiency? Existing research has initially provided a positive answer to the above question [10, 11 , 12 ]. A natural strategy is to treat the reasoning path discovered by ToT for each instance as a target for supervision, and then fine-tune LLMs to improve their CoT reasoning abilities [11 , 12 ]. Several methods have been proposed to improve this approach, including using advanced tree-search techniques like MonteCarlo tree-search (MCTS) and employing external reward models [12 , 10 ] for pruning and planning to gather better reasoning paths as supervision. The effectiveness of these approaches is therefore largely dependent on the quality of the best-discovered reasoning path. In this paper, we identify a limitation in these approaches: they overlook the non-optimal reasoning thoughts generated during the tree-search process, which naturally provides additional preference information. Specifically, ToT inherently generates multiple alternative thoughts at each reasoning step, and pruning is performed according to their evaluated qualities. This tree-search process constitutes a preference over all intermediate thought candidates—thoughts appearing in the bestdiscovered reasoning path are preferred over those that do not. Moreover, this could shed even more insights than the final best-discovered reasoning path, as non-optimal reasoning paths (and thus preferences) exist at each step in the tree-search. Inspired by recently developed reinforcement learning from human feedback (RLHF) techniques like direct preference optimization (DPO) [13], we propose Chain-of-Preference Optimization (CPO) to fully exploit the inherent preference information. Specifically, we construct paired preference thoughts at each reasoning step according to the search tree of ToT and then train LLMs to align with these preferences using the DPO algorithm (as illustrated in Figure 1(c)). The paired preference thoughts are constructed based on the above intuition: at each reasoning step, we categorize thoughts as preferred or dispreferred based on their inclusion in the final paths chosen by ToT. With such preference data, CPO enables LLMs to generate the path preferred by ToT using CoT decoding at inference time. We conduct extensive experiments to evaluate the effectiveness of CPO. Experiments on seven datasets using LLaMA [ 14 ] and Mistral [ 15 ] as base models demonstrate that CPO is highly effective in teaching LLMs the preferred thoughts of ToT at each reasoning step, leading to an average accuracy improvement of up to 4.3% compared to the base models. Additionally, the experiments reveal that CPO can achieve comparable or even superior performance to the ToT method, which on average requires more than 50 times longer for inference.	0	0
using active learning to train neural network-based multi-armed bandits for k-class classification	https://arxiv.org/pdf/2404.12522	Neural Active Learning Beyond Bandits	We study both stream-based and pool-based active learning with neural network approximations. A recent line of works proposed bandit-based approaches that transformed active learning into a bandit problem, achieving both theoretical and empirical success. However, the performance and computational costs of these methods may be susceptible to the number of classes, denoted as K, due to this transformation. Therefore, this paper seeks to answer the question: \"How can we mitigate the adverse impacts of K while retaining the advantages of principled exploration and provable performance guarantees in active learning?\" To tackle this challenge, we propose two algorithms based on the newly designed exploitation and exploration neural networks for stream-based and pool-based active learning. Subsequently, we provide theoretical performance guarantees for both algorithms in a non-parametric setting, demonstrating a slower error-growth rate concerning K for the proposed approaches. We use extensive experiments to evaluate the proposed algorithms, which consistently outperform state-of-the-art baselines.	Active learning is one of the primary areas in machine learning to investigate the learning technique on a small subset of labeled data while acquiring good generalization performance compared to passive learning [ 19 ]. There are mainly two settings of active learning: stream-based and pool-based settings. For the stream-based setting, the learner is presented with an instance drawn from some distribution in each round and is required to decide on-the-fly whether or not to query the label from the oracle. For the pool-based setting, the learner aims to select one or multiple instances from the unlabeled pool and hand them over to the oracle for labeling. It repeats this process until the label budget is exhausted [51]. The essence of active learning is to exploit the knowledge extracted from labeled instances and explore the unlabeled data to maximize information acquisition for long-term benefits. Using neural networks (NNs) to perform active learning has been explored extensively in recent works [48 ; 52 ; 56 ; 6 ]. However, they often lack a provable performance guarantee despite strong empirical performance. To address this issue, a recent line of works [58 ; 14 ] proposed the banditbased approaches to solve the active learning problem, which are equipped with principled exploration and theoretical performance guarantee. In contextual bandits [38 ; 68], the learner is presented with K arms (context vectors) and required to select one arm in each round. Then, the associated reward is observed. [58; 14] transformed the online K-class classification into a bandit problem. Specifically, in one round of stream-based active learning, a data instance xt ∈ Rd is transformed into K long vectors corresponding to K arms, matching K classes: xt,1 = [x⊤ t , 0⊤, · · · , 0⊤]⊤, . . . , xt,K = [0⊤, · · · , 0⊤, x⊤ t ]⊤, where xt,k ∈ RdK , k ∈ [K]. Then, the learner uses an NN model to calculate a score for each arm and selects an arm based on these scores. The index of the selected arm represents the index of the predicted class. This design enables researchers to utilize the exploration strategy and analysis in contextual bandits to solve the active learning problem. Note [58; 14 ] can only handle the stream-based setting of active learning. However, bandit-based approaches bear the following two limitations. First, as the instance xt is transformed into K arms, it is required to calculate a score for all K arms respectively, producing a cost of K times forward-propagation computation of neural networks. This computation cost is scaled by K. Second, the transformed long vector (arm) has (Kd) dimensions, in contrast to the d dimensions of the original instance as the input of the NN model. This potentially amplifies the effects of K on an active learning algorithm's performance. We empirically evaluate [58; 14 ] as shown in Table 1. The results indicate a noticeable degradation in both test accuracy and running time as K increases. In response, in this paper, we aim to mitigate the adverse effects of K on the bandit-based approach in active learning. Our methods are built upon and beyond [ 14 ]. [ 14 ] adopted the idea of [13 ] to employ two neural networks, one for exploitation and another for exploration. As previously mentioned, these two neural networks take the transformed Kd-dimension arm as input. Moreover, in each round, [ 14] decomposed the label vector yt ∈ {0, 1}K into K rewards (scalars), necessitating the training of two neural networks K times for each arm. Next, we summarize our key ideas and contributions to reduce the input dimension back to d and the number of forward propagations to 1 in each round while preserving the essence of exploitation and exploration of neural networks. Methodology. (1) We extend the loss function in active learning from 0-1 loss to Bounded loss, which is more flexible and general. Instead, [58; 14 ] restricted the loss to be 0-1 loss, because they had to define the reward of each class (arm) due to their bandit-based methodology. (2) We re-designed the input and output exploitation and exploration neural networks to directly take the d-dimension instance as input and output the predicted probabilities for K classes synchronously, mitigating the curse of K. The connection between exploitation and exploration neural networks is also reconstructed beyond the standard bandit setting. In other words, we avoid the transformation of active learning to the standard bandit setting. This is the first main contribution of this paper. (3) To facilitate efficient and effective exploration, we introduce the end-to-end embedding (Definition 4.1) as the input of the exploration neural network, which removes the dependence of the input dimension while preserving the essential information. (4) In addition to our proposed stream-based algorithm, referred to NEURONAL-S, we also propose a pool-based active learning algorithm, NEURONAL-P. We bring the redesigned exploitation and exploration network into pool-based setting and propose a novel gap-inverse-based selection strategy tailored for pool-based active learning. This is our second main contribution. Note that the stream-based algorithms cannot be directly converted into the pool-based setting, as discussed in Appendix B. Theoretical analysis. We provide the regret upper bounds for the proposed stream-based algorithm under low-noise conditions on the data distribution. Our results indicate the cumulative regret of NEURONAL-S grows slower than that of [ 58] concerning K by a multiplicative factor at least O(pT log(1 + λ0)) and up to eO(√md), where λ0 is the smallest eigenvalue of Neural Tangent Kernel (NTK) and m is the width of the neural network. This finding helps explain why our algorithms outperform the bandit-based algorithms, particularly when K is large, as shown in Table 1. In the binary classification task, our regret bounds directly remove the dependence of effective dimension  ̃d, which measures the actual underlying dimension in the RKHS space spanned by NTK, discussed in Sec. 5. We also provide a performance analysis for the proposed pool-based algorithm in the non-parametric setting, tailored for neural network models. In contrast, previous works focus on the regime either in parametric settings that require a finite VC dimension [31] or a linear mapping function assumption [ 8 ; 64 ; 28 ]. The above theoretical results are our third main contribution. In addition, Empirical evaluation. In the end, we perform extensive experiments to evaluate the proposed algorithms for both stream-based and pool-based algorithms compared to state-of-the-art baselines. Our evaluation encompasses various metrics, including test accuracy and running time, and we have carried out ablation studies to investigate the impact of hyper-parameters and label budgets. This is our fourth main contribution.	https://arxiv.org/pdf/2210.00423	Improved Algorithms for Neural Active Learning	We improve the theoretical and empirical performance of neural-network(NN)based active learning algorithms for the non-parametric streaming setting. In particular, we introduce two regret metrics by minimizing the population loss that are more suitable in active learning than the one used in state-of-the-art (SOTA) related work. Then, the proposed algorithm leverages the powerful representation of NNs for both exploitation and exploration, has the query decision-maker tailored for k-class classification problems with the performance guarantee, utilizes the full feedback, and updates parameters in a more practical and efficient manner. These careful designs lead to an instance-dependent regret upper bound, roughly improving by a multiplicative factor O(log T ) and removing the curse of input dimensionality. Furthermore, we show that the algorithm can achieve the same performance as the Bayes-optimal classifier in the long run under the hard-margin setting in classification problems. In the end, we use extensive experiments to evaluate the proposed algorithm and SOTA baselines, to show the improved empirical performance.	The Neural Network (NN) is one of the indispensable paradigms in machine learning and is widely used in multifarious supervised-learning tasks [ 23 ]. As more and more complicated NNs are developed, the requirement of the training procedure on the labeled data grows, incurring significant cost of label annotation. Active learning investigates effective techniques on a much smaller labeled data set while attaining the comparable generalization performance to passive learning [19 ]. In this paper, we focus on the classification problem in the streaming setting of active learning with NN models. At every round, the learner receives an instance and is compelled to decide on-the-fly whether or not to observe the label associated with this instance. This problem seeks to maximize the generalization capability of learned NNs in a sequence of rounds, such that the model has robust performance on the unseen data from the same distribution [40]. In active learning, given access to the i.i.d. generated instances from a distribution D, suppose there exist a class of functions F that formulate the mapping from instances to theirs labels. In the parametric setting, i.e., F has finite VC-dimension [ 25 ], existing works [24, 14 , 7] have shown that the active learning algorithms can achieve the convergence rate of  ̃O(1/√N ) to the best population loss in F, where N is the number of label queries. In the non-parametric setting, recent works [ 34 , 35] provide the similar convergence results while suffering from the curse of input dimensionality. Unfortunately, most of NN-based approaches to active learning do not come with the performance guarantee, despite having powerful empirical results. The first performance guarantee for neural active learning has been established in a recent work by [ 48 ], and the analysis is for over-parameterized neural networks with the assistance of Neural Tangent Kernel (NTK). We carefully investigate the limitations of [ 48], which turn into the main motivations of our paper. First, [48 ] transforms the classification problem into a multi-armed bandit problem [ 55 ], to minimize a pseudo regret metric. Yet, on the grounds that they seek to minimize the conditional population loss on a sequence of given data, it is dubious that the pseudo regret used in [ 48 ] can explicitly measure the generalization capability of given algorithms (see Remark 2.1). Second, the training process for NN models is not efficient, as [48] uses vanilla gradient descent and starts from randomly initialized parameters in every round. Third, although [48 ] removes the curse of input dimensionality d, the performance guarantee strongly suffers from another introduced term, the effective dimensionality  ̃d, which can be thought of as the non-linear dimensionalities of Hilbert space spanned by NTK. In the worse case, the magnitude of  ̃d can be an unacceptably large number and thus the performance guarantee collapses. 1.1 Main contributions In this paper, we propose a novel algorithm, I-NeurAL (Improved Algorithms for Neural Active Learning), to tackle the above limitations. Our contributions can be summarized as follows: (1) We consider the k-class classification problem, and we introduce two new regret metrics to minimize the population loss, which can directly reflect the generalization capability of NN-based algorithms. (2) I-NeurAL has a neural exploration strategy with a novel component to decide whether or not to query the label, coming with the performance guarantee. I-NeurAL exploits the full feedback in active learning which is a subtle but effective idea. (3) I-NeurAL is designed to support minibatch Stochastic Gradient Descent (SGD). In particular, at every round, I-NeurAL does mini-batch SGD starting with the parameters of the last round, i.e., with warm start, which is more efficient and practical compared to [48]. (4) Without any noise assumption on the data distribution, we provide an instance-dependent performance guarantee of I-NeurAL for over-parameterized neural networks. Compared to [ 48 ], we remove the curse of both the input dimensionality d and the effective dimensionality  ̃d; Moreover, we roughly improve the regret by a multiplicative factor log(T ), where T is the number of rounds. (5) under a hard-margin assumption on the data distribution, we provide that NN models can achieve the same generalization capability as Bayes-optimal classifier after O(log T ) number of label queries; (6) we conduct extensive experiments on real-world data sets to demonstrate the improved performance of I-NeurAL over state-of-the-art baselines including the closest work [48] which has not provided empirical validation of their proposed algorithms.	0	1
using active learning to train neural network-based multi-armed bandits for k-class classification	https://arxiv.org/pdf/2404.12522	Neural Active Learning Beyond Bandits	We study both stream-based and pool-based active learning with neural network approximations. A recent line of works proposed bandit-based approaches that transformed active learning into a bandit problem, achieving both theoretical and empirical success. However, the performance and computational costs of these methods may be susceptible to the number of classes, denoted as K, due to this transformation. Therefore, this paper seeks to answer the question: \"How can we mitigate the adverse impacts of K while retaining the advantages of principled exploration and provable performance guarantees in active learning?\" To tackle this challenge, we propose two algorithms based on the newly designed exploitation and exploration neural networks for stream-based and pool-based active learning. Subsequently, we provide theoretical performance guarantees for both algorithms in a non-parametric setting, demonstrating a slower error-growth rate concerning K for the proposed approaches. We use extensive experiments to evaluate the proposed algorithms, which consistently outperform state-of-the-art baselines.	Active learning is one of the primary areas in machine learning to investigate the learning technique on a small subset of labeled data while acquiring good generalization performance compared to passive learning [ 19 ]. There are mainly two settings of active learning: stream-based and pool-based settings. For the stream-based setting, the learner is presented with an instance drawn from some distribution in each round and is required to decide on-the-fly whether or not to query the label from the oracle. For the pool-based setting, the learner aims to select one or multiple instances from the unlabeled pool and hand them over to the oracle for labeling. It repeats this process until the label budget is exhausted [51]. The essence of active learning is to exploit the knowledge extracted from labeled instances and explore the unlabeled data to maximize information acquisition for long-term benefits. Using neural networks (NNs) to perform active learning has been explored extensively in recent works [48 ; 52 ; 56 ; 6 ]. However, they often lack a provable performance guarantee despite strong empirical performance. To address this issue, a recent line of works [58 ; 14 ] proposed the banditbased approaches to solve the active learning problem, which are equipped with principled exploration and theoretical performance guarantee. In contextual bandits [38 ; 68], the learner is presented with K arms (context vectors) and required to select one arm in each round. Then, the associated reward is observed. [58; 14] transformed the online K-class classification into a bandit problem. Specifically, in one round of stream-based active learning, a data instance xt ∈ Rd is transformed into K long vectors corresponding to K arms, matching K classes: xt,1 = [x⊤ t , 0⊤, · · · , 0⊤]⊤, . . . , xt,K = [0⊤, · · · , 0⊤, x⊤ t ]⊤, where xt,k ∈ RdK , k ∈ [K]. Then, the learner uses an NN model to calculate a score for each arm and selects an arm based on these scores. The index of the selected arm represents the index of the predicted class. This design enables researchers to utilize the exploration strategy and analysis in contextual bandits to solve the active learning problem. Note [58; 14 ] can only handle the stream-based setting of active learning. However, bandit-based approaches bear the following two limitations. First, as the instance xt is transformed into K arms, it is required to calculate a score for all K arms respectively, producing a cost of K times forward-propagation computation of neural networks. This computation cost is scaled by K. Second, the transformed long vector (arm) has (Kd) dimensions, in contrast to the d dimensions of the original instance as the input of the NN model. This potentially amplifies the effects of K on an active learning algorithm's performance. We empirically evaluate [58; 14 ] as shown in Table 1. The results indicate a noticeable degradation in both test accuracy and running time as K increases. In response, in this paper, we aim to mitigate the adverse effects of K on the bandit-based approach in active learning. Our methods are built upon and beyond [ 14 ]. [ 14 ] adopted the idea of [13 ] to employ two neural networks, one for exploitation and another for exploration. As previously mentioned, these two neural networks take the transformed Kd-dimension arm as input. Moreover, in each round, [ 14] decomposed the label vector yt ∈ {0, 1}K into K rewards (scalars), necessitating the training of two neural networks K times for each arm. Next, we summarize our key ideas and contributions to reduce the input dimension back to d and the number of forward propagations to 1 in each round while preserving the essence of exploitation and exploration of neural networks. Methodology. (1) We extend the loss function in active learning from 0-1 loss to Bounded loss, which is more flexible and general. Instead, [58; 14 ] restricted the loss to be 0-1 loss, because they had to define the reward of each class (arm) due to their bandit-based methodology. (2) We re-designed the input and output exploitation and exploration neural networks to directly take the d-dimension instance as input and output the predicted probabilities for K classes synchronously, mitigating the curse of K. The connection between exploitation and exploration neural networks is also reconstructed beyond the standard bandit setting. In other words, we avoid the transformation of active learning to the standard bandit setting. This is the first main contribution of this paper. (3) To facilitate efficient and effective exploration, we introduce the end-to-end embedding (Definition 4.1) as the input of the exploration neural network, which removes the dependence of the input dimension while preserving the essential information. (4) In addition to our proposed stream-based algorithm, referred to NEURONAL-S, we also propose a pool-based active learning algorithm, NEURONAL-P. We bring the redesigned exploitation and exploration network into pool-based setting and propose a novel gap-inverse-based selection strategy tailored for pool-based active learning. This is our second main contribution. Note that the stream-based algorithms cannot be directly converted into the pool-based setting, as discussed in Appendix B. Theoretical analysis. We provide the regret upper bounds for the proposed stream-based algorithm under low-noise conditions on the data distribution. Our results indicate the cumulative regret of NEURONAL-S grows slower than that of [ 58] concerning K by a multiplicative factor at least O(pT log(1 + λ0)) and up to eO(√md), where λ0 is the smallest eigenvalue of Neural Tangent Kernel (NTK) and m is the width of the neural network. This finding helps explain why our algorithms outperform the bandit-based algorithms, particularly when K is large, as shown in Table 1. In the binary classification task, our regret bounds directly remove the dependence of effective dimension  ̃d, which measures the actual underlying dimension in the RKHS space spanned by NTK, discussed in Sec. 5. We also provide a performance analysis for the proposed pool-based algorithm in the non-parametric setting, tailored for neural network models. In contrast, previous works focus on the regime either in parametric settings that require a finite VC dimension [31] or a linear mapping function assumption [ 8 ; 64 ; 28 ]. The above theoretical results are our third main contribution. In addition, Empirical evaluation. In the end, we perform extensive experiments to evaluate the proposed algorithms for both stream-based and pool-based algorithms compared to state-of-the-art baselines. Our evaluation encompasses various metrics, including test accuracy and running time, and we have carried out ablation studies to investigate the impact of hyper-parameters and label budgets. This is our fourth main contribution.	https://arxiv.org/pdf/2110.08611	Deep Active Learning by Leveraging Training Dynamics	Active learning theories and methods have been extensively studied in classical statistical learning settings. However, deep active learning, i.e., active learning with deep learning models, is usually based on empirical criteria without solid theoretical justification, thus suffering from heavy doubts when some of those fail to provide benefits in real applications. In this paper, by exploring the connection between the generalization performance and the training dynamics, we propose a theory-driven deep active learning method (dynamicAL) which selects samples to maximize training dynamics. In particular, we prove that the convergence speed of training and the generalization performance are positively correlated under the ultra-wide condition and show that maximizing the training dynamics leads to better generalization performance. Furthermore, to scale up to large deep neural networks and data sets, we introduce two relaxations for the subset selection problem and reduce the time complexity from polynomial to constant. Empirical results show that dynamicAL not only outperforms the other baselines consistently but also scales well on large deep learning models. We hope our work would inspire more attempts on bridging the theoretical findings of deep networks and practical impacts of deep active learning in real applications.	Training deep learning (DL) models usually requires large amount of high-quality labeled data [1] to optimize a model with a massive number of parameters. The acquisition of such annotated data is usually time-consuming and expensive, making it unaffordable in the fields that require high domain expertise. A promising approach for minimizing the labeling effort is active learning (AL), which aims to identify and label the maximally informative samples, so that a high-performing classifier can be trained with minimal labeling effort [2]. Under classical statistical learning settings, theories of active learning have been extensively studied from the perspective of VC dimension [3]. As a result, a variety of methods have been proposed, such as (i) the version-space-based approaches, which require maintaining a set of models [4, 5], and (ii) the clustering-based approaches, which assume that the data within the same cluster have pure labels [6]. However, the theoretical analyses for these classical settings may not hold for over-parameterized deep neural networks where the traditional wisdom is ineffective [1]. For example, margin-based methods select the labeling examples in the vicinity of the learned decision boundary [ 7, 8 ]. However, in the over-parameterized regime, every labeled example could potentially be near the learned decision boundary [ 9]. As a result, theoretically, such analysis can hardly guide us to design practical active arXiv:2110.08611v2 [cs.LG] 20 Nov 2022learning methods. Besides, empirically, multiple deep active learning works, borrowing observations and insights from the classical theories and methods, have been observed unable to outperform their passive learning counterparts in a few application scenarios [10, 11]. On the other hand, the analysis of neural network's optimization and generalization performance has witnessed several exciting developments in recent years in terms of the deep learning theory [ 12– 14]. It is shown that the training dynamics of deep neural networks using gradient descent can be characterized by the Neural Tangent Kernel (NTK) of infinite [ 12] or finite [ 15] width networks. This is further leveraged to characterize the generalization of over-parameterized networks through Rademacher complexity analysis [ 13 , 16 ]. We are therefore inspired to ask: How can we design a practical and generic active learning method for deep neural networks with theoretical justifications? To answer this question, we firstly explore the connection between the model performance on testing data and the convergence speed on training data for the over-parameterized deep neural networks. Based on the NTK framework [12 , 13], we theoretically show that if a deep neural network converges faster (“Train Faster”), then it tends to have better generalization performance (“Generalize Better”), which matches the existing observations [17 –21 ]. Motivated by the aforementioned connection, we first introduce Training Dynamics, the derivative of training loss with respect to iteration, as a proxy to quantitatively describe the training process. On top of it, we formally propose our generic and theoretically-motivated deep active learning method, dynamicAL, which will query labels for a subset of unlabeled samples that maximally increase the training dynamics. In order to compute the training dynamics by merely using the unlabeled samples, we leverage two relaxations Pseudo-labeling and Subset Approximation to solve this non-trivial subset selection problem. Our relaxed approaches are capable of effectively estimating the training dynamics as well as efficiently solving the subset selection problem by reducing the complexity from O(N b) to O(b). In theory, we coin a new term Alignment to measure the length of the label vector's projection on the neural tangent kernel space. Then, we demonstrate that higher alignment usually comes with a faster convergence speed and a lower generalization bound. Furthermore, with the help of the maximum mean discrepancy [ 22], we extend the previous analysis to an active learning setting where the i.i.d. assumption may not hold. Finally, we show that alignment is positively correlated with our active learning goal, training dynamics, which implies that maximizing training dynamics will lead to better generalization performance. Regarding experiments, we have empirically verified our theory by conducting extensive experiments on three datasets, CIFAR10 [ 23 ], SVHN [24], and Caltech101 [25 ] using three types of network structures: vanilla CNN, ResNet [ 26], and VGG [ 27 ]. We first show that the result of the subset selection problem delivered by the subset approximation is close to the global optimal solution. Furthermore, under the active learning setting, our method not only outperforms other baselines but also scales well on large deep learning models. The main contributions of our paper can be summarized as follows: • We propose a theory-driven deep active learning method, dynamicAL, inspired by the observation of “train faster, generalize better”. To this end, we introduce the Training Dynamics, as a proxy to describe the training process. • We demonstrate that the convergence speed of training and the generalization performance is strongly (positively) correlated under the ultra-wide condition; we also show that maximizing the training dynamics will lead to a lower generalization error in the scenario of active learning. • Our method is easy to implement. We conduct extensive experiments to evaluate the effectiveness of dynamicAL and empirically show that our method consistently outperforms other methods in a wide range of active learning settings.	0	1
event analysis	https://arxiv.org/pdf/2408.04873	Unsupervised Episode Detection for Large-Scale News Events	Episodic structures are inherently interpretable and adaptable to evolving large-scale key events. However, state-of-the-art automatic event detection methods overlook event episodes and, therefore, struggle with these crucial characteristics. This paper introduces a novel task, episode detection, aimed at identifying episodes from a news corpus containing key event articles. An episode describes a cohesive cluster of core entities (e.g., “protesters”, “police”) performing actions at a specific time and location. Furthermore, an episode is a significant part of a larger group of episodes under a particular key event. Automatically detecting episodes is challenging because, unlike key events and atomic actions, we cannot rely on explicit mentions of times and locations to distinguish between episodes or use semantic similarity to merge inconsistent episode co-references. To address these challenges, we introduce EpiMine, an unsupervised episode detection framework that (1) automatically identifies the most salient, key-event-relevant terms and segments, (2) determines candidate episodes in an article based on natural episodic partitions estimated through shifts in discriminative term combinations, and (3) refines and forms final episode clusters using large language model-based reasoning on the candidate episodes. We construct three diverse, real-world event datasets annotated at the episode level. EpiMine outperforms all baselines on these datasets by an average 59.2% increase across all metrics.	Given the saturation of real-time news accessible at our fingertips, reading and processing the critical information of a key event has become an increasingly daunting challenge. Consequently, research on automatic textual event detection has recently attempted to integrate the manner in which humans neurologically perceive/store events into textual event detection methods. Specifically, neuroscientists studying event representations in human memory find that events are stored in a top-to-bottom hierarchy, as demonstrated in Figure 1. The deeper the hierarchical event level, the more finegrained its corresponding text granularity [ 48 ]: we consider a theme as corpus-level (all articles discussing the 2019 Hong Kong Protests), key event as document-level (an article typically discusses a full one to two day key event), episode as segment-level, and atomic action as sentence or phrase-level. Furthermore, neurological research [3 , 21 ] indicates that events are encoded into memory as episodic structures. Representing events as discrete episodes helps us piece together a coherent narrative by considering the sequence of actions, reactions, and developments over time. This empowers several downstream tasks (e.g., event schema generation, event prediction), which may benefit from insights into the causes and consequences of events that might be missed with a more generalized analysis. Despite its strengths, existing automatic event extraction works fail to consider the episode-level. For example, key event detection specifically seeks to output “a set of thematically coherent documents” for each key event [ 29, 48]. However, it is challenging to manually parse through a large cluster of relevant articles in order to gain an efficient and compact understanding of a given event. To address the lack of interpretability of such article clusters, the adjacent task of timeline summarization [9 , 14 , 23, 39 ] aims to identify the dates and a compact summary for each key event. However, high-level timelines are typically applicable for historical themes and thus unrealistic for currently evolving key events where fine-grained timeline summarization is more suitable. Hence, event chain mining [ 18] attempts to address this by mining a series of temporally-ordered atomic actions at the phraselevel; however, the granularity level is often too fine-grained to properly represent a large-scale key event. Thus, we aim to tackle the novel task of episode detection to pave the way for a more effective event representation. Episode detection aims to detect episodes from a news corpus containing key event articles. An episode can be described as a cohesive cluster of subjects performing actions at a certain time and location, occurring as part of a larger sequence of episodes under a specific key event. Episode detection introduces a unique set of challenges, which we address using our novel framework, EpiMine. EpiMine is an unsupervised episode detection framework that automatically detects meaningful episodic events and their corresponding text segments in a large key event corpus, all without any level of human supervision or labeled training data. EpiMine is comprised of the following components: (1) discriminative co-occurrence detection, (2) episode partitioning, (3) candidate episode estimation, and (4) episode-segment classification. Collectively, they tackle the unique challenges of episode detection, detailed below: Challenge 1: Journalists do not timestamp episodes. Key event detection partitions a thematic corpus into document-level clusters by heavily relying on explicit temporal features, like publication dates [48 ]. For example, an article primarily discusses one key event, which can then be roughly mapped to the article's publication date. However, this assumption fails at the episode-level, where there is no guarantee to have a distinct timestamp associated with each text segment that discusses a new episode. Fortunately, we can take advantage of the idea that journalists naturally partition news articles by sequentially discussing distinct episodes: Example: An article likely completes its discussion of the episode A, protesters storming the Legislaive Council, before episode B, “protesters vandalized the Legislative Chamber” (Figure 3). Hence, in order to partition articles into distinct episode segments, EpiMine must identify whether or not two consecutive segments are discussing the same or different episodes, which brings us to our next challenge. Challenge 2: Episodes contain semantically diverse actions. Each episode features a set of unique atomic actions, which we can utilize for determining whether or not two segments discuss the same episode. However, for clustering actions, existing methods [18 ] rely heavily on semantic similarity. This not realistic for episode-segment clustering: Example: “protesters spray-painted slogans” and “they unfurled the colonial-era flag” will fall under the same episode, but are semantically different and unlikely to be clustered. Alternatively, we can identify salient terms that reflect the same episode (“barriers” and “shoved” are unique to Episode A; “defaced” and “walls” for Episode B), by exploiting corpus-level signals. Specifically, if we frequently see “defaced” mentioned together with “walls” (or their respective synonyms) and not with other terms (and vice-versa), then we consider them a discriminative co-occurrence. Consequently, when such co-occurrences remain consistent across text segments, this indicates the same episode being discussed. Conversely, if a sufficient shift in the combination of terms occurs, then this indicates that a different episode is being discussed. Challenge 3: Articles often do not feature all episodes. Given the real-time nature of reporting, an article may feature only a subset or none of ground-truth episodes from a multi-day key event (e.g., focusing on a high-level analysis of the key event or mentioning other related key events). To minimize this noise, EpiMine seeks to identify the set of articles which maximizes the quantity and quality of potential episodes. It then merges any article partitions across these articles which likely discuss the same episode and employs a large language model (LLM) to provide a more fluent interpretation of the candidate episodes, accounting for the episode's core entity, actions, object, location, and time period. This allows EpiMine to finally map the remaining non-salient article segments to these episodes, pruning any candidates which are not sufficiently supported by the remaining articles. We summarize our core contributions: (1) We introduce the novel task of episode detection, which takes in a key-event corpus and outputs multiple detected episodes and their related segments extracted from the corpus. (2) We propose a novel unsupervised episode detection method, EpiMine, which exploits discriminative term co-occurrences to estimate candidate episode partitions from top articles. It refines these candidate episodes using LLM-based reasoning to output a comprehensive and diverse set of episodes. (3) We construct three novel datasets, reflecting a diverse set of real-world themes and thirty global key events, as no large-scale key event-specific news corpus exists for this task where the key events are guaranteed to contain distinguishable episodes. (4) We compare EpiMine with five other baselines through an extensive set of experiments and case studies performed on our real-world datasets, demonstrating that it outperforms all baselines by, on average, a 59.2% increase across all metrics. Reproducibility: We provide our dataset and source code1 to facilitate further studies.	https://arxiv.org/pdf/2104.05919	Document-Level Event Argument Extraction by Conditional Generation	Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human information seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional  following event templates. We also compile a new document-level event extraction benchmark dataset WIKIEVENTS which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WIKIEVENTS datasets respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model's trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.	By converting a large amount of unstructured text into trigger-argument structures, event extraction models provide unique value in assisting us process volumes of documents to form insights. While real-world events are often described throughout a news document (or even span multiple documents), the scope of operation for existing event extraction models have long been limited to the sentence level. Early work on event extraction originally posed the task as document level role filling (Grishman and Sundheim, 1996) on a set of narrow scenarios and evaluated on small datasets. The release of ACE2, a large scale dataset with complete event annotation, opened the possibility of applying powerful machine learning models which led to substantial improvement in event extraction. The success of such models and the widespread adoption of ACE as the training dataset established sentencelevel event extraction as the mainstream task defintion. This formulation signifies a misalignment between the information seeking behavior in real life and the exhaustive annotation process in creating the datasets. An information seeking session (Mai, 2016) can be divided into 6 stages: task initiation, topic selection, pre-focus exploration, focus information, information collection and search closure (Kuhlthau, 1991). Given a target event ontology, we can safely assume that topic selection is complete and users start from skimming the documents before they discover events of interest, focus on such events and then aggregate all relevant information for the events. In both the “pre-focus exploration” and “information collection” stages, users naturally cross sentence boundaries. Empirically, using sentence boundaries as event scopes conveniently simplifies the problem, but also introduces fundamental flaws: the resulting extractions are incomplete and uninformative. We show two examples of this phenomenon in Figure 1. The first example exemplifies the case of implicit arguments across sentences. The sentence that contains the PaymentBarter argument \"$280.32\" is not the sentence that contains the trigger \"reserve\" for the ExchangeBuySell event. Without a documentlevel model, such arguments would be missed and result in incomplete extraction. In the second example, the arguments are present in the same sentence, but written as pronouns. Such extraction would be uninformative to the reader without cross-sentence coreference resolution. We propose a new end-to-end document-level event argument extraction model by framing the problem as conditional generation given a template. Conditioned on the unfilled template and a given context, the model is asked to generate a filledin template with arguments as shown in Figure 2. Our model does not require entity recognition nor coreference resolution as a preprocessing step and can work with long contexts beyond single sentences. Since templates are usually provided as part of the event ontology definition, this requires no additional human effort. Compared to recent efforts (Du and Cardie, 2020; Feng et al., 2020; Chen et al., 2020) that retarget question answering (QA) models for event extraction, our generationbased model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning thresholds and can extract all arguments in a single pass. In order to evaluate the performance of document-level event extraction, we collect and annotate a new benchmark dataset WIKIEVENTS. This document-level evaluation also allows us to move beyond the nearest mention of the argument and instead seek the most informative mention3 in the entire document context. In particular, only 34.5% of the arguments detected in the same sentence as the trigger can be considered informative. We present this new task of document-level informative argument extraction and show that while this task requires much more cross-sentence infer- 3We prefer name mentions over nominal mentions and only use pronoun mentions when no other mentions exist. ence, our model can still perform reliably well. Since we provide the ontology information (which roles are needed for the event) through the template as an external condition, our model has excellent portability to unseen event types. By pairing up our argument extraction model with a keyword-based zero-shot trigger extraction model, we enable zero-shot transfer for new event types. The major contributions of this paper can be summarized as follows: 1. We address the document-level argument extraction task with an end-to-end neural event argument extraction model by conditional text generation. Our model does not rely on entity extraction nor entity/event coreference resolution. Compared to QA-based approaches, it can easily handle missing arguments and multiple arguments in the same role. 2. We present the first document-level event extraction benchmark dataset with complete event and coreference annotation. We also introduce the new document-level informative argument extraction task, which evaluates the ability of models to learn entity-event relations over long ranges. 3. We release the first end-to-end zero-shot event extraction framework by combining our argument extraction model with a zero-shot event trigger classification model.	1	1
event granularities	https://arxiv.org/pdf/2408.04873	Unsupervised Episode Detection for Large-Scale News Events	Episodic structures are inherently interpretable and adaptable to evolving large-scale key events. However, state-of-the-art automatic event detection methods overlook event episodes and, therefore, struggle with these crucial characteristics. This paper introduces a novel task, episode detection, aimed at identifying episodes from a news corpus containing key event articles. An episode describes a cohesive cluster of core entities (e.g., “protesters”, “police”) performing actions at a specific time and location. Furthermore, an episode is a significant part of a larger group of episodes under a particular key event. Automatically detecting episodes is challenging because, unlike key events and atomic actions, we cannot rely on explicit mentions of times and locations to distinguish between episodes or use semantic similarity to merge inconsistent episode co-references. To address these challenges, we introduce EpiMine, an unsupervised episode detection framework that (1) automatically identifies the most salient, key-event-relevant terms and segments, (2) determines candidate episodes in an article based on natural episodic partitions estimated through shifts in discriminative term combinations, and (3) refines and forms final episode clusters using large language model-based reasoning on the candidate episodes. We construct three diverse, real-world event datasets annotated at the episode level. EpiMine outperforms all baselines on these datasets by an average 59.2% increase across all metrics.	Given the saturation of real-time news accessible at our fingertips, reading and processing the critical information of a key event has become an increasingly daunting challenge. Consequently, research on automatic textual event detection has recently attempted to integrate the manner in which humans neurologically perceive/store events into textual event detection methods. Specifically, neuroscientists studying event representations in human memory find that events are stored in a top-to-bottom hierarchy, as demonstrated in Figure 1. The deeper the hierarchical event level, the more finegrained its corresponding text granularity [ 48 ]: we consider a theme as corpus-level (all articles discussing the 2019 Hong Kong Protests), key event as document-level (an article typically discusses a full one to two day key event), episode as segment-level, and atomic action as sentence or phrase-level. Furthermore, neurological research [3 , 21 ] indicates that events are encoded into memory as episodic structures. Representing events as discrete episodes helps us piece together a coherent narrative by considering the sequence of actions, reactions, and developments over time. This empowers several downstream tasks (e.g., event schema generation, event prediction), which may benefit from insights into the causes and consequences of events that might be missed with a more generalized analysis. Despite its strengths, existing automatic event extraction works fail to consider the episode-level. For example, key event detection specifically seeks to output “a set of thematically coherent documents” for each key event [ 29, 48]. However, it is challenging to manually parse through a large cluster of relevant articles in order to gain an efficient and compact understanding of a given event. To address the lack of interpretability of such article clusters, the adjacent task of timeline summarization [9 , 14 , 23, 39 ] aims to identify the dates and a compact summary for each key event. However, high-level timelines are typically applicable for historical themes and thus unrealistic for currently evolving key events where fine-grained timeline summarization is more suitable. Hence, event chain mining [ 18] attempts to address this by mining a series of temporally-ordered atomic actions at the phraselevel; however, the granularity level is often too fine-grained to properly represent a large-scale key event. Thus, we aim to tackle the novel task of episode detection to pave the way for a more effective event representation. Episode detection aims to detect episodes from a news corpus containing key event articles. An episode can be described as a cohesive cluster of subjects performing actions at a certain time and location, occurring as part of a larger sequence of episodes under a specific key event. Episode detection introduces a unique set of challenges, which we address using our novel framework, EpiMine. EpiMine is an unsupervised episode detection framework that automatically detects meaningful episodic events and their corresponding text segments in a large key event corpus, all without any level of human supervision or labeled training data. EpiMine is comprised of the following components: (1) discriminative co-occurrence detection, (2) episode partitioning, (3) candidate episode estimation, and (4) episode-segment classification. Collectively, they tackle the unique challenges of episode detection, detailed below: Challenge 1: Journalists do not timestamp episodes. Key event detection partitions a thematic corpus into document-level clusters by heavily relying on explicit temporal features, like publication dates [48 ]. For example, an article primarily discusses one key event, which can then be roughly mapped to the article's publication date. However, this assumption fails at the episode-level, where there is no guarantee to have a distinct timestamp associated with each text segment that discusses a new episode. Fortunately, we can take advantage of the idea that journalists naturally partition news articles by sequentially discussing distinct episodes: Example: An article likely completes its discussion of the episode A, protesters storming the Legislaive Council, before episode B, “protesters vandalized the Legislative Chamber” (Figure 3). Hence, in order to partition articles into distinct episode segments, EpiMine must identify whether or not two consecutive segments are discussing the same or different episodes, which brings us to our next challenge. Challenge 2: Episodes contain semantically diverse actions. Each episode features a set of unique atomic actions, which we can utilize for determining whether or not two segments discuss the same episode. However, for clustering actions, existing methods [18 ] rely heavily on semantic similarity. This not realistic for episode-segment clustering: Example: “protesters spray-painted slogans” and “they unfurled the colonial-era flag” will fall under the same episode, but are semantically different and unlikely to be clustered. Alternatively, we can identify salient terms that reflect the same episode (“barriers” and “shoved” are unique to Episode A; “defaced” and “walls” for Episode B), by exploiting corpus-level signals. Specifically, if we frequently see “defaced” mentioned together with “walls” (or their respective synonyms) and not with other terms (and vice-versa), then we consider them a discriminative co-occurrence. Consequently, when such co-occurrences remain consistent across text segments, this indicates the same episode being discussed. Conversely, if a sufficient shift in the combination of terms occurs, then this indicates that a different episode is being discussed. Challenge 3: Articles often do not feature all episodes. Given the real-time nature of reporting, an article may feature only a subset or none of ground-truth episodes from a multi-day key event (e.g., focusing on a high-level analysis of the key event or mentioning other related key events). To minimize this noise, EpiMine seeks to identify the set of articles which maximizes the quantity and quality of potential episodes. It then merges any article partitions across these articles which likely discuss the same episode and employs a large language model (LLM) to provide a more fluent interpretation of the candidate episodes, accounting for the episode's core entity, actions, object, location, and time period. This allows EpiMine to finally map the remaining non-salient article segments to these episodes, pruning any candidates which are not sufficiently supported by the remaining articles. We summarize our core contributions: (1) We introduce the novel task of episode detection, which takes in a key-event corpus and outputs multiple detected episodes and their related segments extracted from the corpus. (2) We propose a novel unsupervised episode detection method, EpiMine, which exploits discriminative term co-occurrences to estimate candidate episode partitions from top articles. It refines these candidate episodes using LLM-based reasoning to output a comprehensive and diverse set of episodes. (3) We construct three novel datasets, reflecting a diverse set of real-world themes and thirty global key events, as no large-scale key event-specific news corpus exists for this task where the key events are guaranteed to contain distinguishable episodes. (4) We compare EpiMine with five other baselines through an extensive set of experiments and case studies performed on our real-world datasets, demonstrating that it outperforms all baselines by, on average, a 59.2% increase across all metrics. Reproducibility: We provide our dataset and source code1 to facilitate further studies.	https://arxiv.org/pdf/2206.04153	Unsupervised Key Event Detection from Massive Text Corpora	Automated event detection from news corpora is a crucial task towards mining fast-evolving structured knowledge. As real-world events have different granularities, from the top-level themes to key events and then to event mentions corresponding to concrete actions, there are generally two lines of research: (1) theme detection tries to identify from a news corpus major themes (e.g., “2019 Hong Kong Protests” versus “2020 U.S. Presidential Election”) which have very distinct semantics; and (2) action extraction aims to extract from a single document mention-level actions (e.g., “the police hit the left arm of the protester”) that are often too fine-grained for comprehending the real-world event. In this paper, we propose a new task, key event detection at the intermediate level, which aims to detect from a news corpus key events (e.g., HK Airport Protest on Aug. 12-14), each happening at a particular time/location and focusing on the same topic. This task can bridge event understanding and structuring and is inherently challenging because of (1) the thematic and temporal closeness of different key events and (2) the scarcity of labeled data due to the fast-evolving nature of news articles. To address these challenges, we develop an unsupervised key event detection framework, EvMine, that (1) extracts temporally frequent peak phrases using a novel ttf-itf score, (2) merges peak phrases into event-indicative feature sets by detecting communities from our designed peak phrase graph that captures document cooccurrences, semantic similarities, and temporal closeness signals, and (3) iteratively retrieves documents related to each key event by training a classifier with automatically generated pseudo labels from the event-indicative feature sets and refining the detected key events using the retrieved documents in each iteration. Extensive experiments and case studies show EvMine all the baseline methods and its ablations on two real-world news corpora.	Automated real-world event discovery has long been studied to help people quickly digest explosive information. Researchers studying human memory find people tend to organize real-world events in a hierarchical way [7, 36 ], ranging from top-level themes (e.g., “2019 Hong Kong (HK) Protests”) to middle level key events (e.g., July 1 Storming Legislative Building), possibly to sub-middle level episodes (e.g., “Protester besieged the legislature”), and down to bottom level actions1 (e.g., “Riot police squirt pepper spray at protesters”). As shown in Figure 2, going up this event structure hierarchy leads to larger and more coarse-grained “events” whereas moving down the hierarchy brings in more fine-grained and concrete mentions of \"events\". The broad spectrum of \"events\", differing in duration and complexity, has fostered a variety of event discovery studies under different task names. One line of research, named Topic Detection and Tracking (TDT) [ 1, 3 , 37 ], aims to detect themes from an input corpus where each theme is represented by a cluster of documents, focusing on distinct thematic topics. For example, documents about “2019 Hong Kong Protests” are thematically very distinct from those of “2020 U.S. Presidential Election”. As a result, content-based document clustering methods [2, 30] can easily separate those themes. However, these methods cannot effectively distinguish the key events of the same/similar themes (e.g., identify documents about HK Legislative Building Storming and HK Airport Sit-In from a corpus related to “2019 HK protests”) [12 ]. Another line of work [ 8 , 10 , 23 , 34 ] is action extraction which tries to extract concrete actions (represented as text spans) from input documents. For example, one action extracted from the sentence in Figure 2 can be “Riot police squirt pepper spray at protesters”. These methods typically require a predefined event schema along with massive human-labeled documents for model learning. Besides, their output event mentions are highly redundant as one real-world event can usually be expressed in different ways in multiple documents, which further prevents humans from seeing the overall picture of the event. In this paper, we propose a new task, key event detection, which aims to detect key events from a corpus about one general event theme, or theme corpus. We assume each document has a publication date in the corpus, and each key event, as an aggregation of actions with concrete event time and focused location, is usually covered by a collection of documents. Since previous studies work on either too coarse or too fine-grained views of events, key event discovery, sitting in the intermediate level, plays an essential role in bridging the real-world event understanding and structuring. As shown in Figure 1, given a theme corpus about “2019 Hong Kong Protests”, we extract key events such as July 1st Storming Legislative Building and Aug. 12-14 Hong Kong International Airport Protest2, which helps people gain insights about the theme and thus compensates the previous TDT studies. Meanwhile, first detecting key events provides subsequent action extraction models with extra clues on what type of actions will most likely appear in each document. Furthermore, the identified key events can directly benefit many downstream tasks like timeline generation [ 13 ], evolutionary analysis [38], and query expansion [26]. Our proposed key event discovery task, while being useful, has its own challenges. First, compared to previous topic detection and tracking task, our task is intrinsically harder because it aims to distinguish key events of the same theme and those key events are often thematically similar and temporally closer to each other. Besides, as new events are happening every day, it is neither realistic nor scalable to curate all event schema in advance or label documents for training supervised event extraction models. To address the above challenges, we propose an unsupervised key event detection framework, EvMine, that requires no humanlabeled training data and can automatically discover key events from a large theme corpus. EvMine contains three major steps. First, we extract event-related “peak phrases” from input corpus based on 2All key events in this paper are manually named for easy understanding. our proposed “temporal term frequency–inverse time frequency” (ttf-itf) measure, where each peak phrase is unusually frequent on a day and thus likely indicates a key event. Second, as some key events can span multiple consecutive days and people have various ways to communicate the same key event, we group detected peak phrases into semantic clusters, which will serve as event-indicative features for selecting key event documents. Specifically, we propose a novel topic-time integrated peak phrase graph that considers document co-occurrence features, pre-trained masked language model (MLM) based semantic similarities, and temporal closeness, based on which we cluster peak phrases with a community detection algorithm. Each peak phrase cluster corresponds to one key event and provides event-indicative features. Finally, for each key event, we use the phrases in its corresponding peak phrase cluster to train a classifier that predicts whether a document is related to this key event. We also introduce a feedback loop that uses the current classification results to improve the phrase-based pseudo labels and find possibly missing key events, leading to an iterative document selection process with automatic refinement in each iteration. To summarize, our contributions are: (1) We introduce a new research problem key event detection, which takes a set of documents related to the same theme as inputs and outputs multiple important key events along with their associated documents. (2) We propose EvMine, a novel unsupervised framework for key event detection. EvMine automatically extracts temporally frequent peak phrases, clusters them with a graph-based method that combines thematic and temporal information, and applies document classification with iterative refinements to retrieve the most relevant documents for each key event. (3) We conduct quantitative evaluation, case studies, and parameter sensitivity analysis on two real-world event theme corpora, and EvMine outperforms all the baseline methods and its own ablations in terms of the ability to detect key events	1	1
Taxonomy completion versus taxonomy expansion in weakly supervised settings	https://arxiv.org/pdf/2201.06771	TaxoCom: Topic Taxonomy Completion with Hierarchical Discovery of Novel Topic Clusters	Topic taxonomies, which represent the latent topic (or category) structure of document collections, provide valuable knowledge of contents in many applications such as web search and information filtering. Recently, several unsupervised methods have been developed to automatically construct the topic taxonomy from a text corpus, but it is challenging to generate the desired taxonomy without any prior knowledge. In this paper, we study how to leverage the partial (or incomplete) information about the topic structure as guidance to find out the complete topic taxonomy. We propose a novel framework for topic taxonomy completion, named TaxoCom, which recursively expands the topic taxonomy by discovering novel sub-topic clusters of terms and documents. To effectively identify novel topics within a hierarchical topic structure, TaxoCom devises its embedding and clustering techniques to be closely-linked with each other: (i) locally discriminative embedding optimizes the text embedding space to be discriminative among known (i.e., given) sub-topics, and (ii) novelty adaptive clustering assigns terms into either one of the known sub-topics or novel sub-topics. Our comprehensive experiments on two real-world datasets demonstrate that TaxoCom not only generates the high-quality topic taxonomy in terms of term coherency and topic coverage but also outperforms all other baselines for a downstream task.	Finding the latent topic structure of an input text corpus, also known as hierarchical topic discovery [ 7, 18 , 33 , 42 , 47 ], has been one of the most important problems for information extraction and semantic analysis of text data. Recently, several studies have focused on topic taxonomy construction [ 33, 47], which aims to generate a tree-structured taxonomy whose node corresponds to a conceptual topic; each node of the topic taxonomy is defined as a cluster of semantically coherent terms representing a single topic. Compared to a conventional entity (or term-level) taxonomy, this cluster-level taxonomy is more appropriate for representing the topic hierarchy of the target corpus with high coverage and low redundancy. To identify hierarchical topic clusters of terms, they mainly performed clustering on a low-dimensional text embedding space where textual semantic information is effectively encoded. However, their output topic taxonomy seems plausible by itself but often fails to match with the complete taxonomy designed by a human curator, because they rely on only the text corpus in an unsupervised manner. To be specific, their quality (e.g., coverage and accuracy) highly depends on the number of sub-topic clusters (i.e., child nodes), which has to be manually controlled by a user. In addition, it is sensitive to the topic imbalance in the document collection, which makes it difficult to find out minor topics. In the absence of any information about the topic hierarchy, the unsupervised methods intrinsically become vulnerable to these problems. On the other hand, for some other text mining tasks or NLP applications, several recent studies have tried to take advantage of auxiliary information about the latent topic structure [ 13, 21, 23 – 26 , 34 ]. Most of them focus on utilizing a hierarchy of topic surface names as additional supervision, because it can be easily given as a user's interests or prior knowledge. Specifically, they retrieve the top-𝐾 relevant terms to each topic [ 21 , 26 ] or train a hierarchical text classifier using unlabeled documents and the topic names [ 24 , 34 ]. Despite their effectiveness, their major limitation is that they are only able to consider the known topics included in the given topic hierarchy. That is, the coverage of the obtained results is strictly limited to the given topics. Since it is very challenging for a user to be aware of a full topic structure, a naive solution to incorporate a user-provided hierarchy of topic names into the topic taxonomy is likely to only partially cover the text corpus. To tackle this limitation, we introduce a new problem setting, named topic taxonomy completion, to construct a complete topic taxonomy by making use of additional topic information assumed to be partial or incomplete. Formally, given a text corpus and its partial hierarchy of topic names, this task aims to identify the term clusters for each topic, while discovering the novel topics that do not exist in the given hierarchy but exist in the corpus. Figure 1 illustrates a toy example of our task, where the novel topics (e.g, arts and hockey) are correctly detected and placed in the right position within the taxonomy. This task can be practically applied not only for the case that a user's incomplete knowledge is available, but also for incremental management of the topic taxonomy. In case that the document collection is constantly growing, and so are their topics, the out-dated topic taxonomy of the previous snapshot can serve as the partial hierarchy to capture emerging topics. The technical challenges of this task can be summarized as follows. First, novel topics should be identified by considering the hierarchical semantic relationship among the topics. In Figure 1, the topic hockey is not novel in terms of the root node, because it obviously belongs to its known sub-topic sports. However, hockey should be detected as a novel sub-topic of sports as it does not belong to any of the known sport sub-categories (i.e., soccer and baseball). Second, the granularity of novel sub-topics and that of known sub-topics need to be kept similar with each other, to achieve the consistency of semantic specificity among sibling nodes. In Figure 1, the root node should insert a single novel sub-topic arts, rather than two novel sub-topics music and dance, based on the semantic specificity of its known sub-topics (i.e., politics and sports). In this work, we propose TaxoCom, a hierarchical topic discovery framework to complete the topic taxonomy by recursively identifying novel sub-topic clusters of terms. For each topic node, TaxoCom performs (i) text embedding and (ii) text clustering, to assign the terms into one of either the existing child nodes (i.e., known sub-topics) or newly-created child nodes (i.e., novel subtopics). It first optimizes locally discriminative embedding which enforces the discrimination among the known sub-topics [ 21 , 26 ] by using the given topic surface names; this helps to make a clear distinction between known and novel sub-topic clusters as well. Then, it performs novelty adaptive clustering which separately finds the clusters on novel-topic terms and known-topic terms, respectively. In particular, TaxoCom selectively assigns the terms into the child nodes, referred to as anchor terms, while filtering out general terms based on their semantic relevance and representativeness. Extensive experiments on real-world datasets demonstrate that TaxoCom successfully completes a topic taxonomy with missing (i.e., novel) topic nodes correctly inserted. Our human evaluation quantitatively validates the superiority of topic taxonomies generated by TaxoCom, in terms of the topic coverage as well as semantic coherence among the topic terms. Furthermore, TaxoCom achieves the best performance among all baseline methods for a downstream task, which trains a weakly supervised text classifier by using the topic taxonomy instead of document-level labels.	https://arxiv.org/pdf/2001.09522	TaxoExpan: Self-supervised Taxonomy Expansion with Position-Enhanced Graph Neural Network	Taxonomies consist of machine-interpretable semantics and provide valuable knowledge for many web applications. For example, online retailers (e.g., Amazon and eBay) use taxonomies for product recommendation, and web search engines (e.g., Google and Bing) leverage taxonomies to enhance query understanding. Enormous efforts have been made on constructing taxonomies either manually or semi-automatically. However, with the fast-growing volume of web content, existing taxonomies will become outdated and fail to capture emerging knowledge. Therefore, in many applications, dynamic expansions of an existing taxonomy are in great demand. In this paper, we study how to expand an existing taxonomy by adding a set of new concepts. We propose a novel self-supervised framework, named TaxoExpan, which automatically generates a set of ⟨query concept, anchor concept⟩ pairs from the existing taxonomy as training data. Using such self-supervision data, TaxoExpan learns a model to predict whether a query concept is the direct hyponym of an anchor concept. We develop two innovative techniques in TaxoExpan: (1) a position-enhanced graph neural network that encodes the local structure of an anchor concept in the existing taxonomy, and (2) a noise-robust training objective that enables the learned model to be insensitive to the label noise in the self-supervision data. Extensive experiments on three large-scale datasets from different domains demonstrate both the effectiveness and the efficiency of TaxoExpan for taxonomy expansion.	Taxonomies have been fundamental to organizing knowledge for centuries [ 45 ]. In today's Web, taxonomies provide valuable knowledge to support many applications such as query understanding [ 17 ], content browsing [54], personalized recommendation [ 18, 63 ], and web search [ 29 , 53 ]. For example, many online retailers (e.g., eBay and Amazon) organize products into categories of different granularities, so that customers can easily search and navigate this category taxonomy to find the items they want to purchase. In addition, web search engines (e.g., Google and Bing) leverage a taxonomy to better understand user queries and improve the search quality. Existing taxonomies are mostly constructed by human experts or in a crowdsourcing manner. Such manual curations are timeconsuming, labor-intensive, and rarely complete. To reduce the human efforts, many automatic taxonomy construction methods [ 31, 41 , 60 ] are proposed. They first identify “is-A” relations (e.g., “iPad” is an “Electronics”) using textual patterns [16, 38] or distributional similarities [ 3, 43 ], and then organize extracted concept pairs into a directed acyclic graph (DAG) as the output taxonomy [10, 14 , 24]. As the web contents and human knowledge are constantly growing, people need to expand an existing taxonomy to include new emerging concepts. Most of previous methods, however, construct a taxonomy entirely from scratch and thus when we add new concepts, we have to re-run the entire taxonomy construction process. Although being intuitive, this approach has several limitations. First, many taxonomies have a top-level design provided by domain experts and such design shall be preserved. Second, a newly constructed taxonomy may not be consistent with the old one, which can lead to instabilities of its dependent downstream applications. Finally, as targeting the scenario of building taxonomy from scratch, most previous methods are unsupervised and cannot leverage signals from the existing taxonomy to construct a new one. In this paper, we study the taxonomy expansion task: given an existing taxonomy and a set of new emerging concepts, we aim to automatically expand the taxonomy to incorporate these new concepts (without changing the existing relations in the given taxonomy).1 Figure 1 shows an example where a taxonomy in computer science domain is expanded to include new subfields (e.g., “Quantum Computing”) and new techniques (e.g., “Meta Learning” and “UDA”). Some previous studies [21 , 22 , 39 ] attempt this task by using an additional set of labeled concepts with their true insertion positions in the existing taxonomy. However, such labeled data are usually small and thus forbid us from learning a more powerful model that captures the subsumption semantics in the existing taxonomy. We propose a novel framework named TaxoExpan to tackle the lack-of-supervision challenge. TaxoExpan formulates a taxonomy as a directed acyclic graph (DAG), automatically generates pseudotraining data from the existing taxonomy, and uses them to learn a matching model for expanding a given taxonomy. Specifically, we view each concept in the existing taxonomy as a query and one of its parent concepts as an anchor. This gives us a set of positive ⟨query concept, anchor concept⟩ pairs. Then, we generate negative pairs by sampling those concepts that are neither the descendants nor the direct parents of the query concept in the existing taxonomy. In Figure 1, for example, the ⟨“GPU ”, “Integrated Circuit”⟩ is a positive pair and ⟨“GPU ”, “Label Propagation”⟩ is a negative pair. We refer to these training pairs as self-supervision data, because they are procedurally generated from the existing taxonomy and no human curation is involved. To make the best use of above self-supervision data, we develop two novel techniques in TaxoExpan. The first one is a positionenhanced graph neural network (GNN) which encodes the local structure of an anchor concept using its ego network (egonet) in the existing taxonomy. If we view this anchor concept as the “parent” of the query concept, this ego network includes the potential “siblings” and “grand parents” of the query concept. We apply graph neural networks (GNNs) to model this ego network. However, regular GNNs fail to distinguish nodes with different relative positions to the query (i.e., some nodes are grand parents of the query while the others are siblings of the query). To address this limitation, we present a simple but effective enhancement to inject such position information into GNNs using position embedding. We show that such embedding can be easily integrated with existing GNN architectures (e.g., GCN [ 23 ] and GAT [ 50 ]) and significantly boosts the prediction performance. The second technique is a new noise-robust training scheme based on the InfoNCE loss [47 ]. Instead of predicting whether each individual ⟨query concept, anchor concept⟩ pair is positive or not, we first group all pairs sharing the same query concept into a single training instance and learn a model to select the positive pair among other negative ones from the group. We show that such training scheme is robust to the label noise and leads to performance gains. We test the effectiveness of TaxoExpan framework on three realworld taxonomies from different domains. Our results show that TaxoExpan can generate high-quality concept taxonomies in scientific domains and achieves state-of-the-art performance on the WordNet taxonomy expansion challenge [22]. Contributions. To summarize, our major contributions include: (1) a self-supervised framework that automatically expands existing taxonomies without manually labeled data; (2) an effective method for enhancing graph neural network by incorporating hierarchical positional information; (3) a new training objective that enables the learned model to be robust to label noises in self-supervision data; and (4) extensive experiments that verify both the effectiveness and the efficiency of TaxoExpan framework on three real-world large-scale taxonomies from different domains. The rest of the paper is organized as follows. Section 2 discusses the related work. Section 3 formalizes our problem. Then, we present our TaxoExpan framework in Section 4 and conduct experiments in Section 5. Finally, we conclude this paper in Section 6.	1	1
Continual pretraining of Bert for retrieval tasks	https://arxiv.org/pdf/2004.12832	ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT	Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.	Over the past few years, the Information Retrieval (IR) community has witnessed the introduction of a host of neural ranking models, including DRMM [7 ], KNRM [4, 36], and Duet [20 , 22 ]. In contrast to prior learning-to-rank methods that rely on hand-craed features, these models employ embedding-based representations of queries and documents and directly model local interactions (i.e., fine-granular relationships) between their contents. Among them, a recent approach has emerged that fine-tunes deep pre-trained language models (LMs) like ELMo [ 29 ] and BERT [5] for estimating relevance. By computing deeply-contextualized semantic representations of query–document pairs, these LMs help bridge the pervasive vocabulary mismatch [21, 42 ] between documents and queries [ 30 ]. Indeed, in the span of just a few months, a number of ranking models based on BERT have achieved state-of-the-art results on various retrieval benchmarks [ 3, 18 , 25 , 39 ] and have been proprietarily adapted for deployment by Google1 and Bing2. However, the remarkable gains delivered by these LMs come at a steep increase in computational cost. Hofst  ̈aer et al. [ 9] and MacAvaney et al. [ 18 ] observe that BERT-based models in the literature are 100-1000× more computationally expensive than prior models—some of which are arguably not inexpensive to begin with [ 13 ]. is quality–cost tradeoff is summarized by Figure 1, which compares two BERT-based rankers [25 , 27 ] against a representative set of ranking models. e figure uses MS MARCO Ranking [ 24 ], a recent collection of 9M passages and 1M queries from Bing's logs. It reports retrieval effectiveness (MRR@10) on the official validation set as well as average query latency (log-scale) using a high-end server that dedicates one Tesla V100 GPU per query for neural re-rankers. Following the re-ranking setup of MS MARCO, ColBERT (re-rank), the Neural Matching Models, and the Deep LMs re-rank the MS MARCO's official top-1000 documents per query. Other methods, including ColBERT (full retrieval), directly retrieve the top-1000 results from the entire collection. As the figure shows, BERT considerably improves search precision, raising MRR@10 by almost 7% against the best previous methods; simultaneously, it increases latency by up to tens of thousands of milliseconds even with a high-end GPU. is poses a challenging tradeoff since raising query response times by as lile as 100ms is known to impact user experience and even measurably diminish revenue [ 17 ]. To tackle this problem, recent work has started exploring using Natural Language Understanding (NLU) techniques to augment traditional retrieval models like BM25 [32 ]. For example, Nogueira et al. [ 26, 28] expand documents with NLU-generated queries before indexing with BM25 scores and Dai & Callan [2] replace BM25's term frequency with NLU-estimated term importance. Despite successfully reducing latency, these approaches generally reduce precision substantially relative to BERT. To reconcile efficiency and contextualization in IR, we propose ColBERT, a ranking model based on contextualized late interaction over BERT. As the name suggests, ColBERT proposes a novel late interaction paradigm for estimating relevance between a query q and a document d. Under late interaction, q and d are separately encoded into two sets of contextual embeddings, and relevance is evaluated using cheap and pruning-friendly computations between both sets—that is, fast computations that enable ranking without exhaustively evaluating every possible candidate. Figure 2 contrasts our proposed late interaction approach with existing neural matching paradigms. On the le, Figure 2 (a) illustrates representation-focused rankers, which independently compute an embedding for q and another for d and estimate relevance as a single similarity score between two vectors [ 12 , 41 ]. Moving to the right, Figure 2 (b) visualizes typical interaction-focused rankers. Instead of summarizing q and d into individual embeddings, these rankers model word- and phrase-level relationships across q and d and match them using a deep neural network (e.g., with CNNs/MLPs [ 22 ] or kernels [ 36 ]). In the simplest case, they feed the neural network an interaction matrix that reflects the similiarity between every pair of words across q and d. Further right, Figure 2 (c) illustrates a more powerful interaction-based paradigm, which models the interactions between words within as well as across q and d at the same time, as in BERT's transformer architecture [25]. ese increasingly expressive architectures are in tension. While interaction-based models (i.e., Figure 2 (b) and (c)) tend to be superior for IR tasks [8 , 21 ], a representation-focused model—by isolating the computations among q and d—makes it possible to precompute document representations offline [ 41 ], greatly reducing the computational load per query. In this work, we observe that the fine-grained matching of interaction-based models and the precomputation of document representations of representation-based models can be combined by retaining yet judiciously delaying the query–document interaction. Figure 2 (d) illustrates an architecture that precisely does so. As illustrated, every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms. is paradigm allows ColBERT to exploit deep LM-based representations while shiing the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., [1, 15]) to retrieve the top-k results directly from a large document collection, substantially improving recall over models that only re-rank the output of term-based retrieval. As Figure 1 illustrates, ColBERT can serve queries in tens or few hundreds of milliseconds. For instance, when used for reranking as in “ColBERT (re-rank)”, it delivers over 170× speedup (and requires 14,000× fewer FLOPs) relative to existing BERT-based models, while being more effective than every non-BERT baseline (§4.2 & 4.3). ColBERT's indexing—the only time it needs to feed documents through BERT—is also practical: it can index the MS MARCO collection of 9M passages in about 3 hours using a single server with four GPUs (§4.5), retaining its effectiveness with a space footprint of as lile as few tens of GiBs. Our extensive ablation study (§4.4) shows that late interaction, its implementation via MaxSim operations, and crucial design choices within our BERTbased encoders are all essential to ColBERT's effectiveness. Our main contributions are as follows. (1) We propose late interaction (§3.1) as a paradigm for efficient and effective neural ranking. (2) We present ColBERT (§3.2 & 3.3), a highly-effective model that employs novel BERT-based query and document encoders within the late interaction paradigm.(3) We show how to leverage ColBERT both for re-ranking on top of a term-based retrieval model (§3.5) and for searching a full collection using vector similarity indexes (§3.6). (4) We evaluate ColBERT on MS MARCO and TREC CAR, two recent passage search collections.	https://arxiv.org/pdf/1810.04805	BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding	We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).	Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016). There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017). Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a “masked language model” (MLM) pre-training objective, inspired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a “next sentence prediction” task that jointly pretrains text-pair representations. The contributions of our paper are as follows: • We demonstrate the importance of bidirectional pre-training for language representations. Unlike Radford et al. (2018), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. • We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. • BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at https://github.com/ google-research/bert.	1	1
contrastive learning on graphs	https://arxiv.org/pdf/2103.00113	Anomaly Detection on Attributed Networks via Contrastive Self-Supervised Learning	Anomaly detection on attributed networks attracts considerable research interests due to wide applications of attributed networks in modeling a wide range of complex systems. Recently, the deep learning-based anomaly detection methods have shown promising results over shallow approaches, especially on networks with high-dimensional attributes and complex structures. However, existing approaches, which employ graph autoencoder as their backbone, do not fully exploit the rich information of the network, resulting in suboptimal performance. Furthermore, these methods do not directly target anomaly detection in their learning objective and fail to scale to large networks due to the full graph training mechanism. To overcome these limitations, in this paper, we present a novel contrastive self-supervised learning framework for anomaly detection on attributed networks. Our framework fully exploits the local information from network data by sampling a novel type of contrastive instance pair, which can capture the relationship between each node and its neighboring substructure in an unsupervised way. Meanwhile, a well-designed graph neural network-based contrastive learning model is proposed to learn informative embedding from high-dimensional attributes and local structure and measure the agreement of each instance pairs with its outputted scores. The multi-round predicted scores by the contrastive learning model are further used to evaluate the abnormality of each node with statistical estimation. In this way, the learning model is trained by a specific anomaly detection-aware target. Furthermore, since the input of the graph neural network module is batches of instance pairs instead of the full network, our framework can adapt to large networks flexibly. Experimental results show that our proposed framework outperforms the state-of-the-art baseline methods on all seven benchmark datasets.	Attributed networks (a.k.a. attributed graphs), where nodes with attributes indicate real-world entities and links indicate the relationship between entities, are ubiquitous in various scenarios, including finance (trading networks) [1], social media (social networks) [2], [3], and e-commerce (itemuser networks) [4], [5]. To utilize attributed network data to solve practical problems, a wide variety of graph analysis tasks have attracted significant research interests in recent years, such as node classification [6], [7], graph classification [8], [9], and link prediction [10], [11]. Among these tasks, anomaly detection task on attributed networks is a vital research problem. Aiming to detect the instances that significantly deviate from the majority of instances [12] (in attributed networks, the data instances are nodes generally), anomaly detection has significant implications in many security-related applications, e.g., fraud detection and social spam detection [13]. However, detecting anomalies effectively on attributed networks is not trivial due to the diversity of anomalies and the lack of supervision. Since attributed networks have both attribute information as well as structural information, they usually contain different types of anomalies. Figure 1 provides an example to illustrate two basic types of anomalies: structural anomaly and contextual anomaly. The attribute information of the structural anomalies is often normal, while they have several abnormal links to other nodes. The contextual anomalies, differently, have natural neighboring structures but their attributes are corrupted (noisy or entirely different from all neighbors). Such diversity makes it difficult to apply anomaly detection methods for attribute-only data (e.g., OC-SVM [14]) or plain networks (e.g., LOF [15]) to attributed networks directly. Therefore, an efficient anomaly detection approach should consider multiple patterns of anomalies. Moreover, resulting from the prohibitive cost for accessing ground-truth labels of anomalies, anomaly detection on attributed networks is predominately carried out in an unsupervised manner [13], [16]. That is to say, the algorithm has to conclude the normal pattern of data from the corrupted networks without supervision. Hence, a key is to fully and reasonably exploit existing information from attributed network data. Recently, various methods have been proposed to deal with the anomaly detection task for attributed networks. The shallow methods, including AMEN [16], Radar [17] and ANOMALOUS [18], leverage shallow learning mechanisms (e.g. ego-network analysis, residual analysis or CUR decomposition) to detect anomalies. Unfortunately, these models cannot fully address the computational challenge on attributed networks and fail to capture the complex interactions between different information modalities due to limitations of shallow mechanisms, especially when the feature is high-dimensional [13]. With the rocketing growth of deep learning for anomaly detection [12], [19], [20], [21], researchers also present deep neural networks-based methods to solve the anomaly detection problem on attributed networks. DOMINANT [13] is one of the representative methods. It constructs a graph autoencoder to reconstruct the attribute and structure information simultaneously, and the abnormality is evaluated by reconstruction error. SpecAE [22] also leverages graph autoencoder to extract low-dimensional embedding, and carries out detection via density estimation. Although existing deep learning-based methods [13], [22] have achieved considerable performance for anomaly detection on graphs, they still have several shortcomings, largely attributed to the autoencoder backbone in their architectures. First, autoencoders aim to learn the latent representation by reconstructing the original data instead of detecting the anomaly itself. Although the anomaly scores can be computed according to reconstruction errors [13], this kind of methods can only achieve suboptimal performance due to the fact that they do not target directly the anomaly detection objective. Second, autoencoder-based methods may not able to fully exploit the rich information of the attributed graph for effective graph representation learning. Specifically, autoencoders simply rebuild the original data and they do not have any refinement for data. However, recent works [23], [24], [25] have shown that more useful information can be mined in an unsupervised way if we design certain pretext tasks carefully based on augmented data. Third, graph autoencoder is the bottleneck to carry out anomaly detection on largescale networks. Generally, the graph convolution operation in graph autoencoder needs to input and reconstruct the full networked data, which is unfeasible due to the explosive memory requirements when the network is large. As an alternative unsupervised learning technique, selfsupervised contrastive learning is a promising solution to address the aforementioned limitations. By learning to contrast the elaborate instance pairs, the model can acquire informative knowledge without manual labels. Contrastive self-supervised learning has nice properties for anomaly detection task. First, contrastive learning mainly studies the matching of pairs of instances, which offers helpful information for anomaly detection. For the normal instance in graphs, there is a potential matching pattern between each node and its neighbors, e.g., the homophily hypothesis. The anomalies, on the opposite, often present when there is an inconsistency/mismatch between attributes and structure, which violates the original matching pattern of networks. Moreover, different types of anomalies have different manners of mismatching: in Figure 1, the structural anomaly has individual abnormal links with uncorrelated nodes, which is partial inconsistency; the contextual anomaly, differently, has mismatched attributes with all neighbors. Contrastive learning, naturally, is capable to learn the matching patterns and capture various mismatching patterns via its intrinsic discriminative mechanism. Second, contrastive learning models provide a specific predicted score to measure the agreement between the elements in each instance pair, and the scale is highly related to the abnormality of instance. Since anomaly detection methods usually output a list of scores or a ranking to represent the abnormality of each node, the predicted scores of contrastive learning model can be utilized for anomaly detection directly. In this way, we can train the model via an objective that is highly relevant to anomaly detection. In this paper, we propose a novel Contrastive self-supervised Learning framework for Anomaly detection on attributed networks (CoLA for abbreviation). By sampling the welldesigned instance pairs from the full network and using them to train the contrastive learning model, the information of network is exploited better. Concretely, our framework focuses on modeling the relationship between each node and its partial neighboring substructure, which can expose the various type of anomalies within networks. Meanwhile, our CoLA framework is trained with a direct target to assist the anomaly detection task. We set the learning objective of our model to discriminate the agreement between the elements within the instance pairs, and the results can be further used to evaluate the abnormality of nodes. Besides, by splitting the network into separated lightweight instance pairs, our anomaly detection framework is compatible with large-scale networks. Specifically, our framework does not need to run graph convolution on full networks, so it successfully avoids the memory explosion problem. To summarize, the main contributions are as follows: We propose a contrastive self-supervised learning framework, CoLA, for the anomaly detection problem on attributed networks. To the best of our knowledge, this is the first contrastive self-supervised learning-based method for graph anomaly detection.IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 3 We present a novel type of contrastive instance pair, “target node v.s. local subgraph”, for attributed networks to adapt to the anomaly detection task, which efficiently captures the local information of a node and its neighboring substructure. We design a contrastive learning model to learn the representative information from the node-subgraph instance pairs and provide discriminative scores for abnormality ranking. The proposed learning model is friendly to largescale networked data. We conduct extensive experiments on various datasets to demonstrate the effectiveness of CoLA and its superiority compared with a range of baseline methods. The rest of this paper is organized as follows. In Section II, we first review the related works. Then, the preliminary definitions and notations are introduced in Section III. Section IV illustrates the overall pipeline and the components of our framework in detail. After that, we analyze the experimental results in Section V and then conclude our work in section VI.	https://arxiv.org/pdf/2310.14525	Graph Ranking Contrastive Learning: A Extremely Simple yet Efficient Method	Graph contrastive learning (GCL) has emerged as a representative graph self-supervised method, achieving significant success. The currently prevalent optimization objective for GCL is InfoNCE. Typically, it employs augmentation techniques to obtain two views, where a node in one view acts as the anchor, the corresponding node in the other view serves as the positive sample, and all other nodes are regarded as negative samples. The goal is to minimize the distance between the anchor node and positive samples and maximize the distance to negative samples. However, due to the lack of label information during training, InfoNCE inevitably treats samples from the same class as negative samples, leading to the issue of false negative samples. This can impair the learned node representations and subsequently hinder performance in downstream tasks. While numerous methods have been proposed to mitigate the impact of false negatives, they still face various challenges. For instance, while increasing the number of negative samples can dilute the impact of false negatives, it concurrently increases computational burden. Thus, we propose GraphRank, a simple yet efficient graph contrastive learning method that addresses the problem of false negative samples by redefining the concept of negative samples to a certain extent, thereby avoiding the issue of false negative samples. The effectiveness of GraphRank is empirically validated through experiments on the node, edge, and graph level tasks.	Graph Neural Networks (GNNs) have become the standard approach for handling graph data, given their ability to leverage the underlying structure and features of graphs for effective analysis. Albeit the immense success achieved by supervised or semisupervised GNNs [ 16 , 31 ] across numerous application domains, their effectiveness is tied to the availability of labeled data for learning robust and impactful node representations. However, obtaining labeled data in real-world scenarios is a costly and timeconsuming endeavor, often constraining the availability of such data in many applications. Consequently, to mitigate this reliance on label data, graph self-supervised learning is attracting increasing attention, with graph contrastive learning emerging as the predominant method. Graph Contrastive Learning (GCL) typically starts with generating several views of a given graph through augmentation techniques. From these different views, one view serves as the anchor, with corresponding nodes in other views as positive examples and all other nodes as negative samples. The goal of GCL is then to bring the positive samples closer to the anchor in the representation space while pushing the negative samples further apart. Among various GCL approaches, InfoNCE [ 25 , 45, 46 ] has been recognized as the most commonly used optimization goal. It upholds the principle of minimizing distances between positive pairs and maximizing those between negative pairs, leveraging the contrastive nature of learning based on their representations. GRACE [ 45] relies on hybrid feature augmentations including node feature masking and edge dropping. Based on this data augmentation strategy, GCA [46 ] further introduces an adaptive augmentation for graph-structured data and make a competitive performance. GraphCL [ 39 ] further extends to graph-level representation to pull two views closer. However, InfoNCE experiences the issue of false negative samples [5, 23 ]. Its optimization objective is to minimize the distance with the positive samples and maximize the distance with the negative samples. Given that InfoNCE treats all nodes except the anchor node as negative samples, it unavoidably treats nodes of the same class as the anchor node as negative samples, which are referred to as false negative samples. Such false negative samples can impair the learned node representation and hinder downstream tasks. As shown in Figure 1, GRACE, a representative graph contrastive learning method using InfoNCE loss, is used to validate the issue of false negative samples across three academic citation network datasets. During the training of the GRACE method, we artificially removed the false negatives, which led to substantial performance improvements across all three datasets. However, in practical scenarios, there is a lack of data label information at the time of training, preventing manual removal of false negative samples based on label information. Therefore, in order to alleviate the issue of false negative samples and improve the performance of graph contrastive learning methods, many works have been explored, which are mainly in three ways. Firstly, increasing the number of negative samples helps dilute the impact of false negative samples. In cases where the quantities of various types of nodes are relatively balanced, the proportion of nodes in the same class is less, and increasing the number of negative samples can alleviate the issue of false negative samples. However, an increase in the number of negative samples bears computational and storage burdens, and as shown in Figure 1, GRACE, even when using all available negative samples, still experiences a notable false negative sample issue. Secondly, some works have proposed mechanisms for screening negative samples to attempt to remove false negative samples, thus improving the quality of negative samples. AUGCL [4] establishes a discriminative model based on collective affinity information to assess the uncertainty of negative samples, thereby facilitating the filtration of negative samples. Additionally, [ 23 ] designes a mechanism based on node similarity to sample high-quality positive and negative samples. Although a negative sample screening mechanism can effectively reduce the sampling of false negative samples, it necessitates the design of a complex and intricate screening mechanism to ensure the selection of high-quality negative samples. This, in turn, would introduce additional computational overhead. Lastly, some works have decided to forego the use of negative samples by employing contrastive learning methods that do not use negative samples, thereby avoiding the issue of false negative samples altogether. BGRL [ 30 ] is a contrastive learning method that does not require negative samples. It obtains two views through augmentation techniques; one view is used for learning the online representation, and the other view is used for learning the target representation. Updates are conducted by maximizing the similarity between these two views. However, its success relies on a relatively complex training strategy, specifically requiring a dual-encoder scheme with momentum update and exponential moving average to stabilize the training process. In light of the shortcomings of these graph contrastive methods above, we propose a new framework for graph self-supervised learning called GraphRank. The GraphRank framework involves generating two augmented graph views by applying random masks to nodes and edges. Subsequently,we utilize a GNN as the encoder and employ rank loss as the objective function for training. Specifically, we select a node 𝑣𝑖 as the target node in view 1, the node 𝑣+ 𝑖 corresponding to it in view 2 as a positive sample, and then randomly pick a node 𝑣 𝑗 from view 2 as a negative sample. The representations of these nodes are derived by the encoder, and then the similarity between the target node and the positive and negative samples are calculated accordingly. By employing rank loss as the objective function, our aim is to ensure that the similarity between the target node and the positive samples is greater than the similarity between the target node and the negative samples. GraphRank can effectively address the problems mentioned above. Firstly, a simple random mask approach is applied to GraphRank to obtain augmented graph data, which does not require sophisticatedly designed graph augmentation techniques to obtain highquality augmented graph data, nor does it require a complicated training strategy to stabilize the training. Secondly, we use rank loss as the objective function. Similar to the contrastive loss, e.g. InfoNCE, rank loss also endeavors to maximize the agreement between the target node and the positive sample. Different from contrastive loss, the purpose of rank loss is to make the similarity between the target node and the positive samples greater than the similarity between the target node and the negative samples, rather than separating the target node from the negative samples as much as possible, as in InfoNCE. Therefore, the rank loss would not separate the negative samples as far apart as possible, even if the negative samples selected were false negative samples. Finally, the calculation of rank loss involves only one positive and one negative sample resulting in a smaller computational overload compared to contrastive losses like InfoNCE. As a result, rank loss exhibits better scalability, making it more feasible for large-scale applications compared to typical contrastive losses.	1	0
multi-model embodied agents + environments	https://arxiv.org/pdf/2010.03768	ALFWorld: Aligning Text and Embodied Environments for Interactive Learning	Given a simple request like Put a washed apple in the kitchen fridge, humans can reason in purely abstract terms by imagining action sequences and scoring their likelihood of success, prototypicality, and efficiency, all without moving a muscle. Once we see the kitchen in question, we can update our abstract plans to fit the scene. Embodied agents require the same abilities, but existing work does not yet provide the infrastructure necessary for both reasoning abstractly and executing concretely. We address this limitation by introducing ALFWorld, a simulator that enables agents to learn abstract, text based policies in TextWorld (Côté et al., 2018) and then execute goals from the ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment. ALFWorld enables the creation of a new BUTLER agent whose abstract knowledge, learned in TextWorld, corresponds directly to concrete, visually grounded actions. In turn, as we demonstrate empirically, this fosters better agent generalization than training only in the visually grounded environment. BUTLER's simple, modular design factors the problem to allow researchers to focus on models for improving every piece of the pipeline (language understanding, planning, navigation, and visual scene understanding).	Consider helping a friend prepare dinner in an unfamiliar house: when your friend asks you to clean and slice an apple for an appetizer, how would you approach the task? Intuitively, one could reason abstractly: (1) find an apple (2) wash the apple in the sink (3) put the clean apple on the cutting board (4) find a knife (5) use the knife to slice the apple (6) put the slices in a bowl. Even in an unfamiliar setting, abstract reasoning can help accomplish the goal by leveraging semantic priors. Priors like locations of objects – apples are commonly found in the kitchen along with implements for cleaning and slicing, object affordances – a sink is useful for washing an apple unlike a refrigerator, pre-conditions – better to wash an apple before slicing it, rather than the converse. We hypothesize that, learning to solve tasks using abstract language, unconstrained by the particulars of the physical world, enables agents to complete embodied tasks in novel environments by leveraging the kinds of semantic priors that are exposed by abstraction and interaction. To test this hypothesis, we have created the novel ALFWorld framework, the first interactive, parallel environment that aligns text descriptions and commands with physically embodied robotic simulation. We build ALFWorld by extending two prior works: TextWorld (Côté et al., 2018) - an engine for interactive text-based games, and ALFRED (Shridhar et al., 2020) - a large scale dataset for visionlanguage instruction following in embodied environments. ALFWorld provides two views of the same underlying world and two modes by which to interact with it: TextWorld, an abstract, text-based environment, generates textual observations of the world and responds to high-level text actions; ALFRED, the embodied simulator, renders the world in high-dimensional images and responds to low-level physical actions as from a robot (Figure 1).1 Unlike prior work on instruction following (MacMahon et al., 2006; Anderson et al., 2018a), which typically uses a static corpus of cross-modal expert demonstrations, we argue that aligned parallel environments like ALFWorld offer a distinct advantage: they allow agents to explore, interact, and learn in the abstract environment of language before encountering the complexities of the embodied environment. While fields such as robotic control use simulators like MuJoCo (Todorov et al., 2012) to provide infinite data through interaction, there has been no analogous mechanism – short of hiring a human around the clock – for providing linguistic feedback and annotations to an embodied agent. TextWorld addresses this discrepancy by providing programmatic and aligned linguistic signals during agent exploration. This facilitates the first work, to our knowledge, in which an embodied agent learns the meaning of complex multi-step policies, expressed in language, directly through interaction. Empowered by the ALFWorld framework, we introduce BUTLER (Building Understanding in Textworld via Language for Embodied Reasoning), an agent that first learns to perform abstract tasks in TextWorld using Imitation Learning (IL) and then transfers the learned policies to embodied tasks in ALFRED. When operating in the embodied world, BUTLER leverages the abstract understanding gained from TextWorld to generate text-based actions; these serve as high-level subgoals that facilitate physical action generation by a low-level controller. Broadly, we find that BUTLER is capable of generalizing in a zero-shot manner from TextWorld to unseen embodied tasks and settings. Our results show that training first in the abstract text-based environment is not only 7× faster, but also yields better performance than training from scratch in the embodied world. These results lend credibility to the hypothesis that solving abstract language-based tasks can help build priors that enable agents to generalize to unfamiliar embodied environments. Our contributions are as follows: § 2 ALFWorld environment: The first parallel interactive text-based and embodied environment. § 3 BUTLER architecture: An agent that learns high-level policies in language that transfer to low-level embodied executions, and whose modular components can be independently upgraded. § 4 Generalization: We demonstrate empirically that BUTLER, trained in the abstract text domain, generalizes better to unseen embodied settings than agents trained from corpora of demonstrations or from scratch in the embodied world.	https://arxiv.org/pdf/2301.12050	Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling	Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to and corrects errors in the LLM, successfully blending noisy internet-scale information from LLMs with knowledge grounded in environment dynamics.	Despite evidence that practical sequential decision making systems require efficient exploitation of prior knowledge regarding a task, the current prevailing paradigm in reinforcement learning (RL) is to train tabula rasa, without any pretraining or external knowledge (Agarwal et al., 2022). In an effort to shift away from this paradigm, we focus on the task of creating embodied RL agents that can effectively exploit large-scale external knowledge sources presented in the form of pretrained large language models (LLMs). LLMs contain potentially useful knowledge for completing tasks and compiling knowledge sources (Petroni et al., 2019). Previous work has attempted to apply knowledge from LLMs to decision-making by generating action plans for executing in an embodied environment (Ichter et al., 2022; Huang et al., 2022b; Song et al., 2022b; Singh et al., 2022; Liang et al., 2022b; Huang et al., 2022a). However, LLMs still often fail when generating plans due to a lack of grounding (Valmeekam et al., 2022). Additionally, many of these agents that rely on LLM knowledge at execution time are limited in performance by the accuracy of LLM output. We hypothesize that if LLMs are instead applied to improving exploration during training, resulting policies will not be constrained by the accuracy of an LLM. Exploration in environments with sparse rewards becomes increasingly difficult as the size of the explorable state space increases. For example, the popular 3D embodied environment Minecraft has a large technology tree of craftable items with complex dependencies and a high branching factor. Before crafting a stone pickaxe in Minecraft an agent must: collect logs, craft logs into planks and then sticks, craft a crafting table from planks, use the crafting table to craft a wooden pickaxe from sticks and planks, use the wooden pickaxe to collect cobblestone, and finally use the crafting table to craft a stone pickaxe from sticks and cobblestone. Reaching a goal item is difficult without expert knowledge of Minecraft via dense rewards (Baker et al., 2022; Hafner et al., 2023) or expert demonstrations (Skrynnik et al., 2021; Patil et al., 2020), making item crafting in Minecraft a longstanding AI challenge (Guss et al., 2019; Fan et al., 2022). We propose DECKARD* (DECision-making for Knowledgable Autonomous Reinforcement-learning Dreamers), an agent that hypothesizes an Abstract World Model (AWM) over subgoals by few-shot prompting an LLM, then exploits the AWM for exploration and verifies the AWM with grounded experience. As seen in Figure 1, DECKARD operates in two phases: (1) the Dream phase where it uses the hypothesized AWM to suggest the next node to explore from the directed acyclic graph (DAG) of subgoals; and (2) the Wake phase where it learns a modular policy of subgoals, each trained on RL objectives, and verifies the hypothesized AWM with grounded environment dynamics. Figure 1 shows two iterations of the DECKARD agent learning the “craft a stone pickaxe” task in Minecraft. During the first Dream phase, the agent has already verified the nodes log and plank, and DECKARD suggests exploring towards the stick subgoal, ignoring nodes such as door that are not predicted to complete the task. Then, during the following Wake phase, DECKARD executes each subgoal in the branch ending in the stick node and then explores until it successfully crafts a stick. If successful, the agent marks the newly discovered node as verified in its AWM before proceeding to the next iteration. We evaluate DECKARD on learning to craft items in the Minecraft technology tree. We show that LLM-guidance is essential to exploration in DECKARD, with a version of our agent without LLM-guidance taking over twice as long to craft most items during open-ended exploration. Whereas, when exploring towards a specific task, DECKARD improves sample-efficiency by an order of magnitude versus comparable agents, (12x the ablated DECKARD without LLM-guidance). Our method is also robust to task decomposition errors in the LLM, consistently outperforming baselines as we introduce errors in the LLM output. DECKARD demonstrates the potential for robustly applying LLMs to RL, thus enabling RL agents to effectively use large-scale, noisy prior knowledge sources for exploration.	1	0
helping students understand their mistakes and misunderstandings	https://arxiv.org/pdf/2406.11709	Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging	Socratic questioning is an effective teaching strategy, encouraging critical thinking and problem-solving. The conversational capabilities of large language models (LLMs) show great potential for providing scalable, real-time student guidance. However, current LLMs often give away solutions directly, making them ineffective instructors. We tackle this issue in the code debugging domain with TreeInstruct, an Instructor agent guided by a novel state space-based planning algorithm. TreeInstruct asks probing questions to help students independently identify and resolve errors. It estimates a student's conceptual and syntactical knowledge to dynamically construct a question tree based on their responses and current knowledge state, effectively addressing both independent and dependent mistakes concurrently in a multi-turn interaction setting. In addition to using an existing single-bug debugging benchmark, we construct a more challenging multi-bug dataset of 150 coding problems, incorrect solutions, and bug fixes -- all carefully constructed and annotated by experts. Extensive evaluation shows TreeInstruct's state-of-the-art performance on both datasets, proving it to be a more effective instructor than baselines. Furthermore, a real-world case study with five students of varying skill levels further demonstrates TreeInstruct's ability to guide students to debug their code efficiently with minimal turns and highly Socratic questioning. 	With the rapidly expanding conversational and reasoning abilities of large language models (LLMs), there has been a substantial rise in demand for exploiting their capabilities within a multitude of educational applications (Kasneci et al., 2023) in order to widen accessibility via personalized feedback. Specifically, several recent works explore the use of LLMs for providing feedback and guidance to students (Wang et al., 2023; Kazemitabaar et al., 2024; Sheese et al., 2024; Lyu et al., 2024). However, LLMs are typically optimized to generate customer-serving, assistant-like responses, which also translates into the types of questions asked. Especially for educational domains, this style of questioning can be suboptimal (Cotton, 1988; Sahamid, 2016; Yang et al., 2005; Wilson, 1987). For instance, if a student is seeking help from an instructor for correcting their mistakes (e.g., debugging their buggy code), we consider two forms of potential responses: assistant-like and instructor-like. As shown in Figure 1, an assistant-like response would not be a successful educational interaction, as it leads to the Assistant directly providing an answer. On the other hand, an Instructor-like response reflects the educational philosophy of Socratic questioning. Socratic questioning is a teaching strategy where the Student independently solves their problem by answering guiding questions, instead of being given the solution directly (Wilson, 1987). This is a more effective learning strategy because the weight of learning falls on the Student as they must put in effort to answer a question as opposed to solely relying on the model (Cotton, 1988; Kasneci et al., 2023). Therefore, we aim to re-orient an LLM to be an Instructor, not an assistant, by asking Socratic questions that (1) help the Student understand their mistakes, and (2) do not directly provide the answer. To tackle these challenges, we propose TreeInstruct based on the following principles: 1. State space estimation: An Instructor plans its conversation with a Student based on the “distance” between their initial answer and the optimal, correct answer within the estimated state space. In other words, it tracks the knowledge state of the Student within this space throughout the Instructor-Student interactions. 2. Tree-based Socratic questioning: An Instructor generates turn-level Socratic questions conditioned on both the Student's current knowledge state and misunderstanding(s), the latter derived from their responses to the Instructor's questions. This step dynamically constructs a Socratic question tree. 3. Adaptive conversation restructuring: An Instructor updates their initial conversation plan based on how the Student is progressing in the conversation, as reflected by updates (or lack thereof) to the Student's knowledge state. This planning can include both questioning and teaching actions. While these principles can apply to many educational domains, this paper focuses on code debugging, which presents unique challenges. Realworld code debugging often involves multiple, potentially interdependent conceptual and syntactical bugs. For instance, Figure 1 shows that first resolving the Student's conceptual misunderstanding of recursion in Fibonacci helps them identify their recursive syntactical bug (Figure 1). However, existing work fails to account for such nuances and assumes single-turn feedback (Kazemitabaar et al., 2024; Wang et al., 2023; Lyu et al., 2024). This ignores the sub-steps required for the Student to understand each bug. In contrast, TreeInstruct constructs a multi-turn debugging plan (state representation), defined as the set of Student misunderstandings and mistakes (state variables) to be resolved in order to comprehend and correct their bug(s). We define all potential paths to complete these tasks as the state space. We traverse the space using Socratic questions and trace which variables have been resolved, grounded based on the Student's responses. While existing LLM-based tutors are effective in fixing the Student's code with high success, they are either prone to directly revealing code answers or cannot be adapted to new Student responses. For example, CodeAid (Kazemitabaar et al., 2024) (specifically, the \"Help Fix Code\" and \"Question from Code\" modules, as these are most similar to our setting) directly provides code or pseudocode 57% of the time, and achieves a mere 55% rate of helpfulness. On the other hand, TreeInstruct exploits the state space to dynamically construct a tree of questions based on (1) incorrect Student responses, or (2) gaps in the Student's knowledge. The sibling and parent-child relationships between questions reflect the manner in which they traverse the state space. Finally, it exploits both the Student's knowledge state and any proposed bug fixes to serve as the dynamic stopping condition. Overall, TreeInstruct takes a more structured approach to multi-turn conversational feedback, as (1) grounding the conversation on the state space representation ensures that all bugs are sufficiently addressed, and (2) constructing a tree based on the Student's current level of understanding allows for more relevant and personalized question generation. We summarize our contributions below: • To the best of our knowledge, TreeInstruct is the first work to explore state space estimation and dynamic tree-based questioning for multi-turn Socratic instruction. • We construct a novel multi-bug debugging dataset with 150 expert-annotated, challenging conceptual and syntactical bugs and their fixes. • Extensive experiments on an existing benchmark and our constructed dataset demonstrate that TreeInstruct can be universally applied to both open and closed source-settings. We also showcase that TreeInstruct's strong Socratic questioning abilities widely outperform all baselines through both (1) rigorous quantitative and qualitative expert evaluation (on average, preferred 78.43% of the time; Student fixes code 24.55% more) and (2) real-world interactions with students of varying coding abilities. Reproducibility: We release our data and source code1 to facilitate further studies.	https://arxiv.org/pdf/2310.10648	Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes	Scaling high-quality tutoring remains a major challenge in education. Due to growing demand, many platforms employ novice tutors who, unlike experienced educators, struggle to address student mistakes and thus fail to seize prime learning opportunities. Our work explores the potential of large language models (LLMs) to close the novice-expert knowledge gap in remediating math mistakes. We contribute Bridge, a method that uses cognitive task analysis to translate an expert's latent thought process into a decision-making model for remediation. This involves an expert identifying (A) the student's error, (B) a remediation strategy, and (C) their intention before generating a response. We construct a dataset of 700 real tutoring conversations, annotated by experts with their decisions. We evaluate state-of-the-art LLMs on our dataset and find that the expert's decision-making model is critical for LLMs to close the gap: responses from GPT4 with expert decisions (e.g., "simplify the problem") are +76% more preferred than without. Additionally, context-sensitive decisions are critical to closing pedagogical gaps: random decisions decrease GPT4's response quality by -97% than expert decisions. Our work shows the potential of embedding expert thought processes in LLM generations to enhance their capability to bridge novice-expert knowledge gaps. Our dataset and code can be found at: \url{this https URL}.	Human tutoring plays a critical role in accelerating student learning, and is one of the primary ways to combat pandemic-related learning losses (Fryer Jr and Howard-Noveck, 2020; Nickow et al., 2020; Robinson and Loeb, 2021; of Education, 2021; Accelerator, 2022). To accommodate the growing demand for tutoring, many tutoring providers engage novice tutors. While novice tutors may exercise the domain knowledge, they often lack the specialized training of professional educators in interacting with students. However, research suggests that novices with proper training can be effective tutors (Nickow et al., 2020). Responding to student mistakes in real-time is a critical area where novice tutors tend to struggle. Mistakes are prime learning opportunities to address misconceptions (Boaler, 2013), but effective responses involve pedagogical expertise in engaging with student’s thinking and building positive rapport (Roorda et al., 2011; Pianta, 2016; Shaughnessy et al., 2021; Robinson, 2022). Novices typically learn from experts to understand the expert’s thought process however hiring experienced educators to provide timely feedback is resourceintensive (Kraft et al., 2018; Kelly et al., 2020). One potential solution is the use of automated tutors (Graesser et al., 2004). With recent advances in large language models (LLMs), this approach has gained even more interest (Khan Academy, 2023). However their ability to remediate is yet to be evaluated. Prior work suggests several shortcomings with LLMs, including lacking reliable subject and pedagogical knowledge (Frieder et al., 2023; Wang and Demszky, 2023; Singer, 2023), that can be mitigated using explicitly thought processes such as through chain-of-thought prompting (Wei et al., 2022). To address these challenges, our work makes several key contributions. First, we build Bridge, a method that leverages cognitive task analysis to elicit the latent thought processes of experts. We apply Bridge to remediation where we collaborate extensively with experienced math educators to translate their thought process into a decisionmaking model. Bridge breaks down the experts’ thought process: illustrated in Figure 1, Step A is to infer the student’s error (e.g., the student guessed); Step B is to determine the remediation strategy (e.g., provide a solution approach); and Step C is to identify the strategy intention (e.g., to help the student understand the concept). We construct a dataset of real-world tutoring conversations, annotated with expert decisions and responses. Our open-source dataset consists of 700 real tutoring sessions conducted with 1st5th grade students in Title I schools, predominantly serving low-income students of color. Following FERPA guidelines, our study is IRB-approved and conducts secondary data analysis based on our Data Use Agreement with the tutoring provider and school district. We conduct a thorough human evaluation to compare the expert, novice and LLMs in remediation. To our knowledge, our work is the first to assess the performance of LLMs such as GPT4 and instruct-tuned Llama-2-70b on remediating student mistakes. We find that the response quality of LLMs significantly improve with the expert’s decision-making process: Response from GPT4 with expert- and self-generated decisions are 76-88% more preferred than GPT4 without. Context-sensitive decisions are also critical to closing the knowledge gap: Random decisions decrease GPT4’s response quality -67% than expert decisions. Complementing our quantitative analysis, our lexical analysis reveals that novices and LLMs without the expert’s decision-making process engage superficially with student’s problemsolving process: They give away the answer or prompt the student to re-attempt without further guidance (“double check”, “try again”).	1	1
event granularities	https://arxiv.org/pdf/2408.04873	Unsupervised Episode Detection for Large-Scale News Events	Episodic structures are inherently interpretable and adaptable to evolving large-scale key events. However, state-of-the-art automatic event detection methods overlook event episodes and, therefore, struggle with these crucial characteristics. This paper introduces a novel task, episode detection, aimed at identifying episodes from a news corpus containing key event articles. An episode describes a cohesive cluster of core entities (e.g., “protesters”, “police”) performing actions at a specific time and location. Furthermore, an episode is a significant part of a larger group of episodes under a particular key event. Automatically detecting episodes is challenging because, unlike key events and atomic actions, we cannot rely on explicit mentions of times and locations to distinguish between episodes or use semantic similarity to merge inconsistent episode co-references. To address these challenges, we introduce EpiMine, an unsupervised episode detection framework that (1) automatically identifies the most salient, key-event-relevant terms and segments, (2) determines candidate episodes in an article based on natural episodic partitions estimated through shifts in discriminative term combinations, and (3) refines and forms final episode clusters using large language model-based reasoning on the candidate episodes. We construct three diverse, real-world event datasets annotated at the episode level. EpiMine outperforms all baselines on these datasets by an average 59.2% increase across all metrics.	Given the saturation of real-time news accessible at our fingertips, reading and processing the critical information of a key event has become an increasingly daunting challenge. Consequently, research on automatic textual event detection has recently attempted to integrate the manner in which humans neurologically perceive/store events into textual event detection methods. Specifically, neuroscientists studying event representations in human memory find that events are stored in a top-to-bottom hierarchy, as demonstrated in Figure 1. The deeper the hierarchical event level, the more finegrained its corresponding text granularity [ 48 ]: we consider a theme as corpus-level (all articles discussing the 2019 Hong Kong Protests), key event as document-level (an article typically discusses a full one to two day key event), episode as segment-level, and atomic action as sentence or phrase-level. Furthermore, neurological research [3 , 21 ] indicates that events are encoded into memory as episodic structures. Representing events as discrete episodes helps us piece together a coherent narrative by considering the sequence of actions, reactions, and developments over time. This empowers several downstream tasks (e.g., event schema generation, event prediction), which may benefit from insights into the causes and consequences of events that might be missed with a more generalized analysis. Despite its strengths, existing automatic event extraction works fail to consider the episode-level. For example, key event detection specifically seeks to output “a set of thematically coherent documents” for each key event [ 29, 48]. However, it is challenging to manually parse through a large cluster of relevant articles in order to gain an efficient and compact understanding of a given event. To address the lack of interpretability of such article clusters, the adjacent task of timeline summarization [9 , 14 , 23, 39 ] aims to identify the dates and a compact summary for each key event. However, high-level timelines are typically applicable for historical themes and thus unrealistic for currently evolving key events where fine-grained timeline summarization is more suitable. Hence, event chain mining [ 18] attempts to address this by mining a series of temporally-ordered atomic actions at the phraselevel; however, the granularity level is often too fine-grained to properly represent a large-scale key event. Thus, we aim to tackle the novel task of episode detection to pave the way for a more effective event representation. Episode detection aims to detect episodes from a news corpus containing key event articles. An episode can be described as a cohesive cluster of subjects performing actions at a certain time and location, occurring as part of a larger sequence of episodes under a specific key event. Episode detection introduces a unique set of challenges, which we address using our novel framework, EpiMine. EpiMine is an unsupervised episode detection framework that automatically detects meaningful episodic events and their corresponding text segments in a large key event corpus, all without any level of human supervision or labeled training data. EpiMine is comprised of the following components: (1) discriminative co-occurrence detection, (2) episode partitioning, (3) candidate episode estimation, and (4) episode-segment classification. Collectively, they tackle the unique challenges of episode detection, detailed below: Challenge 1: Journalists do not timestamp episodes. Key event detection partitions a thematic corpus into document-level clusters by heavily relying on explicit temporal features, like publication dates [48 ]. For example, an article primarily discusses one key event, which can then be roughly mapped to the article's publication date. However, this assumption fails at the episode-level, where there is no guarantee to have a distinct timestamp associated with each text segment that discusses a new episode. Fortunately, we can take advantage of the idea that journalists naturally partition news articles by sequentially discussing distinct episodes: Example: An article likely completes its discussion of the episode A, protesters storming the Legislaive Council, before episode B, “protesters vandalized the Legislative Chamber” (Figure 3). Hence, in order to partition articles into distinct episode segments, EpiMine must identify whether or not two consecutive segments are discussing the same or different episodes, which brings us to our next challenge. Challenge 2: Episodes contain semantically diverse actions. Each episode features a set of unique atomic actions, which we can utilize for determining whether or not two segments discuss the same episode. However, for clustering actions, existing methods [18 ] rely heavily on semantic similarity. This not realistic for episode-segment clustering: Example: “protesters spray-painted slogans” and “they unfurled the colonial-era flag” will fall under the same episode, but are semantically different and unlikely to be clustered. Alternatively, we can identify salient terms that reflect the same episode (“barriers” and “shoved” are unique to Episode A; “defaced” and “walls” for Episode B), by exploiting corpus-level signals. Specifically, if we frequently see “defaced” mentioned together with “walls” (or their respective synonyms) and not with other terms (and vice-versa), then we consider them a discriminative co-occurrence. Consequently, when such co-occurrences remain consistent across text segments, this indicates the same episode being discussed. Conversely, if a sufficient shift in the combination of terms occurs, then this indicates that a different episode is being discussed. Challenge 3: Articles often do not feature all episodes. Given the real-time nature of reporting, an article may feature only a subset or none of ground-truth episodes from a multi-day key event (e.g., focusing on a high-level analysis of the key event or mentioning other related key events). To minimize this noise, EpiMine seeks to identify the set of articles which maximizes the quantity and quality of potential episodes. It then merges any article partitions across these articles which likely discuss the same episode and employs a large language model (LLM) to provide a more fluent interpretation of the candidate episodes, accounting for the episode's core entity, actions, object, location, and time period. This allows EpiMine to finally map the remaining non-salient article segments to these episodes, pruning any candidates which are not sufficiently supported by the remaining articles. We summarize our core contributions: (1) We introduce the novel task of episode detection, which takes in a key-event corpus and outputs multiple detected episodes and their related segments extracted from the corpus. (2) We propose a novel unsupervised episode detection method, EpiMine, which exploits discriminative term co-occurrences to estimate candidate episode partitions from top articles. It refines these candidate episodes using LLM-based reasoning to output a comprehensive and diverse set of episodes. (3) We construct three novel datasets, reflecting a diverse set of real-world themes and thirty global key events, as no large-scale key event-specific news corpus exists for this task where the key events are guaranteed to contain distinguishable episodes. (4) We compare EpiMine with five other baselines through an extensive set of experiments and case studies performed on our real-world datasets, demonstrating that it outperforms all baselines by, on average, a 59.2% increase across all metrics. Reproducibility: We provide our dataset and source code1 to facilitate further studies.	https://dl.acm.org/doi/pdf/10.1145/3543507.3583295	Unsupervised Event Chain Mining from Multiple Documents	Massive and fast-evolving news articles keep emerging on the web. To efectively summarize and provide concise insights into realworld events, we propose a new event knowledge extraction task Event Chain Mining in this paper. Given multiple documents about a super event, it aims to mine a series of salient events in temporal order. For example, the event chain of super event Mexico Earthquake in 2017 is {earthquake hit Mexico, destroy houses, kill people, block roads}. This task can help readers capture the gist of texts quickly, thereby improving reading efciency and deepening text comprehension. To address this task, we regard an event as a cluster of diferent mentions of similar meanings. In this way, we can identify the diferent expressions of events, enrich their semantic knowledge and replenish relation information among them. Taking events as the basic unit, we present a novel unsupervised framework, EMiner. Specifcally, we extract event mentions from texts and merge them with similar meanings into a cluster as a single event. By jointly incorporating both content and commonsense, essential events are then selected and arranged chronologically to form an event chain. Meanwhile, we annotate a multi-document benchmark to build a comprehensive testbed for the proposed task. Extensive experiments are conducted to verify the efectiveness of EMiner in terms of both automatic and human evaluations.	In the information age, a plethora of information resources is at the fngertips of every user. Faced with a variety of complex and lengthy news on the web, how to quickly understand their core idea has become a critical problem with increasing concerns. Generally, massive unstructured news articles can be regarded as the chains of salient events arranged in order [1, 11]. Therefore, extracting event chain knowledge from them is a crucial step in text understanding. Recently, various event-centric tasks have gained signifcant interest, such as event relation extraction [2, 13, 37], salient event identifcation [25, 39], and event process understanding [8, 43]. However, most of these studies highly rely on expert annotations, which are expensive and time-consuming. Subsequent research [20, 38] attempts to alleviate this issue under an unsupervised setting. They extract event schema from large corpus as prior knowledge to assist downstream tasks, such as story generation [40], question answering [34], text summarization [49], and reading comprehension [46]. However, explorations on how to mine event chains from large amounts of unstructured text remain lacking. Such a task can provide a brief summary and help readers to capture the skeleton of texts quickly. To this end, we propose a new task of knowledge extraction, Event Chain Mining. It aims to mine a series of salient events in a temporal order, which can serve as a concise highlight of texts. Specifcally, given a super event1, multiple documents usually report it from diferent perspectives. Moreover, these reports usually share the most salient events among their texts. For example, Figure 1 shows three documents on a super event, Mexico Earthquake in 2017. Most of them mention four essential events in the earthquake, including earthquake hit Mexico, damage houses, kill people, and block roads. These events can provide a sequential highlight of how this disaster occurred. With such a chain of salient events, readers can efectively grasp the whole picture of the news, thus improving reading efciency and deepening reading comprehension. This observation leads to the Event Chain Mining problem, which poses the following challenges: (1) variability of events. An event can be expressed in diferent descriptions. For example, in Figure 1, earthquake rocked southern Mexico and earthquake hit southern Mexico are two diferent mentions, but they describe similar meanings in Mexico Earthquake. The desired event chain should not include both events, as this brings in information redundancy. (2) salience inequality of events. Not all events in the news are equally important. Some of the events could be too general and contain little information,such assay it. Others could be too specifc, not closely tied to the main points, such as The state has 3.44 million people. These events should be fltered to ensure that the fnal event chain includes only the central point of the news. (3) ambiguity of event relationship. Existing methods heavily rely on local contexts to determine the temporal relationships between diferent events. However, due to diferent narrative styles, clear clues are not always provided in the news. Especially when events scatter separately in a long text or multiple documents, it is more challenging to capture their long-distance relations. To address these challenges, we regard an event as a cluster of mentions with similar meanings. In this way, there are three signifcant benefts. First, it naturally helps to address the frst challenge – diferent expressions of the same event. Second, it enhances event semantics by including multiple related mentions, which makes it easier to deepen event understanding and recognize salient events. Third, it enriches the order of information between events. By introducing multiple mentions, more clues about event relations can be obtained from the contexts. As a result, event ordering can be more convincing with the support from the majority of mentions. On top of that, we propose a novel unsupervised event chain mining framework, EMiner, which contains four major steps. Concretely, given a set of texts on the same super event, we frst decompose them into multiple event mentions. We elaborate frequentlyoccurring syntactic patterns and extract all possible event mentions. Then, event mentions of similar meanings are merged into clusters as distinct events. We formulate the event mention merging problem as an online text stream clustering task without requiring a fxed number of clusters. Next, we measure the salience of events to select important ones according to event frequency counting. Finally, we propose a language model empowered event ordering method, which jointly incorporates both of content and commonsense to arrange the salient events in a sequential chain. It not only leverages explicit clues from the content description in multiple documents, but also exploits commonsense knowledge in the pretrained generation models by re-framing the ordering problem as a generation task. By combining two perspectives together, the fnal chronological chain of events can be produced. To build a testbed for the proposed task, we re-annotate an existing multi-document dataset [28] to develop a new benchmark for evaluating event chain mining systems. For multiple documents on a super event, we manually annotate salient events as a brief summary. In addition, we design a comprehensive evaluation system, which can evaluate the model from multiple aspects, such as event semantics and sequential orders. We conduct extensive experiments and the results verify that EMiner can produce a chain of salient events to guide people to understand texts. The major contributions of this paper are summarized as follows: (1) We propose a new task, Event Chain Mining, to summarize and provide concise insights to real-world events. It assists people in quickly capturing the central points of a large amount of unstructured textual data on the web, thus improving reading efciency and deepening comprehension. (2) We present a novel framework EMiner to automatically extract an event chain in an unsupervised setting. According to the cluster completeness, term occurrence, and semantic similarity, our model extracts and merges essential events from multiple documents. EMiner can also reorder salient events by incorporating contents and commonsense knowledge to form the fnal chain. (3) To facilitate research in this direction, we establish a comprehensive testbed with a human-annotated benchmark and evaluation metrics. Experimentally, our proposed EMiner outperforms all baselines in terms of both automatic and human evaluation.	1	1
text classification	https://arxiv.org/pdf/2304.01969	MEGClass: Extremely Weakly Supervised Text Classification via Mutually-Enhancing Text Granularities	Text classification is essential for organizing unstructured text. Traditional methods rely on human annotations or, more recently, a set of class seed words for supervision, which can be costly, particularly for specialized or emerging domains. To address this, using class surface names alone as extremely weak supervision has been proposed. However, existing approaches treat different levels of text granularity (documents, sentences, or words) independently, disregarding inter-granularity class disagreements and the context identifiable exclusively through joint extraction. In order to tackle these issues, we introduce MEGClass, an extremely weakly-supervised text classification method that leverages Mutually-Enhancing Text Granularities. MEGClass utilizes coarse- and fine-grained context signals obtained by jointly considering a document’s most class-indicative words and sentences. This approach enables the learning of a contextualized document representation that captures the most discriminative class indicators. By preserving the heterogeneity of potential classes, MEGClass can select the most informative class-indicative documents as iterative feedback to enhance the initial word-based class representations and ultimately fine-tune a pre-trained text classifier. Extensive experiments on seven benchmark datasets demonstrate that MEGClass outperforms other weakly and extremely weakly supervised methods.	Text classification is a fundamental task in Natural Language Processing (NLP) that enables automatic labeling of massive text corpora, which is necessary in many downstream applications (Rajpurkar et al., 2016; Zhang et al., 2022b; Tang et al., 2015). Prior works train text classifiers in a fully-supervised manner (Yang et al., 2016, 2019; Zhang et al., 2015) that requires a substantial amount of training data, which is expensive and time-consuming, especially in emerging domains that require the suFigure 1: These are the three document types featured within a corpus. While existing methods can only distinguish between (1) and (3), MEGClass addresses all three types as well as minimizing (3) from the constructed pseudo-training dataset. pervision of domain experts. Recent works have explored weakly supervised text classification, where a few labeled documents or class seeds are provided to serve as weak supervision (Meng et al., 2018; Mekala and Shang, 2020; Agichtein and Gravano, 2000; Tao et al., 2018; Zeng et al., 2022). These works typically compile a pseudo-training dataset from a given corpus by assigning a pseudolabel to each document based on its alignment to a specific class. The pseudo-training dataset aims to be a confident substitution for an expert-curated collection of class-indicative documents, ultimately being used to fine-tune a text classifier. The motivation of this work begins with the assumption that there are three types of documents with a ground truth class topic, as illustrated in Figure 1: documents discussing (1) the class topic at the word, sentence, and document level, (2) the class topic at the document-level but with multiple other topics mentioned at the word or sentence level, or (3) multiple topics as well as the class topic at all levels. Existing weakly supervised methods focus on word-level (Meng et al., 2018, 2020b; Mekala and Shang, 2020) or documentlevel (Wang et al., 2021) information independently and only ever associate one pseudo-label to a document, which may allow them to differentiate documents of type (1) from those of type (3). However, this notion is fundamentally unrealistic for real-world data, which often takes on the intergranularity class disagreements exhibited in type (2), and hence encounters the risk of the word- and sentence-level topics overriding the true documentlevel class. For example, the education-news article of type (2) uses strong political terms to discuss the government being the subject matter of a teacher’s lesson to students, which may mistakenly lead it being classified as “politics” instead of “education”. In general, documents may also feature several indeterminate sentences that do not confidently indicate any sort of class alignment. The existing classification methods unfortunately do not address such inconsistency concerns among different levels of granularity. Several approaches (Meng et al., 2018, 2020b) generate pseudo-labels based on the individual words or sentences instead of the entire document and evaluate their confidence in aligning to a single class. This may lead to two glaring issues: (1) many words and sentences are only considered high quality and class-indicative based on the context of their parent document, and (2) they could be representative of classes different from the document’s fundamental topic (e.g., “checks and balances” and “judicial branches” in conflict with “education” in Figure 1). On the other hand, another approach (Wang et al., 2021) heavily relies on both word-level and document-level, but independently– resulting in both overconfident and vague document representations used for generating pseudo-labels. In this study, we attack this problem by utilizing all three levels of text granularity (words, sentences, and documents) in a manner that allows them to mutually enhance each other’s significance in understanding which class a given document aligns with. Furthermore, we consider a document’s class distribution instead of a single class such that we rank documents with a narrower distribution higher and can correct any initial misclassifications (e.g., education > politics in Figure 1) through iterative feedback. Doing so will allow us to exploit three levels of insight into the document’s local (word/sentence) and global (document) class distribution and jointly factor them into our understanding of the document and final pseudo-training dataset construction. In this study, we propose a novel framework MEGClass to perform extremely weakly supervised text classification using mutually enhancing text granularities. Specifically, when only the class surface names are given, MEGClass first identifies class-indicative keywords and uses them to represent classes and sentences. MEGClass then automatically weighs the importance of each sentence in classification for more accurately estimating a document’s class distribution. MEGClass further leverages a multi-head attention network to learn a document representation that reflects the critical information at multiple text granularities. Finally, MEGClass takes the most confident document representations to enhance our initial wordbased class representations through our iterative feedback approach, helping our model better understand what an e.g., “education” document should look like at all levels of granularity. Because our multi-granularity approach can handle all three document types when constructing the final pseudotraining dataset, we allow our fine-tuned final classifier to be more robust to challenging real-world documents. Comparing with existing weakly and extremely weakly supervised methods, MEGClass achieves a stronger performance on most datasets, especially with longer documents and fine-grained classes. Our contributions are summarized as follows: 1. To the best of our knowledge, this is the first work to exploit mutually enhancing text granularities for extremely weakly supervised text classification (only given the class label names). 2. We propose a novel method, MEGClass, which produces a quality pseudo-training dataset through class distribution estimation and contextualized document embeddings, refined by iterative feedback. The pseudotraining dataset is used to fine-tune a text classifier. 3. Experiments on seven datasets demonstrate that MEGClass outperforms existing weakly and extremely weakly supervised methods, significantly in long-document datasets and competitively in shorter, coarse-grained datasets.	https://arxiv.org/pdf/2010.07245	Text Classification Using Label Names Only: A Language Model Self-Training Approach	Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications. Humans can perform classification without seeing any labeled examples but only based on a small set of words describing the categories to be classified. In this paper, we explore the potential of only using the label name of each class to train classification models on unlabeled data, without using any labeled documents. We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as representation learning models for document classification. Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training. We show that our model achieves around 90% accuracy on four benchmark datasets including topic and sentiment classification without using any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name.	Text classification is a classic and fundamental task in Natural Language Processing (NLP) with a wide spectrum of applications such as question answering (Rajpurkar et al., 2016), spam detection (Jindal and Liu, 2007) and sentiment analysis (Pang et al., 2002). Building an automatic text classification model has been viewed as a task of training machine learning models from human-labeled documents. Indeed, many deep learning-based classifiers including CNNs (Kim, 2014; Zhang et al., 2015) and RNNs (Tang et al., 2015a; Yang et al. 2016) have been developed and achieved great success when trained on large-scale labeled documents (usually over tens of thousands), thanks to their strong representation learning power that effectively captures the high-order, long-range semantic dependency in text sequences for accurate classification. Recently, increasing attention has been paid to semi-supervised text classification which requires a much smaller amount of labeled data. The success of semi-supervised methods stems from the usage of abundant unlabeled data: Unlabeled documents provide natural regularization for constraining the model predictions to be invariant to small changes in input (Chen et al., 2020; Miyato et al., 2017; Xie et al., 2019), thus improving the generalization ability of the model. Despite mitigating the annotation burden, semi-supervised methods still require manual efforts from domain experts, which might be difficult or expensive to obtain especially when the number of classes is large. Contrary to existing supervised and semisupervised models which learn from labeled documents, a human expert will just need to understand the label name (i.e., a single or a few representative words) of each class to classify documents. For example, we can easily classify news articles when given the label names such as “sports”, “business”, and “politics” because we are able to understand these topics based on prior knowledge. In this paper, we study the problem of weaklysupervised text classification where only the label name of each class is provided to train a classifier on purely unlabeled data. We propose a language model self-training approach wherein a pre-trained neural language model (LM) (Devlin et al., 2019; Peters et al., 2018; Radford et al., 2018; Yang et al., 2019) is used as both the general knowledge source for category understanding and feature representation learning model for classification. The LM creates contextualized word-level category supervision from unlabeled data to train itself, and then generalizes to document-level classification via a self-training objective. Specifically, we propose the LOTClass model for Label-Name-Only Text Classification built in three steps: (1) We construct a category vocabulary for each class that contains semantically correlated words with the label name using a pre-trained LM. (2) The LM collects high-quality categoryindicative words in the unlabeled corpus to train itself to capture category distinctive information with a contextualized word-level category prediction task. (3) We generalize the LM via documentlevel self-training on abundant unlabeled data. LOTClass achieves around 90% accuracy on four benchmark text classification datasets, AG News, DBPedia, IMDB and Amazon corpora, without learning from any labeled data but only using at most 3 words (1 word in most cases) per class as the label name, outperforming existing weaklysupervised methods significantly and yielding even comparable performance to strong semi-supervised and supervised models. The contributions of this paper are as follows: • We propose a weakly-supervised text classification model LOTClass based on a pre-trained neural LM without any further dependencies2 . LOTClass does not need any labeled documents but only the label name of each class. • We propose a method for finding categoryindicative words and a contextualized word-level category prediction task that trains LM to predict the implied category of a word using its contexts. The LM so trained generalizes well to document-level classification upon self-training on unlabeled corpus. • On four benchmark datasets, LOTClass outperforms significantly weakly-supervised models and has comparable performance to strong semisupervised and supervised models.	1	1
text classification	https://arxiv.org/pdf/2304.01969	MEGClass: Extremely Weakly Supervised Text Classification via Mutually-Enhancing Text Granularities	Text classification is essential for organizing unstructured text. Traditional methods rely on human annotations or, more recently, a set of class seed words for supervision, which can be costly, particularly for specialized or emerging domains. To address this, using class surface names alone as extremely weak supervision has been proposed. However, existing approaches treat different levels of text granularity (documents, sentences, or words) independently, disregarding inter-granularity class disagreements and the context identifiable exclusively through joint extraction. In order to tackle these issues, we introduce MEGClass, an extremely weakly-supervised text classification method that leverages Mutually-Enhancing Text Granularities. MEGClass utilizes coarse- and fine-grained context signals obtained by jointly considering a document’s most class-indicative words and sentences. This approach enables the learning of a contextualized document representation that captures the most discriminative class indicators. By preserving the heterogeneity of potential classes, MEGClass can select the most informative class-indicative documents as iterative feedback to enhance the initial word-based class representations and ultimately fine-tune a pre-trained text classifier. Extensive experiments on seven benchmark datasets demonstrate that MEGClass outperforms other weakly and extremely weakly supervised methods.	Text classification is a fundamental task in Natural Language Processing (NLP) that enables automatic labeling of massive text corpora, which is necessary in many downstream applications (Rajpurkar et al., 2016; Zhang et al., 2022b; Tang et al., 2015). Prior works train text classifiers in a fully-supervised manner (Yang et al., 2016, 2019; Zhang et al., 2015) that requires a substantial amount of training data, which is expensive and time-consuming, especially in emerging domains that require the suFigure 1: These are the three document types featured within a corpus. While existing methods can only distinguish between (1) and (3), MEGClass addresses all three types as well as minimizing (3) from the constructed pseudo-training dataset. pervision of domain experts. Recent works have explored weakly supervised text classification, where a few labeled documents or class seeds are provided to serve as weak supervision (Meng et al., 2018; Mekala and Shang, 2020; Agichtein and Gravano, 2000; Tao et al., 2018; Zeng et al., 2022). These works typically compile a pseudo-training dataset from a given corpus by assigning a pseudolabel to each document based on its alignment to a specific class. The pseudo-training dataset aims to be a confident substitution for an expert-curated collection of class-indicative documents, ultimately being used to fine-tune a text classifier. The motivation of this work begins with the assumption that there are three types of documents with a ground truth class topic, as illustrated in Figure 1: documents discussing (1) the class topic at the word, sentence, and document level, (2) the class topic at the document-level but with multiple other topics mentioned at the word or sentence level, or (3) multiple topics as well as the class topic at all levels. Existing weakly supervised methods focus on word-level (Meng et al., 2018, 2020b; Mekala and Shang, 2020) or documentlevel (Wang et al., 2021) information independently and only ever associate one pseudo-label to a document, which may allow them to differentiate documents of type (1) from those of type (3). However, this notion is fundamentally unrealistic for real-world data, which often takes on the intergranularity class disagreements exhibited in type (2), and hence encounters the risk of the word- and sentence-level topics overriding the true documentlevel class. For example, the education-news article of type (2) uses strong political terms to discuss the government being the subject matter of a teacher’s lesson to students, which may mistakenly lead it being classified as “politics” instead of “education”. In general, documents may also feature several indeterminate sentences that do not confidently indicate any sort of class alignment. The existing classification methods unfortunately do not address such inconsistency concerns among different levels of granularity. Several approaches (Meng et al., 2018, 2020b) generate pseudo-labels based on the individual words or sentences instead of the entire document and evaluate their confidence in aligning to a single class. This may lead to two glaring issues: (1) many words and sentences are only considered high quality and class-indicative based on the context of their parent document, and (2) they could be representative of classes different from the document’s fundamental topic (e.g., “checks and balances” and “judicial branches” in conflict with “education” in Figure 1). On the other hand, another approach (Wang et al., 2021) heavily relies on both word-level and document-level, but independently– resulting in both overconfident and vague document representations used for generating pseudo-labels. In this study, we attack this problem by utilizing all three levels of text granularity (words, sentences, and documents) in a manner that allows them to mutually enhance each other’s significance in understanding which class a given document aligns with. Furthermore, we consider a document’s class distribution instead of a single class such that we rank documents with a narrower distribution higher and can correct any initial misclassifications (e.g., education > politics in Figure 1) through iterative feedback. Doing so will allow us to exploit three levels of insight into the document’s local (word/sentence) and global (document) class distribution and jointly factor them into our understanding of the document and final pseudo-training dataset construction. In this study, we propose a novel framework MEGClass to perform extremely weakly supervised text classification using mutually enhancing text granularities. Specifically, when only the class surface names are given, MEGClass first identifies class-indicative keywords and uses them to represent classes and sentences. MEGClass then automatically weighs the importance of each sentence in classification for more accurately estimating a document’s class distribution. MEGClass further leverages a multi-head attention network to learn a document representation that reflects the critical information at multiple text granularities. Finally, MEGClass takes the most confident document representations to enhance our initial wordbased class representations through our iterative feedback approach, helping our model better understand what an e.g., “education” document should look like at all levels of granularity. Because our multi-granularity approach can handle all three document types when constructing the final pseudotraining dataset, we allow our fine-tuned final classifier to be more robust to challenging real-world documents. Comparing with existing weakly and extremely weakly supervised methods, MEGClass achieves a stronger performance on most datasets, especially with longer documents and fine-grained classes. Our contributions are summarized as follows: 1. To the best of our knowledge, this is the first work to exploit mutually enhancing text granularities for extremely weakly supervised text classification (only given the class label names). 2. We propose a novel method, MEGClass, which produces a quality pseudo-training dataset through class distribution estimation and contextualized document embeddings, refined by iterative feedback. The pseudotraining dataset is used to fine-tune a text classifier. 3. Experiments on seven datasets demonstrate that MEGClass outperforms existing weakly and extremely weakly supervised methods, significantly in long-document datasets and competitively in shorter, coarse-grained datasets.	https://arxiv.org/pdf/2305.13723	PIEClass: Weakly-Supervised Text Classification with Prompting and Noise-Robust Iterative Ensemble Training	Weakly-supervised text classification trains a classifier using the label name of each target class as the only supervision, which largely reduces human annotation efforts. Most existing methods first use the label names as static keyword-based features to generate pseudo labels, which are then used for final classifier training. While reasonable, such a commonly adopted framework suffers from two limitations: (1) keywords can have different meanings in different contexts and some text may not have any keyword, so keyword matching can induce noisy and inadequate pseudo labels; (2) the errors made in the pseudo label generation stage will directly propagate to the classifier training stage without a chance of being corrected. In this paper, we propose a new method, PIEClass, consisting of two modules: (1) a pseudo label acquisition module that uses zero-shot prompting of pre-trained language models (PLM) to get pseudo labels based on contextualized text understanding beyond static keyword matching, and (2) a noise-robust iterative ensemble training module that iteratively trains classifiers and updates pseudo labels by utilizing two PLM fine-tuning methods that regularize each other. Extensive experiments show that PIEClass achieves overall better performance than existing strong baselines on seven benchmark datasets and even achieves similar performance to fully-supervised classifiers on sentiment classification tasks.	Text classification is a fundamental NLP task with a wide range of downstream applications, such as question answering (Rajpurkar et al., 2016), sentiment analysis (Tang et al., 2015), and event detection (Zhang et al., 2022c). Earlier studies train text classifiers in a fully-supervised manner that requires a substantial amount of training data (Zhang et al., 2015; Yang et al., 2016), which are generally costly to obtain. To eliminate the need for labeled training samples, weakly-supervised text classification settings (Meng et al., 2018, 2020; Wang et al., 2021) are proposed, which aim to train text classifiers using the label names of target classes as the only supervision. Such settings are intriguing especially when obtaining high-quality labels is prohibitively expensive. Recent advancements in large generative language models (LLMs) (e.g., ChatGPT, GPT-4 (OpenAI, 2023)) make it a valid approach to directly prompt them in a zero-shot manner for text classification without labeled data. For example, people may provide a restaurant review and ask an LLM “What is the sentiment of this document?”, and the model will generate an answer according to its understanding. However, there are certain limitations of this method for the weakly-supervised text classification setting. First, directly prompting LLMs cannot utilize any domain-specific information hidden in the unlabeled data, because it is intractable to fine-tune such a large model and the prompts can hardly incorporate any corpus-level information, especially for corpora not appearing in LLMs’ pre-training data (e.g., private domains). Second, deploying LLMs is expensive, while many text classification applications require fast real-time inference (e.g., email and review classification). Another line of studies tailored for weaklysupervised text classification aims to train a moderate-size classifier with a task-specific unlabeled corpus. Given the label names, these methods first acquire class-indicative keywords using PLMs (Meng et al., 2020; Wang et al., 2021) or corpus-level co-occurrence features (Zhang et al., 2021, 2022b). The keywords are then used as static features to generate pseudo-labeled documents for fine-tuning the final classifier. Despite their promising performance, the aforementioned weakly-supervised methods may suffer from two major limitations. First, these methods are keyword-driven by using class-indicative keywords as static context-free features to generate pseudo labels with different forms of string matching. However, some texts may not contain such classindicative keywords and keywords may have different meanings in different contexts, so using them as static features can lead to noisy and inadequate pseudo labels. Such an issue is more serious for abstract classes like sentiments that require understanding rhetoric. For example, a food review “It is to die for!” contains the keyword “die” which itself is negative, but the entire review expresses a strong positive sentiment, and keyword-driven methods will likely struggle in these cases. Second, most existing methods are two-stage by conducting pseudo label acquisition and text classifier training in two successive steps. Although different pseudo label acquisition methods are explored to improve their quality (e.g., masked language modeling (Meng et al., 2020), clustering of PLM embeddings (Wang et al., 2021), or large textual entailment models trained with external data (Park and Lee, 2022)), there is still a large performance gap between weakly-supervised and fully-supervised settings, because erroneous pseudo labels in the first stage will propagate to and harm the classifier training stage without a chance to be corrected. To address the limitations of existing works, in this paper, we propose PIEClass: Prompting and Iterative Ensemble Training for WeaklySupervised Text Classification. PIEClass consists of two modules. (1) Pseudo label acquisition via PLM prompting. By designing a task-specific prompt, we can apply a moderate-size PLM to infer the class label of documents based on the entire input sequence, which is thus contextualized and beyond static keyword features. In this work, besides the well-studied prompting method using PLMs pre-trained with the masked language modeling task (MLM) (e.g., BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019)), we also explore a different prompting method for a discriminative pretrained language model, ELECTRA (Clark et al., 2020), and compare them in the experiments. (2) Noise-robust training with iterative ensemble. In each iteration, we train text classifiers using the current pseudo labels and then use the confident predictions to re-select the pseudo labels. In this way, we can gradually expand the pseudo labels which can be used to train better text classifiers. To avoid erroneous pseudo labels accumulated during the iterative process, we propose to utilize two PLM fine-tuning strategies, head token fine-tuning and prompt-based fine-tuning, as two complementary views of the data: One captures the semantics of the entire sequence while the other interprets the contexts based on the prompts. We use the two views to regularize each other and further apply model ensemble to improve the noise robustness of the pseudo label expansion process. To summarize, the contributions of this paper are as follows: (1) We propose to use the contextualization power of PLM prompting to get pseudo labels for the weakly-supervised text classification task instead of static keyword-based features. (2) We explore the prompting method of a discriminative PLM on the text classification task and compare it with prompting methods for MLM. (3) We propose a noise-robust iterative ensemble training method. To deal with noisy pseudo labels, we utilize two PLM fine-tuning strategies that regularize each other and apply model ensemble to enhance the pseudo label quality. (4) On seven benchmark datasets, PIEClass overall performs better than strong baselines and even achieves similar performance to fully-supervised methods.	1	1
text classification	https://arxiv.org/pdf/2304.01969	MEGClass: Extremely Weakly Supervised Text Classification via Mutually-Enhancing Text Granularities	Text classification is essential for organizing unstructured text. Traditional methods rely on human annotations or, more recently, a set of class seed words for supervision, which can be costly, particularly for specialized or emerging domains. To address this, using class surface names alone as extremely weak supervision has been proposed. However, existing approaches treat different levels of text granularity (documents, sentences, or words) independently, disregarding inter-granularity class disagreements and the context identifiable exclusively through joint extraction. In order to tackle these issues, we introduce MEGClass, an extremely weakly-supervised text classification method that leverages Mutually-Enhancing Text Granularities. MEGClass utilizes coarse- and fine-grained context signals obtained by jointly considering a document’s most class-indicative words and sentences. This approach enables the learning of a contextualized document representation that captures the most discriminative class indicators. By preserving the heterogeneity of potential classes, MEGClass can select the most informative class-indicative documents as iterative feedback to enhance the initial word-based class representations and ultimately fine-tune a pre-trained text classifier. Extensive experiments on seven benchmark datasets demonstrate that MEGClass outperforms other weakly and extremely weakly supervised methods.	Text classification is a fundamental task in Natural Language Processing (NLP) that enables automatic labeling of massive text corpora, which is necessary in many downstream applications (Rajpurkar et al., 2016; Zhang et al., 2022b; Tang et al., 2015). Prior works train text classifiers in a fully-supervised manner (Yang et al., 2016, 2019; Zhang et al., 2015) that requires a substantial amount of training data, which is expensive and time-consuming, especially in emerging domains that require the suFigure 1: These are the three document types featured within a corpus. While existing methods can only distinguish between (1) and (3), MEGClass addresses all three types as well as minimizing (3) from the constructed pseudo-training dataset. pervision of domain experts. Recent works have explored weakly supervised text classification, where a few labeled documents or class seeds are provided to serve as weak supervision (Meng et al., 2018; Mekala and Shang, 2020; Agichtein and Gravano, 2000; Tao et al., 2018; Zeng et al., 2022). These works typically compile a pseudo-training dataset from a given corpus by assigning a pseudolabel to each document based on its alignment to a specific class. The pseudo-training dataset aims to be a confident substitution for an expert-curated collection of class-indicative documents, ultimately being used to fine-tune a text classifier. The motivation of this work begins with the assumption that there are three types of documents with a ground truth class topic, as illustrated in Figure 1: documents discussing (1) the class topic at the word, sentence, and document level, (2) the class topic at the document-level but with multiple other topics mentioned at the word or sentence level, or (3) multiple topics as well as the class topic at all levels. Existing weakly supervised methods focus on word-level (Meng et al., 2018, 2020b; Mekala and Shang, 2020) or documentlevel (Wang et al., 2021) information independently and only ever associate one pseudo-label to a document, which may allow them to differentiate documents of type (1) from those of type (3). However, this notion is fundamentally unrealistic for real-world data, which often takes on the intergranularity class disagreements exhibited in type (2), and hence encounters the risk of the word- and sentence-level topics overriding the true documentlevel class. For example, the education-news article of type (2) uses strong political terms to discuss the government being the subject matter of a teacher’s lesson to students, which may mistakenly lead it being classified as “politics” instead of “education”. In general, documents may also feature several indeterminate sentences that do not confidently indicate any sort of class alignment. The existing classification methods unfortunately do not address such inconsistency concerns among different levels of granularity. Several approaches (Meng et al., 2018, 2020b) generate pseudo-labels based on the individual words or sentences instead of the entire document and evaluate their confidence in aligning to a single class. This may lead to two glaring issues: (1) many words and sentences are only considered high quality and class-indicative based on the context of their parent document, and (2) they could be representative of classes different from the document’s fundamental topic (e.g., “checks and balances” and “judicial branches” in conflict with “education” in Figure 1). On the other hand, another approach (Wang et al., 2021) heavily relies on both word-level and document-level, but independently– resulting in both overconfident and vague document representations used for generating pseudo-labels. In this study, we attack this problem by utilizing all three levels of text granularity (words, sentences, and documents) in a manner that allows them to mutually enhance each other’s significance in understanding which class a given document aligns with. Furthermore, we consider a document’s class distribution instead of a single class such that we rank documents with a narrower distribution higher and can correct any initial misclassifications (e.g., education > politics in Figure 1) through iterative feedback. Doing so will allow us to exploit three levels of insight into the document’s local (word/sentence) and global (document) class distribution and jointly factor them into our understanding of the document and final pseudo-training dataset construction. In this study, we propose a novel framework MEGClass to perform extremely weakly supervised text classification using mutually enhancing text granularities. Specifically, when only the class surface names are given, MEGClass first identifies class-indicative keywords and uses them to represent classes and sentences. MEGClass then automatically weighs the importance of each sentence in classification for more accurately estimating a document’s class distribution. MEGClass further leverages a multi-head attention network to learn a document representation that reflects the critical information at multiple text granularities. Finally, MEGClass takes the most confident document representations to enhance our initial wordbased class representations through our iterative feedback approach, helping our model better understand what an e.g., “education” document should look like at all levels of granularity. Because our multi-granularity approach can handle all three document types when constructing the final pseudotraining dataset, we allow our fine-tuned final classifier to be more robust to challenging real-world documents. Comparing with existing weakly and extremely weakly supervised methods, MEGClass achieves a stronger performance on most datasets, especially with longer documents and fine-grained classes. Our contributions are summarized as follows: 1. To the best of our knowledge, this is the first work to exploit mutually enhancing text granularities for extremely weakly supervised text classification (only given the class label names). 2. We propose a novel method, MEGClass, which produces a quality pseudo-training dataset through class distribution estimation and contextualized document embeddings, refined by iterative feedback. The pseudotraining dataset is used to fine-tune a text classifier. 3. Experiments on seven datasets demonstrate that MEGClass outperforms existing weakly and extremely weakly supervised methods, significantly in long-document datasets and competitively in shorter, coarse-grained datasets.	https://arxiv.org/pdf/2010.12794	X-Class: Text Classification with Extremely Weak Supervision	In this paper, we explore text classification with extremely weak supervision, i.e., only relying on the surface text of class names. This is a more challenging setting than the seed-driven weak supervision, which allows a few seed words per class. We opt to attack this problem from a representation learning perspective -- ideal document representations should lead to nearly the same results between clustering and the desired classification. In particular, one can classify the same corpus differently (e.g., based on topics and locations), so document representations should be adaptive to the given class names. We propose a novel framework X-Class to realize the adaptive representations. Specifically, we first estimate class representations by incrementally adding the most similar word to each class until inconsistency arises. Following a tailored mixture of class attention mechanisms, we obtain the document representation via a weighted average of contextualized word representations. With the prior of each document assigned to its nearest class, we then cluster and align the documents to classes. Finally, we pick the most confident documents from each cluster to train a text classifier. Extensive experiments demonstrate that X-Class can rival and even outperform seed-driven weakly supervised methods on 7 benchmark datasets. Our dataset and code are released at this https URL .	Weak supervision has been recently explored in text classification to save human effort. Typical forms of weak supervision include a few labeled documents per class (Meng et al., 2018; Jo and Cinarel, 2019), a few seed words per class (Meng et al., 2018, 2020a; Mekala and Shang, 2020; Mekala et al., 2020), and other similar open-data (Yin et al., 2019).Though much weaker than a fully annotated corpus, these forms still require non-trivial, corpusspecific knowledge from experts. For example, nominating seed words requires experts to consider their relevance to not only the desired classes but also the input corpus; To acquire a few labeled documents per class, unless the classes are balanced, one needs to sample and annotate a much larger number of documents to cover the minority class. In this paper, we focus on extremely weak supervision, i.e., only relying on the surface text of class names. This setting is much more challenging than the ones above, and can be considered as almost-unsupervised text classification. We opt to attack this problem from a representation learning perspective—ideal document representations should lead to nearly the same result between clustering and the desired classification. Recent advances in contextualized representation learning using neural language models have demonstrated the capability of clustering text to domains with high accuracy (Aharoni and Goldberg, 2020). Specifically, a simple average of word representations is sufficient to group documents on the same topic together. However, the same corpus could be classified using various criteria other than topics, such as locations and sentiments. As visualized in Figure 1, such class-invariant representations separate topics well but mix up locations. Therefore, it is a necessity to make document representations adaptive to the user-specified class names. We propose a novel framework X-Class to conduct text classification with extremely weak supervision, as illustrated in Figure 2. Firstly, we estimate class representations by incrementally adding the most similar word to each class and recalculating its representation. Following a tailored mixture of class attention mechanisms, we obtain the document representation via a weighted average of contextualized word representations. These representations are based on pre-trained neural language models, and they are supposed to be in the same latent space. We then adopt clustering methods (e.g., Gaussian Mixture Models) to group the documents into K clusters, where K is the number of desired classes. The clustering method is initialized with the prior knowledge of each document assigned to its nearest class. We preserve this assignment so we can easily align the final clusters to the classes. In the end, we pick confident documents from each cluster to form a pseudo training set, based on which, we can train any document classifier. In our implementation, we use BERT as both the pre-trained language model and the text classifier. Compared with existing weakly supervised methods, X-Class has a stronger and more consistent performance on 7 benchmark datasets, despite some of them using at least 3 seed words per class. It is also worth mentioning that X-Class has a much more mild requirement on the existence of class names in the corpus, whereas existing methods rely on the variety of contexts of the class names. Our contributions are summarized as follows. • We advocate an important but not-well-studied problem of text classification with extremely weak supervision. • We develop a novel framework X-Class to attack this problem from a representation learning perspective. It estimates high-quality, class-oriented document representations based on pre-trained neural language models so that the confident clustering examples could form pseudo training set for any document classifiers to train on. • We show that on 7 benchmark datasets, X-Class achieves comparable and even better performance than existing weakly supervised methods that require more human effort.	1	1