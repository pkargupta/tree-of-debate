focus_paper	opp_paper	topic	title_focus	title_opp	notes
https://arxiv.org/pdf/2406.11709	https://arxiv.org/pdf/2310.10648	helping students fix their mistakes	Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging	Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes	
https://arxiv.org/pdf/2305.10601	https://arxiv.org/pdf/2201.11903	enabling large language model reasoning via prompting	Tree of Thoughts: Deliberate Problem Solving with Large Language Models	Chain-of-Thought Prompting Elicits Reasoning in Large Language Models	
https://arxiv.org/pdf/2404.02078	https://arxiv.org/pdf/2406.09136	using preferences to train language models for better reasoning	Advancing LLM Reasoning Generalists with Preference Trees	Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs	focus uses criticisms/observations to improve the preference tree
https://arxiv.org/pdf/2411.04425	https://arxiv.org/pdf/2402.04333v3	selecting subsets of data to improve language model performance	DELIFT: Data Efficiency in Language model Instruction Fine Tuning	LESS: Selecting Influential Data for Targeted Instruction Tuning	focus paper uses ICL gains while cited paper uses expensive gradient similarity, focus paper also works on 3 use cases while cited works on 1
https://arxiv.org/pdf/2402.04333v3	https://arxiv.org/pdf/1906.01827	using gradient-based information to select subsets of data for improving language model performance	LESS: Selecting Influential Data for Targeted Instruction Tuning	Coresets for Data-efficient Training of Machine Learning Models	both use gradients, but cited paper uses submodular functions to select while focus paper does sim matching to training set
https://arxiv.org/pdf/2402.04333v3	https://arxiv.org/pdf/2301.13287	selecting subsets of data to improve language model performance	LESS: Selecting Influential Data for Targeted Instruction Tuning	MILO : Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning	task is the same but focus paper uses gradients while cited paper uses sentence semantics 
https://arxiv.org/pdf/2404.12522	https://arxiv.org/pdf/2210.00423	using active learning to train neural network-based multi-armed bandits for k-class classification	Neural Active Learning Beyond Bandits	Improved Algorithms for Neural Active Learning	(1) focus paper takes the cited paper method and removes the dependency of the long vector by predicting k scores for each k class simuntaneously, making it more efficient (2) focus paper introduces pool-based setting along with stream-based setting
https://arxiv.org/pdf/2404.12522	https://arxiv.org/pdf/2110.08611	using active learning to train neural network-based multi-armed bandits for k-class classification	Neural Active Learning Beyond Bandits	Deep Active Learning by Leveraging Training Dynamics	focus paper uses confidence-based performance while cited paper uses gradient delta-based active learning metrics AND focus paper introduces streaming/pooling algorithsm
https://arxiv.org/pdf/2408.04873	https://arxiv.org/pdf/2104.05919	event analysis	Unsupervised Episode Detection for Large-Scale News Events	Document-Level Event Argument Extraction by Conditional Generation	episode mining vs event extraction
https://arxiv.org/pdf/2408.04873	https://arxiv.org/pdf/2206.04153	event granularities	Unsupervised Episode Detection for Large-Scale News Events	Unsupervised Key Event Detection from Massive Text Corpora	episode mining vs key events
https://arxiv.org/pdf/2201.06771	https://arxiv.org/pdf/2001.09522	Taxonomy completion versus taxonomy expansion in weakly supervised settings	TaxoCom: Topic Taxonomy Completion with Hierarchical Discovery of Novel Topic Clusters	TaxoExpan: Self-supervised Taxonomy Expansion with Position-Enhanced Graph Neural Network	taxo const vs taxo expan
https://arxiv.org/pdf/2004.12832	https://arxiv.org/pdf/1810.04805	Continual pretraining of Bert for retrieval tasks	ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT	BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding	colbert vs bert 
https://arxiv.org/pdf/2103.00113	https://arxiv.org/pdf/2310.14525	contrastive learning on graphs	Anomaly Detection on Attributed Networks via Contrastive Self-Supervised Learning	Graph Ranking Contrastive Learning: A Extremely Simple yet Efficient Method	node anomaly detection vs (one method to do three tasks:) node classification/link prediction/graph classification
https://arxiv.org/pdf/2010.03768	https://arxiv.org/pdf/2010.03768	multi-model embodied agents + environments	ALFWorld: Aligning Text and Embodied Environments for Interactive Learning	Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling	focus is alfworld, a simulator for embodied agents, and the cited is a methodology for improving reasoning in embodied agents