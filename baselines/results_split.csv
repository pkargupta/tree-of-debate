focus_paper,opp_paper,topic,title_focus,title_opp,summary,author 0,author 1
https://arxiv.org/pdf/2406.11709,https://arxiv.org/pdf/2310.10648,helping students fix their errors,"Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging",Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes,"{ ""similarities"": [""Both papers focus on leveraging AI for educational purposes, particularly in enhancing tutoring effectiveness through adaptive questioning and decision-making."", ""Both papers emphasize the importance of expert guidance in AI-driven educational tools, with Author 0 using cognitive task analysis and Author 1 employing a Socratic questioning approach."", ""Both papers contribute novel datasets to the field: Author 0's real-world tutoring dataset and Author 1's multi-bug debugging dataset.""], ""differences"": [""**Methodological Approach**: Author 0 develops the Bridge method for eliciting expert thought processes, whereas Author 1 introduces a dynamic tree-based Socratic questioning system."", ""**Focus Area**: Author 0 focuses on remediation in general educational settings (1st-5th grade), while Author 1 targets debugging in programming education."", ""**Evaluation Metrics**: Author 0 assesses the effectiveness of LLMs in remediation with a focus on expert-guided decision-making, whereas Author 1 evaluates the performance of its system through quantitative and qualitative metrics, including student preference and code fix rates.""], ""conclusion"": ""While both papers share a common goal of enhancing educational outcomes through AI, they diverge in their methodological approaches, focus areas, and evaluation metrics. Author 0's Bridge method and emphasis on expert-guided decision-making for remediation in general education settings offer a novel contribution. In contrast, Author 1's dynamic tree-based Socratic questioning system, though innovative in the context of programming education, overlaps with Author 0's emphasis on expert guidance but diverges in application. Overall, both papers enrich the educational AI landscape with unique insights and tools."" }","Title: Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging Abstract: Socratic questioning is an effective teaching strategy, encouraging critical thinking and problem-solving. The conversational capabilities of large language models (LLMs) show great potential for providing scalable, real-time student guidance. However, current LLMs often give away solutions directly, making them ineffective instructors. We tackle this issue in the code debugging domain with TreeInstruct , an Instructor agent guided by a novel state space-based planning algorithm. TreeInstruct asks probing questions to help students independently identify and resolve errors. It estimates a student's conceptual and syntactical knowledge to dynamically construct a question tree based on their responses and current knowledge state, effectively addressing both independent and dependent mistakes concurrently in a multi-turn interaction setting. In addition to using an existing single-bug debugging benchmark, we construct a more challenging multi-bug dataset of 150 coding problems, incorrect solutions, and bug fixes- all carefully constructed and annotated by experts. Extensive evaluation shows TreeInstruct's state-ofthe-art performance on both datasets, proving it to be a more effective instructor than baselines. Furthermore, a real-world case study with five students of varying skill levels further demonstrates TreeInstruct's ability to guide students to debug their code efficiently with minimal turns and highly Socratic questioning.
 Introduction With the rapidly expanding conversational and reasoning abilities of large language models (LLMs), there has been a substantial rise in demand for exploiting their capabilities within a multitude of educational applications (Kasneci et al., 2023) in order to widen accessibility via personalized feedback. Specifically, several recent works explore

Figure 1: The Instructor's goal is to generate multi-turn Socratic questions while guiding the Student towards the correct solution.

<!-- image -->

the use of LLMs for providing feedback and guidance to students (Wang et al., 2023; Kazemitabaar et al., 2024; Sheese et al., 2024; Lyu et al., 2024). However, LLMs are typically optimized to generate customer-serving, assistant-like responses, which also translates into the types of questions asked. Especially for educational domains, this style of questioning can be suboptimal (Cotton, 1988; Sahamid, 2016; Yang et al., 2005; Wilson, 1987). For instance, if a student is seeking help from an instructor for correcting their mistakes (e.g., debugging their buggy code), we consider two forms of potential responses: assistant-like and instructor-like . As shown in Figure 1, an assistant-like response would not be a successful educational interaction, as it leads to the Assistant directly providing an answer. On the other hand, an Instructor-like response reflects the educational philosophy of Socratic questioning .

Socratic questioning is a teaching strategy where the Student independently solves their problem by answering guiding questions, instead of being

given the solution directly (Wilson, 1987). This is a more effective learning strategy because the weight of learning falls on the Student as they must put in effort to answer a question as opposed to solely relying on the model (Cotton, 1988; Kasneci et al., 2023). Therefore, we aim to re-orient an LLM to be an Instructor, not an assistant, by asking Socratic questions that (1) help the Student understand their mistakes, and (2) do not directly provide the answer. To tackle these challenges, we propose TreeInstruct based on the following principles:

- 1. State space estimation: An Instructor plans its conversation with a Student based on the ""distance"" between their initial answer and the optimal, correct answer within the estimated state space. In other words, it tracks the knowledge state of the Student within this space throughout the Instructor-Student interactions.
- 2. Tree-based Socratic questioning: An Instructor generates turn-level Socratic questions conditioned on both the Student's current knowledge state and misunderstanding(s), the latter derived from their responses to the Instructor's questions. This step dynamically constructs a Socratic question tree.
- 3. Adaptive conversation restructuring: An Instructor updates their initial conversation plan based on how the Student is progressing in the conversation, as reflected by updates (or lack thereof) to the Student's knowledge state. This planning can include both questioning and teaching actions.

While these principles can apply to many educational domains, this paper focuses on code debugging, which presents unique challenges. Realworld code debugging often involves multiple, potentially interdependent conceptual and syntactical bugs. For instance, Figure 1 shows that first resolving the Student's conceptual misunderstanding of recursion in Fibonacci helps them identify their recursive syntactical bug (Figure 1). However, existing work fails to account for such nuances and assumes single-turn feedback (Kazemitabaar et al., 2024; Wang et al., 2023; Lyu et al., 2024). This ignores the sub-steps required for the Student to understand each bug.

In contrast, TreeInstruct constructs a multi-turn debugging plan ( state representation ), defined as the set of Student misunderstandings and mistakes ( state variables ) to be resolved in order to comprehend and correct their bug(s). We define all

potential paths to complete these tasks as the state space . We traverse the space using Socratic questions and trace which variables have been resolved, grounded based on the Student's responses.

While existing LLM-based tutors are effective in fixing the Student's code with high success, they are either prone to directly revealing code answers or cannot be adapted to new Student responses. For example, CodeAid (Kazemitabaar et al., 2024) (specifically, the ""Help Fix Code"" and ""Question from Code"" modules, as these are most similar to our setting) directly provides code or pseudocode 57% of the time, and achieves a mere 55% rate of helpfulness. On the other hand, TreeInstruct exploits the state space to dynamically construct a tree of questions based on (1) incorrect Student responses, or (2) gaps in the Student's knowledge. The sibling and parent-child relationships between questions reflect the manner in which they traverse the state space. Finally, it exploits both the Student's knowledge state and any proposed bug fixes to serve as the dynamic stopping condition. Overall, TreeInstruct takes a more structured approach to multi-turn conversational feedback, as (1) grounding the conversation on the state space representation ensures that all bugs are sufficiently addressed, and (2) constructing a tree based on the Student's current level of understanding allows for more relevant and personalized question generation.

We summarize our contributions below:

- · To the best of our knowledge, TreeInstruct is the first work to explore state space estimation and dynamic tree-based questioning for multi-turn Socratic instruction.
- · We construct a novel multi-bug debugging dataset with 150 expert-annotated, challenging conceptual and syntactical bugs and their fixes.
- · Extensive experiments on an existing benchmark and our constructed dataset demonstrate that TreeInstruct can be universally applied to both open and closed source-settings. We also showcase that TreeInstruct's strong Socratic questioning abilities widely outperform all baselines through both (1) rigorous quantitative and qualitative expert evaluation (on average, preferred 78.43% of the time; Student fixes code 24.55% more ) and (2) real-world interactions with students of varying coding abilities.

Reproducibility: We release our data and source code 1 to facilitate further studies.","Title: Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes Abstract: Human tutoring plays a critical role in accelerating student learning, and is one of the primary ways to combat pandemic-related learning losses (Fryer Jr and Howard-Noveck, 2020; Nickow et al., 2020; Robinson and Loeb, 2021; of Education, 2021; Accelerator, 2022). To accommodate the growing demand for tutoring, many tutoring providers engage novice tutors. While novice tutors may exercise

the domain knowledge, they often lack the specialized training of professional educators in interacting with students. However, research suggests that novices with proper training can be effective tutors (Nickow et al., 2020).

Responding to student mistakes in real-time is a critical area where novice tutors tend to struggle. Mistakes are prime learning opportunities to address misconceptions (Boaler, 2013), but effective responses involve pedagogical expertise in engaging with student's thinking and building positive rapport (Roorda et al., 2011; Pianta, 2016; Shaughnessy et al., 2021; Robinson, 2022). Novices typically learn from experts to understand the expert's thought process however hiring experienced educators to provide timely feedback is resourceintensive (Kraft et al., 2018; Kelly et al., 2020).

One potential solution is the use of automated tutors (Graesser et al., 2004). With recent advances in large language models (LLMs), this approach has gained even more interest (Khan Academy, 2023). However their ability to remediate is yet to be evaluated. Prior work suggests several shortcomings with LLMs, including lacking reliable subject and pedagogical knowledge (Frieder et al., 2023; Wang and Demszky, 2023; Singer, 2023), that can be mitigated using explicitly thought processes such as through chain-of-thought prompting (Wei et al., 2022).

To address these challenges, our work makes several key contributions. First, we build Bridge , a method that leverages cognitive task analysis to elicit the latent thought processes of experts . We apply Bridge to remediation where we collaborate extensively with experienced math educators to translate their thought process into a decisionmaking model. Bridge breaks down the experts' thought process: illustrated in Figure 1, Step A is to infer the student's error (e.g., the student guessed); Step B is to determine the remediation strategy (e.g., provide a solution approach); and Step C is

Figure 1: 1 ⃝ Closing the knowledge gap at scale. LLMs and novice tutors lack the pedagogical knowledge to engage with student mistakes, yet they are readily available for 1:1 tutoring. Experts like experienced teachers have the pedagogical knowledge, but are hard to scale. 2 ⃝ How do we model the expert's thought process? Our work builds Bridge which leverage cognitive task analysis to translate the latent thought process of experts into a decision-making model. 3 ⃝ Applying Bridge with LLMs. To bridge the knowledge gap, we scale the expert's knowledge with LLMs using the expert-guided decision-making model.

<!-- image -->

to identify the strategy intention (e.g., to help the student understand the concept).

We construct a dataset of real-world tutoring conversations, annotated with expert decisions and responses. Our open-source dataset consists of 700 real tutoring sessions conducted with 1st5th grade students in Title I schools, predominantly serving low-income students of color. Following FERPA guidelines, our study is IRB-approved and conducts secondary data analysis based on our Data Use Agreement with the tutoring provider and school district.

We conduct a thorough human evaluation to compare the expert, novice and LLMs in remediation . To our knowledge, our work is the first to assess the performance of LLMs such as GPT4 and instruct-tuned Llama-2-70b on remediating student mistakes. We find that the response quality of LLMs significantly improve with the expert's decision-making process: Response from GPT4 with expert- and self-generated decisions are 76-88% more preferred than GPT4 without. Context-sensitive decisions are also critical to closing the knowledge gap: Random decisions decrease GPT4's response quality -67% than expert decisions. Complementing our quantitative analy-

 our lexical analysis reveals that novices and LLMs without the expert's decision-making process engage superficially with student's problemsolving process : They give away the answer or prompt the student to re-attempt without further guidance (""double check"", ""try again"").
 Introduction Human tutoring plays a critical role in accelerating student learning, and is one of the primary ways to combat pandemic-related learning losses (Fryer Jr and Howard-Noveck, 2020; Nickow et al., 2020; Robinson and Loeb, 2021; of Education, 2021; Accelerator, 2022). To accommodate the growing demand for tutoring, many tutoring providers engage novice tutors. While novice tutors may exercise

the domain knowledge, they often lack the specialized training of professional educators in interacting with students. However, research suggests that novices with proper training can be effective tutors (Nickow et al., 2020).

Responding to student mistakes in real-time is a critical area where novice tutors tend to struggle. Mistakes are prime learning opportunities to address misconceptions (Boaler, 2013), but effective responses involve pedagogical expertise in engaging with student's thinking and building positive rapport (Roorda et al., 2011; Pianta, 2016; Shaughnessy et al., 2021; Robinson, 2022). Novices typically learn from experts to understand the expert's thought process however hiring experienced educators to provide timely feedback is resourceintensive (Kraft et al., 2018; Kelly et al., 2020).

One potential solution is the use of automated tutors (Graesser et al., 2004). With recent advances in large language models (LLMs), this approach has gained even more interest (Khan Academy, 2023). However their ability to remediate is yet to be evaluated. Prior work suggests several shortcomings with LLMs, including lacking reliable subject and pedagogical knowledge (Frieder et al., 2023; Wang and Demszky, 2023; Singer, 2023), that can be mitigated using explicitly thought processes such as through chain-of-thought prompting (Wei et al., 2022).

To address these challenges, our work makes several key contributions. First, we build Bridge , a method that leverages cognitive task analysis to elicit the latent thought processes of experts . We apply Bridge to remediation where we collaborate extensively with experienced math educators to translate their thought process into a decisionmaking model. Bridge breaks down the experts' thought process: illustrated in Figure 1, Step A is to infer the student's error (e.g., the student guessed); Step B is to determine the remediation strategy (e.g., provide a solution approach); and Step C is

Figure 1: 1 ⃝ Closing the knowledge gap at scale. LLMs and novice tutors lack the pedagogical knowledge to engage with student mistakes, yet they are readily available for 1:1 tutoring. Experts like experienced teachers have the pedagogical knowledge, but are hard to scale. 2 ⃝ How do we model the expert's thought process? Our work builds Bridge which leverage cognitive task analysis to translate the latent thought process of experts into a decision-making model. 3 ⃝ Applying Bridge with LLMs. To bridge the knowledge gap, we scale the expert's knowledge with LLMs using the expert-guided decision-making model.

<!-- image -->

to identify the strategy intention (e.g., to help the student understand the concept).

We construct a dataset of real-world tutoring conversations, annotated with expert decisions and responses. Our open-source dataset consists of 700 real tutoring sessions conducted with 1st5th grade students in Title I schools, predominantly serving low-income students of color. Following FERPA guidelines, our study is IRB-approved and conducts secondary data analysis based on our Data Use Agreement with the tutoring provider and school district.

We conduct a thorough human evaluation to compare the expert, novice and LLMs in remediation . To our knowledge, our work is the first to assess the performance of LLMs such as GPT4 and instruct-tuned Llama-2-70b on remediating student mistakes. We find that the response quality of LLMs significantly improve with the expert's decision-making process: Response from GPT4 with expert- and self-generated decisions are 76-88% more preferred than GPT4 without. Context-sensitive decisions are also critical to closing the knowledge gap: Random decisions decrease GPT4's response quality -67% than expert decisions. Complementing our quantitative analy-

 our lexical analysis reveals that novices and LLMs without the expert's decision-making process engage superficially with student's problemsolving process : They give away the answer or prompt the student to re-attempt without further guidance (""double check"", ""try again"")."
https://arxiv.org/pdf/2305.10601,https://arxiv.org/pdf/2201.11903,enabling large language model reasoning via prompting,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"{ ""similarities"": [""Both papers focus on improving language model performance on reasoning tasks through novel prompting methods."", ""Both papers leverage large language models (Author 0: GPT-4, Author 1: PaLM 540B) to demonstrate their approaches."", ""Both papers emphasize the importance of few-shot learning and efficient adaptation in language models.""], ""differences"": [""**Novelty of Contributions**: Author 0 introduces a more comprehensive framework (Tree of Thoughts) that generalizes and extends the chain-of-thought approach, whereas Author 1 focuses solely on chain-of-thought prompting as a novel method."", ""**Framework vs. Prompting Method**: Author 0 develops a flexible framework (ToT) supporting various levels of thoughts, thought generation, and search algorithms, whereas Author 1 presents a specific prompting structure (triplet prompt) for few-shot learning."", ""**Evaluation Scope and Tasks**: Author 0 evaluates ToT on three novel tasks (Game of 24, Creative Writing, Mini Crosswords) with a broader focus on planning and search, whereas Author 1 primarily focuses on arithmetic, commonsense, and symbolic reasoning benchmarks (including GSM8K)."", ""**Inspiration and Theoretical Roots**: Author 0 draws inspiration from human cognition's dual-process models and problem-solving as search, whereas Author 1 does not explicitly mention such theoretical foundations.""], ""conclusion"": ""While both papers contribute to enhancing language model reasoning capabilities, Author 0's Tree of Thoughts framework offers a more holistic, adaptable approach that builds upon and extends the chain-of-thought concept introduced by Author 1. Author 0's work is novel in its comprehensive framework, broader evaluation scope, and explicit connection to cognitive science and AI roots. In contrast, Author 1's chain-of-thought prompting, though effective, is more specialized and focused on a specific prompting structure for few-shot learning. The two papers complement each other, with Author 0 providing a foundational framework that could potentially incorporate or enhance Author 1's prompting method for even more effective language model reasoning."" }","Title: Tree of Thoughts: Deliberate Problem Solving with Large Language Models Abstract: Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, ""Tree of Thoughts"" (ToT), which generalizes over the popular ""Chain of Thought"" approach to prompting language models, and enables exploration over coherent units of text (""thoughts"") that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm .
 Introduction Originally designed to generate text, scaled-up versions of language models (LMs) such as GPT [25, 26, 1, 23] and PaLM [5] have been shown to be increasingly capable of performing an ever wider range of tasks requiring mathematical, symbolic, commonsense, and knowledge reasoning. It is perhaps surprising that underlying all this progress is still the original autoregressive mechanism for generating text, which makes token-level decisions one by one and in a left-to-right fashion. Is such a simple mechanism sufficient for a LM to be built toward a general problem solver? If not, what problems would challenge the current paradigm, and what should be alternative mechanisms?

The literature on human cognition provides some clues to answer these questions. Research on ""dual process"" models suggests that people have two modes in which they engage with decisions - a fast, automatic, unconscious mode (""System 1"") and a slow, deliberate, conscious mode (""System 2"") [30, 31, 16, 15]. These two modes have previously been connected to a variety of mathematical models used in machine learning. For example, research on reinforcement learning in humans and other animals has explored the circumstances under which they engage in associative ""model free"" learning or more deliberative ""model based"" planning [7]. The simple associative token-level choices of LMs are also reminiscent of ""System 1"", and thus might benefit from augmentation by a more deliberate ""System 2"" planning process that (1) maintains and explores diverse alternatives for current

j ũ Ɯ Ŕ ũ Ɯ

j ũ Ɯ Ŕ ũ Ɯ

ʱ Ê ʲ ˤ G j

ʱ æ ʲ ˤ ĵ É

a Ê Ġ ĵ ŗ Ɠ Ť Ɔ ˤ ſ ĵ Ť ò

ˤ j ũ Ɯ Ŕ ũ Ɯ

ʱ ç ʲ ˤ ĵ É ˁ 

ˤ j ũ Ɯ Ŕ ũ Ɯ

ʱ í ʲ ˤ É ĵ É ˤ ʱ ĵ ũ ŗ ŝ ʲ

Figure 1: Schematic illustrating various approaches to problem solving with LLMs. Each rectangle box represents a thought , which is a coherent language sequence that serves as an intermediate step toward problem solving. See concrete examples of how thoughts are generated, evaluated, and searched in Figures 2,4,6.

<!-- image -->

choices instead of just picking one, and (2) evaluates its current status and actively looks ahead or backtracks to make more global decisions.

To design such a planning process, we return to the origins of artificial intelligence (and cognitive science), drawing inspiration from the planning processes explored by Newell, Shaw, and Simon starting in the 1950s [21, 22]. Newell and colleagues characterized problem solving [21] as search through a combinatorial problem space, represented as a tree. We thus propose the Tree of Thoughts (ToT) framework for general problem solving with language models. As Figure 1 illustrates, while existing methods (detailed below) sample continuous language sequences for problem solving, ToT actively maintains a tree of thoughts, where each thought is a coherent language sequence that serves as an intermediate step toward problem solving (Table 1). Such a high-level semantic unit allows the LM to self-evaluate the progress different intermediate thoughts make towards solving the problem through a deliberate reasoning process that is also instantiated in language (Figures 2,4,6). This implementation of search heuristics via LM self-evaluation and deliberation is novel, as previous search heuristics are either programmed or learned. Finally, we combine this language-based capability to generate and evaluate diverse thoughts with search algorithms, such as breadth-first search (BFS) or depth-first search (DFS), which allow systematic exploration of the tree of thoughts with lookahead and backtracking.

Empirically, we propose three new problems that challenge existing LM inference methods even with the state-of-the-art language model, GPT-4 [23]: Game of 24, Creative Writing, and Crosswords (Table 1). These tasks require deductive, mathematical, commonsense, lexical reasoning abilities, and a way to incorporate systematic planning or search. We show ToT obtains superior results on all three tasks by being general and flexible enough to support different levels of thoughts, different ways to generate and evaluate thoughts, and different search algorithms that adapt to the nature of different problems. We also analyze how such choices affect model performances via systematic ablations and discuss future directions to better train and use LMs.","Title: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Abstract: The NLP landscape has recently been revolutionized by language models (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020, inter alia ). Scaling up the size of language models has been shown to confer a range of benefits, such as improved performance and sample efficiency (Kaplan et al., 2020; Brown et al., 2020, inter alia ). However, scaling up model size alone has not proved sufficient for achieving high performance on challenging tasks such as arithmetic, commonsense, and symbolic reasoning (Rae et al., 2021).

This work explores how the reasoning ability of large language models can be unlocked by a simple method motivated by two ideas. First, techniques for arithmetic reasoning can benefit from generating natural language rationales that lead to the final answer. Prior work has given models the ability to generate natural language intermediate steps by training from scratch (Ling et al., 2017) or finetuning a pretrained model (Cobbe et al., 2021), in addition to neuro-symbolic methods that use formal languages instead of natural language (Roy and Roth, 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Second, large language models offer the exciting

- Finetuned GPT-3 175B
- Prior best
- PaLM 540B: standard prompting
- PaLM 540B: chain-of-thought prompting

Figure 2: PaLM 540B uses chain-ofthought prompting to achieve new stateof-the-art performance on the GSM8K benchmark of math word problems. Finetuned GPT-3 and prior best are from Cobbe et al. (2021).

<!-- image -->

prospect of in-context few-shot learning via prompting . That is, instead of finetuning a separate language model checkpoint for each new task, one can simply ""prompt"" the model with a few input-output exemplars demonstrating the task. Remarkably, this has been successful for a range of simple question-answering tasks (Brown et al., 2020).

Both of the above ideas, however, have key limitations. For rationale-augmented training and finetuning methods, it is costly to create a large set of high quality rationales, which is much more complicated than simple input-output pairs used in normal machine learning. For the traditional fewshot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning abilities, and often does not improve substantially with increasing language model scale (Rae et al., 2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations. Specifically, we explore the ability of language models to perform few-shot prompting for reasoning tasks, given a prompt that consists of triples: 〈 input, chain of thought , output 〉 . A chain of thought is a series of intermediate natural language reasoning steps that lead to the final output, and we refer to this approach as chain-of-thought prompting . An example prompt is shown in Figure 1.

We present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks, showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree. Figure 2 illustrates one such result-on the GSM8K benchmark of math word problems (Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting by a large margin and achieves new state-of-the-art performance. A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of generality. This work underscores how large language models can learn via a few examples with natural language data about the task (c.f. automatically learning the patterns underlying inputs and outputs via a large training dataset).
 Introduction The NLP landscape has recently been revolutionized by language models (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020, inter alia ). Scaling up the size of language models has been shown to confer a range of benefits, such as improved performance and sample efficiency (Kaplan et al., 2020; Brown et al., 2020, inter alia ). However, scaling up model size alone has not proved sufficient for achieving high performance on challenging tasks such as arithmetic, commonsense, and symbolic reasoning (Rae et al., 2021).

This work explores how the reasoning ability of large language models can be unlocked by a simple method motivated by two ideas. First, techniques for arithmetic reasoning can benefit from generating natural language rationales that lead to the final answer. Prior work has given models the ability to generate natural language intermediate steps by training from scratch (Ling et al., 2017) or finetuning a pretrained model (Cobbe et al., 2021), in addition to neuro-symbolic methods that use formal languages instead of natural language (Roy and Roth, 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Second, large language models offer the exciting

- Finetuned GPT-3 175B
- Prior best
- PaLM 540B: standard prompting
- PaLM 540B: chain-of-thought prompting

Figure 2: PaLM 540B uses chain-ofthought prompting to achieve new stateof-the-art performance on the GSM8K benchmark of math word problems. Finetuned GPT-3 and prior best are from Cobbe et al. (2021).

<!-- image -->

prospect of in-context few-shot learning via prompting . That is, instead of finetuning a separate language model checkpoint for each new task, one can simply ""prompt"" the model with a few input-output exemplars demonstrating the task. Remarkably, this has been successful for a range of simple question-answering tasks (Brown et al., 2020).

Both of the above ideas, however, have key limitations. For rationale-augmented training and finetuning methods, it is costly to create a large set of high quality rationales, which is much more complicated than simple input-output pairs used in normal machine learning. For the traditional fewshot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning abilities, and often does not improve substantially with increasing language model scale (Rae et al., 2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations. Specifically, we explore the ability of language models to perform few-shot prompting for reasoning tasks, given a prompt that consists of triples: 〈 input, chain of thought , output 〉 . A chain of thought is a series of intermediate natural language reasoning steps that lead to the final output, and we refer to this approach as chain-of-thought prompting . An example prompt is shown in Figure 1.

We present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks, showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree. Figure 2 illustrates one such result-on the GSM8K benchmark of math word problems (Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting by a large margin and achieves new state-of-the-art performance. A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of generality. This work underscores how large language models can learn via a few examples with natural language data about the task (c.f. automatically learning the patterns underlying inputs and outputs via a large training dataset)."
