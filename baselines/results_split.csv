focus_paper,opp_paper,topic,title_focus,title_opp,summary,document_f,document_o
https://arxiv.org/pdf/2406.11709,https://arxiv.org/pdf/2310.10648,helping students fix their errors,"Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging",Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes," Comparative Summary: **Similarities:**

*   **Focus on Personalized Learning**: Both papers emphasize the importance of tailoring educational experiences to individual students' needs, whether through Socratic questioning (TreeInstruct) or expert-guided decision-making for remediation (Bridge).
*   **Utilization of Advanced AI Techniques**: Both TreeInstruct and Bridge leverage cutting-edge AI methodologies, such as dynamic tree-based questioning and cognitive task analysis, respectively, to enhance educational outcomes.
*   **Emphasis on Critical Thinking and Problem-Solving**: Both approaches aim to foster deeper learning by encouraging students to engage critically with the material, either through Socratic questioning or by addressing the root causes of errors.

**Differences:**

1.  **Methodological Approach**:
    *   **TreeInstruct**: Focuses on state space estimation and dynamic tree-based Socratic questioning for adaptive learning.
    *   **Bridge**: Employs cognitive task analysis to model expert educators' decision-making processes for remediation.
2.  **Educational Context**:
    *   **TreeInstruct**: Primarily geared towards debugging and coding education.
    *   **Bridge**: Applies to a broader range of subjects, with a specific focus on math and reading for 1st-5th grade students.
3.  **Evaluation Metrics and Outcomes**:
    *   **TreeInstruct**: Quantitative evaluation shows outperformance on benchmarks, with qualitative expert evaluation preferring its Socratic questioning.
    *   **Bridge**: Highlights the importance of expert guidance in LLMs for remediation, with a focus on the scalability and accessibility of its approach, particularly for underserved populations.
4.  **Dataset and Reproducibility**:
    *   **TreeInstruct**: Contributes a multi-bug debugging dataset and releases its source code.
    *   **Bridge**: Provides an open-source dataset of real tutoring sessions, emphasizing the value of transparency and reproducibility in educational AI research. </paper2> Comparative Summary: **Similarities:**

*   **Focus on Personalized Learning**: Both papers emphasize the importance of tailoring educational experiences to individual students' needs, whether through Socratic questioning (TreeInstruct) or expert-guided decision-making for remediation (Bridge).
*   **Utilization of Advanced AI Techniques**: Both TreeInstruct and Bridge leverage cutting-edge AI methodologies, such as dynamic tree-based questioning and cognitive task analysis, respectively, to enhance educational outcomes.
*   **Emphasis on Critical Thinking and Problem-Solving**: Both approaches aim to foster deeper learning by encouraging students to engage critically with the material, either through Socratic questioning or by addressing the root causes of errors.

**Differences:**

1.  **Methodological Approach**:
    *   **TreeInstruct**: Focuses on state space estimation and dynamic tree-based Socratic questioning for adaptive learning.
    *   **Bridge**: Employs cognitive task analysis to model expert educators' decision-making processes for remediation.
2.  **Educational Context**:
    *   **TreeInstruct**: Primarily geared towards debugging and coding education.
    *   **Bridge**: Applies to a broader range of subjects, with a specific focus on math and reading for 1st-5th grade students.
3.  **Evaluation Metrics and Outcomes**:
    *   **TreeInstruct**: Quantitative evaluation shows outperformance on benchmarks, with qualitative expert evaluation preferring its Socratic questioning.
    *   **Bridge**: Highlights the importance of expert guidance in LLMs for remediation, with a focus on the scalability and accessibility of its approach, particularly for underserved populations.
4.  **Dataset and Reproducibility**:
    *   **TreeInstruct**: Contributes a multi-bug debugging dataset and releases its source code.
    *   **Bridge**: Provides an open-source dataset of real tutoring sessions, emphasizing the value of transparency and reproducibility in educational AI research. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous Evaluation and Improvement**: Regularly assessing the effectiveness of these AI-driven educational tools in real-world settings to inform future enhancements. **Implications and Future Directions:**

*   **Integration of Approaches**: Exploring the potential of combining TreeInstruct's adaptive Socratic questioning with Bridge's expert-guided decision-making for a more comprehensive learning platform.
*   **Broadening Educational Contexts**: Adapting both approaches for various subjects and age groups to maximize their impact on personalized learning.
*   **Continuous","Title: Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging Abstract: Socratic questioning is an effective teaching strategy, encouraging critical thinking and problem-solving. The conversational capabilities of large language models (LLMs) show great potential for providing scalable, real-time student guidance. However, current LLMs often give away solutions directly, making them ineffective instructors. We tackle this issue in the code debugging domain with TreeInstruct , an Instructor agent guided by a novel state space-based planning algorithm. TreeInstruct asks probing questions to help students independently identify and resolve errors. It estimates a student's conceptual and syntactical knowledge to dynamically construct a question tree based on their responses and current knowledge state, effectively addressing both independent and dependent mistakes concurrently in a multi-turn interaction setting. In addition to using an existing single-bug debugging benchmark, we construct a more challenging multi-bug dataset of 150 coding problems, incorrect solutions, and bug fixes- all carefully constructed and annotated by experts. Extensive evaluation shows TreeInstruct's state-ofthe-art performance on both datasets, proving it to be a more effective instructor than baselines. Furthermore, a real-world case study with five students of varying skill levels further demonstrates TreeInstruct's ability to guide students to debug their code efficiently with minimal turns and highly Socratic questioning.
 Introduction With the rapidly expanding conversational and reasoning abilities of large language models (LLMs), there has been a substantial rise in demand for exploiting their capabilities within a multitude of educational applications (Kasneci et al., 2023) in order to widen accessibility via personalized feedback. Specifically, several recent works explore

Figure 1: The Instructor's goal is to generate multi-turn Socratic questions while guiding the Student towards the correct solution.

<!-- image -->

the use of LLMs for providing feedback and guidance to students (Wang et al., 2023; Kazemitabaar et al., 2024; Sheese et al., 2024; Lyu et al., 2024). However, LLMs are typically optimized to generate customer-serving, assistant-like responses, which also translates into the types of questions asked. Especially for educational domains, this style of questioning can be suboptimal (Cotton, 1988; Sahamid, 2016; Yang et al., 2005; Wilson, 1987). For instance, if a student is seeking help from an instructor for correcting their mistakes (e.g., debugging their buggy code), we consider two forms of potential responses: assistant-like and instructor-like . As shown in Figure 1, an assistant-like response would not be a successful educational interaction, as it leads to the Assistant directly providing an answer. On the other hand, an Instructor-like response reflects the educational philosophy of Socratic questioning .

Socratic questioning is a teaching strategy where the Student independently solves their problem by answering guiding questions, instead of being

given the solution directly (Wilson, 1987). This is a more effective learning strategy because the weight of learning falls on the Student as they must put in effort to answer a question as opposed to solely relying on the model (Cotton, 1988; Kasneci et al., 2023). Therefore, we aim to re-orient an LLM to be an Instructor, not an assistant, by asking Socratic questions that (1) help the Student understand their mistakes, and (2) do not directly provide the answer. To tackle these challenges, we propose TreeInstruct based on the following principles:

- 1. State space estimation: An Instructor plans its conversation with a Student based on the ""distance"" between their initial answer and the optimal, correct answer within the estimated state space. In other words, it tracks the knowledge state of the Student within this space throughout the Instructor-Student interactions.
- 2. Tree-based Socratic questioning: An Instructor generates turn-level Socratic questions conditioned on both the Student's current knowledge state and misunderstanding(s), the latter derived from their responses to the Instructor's questions. This step dynamically constructs a Socratic question tree.
- 3. Adaptive conversation restructuring: An Instructor updates their initial conversation plan based on how the Student is progressing in the conversation, as reflected by updates (or lack thereof) to the Student's knowledge state. This planning can include both questioning and teaching actions.

While these principles can apply to many educational domains, this paper focuses on code debugging, which presents unique challenges. Realworld code debugging often involves multiple, potentially interdependent conceptual and syntactical bugs. For instance, Figure 1 shows that first resolving the Student's conceptual misunderstanding of recursion in Fibonacci helps them identify their recursive syntactical bug (Figure 1). However, existing work fails to account for such nuances and assumes single-turn feedback (Kazemitabaar et al., 2024; Wang et al., 2023; Lyu et al., 2024). This ignores the sub-steps required for the Student to understand each bug.

In contrast, TreeInstruct constructs a multi-turn debugging plan ( state representation ), defined as the set of Student misunderstandings and mistakes ( state variables ) to be resolved in order to comprehend and correct their bug(s). We define all

potential paths to complete these tasks as the state space . We traverse the space using Socratic questions and trace which variables have been resolved, grounded based on the Student's responses.

While existing LLM-based tutors are effective in fixing the Student's code with high success, they are either prone to directly revealing code answers or cannot be adapted to new Student responses. For example, CodeAid (Kazemitabaar et al., 2024) (specifically, the ""Help Fix Code"" and ""Question from Code"" modules, as these are most similar to our setting) directly provides code or pseudocode 57% of the time, and achieves a mere 55% rate of helpfulness. On the other hand, TreeInstruct exploits the state space to dynamically construct a tree of questions based on (1) incorrect Student responses, or (2) gaps in the Student's knowledge. The sibling and parent-child relationships between questions reflect the manner in which they traverse the state space. Finally, it exploits both the Student's knowledge state and any proposed bug fixes to serve as the dynamic stopping condition. Overall, TreeInstruct takes a more structured approach to multi-turn conversational feedback, as (1) grounding the conversation on the state space representation ensures that all bugs are sufficiently addressed, and (2) constructing a tree based on the Student's current level of understanding allows for more relevant and personalized question generation.

We summarize our contributions below:

- · To the best of our knowledge, TreeInstruct is the first work to explore state space estimation and dynamic tree-based questioning for multi-turn Socratic instruction.
- · We construct a novel multi-bug debugging dataset with 150 expert-annotated, challenging conceptual and syntactical bugs and their fixes.
- · Extensive experiments on an existing benchmark and our constructed dataset demonstrate that TreeInstruct can be universally applied to both open and closed source-settings. We also showcase that TreeInstruct's strong Socratic questioning abilities widely outperform all baselines through both (1) rigorous quantitative and qualitative expert evaluation (on average, preferred 78.43% of the time; Student fixes code 24.55% more ) and (2) real-world interactions with students of varying coding abilities.

Reproducibility: We release our data and source code 1 to facilitate further studies.","Title: Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes Abstract: Human tutoring plays a critical role in accelerating student learning, and is one of the primary ways to combat pandemic-related learning losses (Fryer Jr and Howard-Noveck, 2020; Nickow et al., 2020; Robinson and Loeb, 2021; of Education, 2021; Accelerator, 2022). To accommodate the growing demand for tutoring, many tutoring providers engage novice tutors. While novice tutors may exercise

the domain knowledge, they often lack the specialized training of professional educators in interacting with students. However, research suggests that novices with proper training can be effective tutors (Nickow et al., 2020).

Responding to student mistakes in real-time is a critical area where novice tutors tend to struggle. Mistakes are prime learning opportunities to address misconceptions (Boaler, 2013), but effective responses involve pedagogical expertise in engaging with student's thinking and building positive rapport (Roorda et al., 2011; Pianta, 2016; Shaughnessy et al., 2021; Robinson, 2022). Novices typically learn from experts to understand the expert's thought process however hiring experienced educators to provide timely feedback is resourceintensive (Kraft et al., 2018; Kelly et al., 2020).

One potential solution is the use of automated tutors (Graesser et al., 2004). With recent advances in large language models (LLMs), this approach has gained even more interest (Khan Academy, 2023). However their ability to remediate is yet to be evaluated. Prior work suggests several shortcomings with LLMs, including lacking reliable subject and pedagogical knowledge (Frieder et al., 2023; Wang and Demszky, 2023; Singer, 2023), that can be mitigated using explicitly thought processes such as through chain-of-thought prompting (Wei et al., 2022).

To address these challenges, our work makes several key contributions. First, we build Bridge , a method that leverages cognitive task analysis to elicit the latent thought processes of experts . We apply Bridge to remediation where we collaborate extensively with experienced math educators to translate their thought process into a decisionmaking model. Bridge breaks down the experts' thought process: illustrated in Figure 1, Step A is to infer the student's error (e.g., the student guessed); Step B is to determine the remediation strategy (e.g., provide a solution approach); and Step C is

Figure 1: 1 ⃝ Closing the knowledge gap at scale. LLMs and novice tutors lack the pedagogical knowledge to engage with student mistakes, yet they are readily available for 1:1 tutoring. Experts like experienced teachers have the pedagogical knowledge, but are hard to scale. 2 ⃝ How do we model the expert's thought process? Our work builds Bridge which leverage cognitive task analysis to translate the latent thought process of experts into a decision-making model. 3 ⃝ Applying Bridge with LLMs. To bridge the knowledge gap, we scale the expert's knowledge with LLMs using the expert-guided decision-making model.

<!-- image -->

to identify the strategy intention (e.g., to help the student understand the concept).

We construct a dataset of real-world tutoring conversations, annotated with expert decisions and responses. Our open-source dataset consists of 700 real tutoring sessions conducted with 1st5th grade students in Title I schools, predominantly serving low-income students of color. Following FERPA guidelines, our study is IRB-approved and conducts secondary data analysis based on our Data Use Agreement with the tutoring provider and school district.

We conduct a thorough human evaluation to compare the expert, novice and LLMs in remediation . To our knowledge, our work is the first to assess the performance of LLMs such as GPT4 and instruct-tuned Llama-2-70b on remediating student mistakes. We find that the response quality of LLMs significantly improve with the expert's decision-making process: Response from GPT4 with expert- and self-generated decisions are 76-88% more preferred than GPT4 without. Context-sensitive decisions are also critical to closing the knowledge gap: Random decisions decrease GPT4's response quality -67% than expert decisions. Complementing our quantitative analy-

 our lexical analysis reveals that novices and LLMs without the expert's decision-making process engage superficially with student's problemsolving process : They give away the answer or prompt the student to re-attempt without further guidance (""double check"", ""try again"").
 Introduction Human tutoring plays a critical role in accelerating student learning, and is one of the primary ways to combat pandemic-related learning losses (Fryer Jr and Howard-Noveck, 2020; Nickow et al., 2020; Robinson and Loeb, 2021; of Education, 2021; Accelerator, 2022). To accommodate the growing demand for tutoring, many tutoring providers engage novice tutors. While novice tutors may exercise

the domain knowledge, they often lack the specialized training of professional educators in interacting with students. However, research suggests that novices with proper training can be effective tutors (Nickow et al., 2020).

Responding to student mistakes in real-time is a critical area where novice tutors tend to struggle. Mistakes are prime learning opportunities to address misconceptions (Boaler, 2013), but effective responses involve pedagogical expertise in engaging with student's thinking and building positive rapport (Roorda et al., 2011; Pianta, 2016; Shaughnessy et al., 2021; Robinson, 2022). Novices typically learn from experts to understand the expert's thought process however hiring experienced educators to provide timely feedback is resourceintensive (Kraft et al., 2018; Kelly et al., 2020).

One potential solution is the use of automated tutors (Graesser et al., 2004). With recent advances in large language models (LLMs), this approach has gained even more interest (Khan Academy, 2023). However their ability to remediate is yet to be evaluated. Prior work suggests several shortcomings with LLMs, including lacking reliable subject and pedagogical knowledge (Frieder et al., 2023; Wang and Demszky, 2023; Singer, 2023), that can be mitigated using explicitly thought processes such as through chain-of-thought prompting (Wei et al., 2022).

To address these challenges, our work makes several key contributions. First, we build Bridge , a method that leverages cognitive task analysis to elicit the latent thought processes of experts . We apply Bridge to remediation where we collaborate extensively with experienced math educators to translate their thought process into a decisionmaking model. Bridge breaks down the experts' thought process: illustrated in Figure 1, Step A is to infer the student's error (e.g., the student guessed); Step B is to determine the remediation strategy (e.g., provide a solution approach); and Step C is

Figure 1: 1 ⃝ Closing the knowledge gap at scale. LLMs and novice tutors lack the pedagogical knowledge to engage with student mistakes, yet they are readily available for 1:1 tutoring. Experts like experienced teachers have the pedagogical knowledge, but are hard to scale. 2 ⃝ How do we model the expert's thought process? Our work builds Bridge which leverage cognitive task analysis to translate the latent thought process of experts into a decision-making model. 3 ⃝ Applying Bridge with LLMs. To bridge the knowledge gap, we scale the expert's knowledge with LLMs using the expert-guided decision-making model.

<!-- image -->

to identify the strategy intention (e.g., to help the student understand the concept).

We construct a dataset of real-world tutoring conversations, annotated with expert decisions and responses. Our open-source dataset consists of 700 real tutoring sessions conducted with 1st5th grade students in Title I schools, predominantly serving low-income students of color. Following FERPA guidelines, our study is IRB-approved and conducts secondary data analysis based on our Data Use Agreement with the tutoring provider and school district.

We conduct a thorough human evaluation to compare the expert, novice and LLMs in remediation . To our knowledge, our work is the first to assess the performance of LLMs such as GPT4 and instruct-tuned Llama-2-70b on remediating student mistakes. We find that the response quality of LLMs significantly improve with the expert's decision-making process: Response from GPT4 with expert- and self-generated decisions are 76-88% more preferred than GPT4 without. Context-sensitive decisions are also critical to closing the knowledge gap: Random decisions decrease GPT4's response quality -67% than expert decisions. Complementing our quantitative analy-

 our lexical analysis reveals that novices and LLMs without the expert's decision-making process engage superficially with student's problemsolving process : They give away the answer or prompt the student to re-attempt without further guidance (""double check"", ""try again"")."
https://arxiv.org/pdf/2305.10601,https://arxiv.org/pdf/2201.11903,enabling large language model reasoning via prompting,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models," **Comparative Summary** 

**Similarities:**

*   **Reasoning and Problem-Solving Focus**: Both papers concentrate on enhancing language models' reasoning and problem-solving capabilities, albeit through different methodologies.
*   **Innovative Prompting Techniques**: Both ToT and Chain-of-Thought Prompting introduce novel prompting methods designed to improve language model performance on complex tasks.
*   **Empirical Evaluations and State-of-the-Art Performance**: Both papers present comprehensive evaluations and achieve state-of-the-art performance on specific benchmarks (Game of 24 for ToT, GSM8K for Chain-of-Thought Prompting).

**Differences:**

1.  **Approach to Reasoning**:
    *   **ToT**: Emphasizes deliberate, multi-step problem-solving with a focus on exploring coherent units of text (""thoughts"") and self-evaluation.
    *   **Chain-of-Thought Prompting**: Centers around a specific prompt structure that includes intermediate reasoning steps to facilitate effective few-shot learning.
2.  **Framework vs. Prompting Method**:
    *   **ToT**: Presents a more comprehensive framework that can be adapted to various tasks and incorporates multiple reasoning paths and search heuristics.
    *   **Chain-of-Thought Prompting**: Focuses on a prompting technique that can be applied to existing large language models to enhance their reasoning capabilities.
3.  **Evaluation Tasks and Models**:
    *   **ToT**: Evaluated on Game of 24, Creative Writing, and Mini Crosswords with a custom setup.
    *   **Chain-of-Thought Prompting**: Assessed on arithmetic, commonsense, and symbolic reasoning benchmarks (including GSM8K) using PaLM 540B.
4.  **Inspiration and Theoretical Foundations**:
    *   **ToT**: Draws inspiration from human cognition's ""dual process"" models and problem-solving as search through a combinatorial problem space.
    *   **Chain-of-Thought Prompting**: Combines the strengths of rationale-augmented training and few-shot prompting without explicit reference to cognitive science theories. </paper2> **Comparative Summary** 

**Similarities:**

*   **Reasoning and Problem-Solving Focus**: Both papers concentrate on enhancing language models' reasoning and problem-solving capabilities, albeit through different methodologies.
*   **Innovative Prompting Techniques**: Both ToT and Chain-of-Thought Prompting introduce novel prompting methods designed to improve language model performance on complex tasks.
*   **Empirical Evaluations and State-of-the-Art Performance**: Both papers present comprehensive evaluations and achieve state-of-the-art performance on specific benchmarks (Game of 24 for ToT, GSM8K for Chain-of-Thought Prompting).

**Differences:**

1.  **Approach to Reasoning**:
    *   **ToT**: Emphasizes deliberate, multi-step problem-solving with a focus on exploring coherent units of text (""thoughts"") and self-evaluation.
    *   **Chain-of-Thought Prompting**: Centers around a specific prompt structure that includes intermediate reasoning steps to facilitate effective few-shot learning.
2.  **Framework vs. Prompting Method**:
    *   **ToT**: Presents a more comprehensive framework that can be adapted to various tasks and incorporates multiple reasoning paths and search heuristics.
    *   **Chain-of-Thought Prompting**: Focuses on a prompting technique that can be applied to existing large language models to enhance their reasoning capabilities.
3.  **Evaluation Tasks and Models**:
    *   **ToT**: Evaluated on Game of 24, Creative Writing, and Mini Crosswords with a custom setup.
    *   **Chain-of-Thought Prompting**: Assessed on arithmetic, commonsense, and symbolic reasoning benchmarks (including GSM8K) using PaLM 540B.
4.  **Inspiration and Theoretical Foundations**:
    *   **ToT**: Draws inspiration from human cognition's ""dual process"" models and problem-solving as search through a combinatorial problem space.
    *   **Chain-of-Thought Prompting**: Combines the strengths of rationale-augmented training and few-shot prompting without explicit reference to cognitive science theories. 

**Future Directions for Integration or Comparison:**

1.  **Combining ToT with Chain-of-Thought Prompting**: Investigate the potential of integrating the deliberate problem-solving framework of ToT with the effective prompting structure of Chain-of-Thought Prompting for enhanced performance.
2.  **Comparative Analysis on Shared Tasks**: Evaluate both approaches on a common set of tasks to provide a direct comparison of their strengths and weaknesses.
3.  **Exploring Cognitive Science Foundations for Chain-of-Thought Prompting**: Investigate whether incorporating insights from cognitive science, similar to ToT, could further enhance the effectiveness of Chain-of-Thought Prompting. 

**Conclusion:**

Both papers contribute significantly to the advancement of language models' reasoning capabilities, each with its unique approach and strengths. A deeper integration or comparative analysis of these methodologies could lead to even more powerful tools for complex problem-solving.","Title: Tree of Thoughts: Deliberate Problem Solving with Large Language Models Abstract: Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, ""Tree of Thoughts"" (ToT), which generalizes over the popular ""Chain of Thought"" approach to prompting language models, and enables exploration over coherent units of text (""thoughts"") that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm .
 Introduction Originally designed to generate text, scaled-up versions of language models (LMs) such as GPT [25, 26, 1, 23] and PaLM [5] have been shown to be increasingly capable of performing an ever wider range of tasks requiring mathematical, symbolic, commonsense, and knowledge reasoning. It is perhaps surprising that underlying all this progress is still the original autoregressive mechanism for generating text, which makes token-level decisions one by one and in a left-to-right fashion. Is such a simple mechanism sufficient for a LM to be built toward a general problem solver? If not, what problems would challenge the current paradigm, and what should be alternative mechanisms?

The literature on human cognition provides some clues to answer these questions. Research on ""dual process"" models suggests that people have two modes in which they engage with decisions - a fast, automatic, unconscious mode (""System 1"") and a slow, deliberate, conscious mode (""System 2"") [30, 31, 16, 15]. These two modes have previously been connected to a variety of mathematical models used in machine learning. For example, research on reinforcement learning in humans and other animals has explored the circumstances under which they engage in associative ""model free"" learning or more deliberative ""model based"" planning [7]. The simple associative token-level choices of LMs are also reminiscent of ""System 1"", and thus might benefit from augmentation by a more deliberate ""System 2"" planning process that (1) maintains and explores diverse alternatives for current

j ũ Ɯ Ŕ ũ Ɯ

j ũ Ɯ Ŕ ũ Ɯ

ʱ Ê ʲ ˤ G j

ʱ æ ʲ ˤ ĵ É

a Ê Ġ ĵ ŗ Ɠ Ť Ɔ ˤ ſ ĵ Ť ò

ˤ j ũ Ɯ Ŕ ũ Ɯ

ʱ ç ʲ ˤ ĵ É ˁ 

ˤ j ũ Ɯ Ŕ ũ Ɯ

ʱ í ʲ ˤ É ĵ É ˤ ʱ ĵ ũ ŗ ŝ ʲ

Figure 1: Schematic illustrating various approaches to problem solving with LLMs. Each rectangle box represents a thought , which is a coherent language sequence that serves as an intermediate step toward problem solving. See concrete examples of how thoughts are generated, evaluated, and searched in Figures 2,4,6.

<!-- image -->

choices instead of just picking one, and (2) evaluates its current status and actively looks ahead or backtracks to make more global decisions.

To design such a planning process, we return to the origins of artificial intelligence (and cognitive science), drawing inspiration from the planning processes explored by Newell, Shaw, and Simon starting in the 1950s [21, 22]. Newell and colleagues characterized problem solving [21] as search through a combinatorial problem space, represented as a tree. We thus propose the Tree of Thoughts (ToT) framework for general problem solving with language models. As Figure 1 illustrates, while existing methods (detailed below) sample continuous language sequences for problem solving, ToT actively maintains a tree of thoughts, where each thought is a coherent language sequence that serves as an intermediate step toward problem solving (Table 1). Such a high-level semantic unit allows the LM to self-evaluate the progress different intermediate thoughts make towards solving the problem through a deliberate reasoning process that is also instantiated in language (Figures 2,4,6). This implementation of search heuristics via LM self-evaluation and deliberation is novel, as previous search heuristics are either programmed or learned. Finally, we combine this language-based capability to generate and evaluate diverse thoughts with search algorithms, such as breadth-first search (BFS) or depth-first search (DFS), which allow systematic exploration of the tree of thoughts with lookahead and backtracking.

Empirically, we propose three new problems that challenge existing LM inference methods even with the state-of-the-art language model, GPT-4 [23]: Game of 24, Creative Writing, and Crosswords (Table 1). These tasks require deductive, mathematical, commonsense, lexical reasoning abilities, and a way to incorporate systematic planning or search. We show ToT obtains superior results on all three tasks by being general and flexible enough to support different levels of thoughts, different ways to generate and evaluate thoughts, and different search algorithms that adapt to the nature of different problems. We also analyze how such choices affect model performances via systematic ablations and discuss future directions to better train and use LMs.","Title: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Abstract: The NLP landscape has recently been revolutionized by language models (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020, inter alia ). Scaling up the size of language models has been shown to confer a range of benefits, such as improved performance and sample efficiency (Kaplan et al., 2020; Brown et al., 2020, inter alia ). However, scaling up model size alone has not proved sufficient for achieving high performance on challenging tasks such as arithmetic, commonsense, and symbolic reasoning (Rae et al., 2021).

This work explores how the reasoning ability of large language models can be unlocked by a simple method motivated by two ideas. First, techniques for arithmetic reasoning can benefit from generating natural language rationales that lead to the final answer. Prior work has given models the ability to generate natural language intermediate steps by training from scratch (Ling et al., 2017) or finetuning a pretrained model (Cobbe et al., 2021), in addition to neuro-symbolic methods that use formal languages instead of natural language (Roy and Roth, 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Second, large language models offer the exciting

- Finetuned GPT-3 175B
- Prior best
- PaLM 540B: standard prompting
- PaLM 540B: chain-of-thought prompting

Figure 2: PaLM 540B uses chain-ofthought prompting to achieve new stateof-the-art performance on the GSM8K benchmark of math word problems. Finetuned GPT-3 and prior best are from Cobbe et al. (2021).

<!-- image -->

prospect of in-context few-shot learning via prompting . That is, instead of finetuning a separate language model checkpoint for each new task, one can simply ""prompt"" the model with a few input-output exemplars demonstrating the task. Remarkably, this has been successful for a range of simple question-answering tasks (Brown et al., 2020).

Both of the above ideas, however, have key limitations. For rationale-augmented training and finetuning methods, it is costly to create a large set of high quality rationales, which is much more complicated than simple input-output pairs used in normal machine learning. For the traditional fewshot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning abilities, and often does not improve substantially with increasing language model scale (Rae et al., 2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations. Specifically, we explore the ability of language models to perform few-shot prompting for reasoning tasks, given a prompt that consists of triples: 〈 input, chain of thought , output 〉 . A chain of thought is a series of intermediate natural language reasoning steps that lead to the final output, and we refer to this approach as chain-of-thought prompting . An example prompt is shown in Figure 1.

We present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks, showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree. Figure 2 illustrates one such result-on the GSM8K benchmark of math word problems (Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting by a large margin and achieves new state-of-the-art performance. A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of generality. This work underscores how large language models can learn via a few examples with natural language data about the task (c.f. automatically learning the patterns underlying inputs and outputs via a large training dataset).
 Introduction The NLP landscape has recently been revolutionized by language models (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020, inter alia ). Scaling up the size of language models has been shown to confer a range of benefits, such as improved performance and sample efficiency (Kaplan et al., 2020; Brown et al., 2020, inter alia ). However, scaling up model size alone has not proved sufficient for achieving high performance on challenging tasks such as arithmetic, commonsense, and symbolic reasoning (Rae et al., 2021).

This work explores how the reasoning ability of large language models can be unlocked by a simple method motivated by two ideas. First, techniques for arithmetic reasoning can benefit from generating natural language rationales that lead to the final answer. Prior work has given models the ability to generate natural language intermediate steps by training from scratch (Ling et al., 2017) or finetuning a pretrained model (Cobbe et al., 2021), in addition to neuro-symbolic methods that use formal languages instead of natural language (Roy and Roth, 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Second, large language models offer the exciting

- Finetuned GPT-3 175B
- Prior best
- PaLM 540B: standard prompting
- PaLM 540B: chain-of-thought prompting

Figure 2: PaLM 540B uses chain-ofthought prompting to achieve new stateof-the-art performance on the GSM8K benchmark of math word problems. Finetuned GPT-3 and prior best are from Cobbe et al. (2021).

<!-- image -->

prospect of in-context few-shot learning via prompting . That is, instead of finetuning a separate language model checkpoint for each new task, one can simply ""prompt"" the model with a few input-output exemplars demonstrating the task. Remarkably, this has been successful for a range of simple question-answering tasks (Brown et al., 2020).

Both of the above ideas, however, have key limitations. For rationale-augmented training and finetuning methods, it is costly to create a large set of high quality rationales, which is much more complicated than simple input-output pairs used in normal machine learning. For the traditional fewshot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning abilities, and often does not improve substantially with increasing language model scale (Rae et al., 2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations. Specifically, we explore the ability of language models to perform few-shot prompting for reasoning tasks, given a prompt that consists of triples: 〈 input, chain of thought , output 〉 . A chain of thought is a series of intermediate natural language reasoning steps that lead to the final output, and we refer to this approach as chain-of-thought prompting . An example prompt is shown in Figure 1.

We present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks, showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree. Figure 2 illustrates one such result-on the GSM8K benchmark of math word problems (Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting by a large margin and achieves new state-of-the-art performance. A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of generality. This work underscores how large language models can learn via a few examples with natural language data about the task (c.f. automatically learning the patterns underlying inputs and outputs via a large training dataset)."
