focus_paper,opp_paper,topic,title_focus,title_opp,notes
https://arxiv.org/pdf/2406.11709,https://arxiv.org/pdf/2310.10648,helping students fix their mistakes,"Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging",Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes,
https://arxiv.org/pdf/2305.10601,https://arxiv.org/pdf/2201.11903,enabling large language model reasoning via prompting,Tree of Thoughts: Deliberate Problem Solving with Large Language Models,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,
https://arxiv.org/pdf/2404.02078,https://arxiv.org/pdf/2406.09136,using preferences to train language models for better reasoning,Advancing LLM Reasoning Generalists with Preference Trees,Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs,focus uses criticisms/observations to improve the preference tree
https://arxiv.org/pdf/2411.04425,https://arxiv.org/pdf/2402.04333v3,selecting subsets of data to improve language model performance,DELIFT: Data Efficiency in Language model Instruction Fine Tuning,LESS: Selecting Influential Data for Targeted Instruction Tuning,"focus paper uses ICL gains while cited paper uses expensive gradient similarity, focus paper also works on 3 use cases while cited works on 1"
https://arxiv.org/pdf/2402.04333v3,https://arxiv.org/pdf/1906.01827,using gradient-based information to select subsets of data for improving language model performance,LESS: Selecting Influential Data for Targeted Instruction Tuning,Coresets for Data-efficient Training of Machine Learning Models,"both use gradients, but cited paper uses submodular functions to select while focus paper does sim matching to training set"
https://arxiv.org/pdf/2402.04333v3,https://arxiv.org/pdf/2301.13287,selecting subsets of data to improve language model performance,LESS: Selecting Influential Data for Targeted Instruction Tuning,MILO : Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning,task is the same but focus paper uses gradients while cited paper uses sentence semantics 
https://arxiv.org/pdf/2404.12522,https://arxiv.org/pdf/2210.00423,using active learning to train neural network-based multi-armed bandits for k-class classification,Neural Active Learning Beyond Bandits,Improved Algorithms for Neural Active Learning,"(1) focus paper takes the cited paper method and removes the dependency of the long vector by predicting k scores for each k class simuntaneously, making it more efficient (2) focus paper introduces pool-based setting along with stream-based setting"
https://arxiv.org/pdf/2404.12522,https://arxiv.org/pdf/2110.08611,using active learning to train neural network-based multi-armed bandits for k-class classification,Neural Active Learning Beyond Bandits,Deep Active Learning by Leveraging Training Dynamics,focus paper uses confidence-based performance while cited paper uses gradient delta-based active learning metrics AND focus paper introduces streaming/pooling algorithsm