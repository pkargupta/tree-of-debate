focus_paper	f_abstract	f_intro	title_focus	opp_paper	o_abstract	o_intro	title_opp	topic
https://arxiv.org/pdf/2406.11709	Socratic questioning is an effective teaching strategy, encouraging critical thinking and problem-solving. The conversational capabilities of large language models (LLMs) show great potential for providing scalable, real-time student guidance. However, current LLMs often give away solutions directly, making them ineffective instructors. We tackle this issue in the code debugging domain with TreeInstruct, an Instructor agent guided by a novel state space-based planning algorithm. TreeInstruct asks probing questions to help students independently identify and resolve errors. It estimates a student's conceptual and syntactical knowledge to dynamically construct a question tree based on their responses and current knowledge state, effectively addressing both independent and dependent mistakes concurrently in a multi-turn interaction setting.In addition to using an existing single-bug debugging benchmark, we construct a more challenging multi-bug dataset of 150 coding problems, incorrect solutions, and bug fixesall carefully constructed and annotated by experts. Extensive evaluation shows TreeInstruct's state-ofthe-art performance on both datasets, proving it to be a more effective instructor than baselines. Furthermore, a real-world case study with five students of varying skill levels further demonstrates TreeInstruct's ability to guide students to debug their code efficiently with minimal turns and highly Socratic questioning.	"With the rapidly expanding conversational and reasoning abilities of large language models (LLMs), there has been a substantial rise in demand for exploiting their capabilities within a multitude of educational applications (Kasneci et al., 2023) in order to widen accessibility via personalized feedback. Specifically, several recent works explore the use of LLMs for providing feedback and guidance to students (Wang et al., 2023; Kazemitabaar et al., 2024; Sheese et al., 2024; Lyu et al., 2024). However, LLMs are typically optimized to generate customer-serving, assistant-like responses, which also translates into the types of questions asked. Especially for educational domains, this style of questioning can be suboptimal (Cotton, 1988; Sahamid, 2016; Yang et al., 2005; Wilson, 1987). For instance, if a student is seeking help from an instructor for correcting their mistakes (e.g., debugging their buggy code), we consider two forms of potential responses: assistant-like and instructor-like. As shown in Figure 1, an assistant-like response would not be a successful educational interaction, as it leads to the Assistant directly providing an answer. On the other hand, an Instructor-like response reflects the educational philosophy of Socratic questioning. Socratic questioning is a teaching strategy where the Student independently solves their problem by answering guiding questions, instead of being arXiv:2406.11709v4 [cs.CL] 7 Nov 2024given the solution directly (Wilson, 1987). This is a more effective learning strategy because the weight of learning falls on the Student as they must put in effort to answer a question as opposed to solely relying on the model (Cotton, 1988; Kasneci et al., 2023). Therefore, we aim to re-orient an LLM to be an Instructor, not an assistant, by asking Socratic questions that (1) help the Student understand their mistakes, and (2) do not directly provide the answer. To tackle these challenges, we propose TreeInstruct based on the following principles: 1. State space estimation: An Instructor plans its conversation with a Student based on the “distance” between their initial answer and the optimal, correct answer within the estimated state space. In other words, it tracks the knowledge state of the Student within this space throughout the Instructor-Student interactions. 2. Tree-based Socratic questioning: An Instructor generates turn-level Socratic questions conditioned on both the Student’s current knowledge state and misunderstanding(s), the latter derived from their responses to the Instructor’s questions. This step dynamically constructs a Socratic question tree. 3. Adaptive conversation restructuring: An Instructor updates their initial conversation plan based on how the Student is progressing in the conversation, as reflected by updates (or lack thereof) to the Student’s knowledge state. This planning can include both questioning and teaching actions. While these principles can apply to many educational domains, this paper focuses on code debugging, which presents unique challenges. Realworld code debugging often involves multiple, potentially interdependent conceptual and syntactical bugs. For instance, Figure 1 shows that first resolving the Student’s conceptual misunderstanding of recursion in Fibonacci helps them identify their recursive syntactical bug (Figure 1). However, existing work fails to account for such nuances and assumes single-turn feedback (Kazemitabaar et al., 2024; Wang et al., 2023; Lyu et al., 2024). This ignores the sub-steps required for the Student to understand each bug. In contrast, TreeInstruct constructs a multi-turn debugging plan (state representation), defined as the set of Student misunderstandings and mistakes (state variables) to be resolved in order to comprehend and correct their bug(s). We define all potential paths to complete these tasks as the state space. We traverse the space using Socratic questions and trace which variables have been resolved, grounded based on the Student’s responses. While existing LLM-based tutors are effective in fixing the Student’s code with high success, they are either prone to directly revealing code answers or cannot be adapted to new Student responses. For example, CodeAid (Kazemitabaar et al., 2024) (specifically, the ""Help Fix Code"" and ""Question from Code"" modules, as these are most similar to our setting) directly provides code or pseudocode 57% of the time, and achieves a mere 55% rate of helpfulness. On the other hand, TreeInstruct exploits the state space to dynamically construct a tree of questions based on (1) incorrect Student responses, or (2) gaps in the Student’s knowledge. The sibling and parent-child relationships between questions reflect the manner in which they traverse the state space. Finally, it exploits both the Student’s knowledge state and any proposed bug fixes to serve as the dynamic stopping condition. Overall, TreeInstruct takes a more structured approach to multi-turn conversational feedback, as (1) grounding the conversation on the state space representation ensures that all bugs are sufficiently addressed, and (2) constructing a tree based on the Student’s current level of understanding allows for more relevant and personalized question generation. We summarize our contributions below: • To the best of our knowledge, TreeInstruct is the first work to explore state space estimation and dynamic tree-based questioning for multi-turn Socratic instruction. • We construct a novel multi-bug debugging dataset with 150 expert-annotated, challenging conceptual and syntactical bugs and their fixes. • Extensive experiments on an existing benchmark and our constructed dataset demonstrate that TreeInstruct can be universally applied to both open and closed source-settings. We also showcase that TreeInstruct’s strong Socratic questioning abilities widely outperform all baselines through both (1) rigorous quantitative and qualitative expert evaluation (on average, preferred 78.43% of the time; Student fixes code 24.55% more) and (2) real-world interactions with students of varying coding abilities. Reproducibility: We release our data and source code1 to facilitate further studies."	Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging	https://arxiv.org/pdf/2310.10648	Scaling high-quality tutoring remains a major challenge in education. Due to growing demand, many platforms employ novice tutors who, unlike experienced educators, struggle to address student mistakes and thus fail to seize prime learning opportunities. Our work explores the potential of large language models (LLMs) to close the novice-expert knowledge gap in remediating math mistakes. We contribute Bridge, a method that uses cognitive task analysis to translate an expert’s latent thought process into a decision-making model for remediation. This involves an expert identifying (A) the student’s error, (B) a remediation strategy, and (C) their intention before generating a response. We construct a dataset of 700 real tutoring conversations, annotated by experts with their decisions. We evaluate state-of-the-art LLMs on our dataset and find that the expert’s decisionmaking model is critical for LLMs to close the gap: responses from GPT4 with expert decisions (e.g., “simplify the problem”) are +76% more preferred than without. Additionally, context-sensitive decisions are critical to closing pedagogical gaps: random decisions decrease GPT4’s response quality by -97% than expert decisions. Our work shows the potential of embedding expert thought processes in LLM generations to enhance their capability to bridge novice-expert knowledge gaps. Our dataset and code can be found at: https://github.com/rosewang2008/bridge.	Human tutoring plays a critical role in accelerating student learning, and is one of the primary ways to combat pandemic-related learning losses (Fryer Jr and Howard-Noveck, 2020; Nickow et al., 2020; Robinson and Loeb, 2021; of Education, 2021; Accelerator, 2022). To accommodate the growing demand for tutoring, many tutoring providers engage novice tutors. While novice tutors may exercise the domain knowledge, they often lack the specialized training of professional educators in interacting with students. However, research suggests that novices with proper training can be effective tutors (Nickow et al., 2020). Responding to student mistakes in real-time is a critical area where novice tutors tend to struggle. Mistakes are prime learning opportunities to address misconceptions (Boaler, 2013), but effective responses involve pedagogical expertise in engaging with student’s thinking and building positive rapport (Roorda et al., 2011; Pianta, 2016; Shaughnessy et al., 2021; Robinson, 2022). Novices typically learn from experts to understand the expert’s thought process however hiring experienced educators to provide timely feedback is resourceintensive (Kraft et al., 2018; Kelly et al., 2020). One potential solution is the use of automated tutors (Graesser et al., 2004). With recent advances in large language models (LLMs), this approach has gained even more interest (Khan Academy, 2023). However their ability to remediate is yet to be evaluated. Prior work suggests several shortcomings with LLMs, including lacking reliable subject and pedagogical knowledge (Frieder et al., 2023; Wang and Demszky, 2023; Singer, 2023), that can be mitigated using explicitly thought processes such as through chain-of-thought prompting (Wei et al., 2022). To address these challenges, our work makes several key contributions. First, we build Bridge, a method that leverages cognitive task analysis to elicit the latent thought processes of experts. We apply Bridge to remediation where we collaborate extensively with experienced math educators to translate their thought process into a decisionmaking model. Bridge breaks down the experts’ thought process: illustrated in Figure 1, Step A is to infer the student’s error (e.g., the student guessed); Step B is to determine the remediation strategy (e.g., provide a solution approach); and Step C is arXiv:2310.10648v3 [cs.CL] 6 Apr 2024Figure 1: 1⃝ Closing the knowledge gap at scale. LLMs and novice tutors lack the pedagogical knowledge to engage with student mistakes, yet they are readily available for 1:1 tutoring. Experts like experienced teachers have the pedagogical knowledge, but are hard to scale. 2⃝ How do we model the expert’s thought process? Our work builds Bridge which leverage cognitive task analysis to translate the latent thought process of experts into a decision-making model. 3⃝ Applying Bridge with LLMs. To bridge the knowledge gap, we scale the expert’s knowledge with LLMs using the expert-guided decision-making model. to identify the strategy intention (e.g., to help the student understand the concept). We construct a dataset of real-world tutoring conversations, annotated with expert decisions and responses. Our open-source dataset consists of 700 real tutoring sessions conducted with 1st5th grade students in Title I schools, predominantly serving low-income students of color. Following FERPA guidelines, our study is IRB-approved and conducts secondary data analysis based on our Data Use Agreement with the tutoring provider and school district. We conduct a thorough human evaluation to compare the expert, novice and LLMs in remediation. To our knowledge, our work is the first to assess the performance of LLMs such as GPT4 and instruct-tuned Llama-2-70b on remediating student mistakes. We find that the response quality of LLMs significantly improve with the expert’s decision-making process: Response from GPT4 with expert- and self-generated decisions are 76-88% more preferred than GPT4 without. Context-sensitive decisions are also critical to closing the knowledge gap: Random decisions decrease GPT4’s response quality -67% than expert decisions. Complementing our quantitative analysis, our lexical analysis reveals that novices and LLMs without the expert’s decision-making process engage superficially with student’s problemsolving process: They give away the answer or prompt the student to re-attempt without further guidance (“double check”, “try again”).	Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes	helping students fix their mistakes
https://arxiv.org/pdf/2305.10601	Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, “Tree of Thoughts” (ToT), which generalizes over the popular “Chain of Thought” approach to prompting language models, and enables exploration over coherent units of text (“thoughts”) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models’ problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.	Originally designed to generate text, scaled-up versions of language models (LMs) such as GPT [ 25 , 26 , 1, 23] and PaLM [5] have been shown to be increasingly capable of performing an ever wider range of tasks requiring mathematical, symbolic, commonsense, and knowledge reasoning. It is perhaps surprising that underlying all this progress is still the original autoregressive mechanism for generating text, which makes token-level decisions one by one and in a left-to-right fashion. Is such a simple mechanism sufficient for a LM to be built toward a general problem solver? If not, what problems would challenge the current paradigm, and what should be alternative mechanisms? The literature on human cognition provides some clues to answer these questions. Research on “dual process” models suggests that people have two modes in which they engage with decisions – a fast, automatic, unconscious mode (“System 1”) and a slow, deliberate, conscious mode (“System 2”) [ 30 , 31, 16 , 15 ]. These two modes have previously been connected to a variety of mathematical models used in machine learning. For example, research on reinforcement learning in humans and other animals has explored the circumstances under which they engage in associative “model free” learning or more deliberative “model based” planning [7 ]. The simple associative token-level choices of LMs are also reminiscent of “System 1”, and thus might benefit from augmentation by a more deliberate “System 2” planning process that (1) maintains and explores diverse alternatives for current choices instead of just picking one, and (2) evaluates its current status and actively looks ahead or backtracks to make more global decisions. To design such a planning process, we return to the origins of artificial intelligence (and cognitive science), drawing inspiration from the planning processes explored by Newell, Shaw, and Simon starting in the 1950s [ 21, 22 ]. Newell and colleagues characterized problem solving [21 ] as search through a combinatorial problem space, represented as a tree. We thus propose the Tree of Thoughts (ToT) framework for general problem solving with language models. As Figure 1 illustrates, while existing methods (detailed below) sample continuous language sequences for problem solving, ToT actively maintains a tree of thoughts, where each thought is a coherent language sequence that serves as an intermediate step toward problem solving (Table 1). Such a high-level semantic unit allows the LM to self-evaluate the progress different intermediate thoughts make towards solving the problem through a deliberate reasoning process that is also instantiated in language (Figures 2,4,6). This implementation of search heuristics via LM self-evaluation and deliberation is novel, as previous search heuristics are either programmed or learned. Finally, we combine this language-based capability to generate and evaluate diverse thoughts with search algorithms, such as breadth-first search (BFS) or depth-first search (DFS), which allow systematic exploration of the tree of thoughts with lookahead and backtracking. Empirically, we propose three new problems that challenge existing LM inference methods even with the state-of-the-art language model, GPT-4 [ 23 ]: Game of 24, Creative Writing, and Crosswords (Table 1). These tasks require deductive, mathematical, commonsense, lexical reasoning abilities, and a way to incorporate systematic planning or search. We show ToT obtains superior results on all three tasks by being general and flexible enough to support different levels of thoughts, different ways to generate and evaluate thoughts, and different search algorithms that adapt to the nature of different problems. We also analyze how such choices affect model performances via systematic ablations and discuss future directions to better train and use LMs.	Tree of Thoughts: Deliberate Problem Solving with Large Language Models	https://arxiv.org/pdf/2201.11903	We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.	The NLP landscape has recently been revolutionized by language models (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020, inter alia). Scaling up the size of language models has been shown to confer a range of benefits, such as improved performance and sample efficiency (Kaplan et al., 2020; Brown et al., 2020, inter alia). However, scaling up model size alone has not proved sufficient for achieving high performance on challenging tasks such as arithmetic, commonsense, and symbolic reasoning (Rae et al., 2021). This work explores how the reasoning ability of large language models can be unlocked by a simple method motivated by two ideas. First, techniques for arithmetic reasoning can benefit from generating natural language rationales that lead to the final answer. Prior work has given models the ability to generate natural language intermediate steps by training from scratch (Ling et al., 2017) or finetuning a pretrained model (Cobbe et al., 2021), in addition to neuro-symbolic methods that use formal languages instead of natural language (Roy and Roth, 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Second, large language models offer the exciting prospect of in-context few-shot learning via prompting. That is, instead of finetuning a separate language model checkpoint for each new task, one can simply “prompt” the model with a few input–output exemplars demonstrating the task. Remarkably, this has been successful for a range of simple question-answering tasks (Brown et al., 2020). Both of the above ideas, however, have key limitations. For rationale-augmented training and finetuning methods, it is costly to create a large set of high quality rationales, which is much more complicated than simple input–output pairs used in normal machine learning. For the traditional fewshot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning abilities, and often does not improve substantially with increasing language model scale (Rae et al., 2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations. Specifically, we explore the ability of language models to perform few-shot prompting for reasoning tasks, given a prompt that consists of triples: 〈input, chain of thought, output〉. A chain of thought is a series of intermediate natural language reasoning steps that lead to the final output, and we refer to this approach as chain-of-thought prompting. An example prompt is shown in Figure 1. We present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks, showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree. Figure 2 illustrates one such result—on the GSM8K benchmark of math word problems (Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting by a large margin and achieves new state-of-the-art performance. A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of generality. This work underscores how large language models can learn via a few examples with natural language data about the task (c.f. automatically learning the patterns underlying inputs and outputs via a large training dataset).	Chain-of-Thought Prompting Elicits Reasoning in Large Language Models	enabling large language model reasoning via prompting
https://arxiv.org/pdf/2404.02078	We introduce EURUS, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, EURUS models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, EURUS-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of EURUS can be primarily attributed to ULTRAINTERACT, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. ULTRAINTERACT can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting of (1) reasoning chains with diverse planning strategies in a unified format, (2) multi-turn interaction trajectories with the environment and the critique, and (3) pairwise data to facilitate preference learning. ULTRAINTERACT allows us to conduct an in-depth exploration of preference learning for reasoning tasks. Our investigation reveals that some well-established preference learning algorithms may be less suitable for reasoning tasks compared to their effectiveness in general conversations. Inspired by this, we derive a novel reward modeling objective which, together with ULTRAINTERACT, leads to a strong reward model.	Current alignment techniques have significantly advanced the development of open-source large language models (LLMs) that effectively meet user expectations and align with human values (Touvron et al., 2023; Tunstall et al., 2023). On complex reasoning, success has been achieved by specializing models for specific capabilities, such as coding (Wei et al., 2023; Guo et al., 2024a; Zheng et al., 2024) and solving math problems (Fu et al., 2023; Yue et al., 2023; Luo et al., 2023a; Toshniwal et al., 2024). However, these models still fall short, by large margins, of the most advanced proprietary models in their all-around capabilities to tackle a diverse range of challenging problems. We conjecture that this performance gap can be primarily attributed to (1) the lack of high-quality alignment data and (2) the underexploration of preference learning techniques for improving models’ complex reasoning capabilities. In this paper, we take strides towards bridging this gap by addressing both factors and developing EURUS. EURUS consists of a suite of LLMs finetuned from Mistral-7B (Jiang et al., 2023a) and CodeLLaMA-70B (Roziere et al., 2023). Across a diverse set of complex reasoning benchmarks that are mostly out-of-distribution (OOD), EURUS achieves state-of-the-art overall performance among all open-source models. In particular, EURUS excels in solving challenging problems that often require sophisticated planning, reasoning, tool integration, and the ability to interact with and learn from the environment and users. As shown in Figure 1, on university-level STEM questions TheoremQA (Chen et al., 2023) and competition-level coding problems LeetCode Contest (Guo et al., 2024a), EURUS-70B significantly outperforms all open-source models, achieving comparable performance to GPT-3.5 Turbo. EURUS models are trained on ULTRAINTERACT, our newly-curated, large-scale, and high-quality alignment data specifically designed to improve LLMs’ reasoning capabilities. ULTRAINTERACT consists of a diverse set of instructions spanning math, coding, and logical reasoning problems from 12 established datasets. For each instruction, ULTRAINTERACT collects a preference tree that includes: (1) Diverse planning strategies in a unified pattern, such as sequential processing (Wei et al., 2022) and tool creation (Qian et al., 2023), followed by executing step-by-step actions formatted in either text or code, to provide divserse reasoning trajectories. (2) Multi-turn interaction trajectories with the environment and the critique, to improve models’ capabilities to learn from feedback and correct previous errors (Wang et al., 2023b). (3) Paired correct and incorrect actions organized in tree structures, to facilitate preference learning. In total, ULTRAINTERACT contains 86K instructions and 220K action pairs, where each pair consists of an instruction, a correct response, and an incorrect one. Conceptually, ULTRAINTERACT’s data resemble imbalanced binary trees as shown in Figure 2. ULTRAINTERACT can be used in both supervised fine-tuning and preference learning. Our experiments show that, using ULTRAINTERACT along with established datasets in instruction fine-tuning already achieves strong performance. ULTRAINTERACT further facilitates preference learning for reasoning tasks, improving the performance even further with KTO (Ethayarajh et al., 2024) and NCA (Chen et al., 2024a). Surprisingly, applied to an instruction finetuned EURUS model, DPO (Rafailov et al., 2023) hurts the performance. Through careful analysis, we provide evidence that the performance in reasoning correlates with the value of rewards of chosen data—a higher final reward often indicates a better reasoning capability. Besides, our investigation suggests that DPO may be less suitable for reasoning tasks than KTO and NCA. Inspired by this fresh finding, we devise a new objective for reward modeling to augment the Bradley-Terry objective (Bradley & Terry, 1952), explicitly encouraging training to increase the absolute rewards of chosen solution and decrease those of rejected data. Furthermore, ULTRAINTERACT leads to our reward model EURUS-RM-7B, which achieves a better correlation with human annotators than all existing models on AutoJ (Li et al., 2023a) and MT-Bench (Zheng et al., 2023), including GPT-4 (OpenAI, 2023). EURUS-RM-7B demonstrates especially strong preference modeling performance on reasoning tasks. Checkpoints of our EURUS models, accompanying ULTRAINTERACT alignment data to reproduce this research, will be publicly available.	Advancing LLM Reasoning Generalists with Preference Trees	https://arxiv.org/pdf/2406.09136	The recent development of chain-of-thought (CoT) decoding has enabled large language models (LLMs) to generate explicit logical reasoning paths for complex problem-solving. However, research indicates that these paths are not always deliberate and optimal. The tree-of-thought (ToT) method employs tree-searching to extensively explore the reasoning space and find better reasoning paths that CoT decoding might overlook. This deliberation, however, comes at the cost of significantly increased inference complexity. In this work, we demonstrate that fine-tuning LLMs leveraging the search tree constructed by ToT allows CoT to achieve similar or better performance, thereby avoiding the substantial inference burden. This is achieved through Chain of Preference Optimization (CPO), where LLMs are fine-tuned to align each step of the CoT reasoning paths with those of ToT using the inherent preference information in the tree-search process. Extensive experimental results show that CPO significantly improves LLM performance in solving a variety of complex problems, including question answering, fact verification, and arithmetic reasoning, demonstrating its effectiveness. Our code is available at https://github.com/sail-sg/CPO.	Recent advances in large language models (LLMs) have shown that constructing reasoning chains is critical to improving their problem-solving capabilities [1 , 2, 3, 4 , 5 , 6, 7]. A representative method is chain-of-thought (CoT) [1], which prompts LLMs to generate intermediate reasoning steps, i.e., thoughts, thereby constructing explicit reasoning paths (as depicted in Figure 1(a)). While straightforward and intuitive, recent research observes that CoT can often overlook optimal reasoning paths and exhibit an unconscious style of answering due to its single-path focus [ 8, 9]. To foster a more deliberate and conscious reasoning style, Yao et al. [8] propose tree-of-thought (ToT), which generates multiple branching thoughts at each step of the reasoning process and conducts selfevaluation for pruning and planning to search for reasoning paths (as shown in Figure 1(b)). However, despite improving reasoning quality, ToT significantly increases computational complexity, which limits its practical application. This raises the question: Can the strategic depth of ToT be integrated into CoT to enhance its effectiveness while maintaining efficiency? Existing research has initially provided a positive answer to the above question [10, 11 , 12 ]. A natural strategy is to treat the reasoning path discovered by ToT for each instance as a target for supervision, and then fine-tune LLMs to improve their CoT reasoning abilities [11 , 12 ]. Several methods have been proposed to improve this approach, including using advanced tree-search techniques like MonteCarlo tree-search (MCTS) and employing external reward models [12 , 10 ] for pruning and planning to gather better reasoning paths as supervision. The effectiveness of these approaches is therefore largely dependent on the quality of the best-discovered reasoning path. In this paper, we identify a limitation in these approaches: they overlook the non-optimal reasoning thoughts generated during the tree-search process, which naturally provides additional preference information. Specifically, ToT inherently generates multiple alternative thoughts at each reasoning step, and pruning is performed according to their evaluated qualities. This tree-search process constitutes a preference over all intermediate thought candidates—thoughts appearing in the bestdiscovered reasoning path are preferred over those that do not. Moreover, this could shed even more insights than the final best-discovered reasoning path, as non-optimal reasoning paths (and thus preferences) exist at each step in the tree-search. Inspired by recently developed reinforcement learning from human feedback (RLHF) techniques like direct preference optimization (DPO) [13], we propose Chain-of-Preference Optimization (CPO) to fully exploit the inherent preference information. Specifically, we construct paired preference thoughts at each reasoning step according to the search tree of ToT and then train LLMs to align with these preferences using the DPO algorithm (as illustrated in Figure 1(c)). The paired preference thoughts are constructed based on the above intuition: at each reasoning step, we categorize thoughts as preferred or dispreferred based on their inclusion in the final paths chosen by ToT. With such preference data, CPO enables LLMs to generate the path preferred by ToT using CoT decoding at inference time. We conduct extensive experiments to evaluate the effectiveness of CPO. Experiments on seven datasets using LLaMA [ 14 ] and Mistral [ 15 ] as base models demonstrate that CPO is highly effective in teaching LLMs the preferred thoughts of ToT at each reasoning step, leading to an average accuracy improvement of up to 4.3% compared to the base models. Additionally, the experiments reveal that CPO can achieve comparable or even superior performance to the ToT method, which on average requires more than 50 times longer for inference.	Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs	using preferences to train language models for better reasoning
https://arxiv.org/pdf/2411.04425	Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model’s responses to other samples, effectively measuring the informational value relative to the model’s current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.	Fine-tuning large language models (LLMs) is pivotal for adapting these powerful architectures (Devlin et al., 2019; Brown et al., 2020a; Touvron et al., 2023) to specialized tasks such as intricate reasoning, precise question-answering, and the seamless integration of new information (Ouyang et al., 2022). This transformation—from a general-purpose model to a task-specific agent—heavily relies on the quality and nature of the data employed during fine-tuning, which critically determines the model’s subsequent performance (Wei et al., 2022; Zhou et al., 2023; Hoffmann et al., 2024). The effectiveness of fine-tuning hinges on the quality, diversity, and relevance of the selected data (Gururangan et al., 2020; Wei et al., 2022; Zhou et al., 2023). High-quality data ensures accurate learning, diverse data enhances generalization, and relevant data aligns the model’s capabilities with specific application needs. However, optimizing data selection across different fine-tuning phases remains a significant challenge, leading to our central research question: How can we create a unified framework for efficient data selection across all fine-tuning stages of LLMs, while optimizing performance and maximizing data efficiency? To address this challenge, we present DELIFT (Data Efficient Language model Instruction Fine- Tuning), a novel, unified, and computationally efficient algorithm engineered to optimize data selection across all stages of the fine-tuning process. The key innovation of DELIFT lies in its pairwise utility metric, which assesses the informational value of data samples relative to both the model’s current capabilities and other samples within the dataset. This metric, combined with submodular optimization techniques, allows DELIFT to efficiently select optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. 1 arXiv:2411.04425v2 [cs.CL] 10 Nov 2024The typical fine-tuning process comprises three key stages: 1. Instruction Tuning: Enhances the model’s ability to follow general instructions (Mishra et al., 2022; Wei et al., 2022; Longpre et al., 2023); 2. Task-Specific Fine-Tuning: Refines the model’s expertise in specific domains (Gururangan et al., 2020; Cobbe et al., 2021); 3. Continual Fine-tuning: Enables the model to integrate new information while mitigating catastrophic forgetting (Madotto et al., 2021; Wu et al., 2024). DELIFT is able to optimize data selection processes across all three stages. Additionally, DELIFT offers significant benefits for In-Context Learning (ICL) (Brown et al., 2020b; Xue et al., 2024). By utilizing the selected subsets as the ICL example pool, DELIFT achieves similar or better performance compared to using the entire dataset, thereby enhancing data efficiency in ICL scenarios. This dual functionality is empirically validated in our experimental results. Existing data selection methodologies often fail to address the nuanced requirements of the aforementioned distinct fine-tuning stages. Many approaches are tailored to a single stage, lacking the adaptability needed for comprehensive fine-tuning (Xia et al., 2024; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024). Others depend on computationally intensive procedures, such as exhaustive gradient computations, rendering them impractical for large-scale models and datasets (Killamsetty et al., 2021b;a; Xia et al., 2024; Zhang et al., 2024). Additionally, some methods utilize features obtained from an independent model that are not specifically aligned with the model undergoing fine-tuning, reducing their effectiveness (Killamsetty et al., 2023; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024; Du et al., 2023). DELIFT addresses these limitations by adapting to the unique requirements of each fine-tuning stage. 1. Instruction Tuning: Selects diverse data to enhance general instruction-following capabilities; 2. Task-Specific Fine-Tuning: Prioritizes data that is aligned with the target task, to refine specialized expertise; 3. Continual Fine-tuning: Identifies novel, complementary information to expand the model’s knowledge base while safeguarding against catastrophic forgetting. Figure 1 illustrates how DELIFT optimizes data selection across these stages, demonstrating the selection and pruning processes in each fine-tuning phase. By leveraging submodular optimization techniques (Fujishige, 2005; Bilmes, 2022) and submodular information measures (Iyer et al., 2021), DELIFT efficiently selects optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. This approach effectively balances data utility and computational efficiency. Our key contributions are as follows: 1) Versatile Pairwise Utility Metric: A novel, easy-to-compute metric for assessing data informativeness, incorporating model feedback applicable across all fine-tuning stages. 2) Unified Data Selection Algorithm: DELIFT systematically optimizes data selection for instruction tuning, task-specific fine-tuning, and continual fine-tuning within a single framework. 3) Computational Efficiency: Circumvents resource-intensive operations, ensuring scalability to large datasets and models. DELIFT achieves at least 70% reduction in computational time compared to gradient-based methods on benchmark tasks. 4) Enhanced Performance with Reduced Data: Demonstrates the ability to reduce fine-tuning data size by up to 70% without compromising performance, and achieves comparable efficacy as to utilizing the full dataset. 5) Improvement over Existing Methods: Outperforms current data selection techniques by up to 26% in effectiveness across diverse tasks and model scales (see Section 4). The remainder of this paper is organized as follows: Section 2 provides background on fine-tuning LLMs and reviews related work. Section 3 details the methodology behind DELIFT, including the development of our pairwise utility metric and the submodular optimization process. Section 4 presents experimental results that showcase the effectiveness and efficiency of our method. Section 5 discusses the implications of our findings and potential future directions. Finally, we release our code base for further research.	DELIFT: Data Efficiency in Language model Instruction Fine Tuning	https://arxiv.org/pdf/2402.04333v3	Instruction tuning has unlocked powerful capabilities in large language models (LLMs), using combined datasets to develop general-purpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application. To facilitate future work, we release code and data at princetonnlp/LESS.	nstruction tuning has made large language models (LLMs) adept at following human instructions (Ouyang et al., 2022) as versatile chatbots (OpenAI, 2022; 2023; Anthropic, 2023; Google, 2023). Recent efforts curating highly diverse and wide-ranging instruction tuning datasets (Taori et al., 2023; Wang et al.; Mukherjee et al., 2023; Xu et al., 2023, inter alia) induce remarkably strong generalization even from a small number of examples (Zhou et al., 2023). Regardless, it remains an open problem to understand how to best utilize these various datasets. Many real-world applications call for cultivating a specific suite of capabilities in LLMs (e.g., reasoning skills). However, training LLMs with mixed instruction tuning datasets can hinder the development of these specific capabilities. For example, Wang et al. (2023b) demonstrates that LLMs trained on a mix of instruction tuning datasets exhibit worse performance than those trained on a subset of the data. Additionally, considering the broad spectrum of user queries and the multitude of skills required to respond to them, there may not always be enough in-domain data available. Therefore, we hope to be able to effectively use the general instruction tuning data to improve specific capabilities. We frame this setting as targeted instruction tuning: Given just a handful of examples embodying a specific capability, how can we effectively select relevant fine-tuning data from a large collection of instruction datasets? We approach this problem by prioritizing training on data that directly minimizes loss on a target task instead of relying on surface form features (Gururangan et al., 2020; Xie et al., 2023b). Inspired by past works estimating the influence of individual training datapoints with gradient information (Pruthi et al., 2020; Han et al., 2023), we design an optimizer-aware approach to select such data. However, straightforward application of this influence formulation faces several challenges unique to the instruction tuning setting: (1) LLMs are traditionally fine-tuned with the Adam optimizer (Kingma & Ba, 2015) instead of the canonical SGD optimizer; (2) using sequence-level gradients of variable-length instruction data can derail the influence estimation; and (3) the large number of trainable parameters in LLMs makes the computation and storage of gradient information extremely resource-intensive. 1 arXiv:2402.04333v3 [cs.CL] 13 Jun 2024LESS: Selecting Influential Data for Targeted Instruction Tuning We address these concerns in LESS, an algorithm that performs Low-rank gradiEnt Similarity Search to select relevant instruction tuning data for a target application, which exhibits the following properties: 1. Compatible with instruction tuning with Adam (§2 and §3): LESS adapts the gradient features from classical influence formulations (Pruthi et al., 2020) to work with the Adam optimizer and variable-length instruction data. The optimization insights and influence formulation may be of independent interest as well. 2. Efficient (§4.1): LESS uses LoRA (Hu et al., 2021) and random projections (Johnson & Lindenstrauss, 1984) to construct a gradient datastore with lowdimensional, easily manipulable gradient features that permit efficient and effective dataset selection. The gradient datastore can be reused for new target tasks. 3. Transferable (§5.3): Data selected using small models’ gradient features induce strong performance in large models and models from different families, adding to the efficiency of LESS (Table 2). 4. Interpretable (§6.2): Qualitative analysis shows that LESS selects data with similar reasoning and skill types as the target task, whereas existing approaches often select data based on surface form cues (e.g., language or topic). We evaluate our approach on three diverse downstream datasets—MMLU (Hendrycks et al., 2020), TY- DIQA (Clark et al., 2020), and BBH (Suzgun et al., 2023)—each containing distinct subtasks that effectively simulate targeted instruction tuning scenarios. Results show that LESS often selects a small subset of the data (5%) that outperforms training on the full dataset, and the selected subset remains universally effective across model scales and families (Table 2). Comparisons with other data selection methods show that LESS is the only consistently effective approach, justifying its relatively high computational cost.	LESS: Selecting Influential Data for Targeted Instruction Tuning	selecting subsets of data to improve language model performance
https://arxiv.org/pdf/2402.04333v3	Instruction tuning has unlocked powerful capabilities in large language models (LLMs), using combined datasets to develop general-purpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application. To facilitate future work, we release code and data at princetonnlp/LESS.	nstruction tuning has made large language models (LLMs) adept at following human instructions (Ouyang et al., 2022) as versatile chatbots (OpenAI, 2022; 2023; Anthropic, 2023; Google, 2023). Recent efforts curating highly diverse and wide-ranging instruction tuning datasets (Taori et al., 2023; Wang et al.; Mukherjee et al., 2023; Xu et al., 2023, inter alia) induce remarkably strong generalization even from a small number of examples (Zhou et al., 2023). Regardless, it remains an open problem to understand how to best utilize these various datasets. Many real-world applications call for cultivating a specific suite of capabilities in LLMs (e.g., reasoning skills). However, training LLMs with mixed instruction tuning datasets can hinder the development of these specific capabilities. For example, Wang et al. (2023b) demonstrates that LLMs trained on a mix of instruction tuning datasets exhibit worse performance than those trained on a subset of the data. Additionally, considering the broad spectrum of user queries and the multitude of skills required to respond to them, there may not always be enough in-domain data available. Therefore, we hope to be able to effectively use the general instruction tuning data to improve specific capabilities. We frame this setting as targeted instruction tuning: Given just a handful of examples embodying a specific capability, how can we effectively select relevant fine-tuning data from a large collection of instruction datasets? We approach this problem by prioritizing training on data that directly minimizes loss on a target task instead of relying on surface form features (Gururangan et al., 2020; Xie et al., 2023b). Inspired by past works estimating the influence of individual training datapoints with gradient information (Pruthi et al., 2020; Han et al., 2023), we design an optimizer-aware approach to select such data. However, straightforward application of this influence formulation faces several challenges unique to the instruction tuning setting: (1) LLMs are traditionally fine-tuned with the Adam optimizer (Kingma & Ba, 2015) instead of the canonical SGD optimizer; (2) using sequence-level gradients of variable-length instruction data can derail the influence estimation; and (3) the large number of trainable parameters in LLMs makes the computation and storage of gradient information extremely resource-intensive. 1 arXiv:2402.04333v3 [cs.CL] 13 Jun 2024LESS: Selecting Influential Data for Targeted Instruction Tuning We address these concerns in LESS, an algorithm that performs Low-rank gradiEnt Similarity Search to select relevant instruction tuning data for a target application, which exhibits the following properties: 1. Compatible with instruction tuning with Adam (§2 and §3): LESS adapts the gradient features from classical influence formulations (Pruthi et al., 2020) to work with the Adam optimizer and variable-length instruction data. The optimization insights and influence formulation may be of independent interest as well. 2. Efficient (§4.1): LESS uses LoRA (Hu et al., 2021) and random projections (Johnson & Lindenstrauss, 1984) to construct a gradient datastore with lowdimensional, easily manipulable gradient features that permit efficient and effective dataset selection. The gradient datastore can be reused for new target tasks. 3. Transferable (§5.3): Data selected using small models’ gradient features induce strong performance in large models and models from different families, adding to the efficiency of LESS (Table 2). 4. Interpretable (§6.2): Qualitative analysis shows that LESS selects data with similar reasoning and skill types as the target task, whereas existing approaches often select data based on surface form cues (e.g., language or topic). We evaluate our approach on three diverse downstream datasets—MMLU (Hendrycks et al., 2020), TY- DIQA (Clark et al., 2020), and BBH (Suzgun et al., 2023)—each containing distinct subtasks that effectively simulate targeted instruction tuning scenarios. Results show that LESS often selects a small subset of the data (5%) that outperforms training on the full dataset, and the selected subset remains universally effective across model scales and families (Table 2). Comparisons with other data selection methods show that LESS is the only consistently effective approach, justifying its relatively high computational cost.	LESS: Selecting Influential Data for Targeted Instruction Tuning	https://arxiv.org/pdf/1906.01827	Incremental gradient (IG) methods, such as stochastic gradient descent and its variants are commonly used for large scale optimization in machine learning. Despite the sustained effort to make IG methods more data-efficient, it remains an open question how to select a training data subset that can theoretically and practically perform on par with the full dataset. Here we develop CRAIG, a method to select a weighted subset (or coreset) of training data that closely estimates the full gradient by maximizing a submodular function. We prove that applying IG to this subset is guaranteed to converge to the (near)optimal solution with the same convergence rate as that of IG for convex optimization. As a result, CRAIG achieves a speedup that is inversely proportional to the size of the subset. To our knowledge, this is the first rigorous method for data-efficient training of general machine learning models. Our extensive set of experiments show that CRAIG, while achieving practically the same solution, speeds up various IG methods by up to 6x for logistic regression and 3x for training deep neural networks†.	Mathematical optimization lies at the core of training largescale machine learning systems, and is now widely used over massive data sets with great practical success, assuming sufficient data resources are available. Achieving this success, however, also requires large amounts of (often GPU) computing, as well as concomitant financial expenditures and energy usage (Strubell et al., 2019). Significantly decreasing these costs without decreasing the learnt system’s resulting accuracy is one of the grand challenges of machine learning and artificial intelligence today (Asi & Duchi, 2019). Training machine learning models often reduces to optimizing a regularized empirical risk function. Given a convex loss l, and a μ-strongly convex regularizer r, one aims to find model parameter vector w∗ over the parameter space W that minimizes the loss f over the training data V : w∗ ∈ arg minw∈W f (w), f (w) := ∑ i∈V fi(w) + r(w), fi(w) = l(w, (xi, yi)), (1) where V = {1, . . . , n} is an index set of the training data, and functions fi : Rd → R are associated with training examples (xi, yi), where xi ∈ Rd is the feature vector, and yi is the point i’s label. Standard Gradient Descent can find the minimizer of this problem, but requires repeated computations of the full gradient ∇f (w)—sum of the gradients over all training data points/functions i—and is therefore prohibitive for massive data sets. This issue is further exacerbated in case of deep neural networks where gradient computations (backpropagation) are expensive. Incremental Gradient (IG) methods, such as Stochastic Gradient Descent (SGD) and its accelerated variants, including SGD with momentum (Qian, 1999), Adagrad (Duchi et al., 2011), Adam (Kingma & Ba, 2014), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013) iteratively estimate the gradient on random subsets/batches of training data. While this provides an unbiased estimate of the full gradient, the randomized batches introduce variance in the gradient estimate (Hofmann et al., 2015), and therefore stochastic gradient methods are in general slow to converge (Johnson & Zhang, 2013; Defazio et al., 2014). The majority of the work speeding up IG methods has thus primarily focused on reducing the variance of the gradient estimate (SAGA (Defazio et al., 2014), SVRG (Johnson & Zhang, 2013), Katysha (Allen-Zhu, 2017)) or more carefully selecting the gradient stepsize (Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), Adam (Kingma & Ba, 2014)). However, the direction that remains largely unexplored is how to carefully select a small subset S ⊆ V of the fullCoresets for Data-efficient Training of Machine Learning Models PAGE 2 training data V , so that the model is trained only on the subset S while still (approximately) converging to the globally optimal solution (i.e., the model parameters that would be obtained if training/optimizing on the full V ). If such a subset S can be quickly found, then this would directly lead to a speedup of |V |/|S| (which can be very large if |S| ≪ |V |) per epoch of IG. There are four main challenges in finding such a subset S. First is that a guiding principle for selecting S is unclear. For example, selecting training points close to the decision boundary might allow the model to fine tune the decision boundary, while picking the most diverse set of data points would allow the model to get a better sense of the training data distribution. Second is that finding S must be fast, as otherwise identifying the set S may take longer than the actual optimization, and so no overall speed-up would be achieved. Third is that finding a subset S is not enough. One also has to decide on a gradient stepsize for each data point in S, as they affect the convergence. And last, while the method might work well empirically on some data sets, one also requires theoretical understanding and mathematical convergence guarantees. Here we develop Coresets for Accelerating Incremental Gradient descent (CRAIG), for selecting a subset of training data points to speed up training of large machine learning models. Our key idea is to select a weighted subset S of training data V that best approximates the full gradient of V . We prove that the subset S that minimizes an upper-bound on the error of estimating the full gradient maximizes a submodular facility location function. Hence, S can be efficiently found using a fast greedy algorithm. We also provide theoretical analysis of CRAIG and prove its convergence. Most importantly, we show that any incremental gradient method (IG) on S converges in the same number epochs as the same IG would on the full V , which means that we obtain a speed-up inversely proportional to the size of S. In particular, for a μ-strongly convex risk function and a subset S selected by CRAIG that estimates the full gradient by an error of at most ǫ, we prove that IG on S with diminishing stepsize αk = α/kτ at epoch k (with 0 < τ < 1 and 0 < α), converges to an 2Rǫ/μ2 neighborhood of the optimal solution at rate O(1/√k). Here, R = min{d0, (rγmaxC + ǫ)/μ} where d0 is the initial distance to the optimum, C is an upperbound on the norm of the gradients, r = |S|, and γmax is the largest weight for the elements in the subset obtained by CRAIG. Moreover, we prove that if in addition to the strong convexity, component functions have smooth gradients, IG with the same diminishing step size on subset S converges to a 2ǫ/μ neighborhood of the optimum solution at rate O(1/kτ ). The above implies that IG on S converges to the same solution and in the same number of epochs as IG on the full V . But because every epoch only uses a subset S of the data, it requires fewer gradient computations and thus leads to a |V |/|S| speedup over traditional IG methods, while still (approximately) converging to the optimal solution. We also note that CRAIG is complementary to various incremental gradient (IG) methods (SGD, SAGA, SVRG, Adam), and such methods can be used on the subset S found by CRAIG. We also demonstrate the effectiveness of CRAIG via an extensive set of experiments using logistic regression (a convex optimization problem) as well as training deep neural networks (non-convex optimization problems). We show that CRAIG speeds up incremental gradient methods, including SGD, SAGA, and SVRG. In particular, CRAIG while achieving practically the same loss and accuracy as the underlying incremental gradient descent methods, speeds up gradient methods by up to 6x for convex and 3x for non-convex loss functions.	Coresets for Data-efficient Training of Machine Learning Models	using gradient-based information to select subsets of data for improving language model performance
https://arxiv.org/pdf/2402.04333v3	Instruction tuning has unlocked powerful capabilities in large language models (LLMs), using combined datasets to develop general-purpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application. To facilitate future work, we release code and data at princetonnlp/LESS.	nstruction tuning has made large language models (LLMs) adept at following human instructions (Ouyang et al., 2022) as versatile chatbots (OpenAI, 2022; 2023; Anthropic, 2023; Google, 2023). Recent efforts curating highly diverse and wide-ranging instruction tuning datasets (Taori et al., 2023; Wang et al.; Mukherjee et al., 2023; Xu et al., 2023, inter alia) induce remarkably strong generalization even from a small number of examples (Zhou et al., 2023). Regardless, it remains an open problem to understand how to best utilize these various datasets. Many real-world applications call for cultivating a specific suite of capabilities in LLMs (e.g., reasoning skills). However, training LLMs with mixed instruction tuning datasets can hinder the development of these specific capabilities. For example, Wang et al. (2023b) demonstrates that LLMs trained on a mix of instruction tuning datasets exhibit worse performance than those trained on a subset of the data. Additionally, considering the broad spectrum of user queries and the multitude of skills required to respond to them, there may not always be enough in-domain data available. Therefore, we hope to be able to effectively use the general instruction tuning data to improve specific capabilities. We frame this setting as targeted instruction tuning: Given just a handful of examples embodying a specific capability, how can we effectively select relevant fine-tuning data from a large collection of instruction datasets? We approach this problem by prioritizing training on data that directly minimizes loss on a target task instead of relying on surface form features (Gururangan et al., 2020; Xie et al., 2023b). Inspired by past works estimating the influence of individual training datapoints with gradient information (Pruthi et al., 2020; Han et al., 2023), we design an optimizer-aware approach to select such data. However, straightforward application of this influence formulation faces several challenges unique to the instruction tuning setting: (1) LLMs are traditionally fine-tuned with the Adam optimizer (Kingma & Ba, 2015) instead of the canonical SGD optimizer; (2) using sequence-level gradients of variable-length instruction data can derail the influence estimation; and (3) the large number of trainable parameters in LLMs makes the computation and storage of gradient information extremely resource-intensive. 1 arXiv:2402.04333v3 [cs.CL] 13 Jun 2024LESS: Selecting Influential Data for Targeted Instruction Tuning We address these concerns in LESS, an algorithm that performs Low-rank gradiEnt Similarity Search to select relevant instruction tuning data for a target application, which exhibits the following properties: 1. Compatible with instruction tuning with Adam (§2 and §3): LESS adapts the gradient features from classical influence formulations (Pruthi et al., 2020) to work with the Adam optimizer and variable-length instruction data. The optimization insights and influence formulation may be of independent interest as well. 2. Efficient (§4.1): LESS uses LoRA (Hu et al., 2021) and random projections (Johnson & Lindenstrauss, 1984) to construct a gradient datastore with lowdimensional, easily manipulable gradient features that permit efficient and effective dataset selection. The gradient datastore can be reused for new target tasks. 3. Transferable (§5.3): Data selected using small models’ gradient features induce strong performance in large models and models from different families, adding to the efficiency of LESS (Table 2). 4. Interpretable (§6.2): Qualitative analysis shows that LESS selects data with similar reasoning and skill types as the target task, whereas existing approaches often select data based on surface form cues (e.g., language or topic). We evaluate our approach on three diverse downstream datasets—MMLU (Hendrycks et al., 2020), TY- DIQA (Clark et al., 2020), and BBH (Suzgun et al., 2023)—each containing distinct subtasks that effectively simulate targeted instruction tuning scenarios. Results show that LESS often selects a small subset of the data (5%) that outperforms training on the full dataset, and the selected subset remains universally effective across model scales and families (Table 2). Comparisons with other data selection methods show that LESS is the only consistently effective approach, justifying its relatively high computational cost.	LESS: Selecting Influential Data for Targeted Instruction Tuning	https://arxiv.org/pdf/2301.13287	Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results indicate that MILO can train models 3 × −10× faster and tune hyperparameters 20 × −75× faster than full-dataset training or tuning without compromising performance.	"Deep learning has achieved remarkable success in a multitude of machine learning tasks, including natural language processing, computer vision, and speech recognition in recent years. This success is partially due to the availability of massive training datasets and the capacity to train large-scale neural networks. However, training deep models on extensive datasets is computationally demanding, incurring significant financial costs and generating substantial CO2 emissions [63, 59 ]. Bhavya et al. [ 2] overview several research trajectories aimed at enhancing model convergence and reducing training time and costs, including data subset selection, curriculum learning, model architecture improvements, and optimization algorithm enhancements. Our work specifically focuses on selecting useful, generalizable data subsets for efficient deep neural network training. Recent research [66 , 5 , 21 ] suggests that many current training datasets are redundant, indicating that a non-redundant, informative subset could achieve comparable performance to training on the full dataset. To identify such informative samples, metrics such as prediction uncertainty [ 7], prediction flips [ 66 , 72 ], loss [42 ], or gradient/gradient-norm [ 21, 47 , 28, 27 , 29 ] have been applied. These metrics, calculated using either the downstream machine learning model or a lightweight surrogate [ 7], require a fully converged model, a requirement that runs counter to the goal of efficient training. To address this, existing subset selection algorithms [ 47 , 28 , 27, 29, 30 , 54 ] for efficient learning utilize the downstream model during training for heuristic computation and periodically update the subset as the model-dependent metrics for each sample evolve. Drawbacks of Model-Dependent Subset Selection: Despite the theoretical advantages of existing subset selection strategies[ 28 , 27, 47 ], they often fall short in computational efficiency compared to adaptive random subset selection, which selects random subsets periodically. This is primarily because traditional approaches rely on downstream models and typically require the calculation of sample metrics, such as gradients, before each subset selection step. Furthermore, these computationally demanding subset selection steps occur during model training. For instance, Figure 1 compares the convergence rate of the ResNet18 model on the CIFAR100 dataset, in terms of both time and epochs. Here, the model uses 10% subsets chosen every epoch by GradMatchPB [ 27 ], a state-of-the-art data subset selection strategy for efficient training, CraigPB [ 47 ], and Adaptive-Random (where a new 10% subset is randomly selected periodically). The selection of a new subset every epoch is done to demonstrate the maximum performance attainable by GradMatchPB and CraigPB. The results indicate that GradMatchPB achieves faster epoch convergence than both Adaptive-Random and CraigPB when selecting a new subset each epoch. However, due to the necessity for a computationally intensive subset selection step every epoch, both GradMatchPB and CraigPB demonstrate considerable inefficiency in terms of training time. In their respective studies, Killamsetty et al. [27 ] and Mirzasoleiman et al. [47] suggested selecting a new subset every R epochs to enhance training efficiency. However, this comes at the cost of the model’s convergence rate. Lastly, model-dependent subset selection requires a computationally intensive subset selection steps each time a new model is trained. Model-Agnostic Selection: Our primary insight is that a model-agnostic subset selection framework can circumvent the computationally intensive subset selection steps during model training by conducting subset selection in the preprocessing phase. By pre-selecting subsets and storing them as metadata with each dataset, we can train numerous models without incurring additional costs, effectively spreading out the expense of subset selection. In this work, we endeavor to answer the following question: Is it possible to develop a model-agnostic subset selection method that selects new subsets in a minimal amount of time, yet achieves superior model convergence without significant compromise in test accuracy or generalization performance? 1.1 Contributions MILO Framework: In this study, we introduce MILO, a model-agnostic subset selection framework designed for efficient model training and tuning. MILO employs submodular measures [ 13 , 23 ], which capture higher-order interactions between data samples for subset selection. We utilize pre-trained large language models [ 55 ] and pre-trained vision transformers [25 ] as feature encoders due to their zero-shot feature encoding capabilities. These encoders compute the sample metrics in a nominal amount of time, and this computation is independent of the downstream model, rendering MILO model-agnostic. Figure 3 provides a visual representation of MILO for model training, comprising two steps: a) A pre-processing step that involves the selection of multiple subsets from the training dataset using ""Stochastic-Greedy Exploration (SGE)"" (refer to Section 3.1.1), and the construction of a probability distribution over the entire dataset for subset sampling through ""Weighted Random Exploration (WRE)"" (refer to Section 3.1.2); b) A model training scheme that involves training the model on a curriculum of easy-to-hard subsets (refer to Section 3.1.3) using the subsets selected and the probability distribution constructed in the pre-processing step. We also present a class-wise partitioning trick to significantly minimize the memory footprint of MILO (refer to Section 3.2). Effectiveness of MILO : Through extensive experiments on multiple real-world datasets, we empirically demonstrate the effectiveness of the MILO framework for efficient training and hyperparameter tuning. In Figure 2, we provide a summary of the speedup achieved by MILO compared to full 2(a) Efficient Training (b) Efficient Tuning Figure 2: This figure illustrates the MILO’s tradeoff between speedup and accuracy degradation compared to full data training and tuning. For model training, speedups of 3× to 10× were achieved with less than a 1.5% accuracy drop, and for hyper-parameter tuning, MILO achieves speedups of 20× to 75× with less than a 0.15% accuracy drop. data training and tuning, along with the corresponding relative performance. Our results show that MILO can train models 3x to 10x faster and tune hyperparameters 20x to 75x faster, with minimal loss in performance. Furthermore, we consistently outperform the subset selection baselines that were considered. Additionally, MILO exhibits significantly faster initial model convergence and maintains faster convergence throughout training compared to other baselines. These qualities make it highly effective for applications such as hyperparameter tuning and neural architecture search, where the ability to distinguish good models from bad models early in the tuning process is crucial for enhancing efficiency [38, 37]. Related Work: In recent years, data-subset selection strategies have achieved success in various machine learning applications such as speech recognition [ 68 , 67], machine translation [ 32 ], active learning [ 60 , 1 , 33 ], hyper-parameter tuning [ 30 ], continual learning [65 ], domain adaptation [20 ], and computer vision [22]. A recent empirical study [5] demonstrated that datasets frequently contain semantic redundancies. These redundancies can be removed during training without affecting performance. In response, data pruning methods [66 , 51, 62] have developed principled selection criteria for pruning redundant samples before model training, with minimal loss in performance. However, both existing data pruning and compute-efficient learning approaches are model-dependent, resulting in them being computationally expensive. In contrast, our method is model-agnostic and capable of identifying high-quality subsets by intelligent data exploration. Furthermore, our approach can achieve superior model convergence by utilizing an easy-to-hard curriculum, transitioning from representative to diverse subsets during training."	MILO : Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning	selecting subsets of data to improve language model performance
https://arxiv.org/pdf/2404.12522	"We study both stream-based and pool-based active learning with neural network approximations. A recent line of works proposed bandit-based approaches that transformed active learning into a bandit problem, achieving both theoretical and empirical success. However, the performance and computational costs of these methods may be susceptible to the number of classes, denoted as K, due to this transformation. Therefore, this paper seeks to answer the question: ""How can we mitigate the adverse impacts of K while retaining the advantages of principled exploration and provable performance guarantees in active learning?"" To tackle this challenge, we propose two algorithms based on the newly designed exploitation and exploration neural networks for stream-based and pool-based active learning. Subsequently, we provide theoretical performance guarantees for both algorithms in a non-parametric setting, demonstrating a slower error-growth rate concerning K for the proposed approaches. We use extensive experiments to evaluate the proposed algorithms, which consistently outperform state-of-the-art baselines."	Active learning is one of the primary areas in machine learning to investigate the learning technique on a small subset of labeled data while acquiring good generalization performance compared to passive learning [ 19 ]. There are mainly two settings of active learning: stream-based and pool-based settings. For the stream-based setting, the learner is presented with an instance drawn from some distribution in each round and is required to decide on-the-fly whether or not to query the label from the oracle. For the pool-based setting, the learner aims to select one or multiple instances from the unlabeled pool and hand them over to the oracle for labeling. It repeats this process until the label budget is exhausted [51]. The essence of active learning is to exploit the knowledge extracted from labeled instances and explore the unlabeled data to maximize information acquisition for long-term benefits. Using neural networks (NNs) to perform active learning has been explored extensively in recent works [48 ; 52 ; 56 ; 6 ]. However, they often lack a provable performance guarantee despite strong empirical performance. To address this issue, a recent line of works [58 ; 14 ] proposed the banditbased approaches to solve the active learning problem, which are equipped with principled exploration and theoretical performance guarantee. In contextual bandits [38 ; 68], the learner is presented with K arms (context vectors) and required to select one arm in each round. Then, the associated reward is observed. [58; 14] transformed the online K-class classification into a bandit problem. Specifically, in one round of stream-based active learning, a data instance xt ∈ Rd is transformed into K long vectors corresponding to K arms, matching K classes: xt,1 = [x⊤ t , 0⊤, · · · , 0⊤]⊤, . . . , xt,K = [0⊤, · · · , 0⊤, x⊤ t ]⊤, where xt,k ∈ RdK , k ∈ [K]. Then, the learner uses an NN model to calculate a score for each arm and selects an arm based on these scores. The index of the selected arm represents the index of the predicted class. This design enables researchers to utilize the exploration strategy and analysis in contextual bandits to solve the active learning problem. Note [58; 14 ] can only handle the stream-based setting of active learning. However, bandit-based approaches bear the following two limitations. First, as the instance xt is transformed into K arms, it is required to calculate a score for all K arms respectively, producing a cost of K times forward-propagation computation of neural networks. This computation cost is scaled by K. Second, the transformed long vector (arm) has (Kd) dimensions, in contrast to the d dimensions of the original instance as the input of the NN model. This potentially amplifies the effects of K on an active learning algorithm’s performance. We empirically evaluate [58; 14 ] as shown in Table 1. The results indicate a noticeable degradation in both test accuracy and running time as K increases. In response, in this paper, we aim to mitigate the adverse effects of K on the bandit-based approach in active learning. Our methods are built upon and beyond [ 14 ]. [ 14 ] adopted the idea of [13 ] to employ two neural networks, one for exploitation and another for exploration. As previously mentioned, these two neural networks take the transformed Kd-dimension arm as input. Moreover, in each round, [ 14] decomposed the label vector yt ∈ {0, 1}K into K rewards (scalars), necessitating the training of two neural networks K times for each arm. Next, we summarize our key ideas and contributions to reduce the input dimension back to d and the number of forward propagations to 1 in each round while preserving the essence of exploitation and exploration of neural networks. Methodology. (1) We extend the loss function in active learning from 0-1 loss to Bounded loss, which is more flexible and general. Instead, [58; 14 ] restricted the loss to be 0-1 loss, because they had to define the reward of each class (arm) due to their bandit-based methodology. (2) We re-designed the input and output exploitation and exploration neural networks to directly take the d-dimension instance as input and output the predicted probabilities for K classes synchronously, mitigating the curse of K. The connection between exploitation and exploration neural networks is also reconstructed beyond the standard bandit setting. In other words, we avoid the transformation of active learning to the standard bandit setting. This is the first main contribution of this paper. (3) To facilitate efficient and effective exploration, we introduce the end-to-end embedding (Definition 4.1) as the input of the exploration neural network, which removes the dependence of the input dimension while preserving the essential information. (4) In addition to our proposed stream-based algorithm, referred to NEURONAL-S, we also propose a pool-based active learning algorithm, NEURONAL-P. We bring the redesigned exploitation and exploration network into pool-based setting and propose a novel gap-inverse-based selection strategy tailored for pool-based active learning. This is our second main contribution. Note that the stream-based algorithms cannot be directly converted into the pool-based setting, as discussed in Appendix B. Theoretical analysis. We provide the regret upper bounds for the proposed stream-based algorithm under low-noise conditions on the data distribution. Our results indicate the cumulative regret of NEURONAL-S grows slower than that of [ 58] concerning K by a multiplicative factor at least O(pT log(1 + λ0)) and up to eO(√md), where λ0 is the smallest eigenvalue of Neural Tangent Kernel (NTK) and m is the width of the neural network. This finding helps explain why our algorithms outperform the bandit-based algorithms, particularly when K is large, as shown in Table 1. In the binary classification task, our regret bounds directly remove the dependence of effective dimension  ̃d, which measures the actual underlying dimension in the RKHS space spanned by NTK, discussed in Sec. 5. We also provide a performance analysis for the proposed pool-based algorithm in the non-parametric setting, tailored for neural network models. In contrast, previous works focus on the regime either in parametric settings that require a finite VC dimension [31] or a linear mapping function assumption [ 8 ; 64 ; 28 ]. The above theoretical results are our third main contribution. In addition, Empirical evaluation. In the end, we perform extensive experiments to evaluate the proposed algorithms for both stream-based and pool-based algorithms compared to state-of-the-art baselines. Our evaluation encompasses various metrics, including test accuracy and running time, and we have carried out ablation studies to investigate the impact of hyper-parameters and label budgets. This is our fourth main contribution.	Neural Active Learning Beyond Bandits	https://arxiv.org/pdf/2210.00423	We improve the theoretical and empirical performance of neural-network(NN)based active learning algorithms for the non-parametric streaming setting. In particular, we introduce two regret metrics by minimizing the population loss that are more suitable in active learning than the one used in state-of-the-art (SOTA) related work. Then, the proposed algorithm leverages the powerful representation of NNs for both exploitation and exploration, has the query decision-maker tailored for k-class classification problems with the performance guarantee, utilizes the full feedback, and updates parameters in a more practical and efficient manner. These careful designs lead to an instance-dependent regret upper bound, roughly improving by a multiplicative factor O(log T ) and removing the curse of input dimensionality. Furthermore, we show that the algorithm can achieve the same performance as the Bayes-optimal classifier in the long run under the hard-margin setting in classification problems. In the end, we use extensive experiments to evaluate the proposed algorithm and SOTA baselines, to show the improved empirical performance.	The Neural Network (NN) is one of the indispensable paradigms in machine learning and is widely used in multifarious supervised-learning tasks [ 23 ]. As more and more complicated NNs are developed, the requirement of the training procedure on the labeled data grows, incurring significant cost of label annotation. Active learning investigates effective techniques on a much smaller labeled data set while attaining the comparable generalization performance to passive learning [19 ]. In this paper, we focus on the classification problem in the streaming setting of active learning with NN models. At every round, the learner receives an instance and is compelled to decide on-the-fly whether or not to observe the label associated with this instance. This problem seeks to maximize the generalization capability of learned NNs in a sequence of rounds, such that the model has robust performance on the unseen data from the same distribution [40]. In active learning, given access to the i.i.d. generated instances from a distribution D, suppose there exist a class of functions F that formulate the mapping from instances to theirs labels. In the parametric setting, i.e., F has finite VC-dimension [ 25 ], existing works [24, 14 , 7] have shown that the active learning algorithms can achieve the convergence rate of  ̃O(1/√N ) to the best population loss in F, where N is the number of label queries. In the non-parametric setting, recent works [ 34 , 35] provide the similar convergence results while suffering from the curse of input dimensionality. Unfortunately, most of NN-based approaches to active learning do not come with the performance guarantee, despite having powerful empirical results. The first performance guarantee for neural active learning has been established in a recent work by [ 48 ], and the analysis is for over-parameterized neural networks with the assistance of Neural Tangent Kernel (NTK). We carefully investigate the limitations of [ 48], which turn into the main motivations of our paper. First, [48 ] transforms the classification problem into a multi-armed bandit problem [ 55 ], to minimize a pseudo regret metric. Yet, on the grounds that they seek to minimize the conditional population loss on a sequence of given data, it is dubious that the pseudo regret used in [ 48 ] can explicitly measure the generalization capability of given algorithms (see Remark 2.1). Second, the training process for NN models is not efficient, as [48] uses vanilla gradient descent and starts from randomly initialized parameters in every round. Third, although [48 ] removes the curse of input dimensionality d, the performance guarantee strongly suffers from another introduced term, the effective dimensionality  ̃d, which can be thought of as the non-linear dimensionalities of Hilbert space spanned by NTK. In the worse case, the magnitude of  ̃d can be an unacceptably large number and thus the performance guarantee collapses. 1.1 Main contributions In this paper, we propose a novel algorithm, I-NeurAL (Improved Algorithms for Neural Active Learning), to tackle the above limitations. Our contributions can be summarized as follows: (1) We consider the k-class classification problem, and we introduce two new regret metrics to minimize the population loss, which can directly reflect the generalization capability of NN-based algorithms. (2) I-NeurAL has a neural exploration strategy with a novel component to decide whether or not to query the label, coming with the performance guarantee. I-NeurAL exploits the full feedback in active learning which is a subtle but effective idea. (3) I-NeurAL is designed to support minibatch Stochastic Gradient Descent (SGD). In particular, at every round, I-NeurAL does mini-batch SGD starting with the parameters of the last round, i.e., with warm start, which is more efficient and practical compared to [48]. (4) Without any noise assumption on the data distribution, we provide an instance-dependent performance guarantee of I-NeurAL for over-parameterized neural networks. Compared to [ 48 ], we remove the curse of both the input dimensionality d and the effective dimensionality  ̃d; Moreover, we roughly improve the regret by a multiplicative factor log(T ), where T is the number of rounds. (5) under a hard-margin assumption on the data distribution, we provide that NN models can achieve the same generalization capability as Bayes-optimal classifier after O(log T ) number of label queries; (6) we conduct extensive experiments on real-world data sets to demonstrate the improved performance of I-NeurAL over state-of-the-art baselines including the closest work [48] which has not provided empirical validation of their proposed algorithms.	Improved Algorithms for Neural Active Learning	using active learning to train neural network-based multi-armed bandits for k-class classification
https://arxiv.org/pdf/2404.12522	"We study both stream-based and pool-based active learning with neural network approximations. A recent line of works proposed bandit-based approaches that transformed active learning into a bandit problem, achieving both theoretical and empirical success. However, the performance and computational costs of these methods may be susceptible to the number of classes, denoted as K, due to this transformation. Therefore, this paper seeks to answer the question: ""How can we mitigate the adverse impacts of K while retaining the advantages of principled exploration and provable performance guarantees in active learning?"" To tackle this challenge, we propose two algorithms based on the newly designed exploitation and exploration neural networks for stream-based and pool-based active learning. Subsequently, we provide theoretical performance guarantees for both algorithms in a non-parametric setting, demonstrating a slower error-growth rate concerning K for the proposed approaches. We use extensive experiments to evaluate the proposed algorithms, which consistently outperform state-of-the-art baselines."	Active learning is one of the primary areas in machine learning to investigate the learning technique on a small subset of labeled data while acquiring good generalization performance compared to passive learning [ 19 ]. There are mainly two settings of active learning: stream-based and pool-based settings. For the stream-based setting, the learner is presented with an instance drawn from some distribution in each round and is required to decide on-the-fly whether or not to query the label from the oracle. For the pool-based setting, the learner aims to select one or multiple instances from the unlabeled pool and hand them over to the oracle for labeling. It repeats this process until the label budget is exhausted [51]. The essence of active learning is to exploit the knowledge extracted from labeled instances and explore the unlabeled data to maximize information acquisition for long-term benefits. Using neural networks (NNs) to perform active learning has been explored extensively in recent works [48 ; 52 ; 56 ; 6 ]. However, they often lack a provable performance guarantee despite strong empirical performance. To address this issue, a recent line of works [58 ; 14 ] proposed the banditbased approaches to solve the active learning problem, which are equipped with principled exploration and theoretical performance guarantee. In contextual bandits [38 ; 68], the learner is presented with K arms (context vectors) and required to select one arm in each round. Then, the associated reward is observed. [58; 14] transformed the online K-class classification into a bandit problem. Specifically, in one round of stream-based active learning, a data instance xt ∈ Rd is transformed into K long vectors corresponding to K arms, matching K classes: xt,1 = [x⊤ t , 0⊤, · · · , 0⊤]⊤, . . . , xt,K = [0⊤, · · · , 0⊤, x⊤ t ]⊤, where xt,k ∈ RdK , k ∈ [K]. Then, the learner uses an NN model to calculate a score for each arm and selects an arm based on these scores. The index of the selected arm represents the index of the predicted class. This design enables researchers to utilize the exploration strategy and analysis in contextual bandits to solve the active learning problem. Note [58; 14 ] can only handle the stream-based setting of active learning. However, bandit-based approaches bear the following two limitations. First, as the instance xt is transformed into K arms, it is required to calculate a score for all K arms respectively, producing a cost of K times forward-propagation computation of neural networks. This computation cost is scaled by K. Second, the transformed long vector (arm) has (Kd) dimensions, in contrast to the d dimensions of the original instance as the input of the NN model. This potentially amplifies the effects of K on an active learning algorithm’s performance. We empirically evaluate [58; 14 ] as shown in Table 1. The results indicate a noticeable degradation in both test accuracy and running time as K increases. In response, in this paper, we aim to mitigate the adverse effects of K on the bandit-based approach in active learning. Our methods are built upon and beyond [ 14 ]. [ 14 ] adopted the idea of [13 ] to employ two neural networks, one for exploitation and another for exploration. As previously mentioned, these two neural networks take the transformed Kd-dimension arm as input. Moreover, in each round, [ 14] decomposed the label vector yt ∈ {0, 1}K into K rewards (scalars), necessitating the training of two neural networks K times for each arm. Next, we summarize our key ideas and contributions to reduce the input dimension back to d and the number of forward propagations to 1 in each round while preserving the essence of exploitation and exploration of neural networks. Methodology. (1) We extend the loss function in active learning from 0-1 loss to Bounded loss, which is more flexible and general. Instead, [58; 14 ] restricted the loss to be 0-1 loss, because they had to define the reward of each class (arm) due to their bandit-based methodology. (2) We re-designed the input and output exploitation and exploration neural networks to directly take the d-dimension instance as input and output the predicted probabilities for K classes synchronously, mitigating the curse of K. The connection between exploitation and exploration neural networks is also reconstructed beyond the standard bandit setting. In other words, we avoid the transformation of active learning to the standard bandit setting. This is the first main contribution of this paper. (3) To facilitate efficient and effective exploration, we introduce the end-to-end embedding (Definition 4.1) as the input of the exploration neural network, which removes the dependence of the input dimension while preserving the essential information. (4) In addition to our proposed stream-based algorithm, referred to NEURONAL-S, we also propose a pool-based active learning algorithm, NEURONAL-P. We bring the redesigned exploitation and exploration network into pool-based setting and propose a novel gap-inverse-based selection strategy tailored for pool-based active learning. This is our second main contribution. Note that the stream-based algorithms cannot be directly converted into the pool-based setting, as discussed in Appendix B. Theoretical analysis. We provide the regret upper bounds for the proposed stream-based algorithm under low-noise conditions on the data distribution. Our results indicate the cumulative regret of NEURONAL-S grows slower than that of [ 58] concerning K by a multiplicative factor at least O(pT log(1 + λ0)) and up to eO(√md), where λ0 is the smallest eigenvalue of Neural Tangent Kernel (NTK) and m is the width of the neural network. This finding helps explain why our algorithms outperform the bandit-based algorithms, particularly when K is large, as shown in Table 1. In the binary classification task, our regret bounds directly remove the dependence of effective dimension  ̃d, which measures the actual underlying dimension in the RKHS space spanned by NTK, discussed in Sec. 5. We also provide a performance analysis for the proposed pool-based algorithm in the non-parametric setting, tailored for neural network models. In contrast, previous works focus on the regime either in parametric settings that require a finite VC dimension [31] or a linear mapping function assumption [ 8 ; 64 ; 28 ]. The above theoretical results are our third main contribution. In addition, Empirical evaluation. In the end, we perform extensive experiments to evaluate the proposed algorithms for both stream-based and pool-based algorithms compared to state-of-the-art baselines. Our evaluation encompasses various metrics, including test accuracy and running time, and we have carried out ablation studies to investigate the impact of hyper-parameters and label budgets. This is our fourth main contribution.	Neural Active Learning Beyond Bandits	https://arxiv.org/pdf/2110.08611	Active learning theories and methods have been extensively studied in classical statistical learning settings. However, deep active learning, i.e., active learning with deep learning models, is usually based on empirical criteria without solid theoretical justification, thus suffering from heavy doubts when some of those fail to provide benefits in real applications. In this paper, by exploring the connection between the generalization performance and the training dynamics, we propose a theory-driven deep active learning method (dynamicAL) which selects samples to maximize training dynamics. In particular, we prove that the convergence speed of training and the generalization performance are positively correlated under the ultra-wide condition and show that maximizing the training dynamics leads to better generalization performance. Furthermore, to scale up to large deep neural networks and data sets, we introduce two relaxations for the subset selection problem and reduce the time complexity from polynomial to constant. Empirical results show that dynamicAL not only outperforms the other baselines consistently but also scales well on large deep learning models. We hope our work would inspire more attempts on bridging the theoretical findings of deep networks and practical impacts of deep active learning in real applications.	Training deep learning (DL) models usually requires large amount of high-quality labeled data [1] to optimize a model with a massive number of parameters. The acquisition of such annotated data is usually time-consuming and expensive, making it unaffordable in the fields that require high domain expertise. A promising approach for minimizing the labeling effort is active learning (AL), which aims to identify and label the maximally informative samples, so that a high-performing classifier can be trained with minimal labeling effort [2]. Under classical statistical learning settings, theories of active learning have been extensively studied from the perspective of VC dimension [3]. As a result, a variety of methods have been proposed, such as (i) the version-space-based approaches, which require maintaining a set of models [4, 5], and (ii) the clustering-based approaches, which assume that the data within the same cluster have pure labels [6]. However, the theoretical analyses for these classical settings may not hold for over-parameterized deep neural networks where the traditional wisdom is ineffective [1]. For example, margin-based methods select the labeling examples in the vicinity of the learned decision boundary [ 7, 8 ]. However, in the over-parameterized regime, every labeled example could potentially be near the learned decision boundary [ 9]. As a result, theoretically, such analysis can hardly guide us to design practical active arXiv:2110.08611v2 [cs.LG] 20 Nov 2022learning methods. Besides, empirically, multiple deep active learning works, borrowing observations and insights from the classical theories and methods, have been observed unable to outperform their passive learning counterparts in a few application scenarios [10, 11]. On the other hand, the analysis of neural network’s optimization and generalization performance has witnessed several exciting developments in recent years in terms of the deep learning theory [ 12– 14]. It is shown that the training dynamics of deep neural networks using gradient descent can be characterized by the Neural Tangent Kernel (NTK) of infinite [ 12] or finite [ 15] width networks. This is further leveraged to characterize the generalization of over-parameterized networks through Rademacher complexity analysis [ 13 , 16 ]. We are therefore inspired to ask: How can we design a practical and generic active learning method for deep neural networks with theoretical justifications? To answer this question, we firstly explore the connection between the model performance on testing data and the convergence speed on training data for the over-parameterized deep neural networks. Based on the NTK framework [12 , 13], we theoretically show that if a deep neural network converges faster (“Train Faster”), then it tends to have better generalization performance (“Generalize Better”), which matches the existing observations [17 –21 ]. Motivated by the aforementioned connection, we first introduce Training Dynamics, the derivative of training loss with respect to iteration, as a proxy to quantitatively describe the training process. On top of it, we formally propose our generic and theoretically-motivated deep active learning method, dynamicAL, which will query labels for a subset of unlabeled samples that maximally increase the training dynamics. In order to compute the training dynamics by merely using the unlabeled samples, we leverage two relaxations Pseudo-labeling and Subset Approximation to solve this non-trivial subset selection problem. Our relaxed approaches are capable of effectively estimating the training dynamics as well as efficiently solving the subset selection problem by reducing the complexity from O(N b) to O(b). In theory, we coin a new term Alignment to measure the length of the label vector’s projection on the neural tangent kernel space. Then, we demonstrate that higher alignment usually comes with a faster convergence speed and a lower generalization bound. Furthermore, with the help of the maximum mean discrepancy [ 22], we extend the previous analysis to an active learning setting where the i.i.d. assumption may not hold. Finally, we show that alignment is positively correlated with our active learning goal, training dynamics, which implies that maximizing training dynamics will lead to better generalization performance. Regarding experiments, we have empirically verified our theory by conducting extensive experiments on three datasets, CIFAR10 [ 23 ], SVHN [24], and Caltech101 [25 ] using three types of network structures: vanilla CNN, ResNet [ 26], and VGG [ 27 ]. We first show that the result of the subset selection problem delivered by the subset approximation is close to the global optimal solution. Furthermore, under the active learning setting, our method not only outperforms other baselines but also scales well on large deep learning models. The main contributions of our paper can be summarized as follows: • We propose a theory-driven deep active learning method, dynamicAL, inspired by the observation of “train faster, generalize better”. To this end, we introduce the Training Dynamics, as a proxy to describe the training process. • We demonstrate that the convergence speed of training and the generalization performance is strongly (positively) correlated under the ultra-wide condition; we also show that maximizing the training dynamics will lead to a lower generalization error in the scenario of active learning. • Our method is easy to implement. We conduct extensive experiments to evaluate the effectiveness of dynamicAL and empirically show tha	Deep Active Learning by Leveraging Training Dynamics	using active learning to train neural network-based multi-armed bandits for k-class classification
https://arxiv.org/pdf/2408.04873	Episodic structures are inherently interpretable and adaptable to evolving large-scale key events. However, state-of-the-art automatic event detection methods overlook event episodes and, therefore, struggle with these crucial characteristics. This paper introduces a novel task, episode detection, aimed at identifying episodes from a news corpus containing key event articles. An episode describes a cohesive cluster of core entities (e.g., “protesters”, “police”) performing actions at a specific time and location. Furthermore, an episode is a significant part of a larger group of episodes under a particular key event. Automatically detecting episodes is challenging because, unlike key events and atomic actions, we cannot rely on explicit mentions of times and locations to distinguish between episodes or use semantic similarity to merge inconsistent episode co-references. To address these challenges, we introduce EpiMine, an unsupervised episode detection framework that (1) automatically identifies the most salient, key-event-relevant terms and segments, (2) determines candidate episodes in an article based on natural episodic partitions estimated through shifts in discriminative term combinations, and (3) refines and forms final episode clusters using large language model-based reasoning on the candidate episodes. We construct three diverse, real-world event datasets annotated at the episode level. EpiMine outperforms all baselines on these datasets by an average 59.2% increase across all metrics.	Given the saturation of real-time news accessible at our fingertips, reading and processing the critical information of a key event has become an increasingly daunting challenge. Consequently, research on automatic textual event detection has recently attempted to integrate the manner in which humans neurologically perceive/store events into textual event detection methods. Specifically, neuroscientists studying event representations in human memory find that events are stored in a top-to-bottom hierarchy, as demonstrated in Figure 1. The deeper the hierarchical event level, the more finegrained its corresponding text granularity [ 48 ]: we consider a theme as corpus-level (all articles discussing the 2019 Hong Kong Protests), key event as document-level (an article typically discusses a full one to two day key event), episode as segment-level, and atomic action as sentence or phrase-level. Furthermore, neurological research [3 , 21 ] indicates that events are encoded into memory as episodic structures. Representing events as discrete episodes helps us piece together a coherent narrative by considering the sequence of actions, reactions, and developments over time. This empowers several downstream tasks (e.g., event schema generation, event prediction), which may benefit from insights into the causes and consequences of events that might be missed with a more generalized analysis. Despite its strengths, existing automatic event extraction works fail to consider the episode-level. For example, key event detection specifically seeks to output “a set of thematically coherent documents” for each key event [ 29, 48]. However, it is challenging to manually parse through a large cluster of relevant articles in order to gain an efficient and compact understanding of a given event. To address the lack of interpretability of such article clusters, the adjacent task of timeline summarization [9 , 14 , 23, 39 ] aims to identify the dates and a compact summary for each key event. However, high-level timelines are typically applicable for historical themes and thus unrealistic for currently evolving key events where fine-grained timeline summarization is more suitable. Hence, event chain mining [ 18] attempts to address this by mining a series of temporally-ordered atomic actions at the phraselevel; however, the granularity level is often too fine-grained to properly represent a large-scale key event. Thus, we aim to tackle the novel task of episode detection to pave the way for a more effective event representation. Episode detection aims to detect episodes from a news corpus containing key event articles. An episode can be described as a cohesive cluster of subjects performing actions at a certain time and location, occurring as part of a larger sequence of episodes under a specific key event. Episode detection introduces a unique set of challenges, which we address using our novel framework, EpiMine. EpiMine is an unsupervised episode detection framework that automatically detects meaningful episodic events and their corresponding text segments in a large key event corpus, all without any level of human supervision or labeled training data. EpiMine is comprised of the following components: (1) discriminative co-occurrence detection, (2) episode partitioning, (3) candidate episode estimation, and (4) episode-segment classification. Collectively, they tackle the unique challenges of episode detection, detailed below: Challenge 1: Journalists do not timestamp episodes. Key event detection partitions a thematic corpus into document-level clusters by heavily relying on explicit temporal features, like publication dates [48 ]. For example, an article primarily discusses one key event, which can then be roughly mapped to the article’s publication date. However, this assumption fails at the episode-level, where there is no guarantee to have a distinct timestamp associated with each text segment that discusses a new episode. Fortunately, we can take advantage of the idea that journalists naturally partition news articles by sequentially discussing distinct episodes: Example: An article likely completes its discussion of the episode A, protesters storming the Legislaive Council, before episode B, “protesters vandalized the Legislative Chamber” (Figure 3). Hence, in order to partition articles into distinct episode segments, EpiMine must identify whether or not two consecutive segments are discussing the same or different episodes, which brings us to our next challenge. Challenge 2: Episodes contain semantically diverse actions. Each episode features a set of unique atomic actions, which we can utilize for determining whether or not two segments discuss the same episode. However, for clustering actions, existing methods [18 ] rely heavily on semantic similarity. This not realistic for episode-segment clustering: Example: “protesters spray-painted slogans” and “they unfurled the colonial-era flag” will fall under the same episode, but are semantically different and unlikely to be clustered. Alternatively, we can identify salient terms that reflect the same episode (“barriers” and “shoved” are unique to Episode A; “defaced” and “walls” for Episode B), by exploiting corpus-level signals. Specifically, if we frequently see “defaced” mentioned together with “walls” (or their respective synonyms) and not with other terms (and vice-versa), then we consider them a discriminative co-occurrence. Consequently, when such co-occurrences remain consistent across text segments, this indicates the same episode being discussed. Conversely, if a sufficient shift in the combination of terms occurs, then this indicates that a different episode is being discussed. Challenge 3: Articles often do not feature all episodes. Given the real-time nature of reporting, an article may feature only a subset or none of ground-truth episodes from a multi-day key event (e.g., focusing on a high-level analysis of the key event or mentioning other related key events). To minimize this noise, EpiMine seeks to identify the set of articles which maximizes the quantity and quality of potential episodes. It then merges any article partitions across these articles which likely discuss the same episode and employs a large language model (LLM) to provide a more fluent interpretation of the candidate episodes, accounting for the episode’s core entity, actions, object, location, and time period. This allows EpiMine to finally map the remaining non-salient article segments to these episodes, pruning any candidates which are not sufficiently supported by the remaining articles. We summarize our core contributions: (1) We introduce the novel task of episode detection, which takes in a key-event corpus and outputs multiple detected episodes and their related segments extracted from the corpus. (2) We propose a novel unsupervised episode detection method, EpiMine, which exploits discriminative term co-occurrences to estimate candidate episode partitions from top articles. It refines these candidate episodes using LLM-based reasoning to output a comprehensive and diverse set of episodes. (3) We construct three novel datasets, reflecting a diverse set of real-world themes and thirty global key events, as no large-scale key event-specific news corpus exists for this task where the key events are guaranteed to contain distinguishable episodes. (4) We compare EpiMine with five other baselines through an extensive set of experiments and case studies performed on our real-world datasets, demonstrating that it outperforms all baselines by, on average, a 59.2% increase across all metrics. Reproducibility: We provide our dataset and source code1 to facilitate further studies.	Unsupervised Episode Detection for Large-Scale News Events	https://arxiv.org/pdf/2104.05919	Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human information seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional  following event templates. We also compile a new document-level event extraction benchmark dataset WIKIEVENTS which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WIKIEVENTS datasets respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model’s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.	"By converting a large amount of unstructured text into trigger-argument structures, event extraction models provide unique value in assisting us process volumes of documents to form insights. While real-world events are often described throughout a news document (or even span multiple documents), the scope of operation for existing event extraction models have long been limited to the sentence level. Early work on event extraction originally posed the task as document level role filling (Grishman and Sundheim, 1996) on a set of narrow scenarios and evaluated on small datasets. The release of ACE2, a large scale dataset with complete event annotation, opened the possibility of applying powerful machine learning models which led to substantial improvement in event extraction. The success of such models and the widespread adoption of ACE as the training dataset established sentencelevel event extraction as the mainstream task defintion. This formulation signifies a misalignment between the information seeking behavior in real life and the exhaustive annotation process in creating the datasets. An information seeking session (Mai, 2016) can be divided into 6 stages: task initiation, topic selection, pre-focus exploration, focus information, information collection and search closure (Kuhlthau, 1991). Given a target event ontology, we can safely assume that topic selection is complete and users start from skimming the documents before they discover events of interest, focus on such events and then aggregate all relevant information for the events. In both the “pre-focus exploration” and “information collection” stages, users naturally cross sentence boundaries. Empirically, using sentence boundaries as event scopes conveniently simplifies the problem, but also introduces fundamental flaws: the resulting extractions are incomplete and uninformative. We show two examples of this phenomenon in Figure 1. The first example exemplifies the case of implicit arguments across sentences. The sentence that contains the PaymentBarter argument ""$280.32"" is not the sentence that contains the trigger ""reserve"" for the ExchangeBuySell event. Without a documentlevel model, such arguments would be missed and result in incomplete extraction. In the second example, the arguments are present in the same sentence, but written as pronouns. Such extraction would be uninformative to the reader without cross-sentence coreference resolution. We propose a new end-to-end document-level event argument extraction model by framing the problem as conditional generation given a template. Conditioned on the unfilled template and a given context, the model is asked to generate a filledin template with arguments as shown in Figure 2. Our model does not require entity recognition nor coreference resolution as a preprocessing step and can work with long contexts beyond single sentences. Since templates are usually provided as part of the event ontology definition, this requires no additional human effort. Compared to recent efforts (Du and Cardie, 2020; Feng et al., 2020; Chen et al., 2020) that retarget question answering (QA) models for event extraction, our generationbased model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning thresholds and can extract all arguments in a single pass. In order to evaluate the performance of document-level event extraction, we collect and annotate a new benchmark dataset WIKIEVENTS. This document-level evaluation also allows us to move beyond the nearest mention of the argument and instead seek the most informative mention3 in the entire document context. In particular, only 34.5% of the arguments detected in the same sentence as the trigger can be considered informative. We present this new task of document-level informative argument extraction and show that while this task requires much more cross-sentence infer- 3We prefer name mentions over nominal mentions and only use pronoun mentions when no other mentions exist. ence, our model can still perform reliably well. Since we provide the ontology information (which roles are needed for the event) through the template as an external condition, our model has excellent portability to unseen event types. By pairing up our argument extraction model with a keyword-based zero-shot trigger extraction model, we enable zero-shot transfer for new event types. The major contributions of this paper can be summarized as follows: 1. We address the document-level argument extraction task with an end-to-end neural event argument extraction model by conditional text generation. Our model does not rely on entity extraction nor entity/event coreference resolution. Compared to QA-based approaches, it can easily handle missing arguments and multiple arguments in the same role. 2. We present the first document-level event extraction benchmark dataset with complete event and coreference annotation. We also introduce the new document-level informative argument extraction task, which evaluates the ability of models to learn entity-event relations over long ranges. 3. We release the first end-to-end zero-shot event extraction framework by combining our argument extraction model with a zero-shot event trigger classification model."	Document-Level Event Argument Extraction by Conditional Generation	event analysis
https://arxiv.org/pdf/2408.04873	Episodic structures are inherently interpretable and adaptable to evolving large-scale key events. However, state-of-the-art automatic event detection methods overlook event episodes and, therefore, struggle with these crucial characteristics. This paper introduces a novel task, episode detection, aimed at identifying episodes from a news corpus containing key event articles. An episode describes a cohesive cluster of core entities (e.g., “protesters”, “police”) performing actions at a specific time and location. Furthermore, an episode is a significant part of a larger group of episodes under a particular key event. Automatically detecting episodes is challenging because, unlike key events and atomic actions, we cannot rely on explicit mentions of times and locations to distinguish between episodes or use semantic similarity to merge inconsistent episode co-references. To address these challenges, we introduce EpiMine, an unsupervised episode detection framework that (1) automatically identifies the most salient, key-event-relevant terms and segments, (2) determines candidate episodes in an article based on natural episodic partitions estimated through shifts in discriminative term combinations, and (3) refines and forms final episode clusters using large language model-based reasoning on the candidate episodes. We construct three diverse, real-world event datasets annotated at the episode level. EpiMine outperforms all baselines on these datasets by an average 59.2% increase across all metrics.	Given the saturation of real-time news accessible at our fingertips, reading and processing the critical information of a key event has become an increasingly daunting challenge. Consequently, research on automatic textual event detection has recently attempted to integrate the manner in which humans neurologically perceive/store events into textual event detection methods. Specifically, neuroscientists studying event representations in human memory find that events are stored in a top-to-bottom hierarchy, as demonstrated in Figure 1. The deeper the hierarchical event level, the more finegrained its corresponding text granularity [ 48 ]: we consider a theme as corpus-level (all articles discussing the 2019 Hong Kong Protests), key event as document-level (an article typically discusses a full one to two day key event), episode as segment-level, and atomic action as sentence or phrase-level. Furthermore, neurological research [3 , 21 ] indicates that events are encoded into memory as episodic structures. Representing events as discrete episodes helps us piece together a coherent narrative by considering the sequence of actions, reactions, and developments over time. This empowers several downstream tasks (e.g., event schema generation, event prediction), which may benefit from insights into the causes and consequences of events that might be missed with a more generalized analysis. Despite its strengths, existing automatic event extraction works fail to consider the episode-level. For example, key event detection specifically seeks to output “a set of thematically coherent documents” for each key event [ 29, 48]. However, it is challenging to manually parse through a large cluster of relevant articles in order to gain an efficient and compact understanding of a given event. To address the lack of interpretability of such article clusters, the adjacent task of timeline summarization [9 , 14 , 23, 39 ] aims to identify the dates and a compact summary for each key event. However, high-level timelines are typically applicable for historical themes and thus unrealistic for currently evolving key events where fine-grained timeline summarization is more suitable. Hence, event chain mining [ 18] attempts to address this by mining a series of temporally-ordered atomic actions at the phraselevel; however, the granularity level is often too fine-grained to properly represent a large-scale key event. Thus, we aim to tackle the novel task of episode detection to pave the way for a more effective event representation. Episode detection aims to detect episodes from a news corpus containing key event articles. An episode can be described as a cohesive cluster of subjects performing actions at a certain time and location, occurring as part of a larger sequence of episodes under a specific key event. Episode detection introduces a unique set of challenges, which we address using our novel framework, EpiMine. EpiMine is an unsupervised episode detection framework that automatically detects meaningful episodic events and their corresponding text segments in a large key event corpus, all without any level of human supervision or labeled training data. EpiMine is comprised of the following components: (1) discriminative co-occurrence detection, (2) episode partitioning, (3) candidate episode estimation, and (4) episode-segment classification. Collectively, they tackle the unique challenges of episode detection, detailed below: Challenge 1: Journalists do not timestamp episodes. Key event detection partitions a thematic corpus into document-level clusters by heavily relying on explicit temporal features, like publication dates [48 ]. For example, an article primarily discusses one key event, which can then be roughly mapped to the article’s publication date. However, this assumption fails at the episode-level, where there is no guarantee to have a distinct timestamp associated with each text segment that discusses a new episode. Fortunately, we can take advantage of the idea that journalists naturally partition news articles by sequentially discussing distinct episodes: Example: An article likely completes its discussion of the episode A, protesters storming the Legislaive Council, before episode B, “protesters vandalized the Legislative Chamber” (Figure 3). Hence, in order to partition articles into distinct episode segments, EpiMine must identify whether or not two consecutive segments are discussing the same or different episodes, which brings us to our next challenge. Challenge 2: Episodes contain semantically diverse actions. Each episode features a set of unique atomic actions, which we can utilize for determining whether or not two segments discuss the same episode. However, for clustering actions, existing methods [18 ] rely heavily on semantic similarity. This not realistic for episode-segment clustering: Example: “protesters spray-painted slogans” and “they unfurled the colonial-era flag” will fall under the same episode, but are semantically different and unlikely to be clustered. Alternatively, we can identify salient terms that reflect the same episode (“barriers” and “shoved” are unique to Episode A; “defaced” and “walls” for Episode B), by exploiting corpus-level signals. Specifically, if we frequently see “defaced” mentioned together with “walls” (or their respective synonyms) and not with other terms (and vice-versa), then we consider them a discriminative co-occurrence. Consequently, when such co-occurrences remain consistent across text segments, this indicates the same episode being discussed. Conversely, if a sufficient shift in the combination of terms occurs, then this indicates that a different episode is being discussed. Challenge 3: Articles often do not feature all episodes. Given the real-time nature of reporting, an article may feature only a subset or none of ground-truth episodes from a multi-day key event (e.g., focusing on a high-level analysis of the key event or mentioning other related key events). To minimize this noise, EpiMine seeks to identify the set of articles which maximizes the quantity and quality of potential episodes. It then merges any article partitions across these articles which likely discuss the same episode and employs a large language model (LLM) to provide a more fluent interpretation of the candidate episodes, accounting for the episode’s core entity, actions, object, location, and time period. This allows EpiMine to finally map the remaining non-salient article segments to these episodes, pruning any candidates which are not sufficiently supported by the remaining articles. We summarize our core contributions: (1) We introduce the novel task of episode detection, which takes in a key-event corpus and outputs multiple detected episodes and their related segments extracted from the corpus. (2) We propose a novel unsupervised episode detection method, EpiMine, which exploits discriminative term co-occurrences to estimate candidate episode partitions from top articles. It refines these candidate episodes using LLM-based reasoning to output a comprehensive and diverse set of episodes. (3) We construct three novel datasets, reflecting a diverse set of real-world themes and thirty global key events, as no large-scale key event-specific news corpus exists for this task where the key events are guaranteed to contain distinguishable episodes. (4) We compare EpiMine with five other baselines through an extensive set of experiments and case studies performed on our real-world datasets, demonstrating that it outperforms all baselines by, on average, a 59.2% increase across all metrics. Reproducibility: We provide our dataset and source code1 to facilitate further studies.	Unsupervised Episode Detection for Large-Scale News Events	https://arxiv.org/pdf/2206.04153	Automated event detection from news corpora is a crucial task towards mining fast-evolving structured knowledge. As real-world events have different granularities, from the top-level themes to key events and then to event mentions corresponding to concrete actions, there are generally two lines of research: (1) theme detection tries to identify from a news corpus major themes (e.g., “2019 Hong Kong Protests” versus “2020 U.S. Presidential Election”) which have very distinct semantics; and (2) action extraction aims to extract from a single document mention-level actions (e.g., “the police hit the left arm of the protester”) that are often too fine-grained for comprehending the real-world event. In this paper, we propose a new task, key event detection at the intermediate level, which aims to detect from a news corpus key events (e.g., HK Airport Protest on Aug. 12-14), each happening at a particular time/location and focusing on the same topic. This task can bridge event understanding and structuring and is inherently challenging because of (1) the thematic and temporal closeness of different key events and (2) the scarcity of labeled data due to the fast-evolving nature of news articles. To address these challenges, we develop an unsupervised key event detection framework, EvMine, that (1) extracts temporally frequent peak phrases using a novel ttf-itf score, (2) merges peak phrases into event-indicative feature sets by detecting communities from our designed peak phrase graph that captures document cooccurrences, semantic similarities, and temporal closeness signals, and (3) iteratively retrieves documents related to each key event by training a classifier with automatically generated pseudo labels from the event-indicative feature sets and refining the detected key events using the retrieved documents in each iteration. Extensive experiments and case studies show EvMine all the baseline methods and its ablations on two real-world news corpora.	"Automated real-world event discovery has long been studied to help people quickly digest explosive information. Researchers studying human memory find people tend to organize real-world events in a hierarchical way [7, 36 ], ranging from top-level themes (e.g., “2019 Hong Kong (HK) Protests”) to middle level key events (e.g., July 1 Storming Legislative Building), possibly to sub-middle level episodes (e.g., “Protester besieged the legislature”), and down to bottom level actions1 (e.g., “Riot police squirt pepper spray at protesters”). As shown in Figure 2, going up this event structure hierarchy leads to larger and more coarse-grained “events” whereas moving down the hierarchy brings in more fine-grained and concrete mentions of ""events"". The broad spectrum of ""events"", differing in duration and complexity, has fostered a variety of event discovery studies under different task names. One line of research, named Topic Detection and Tracking (TDT) [ 1, 3 , 37 ], aims to detect themes from an input corpus where each theme is represented by a cluster of documents, focusing on distinct thematic topics. For example, documents about “2019 Hong Kong Protests” are thematically very distinct from those of “2020 U.S. Presidential Election”. As a result, content-based document clustering methods [2, 30] can easily separate those themes. However, these methods cannot effectively distinguish the key events of the same/similar themes (e.g., identify documents about HK Legislative Building Storming and HK Airport Sit-In from a corpus related to “2019 HK protests”) [12 ]. Another line of work [ 8 , 10 , 23 , 34 ] is action extraction which tries to extract concrete actions (represented as text spans) from input documents. For example, one action extracted from the sentence in Figure 2 can be “Riot police squirt pepper spray at protesters”. These methods typically require a predefined event schema along with massive human-labeled documents for model learning. Besides, their output event mentions are highly redundant as one real-world event can usually be expressed in different ways in multiple documents, which further prevents humans from seeing the overall picture of the event. In this paper, we propose a new task, key event detection, which aims to detect key events from a corpus about one general event theme, or theme corpus. We assume each document has a publication date in the corpus, and each key event, as an aggregation of actions with concrete event time and focused location, is usually covered by a collection of documents. Since previous studies work on either too coarse or too fine-grained views of events, key event discovery, sitting in the intermediate level, plays an essential role in bridging the real-world event understanding and structuring. As shown in Figure 1, given a theme corpus about “2019 Hong Kong Protests”, we extract key events such as July 1st Storming Legislative Building and Aug. 12-14 Hong Kong International Airport Protest2, which helps people gain insights about the theme and thus compensates the previous TDT studies. Meanwhile, first detecting key events provides subsequent action extraction models with extra clues on what type of actions will most likely appear in each document. Furthermore, the identified key events can directly benefit many downstream tasks like timeline generation [ 13 ], evolutionary analysis [38], and query expansion [26]. Our proposed key event discovery task, while being useful, has its own challenges. First, compared to previous topic detection and tracking task, our task is intrinsically harder because it aims to distinguish key events of the same theme and those key events are often thematically similar and temporally closer to each other. Besides, as new events are happening every day, it is neither realistic nor scalable to curate all event schema in advance or label documents for training supervised event extraction models. To address the above challenges, we propose an unsupervised key event detection framework, EvMine, that requires no humanlabeled training data and can automatically discover key events from a large theme corpus. EvMine contains three major steps. First, we extract event-related “peak phrases” from input corpus based on 2All key events in this paper are manually named for easy understanding. our proposed “temporal term frequency–inverse time frequency” (ttf-itf) measure, where each peak phrase is unusually frequent on a day and thus likely indicates a key event. Second, as some key events can span multiple consecutive days and people have various ways to communicate the same key event, we group detected peak phrases into semantic clusters, which will serve as event-indicative features for selecting key event documents. Specifically, we propose a novel topic-time integrated peak phrase graph that considers document co-occurrence features, pre-trained masked language model (MLM) based semantic similarities, and temporal closeness, based on which we cluster peak phrases with a community detection algorithm. Each peak phrase cluster corresponds to one key event and provides event-indicative features. Finally, for each key event, we use the phrases in its corresponding peak phrase cluster to train a classifier that predicts whether a document is related to this key event. We also introduce a feedback loop that uses the current classification results to improve the phrase-based pseudo labels and find possibly missing key events, leading to an iterative document selection process with automatic refinement in each iteration. To summarize, our contributions are: (1) We introduce a new research problem key event detection, which takes a set of documents related to the same theme as inputs and outputs multiple important key events along with their associated documents. (2) We propose EvMine, a novel unsupervised framework for key event detection. EvMine automatically extracts temporally frequent peak phrases, clusters them with a graph-based method that combines thematic and temporal information, and applies document classification with iterative refinements to retrieve the most relevant documents for each key event. (3) We conduct quantitative evaluation, case studies, and parameter sensitivity analysis on two real-world event theme corpora, and EvMine outperforms all the baseline methods and its own ablations in terms of the ability to detect key events"	Unsupervised Key Event Detection from Massive Text Corpora	event granularities
https://arxiv.org/pdf/2201.06771	Topic taxonomies, which represent the latent topic (or category) structure of document collections, provide valuable knowledge of contents in many applications such as web search and information filtering. Recently, several unsupervised methods have been developed to automatically construct the topic taxonomy from a text corpus, but it is challenging to generate the desired taxonomy without any prior knowledge. In this paper, we study how to leverage the partial (or incomplete) information about the topic structure as guidance to find out the complete topic taxonomy. We propose a novel framework for topic taxonomy completion, named TaxoCom, which recursively expands the topic taxonomy by discovering novel sub-topic clusters of terms and documents. To effectively identify novel topics within a hierarchical topic structure, TaxoCom devises its embedding and clustering techniques to be closely-linked with each other: (i) locally discriminative embedding optimizes the text embedding space to be discriminative among known (i.e., given) sub-topics, and (ii) novelty adaptive clustering assigns terms into either one of the known sub-topics or novel sub-topics. Our comprehensive experiments on two real-world datasets demonstrate that TaxoCom not only generates the high-quality topic taxonomy in terms of term coherency and topic coverage but also outperforms all other baselines for a downstream task.	Finding the latent topic structure of an input text corpus, also known as hierarchical topic discovery [ 7, 18 , 33 , 42 , 47 ], has been one of the most important problems for information extraction and semantic analysis of text data. Recently, several studies have focused on topic taxonomy construction [ 33, 47], which aims to generate a tree-structured taxonomy whose node corresponds to a conceptual topic; each node of the topic taxonomy is defined as a cluster of semantically coherent terms representing a single topic. Compared to a conventional entity (or term-level) taxonomy, this cluster-level taxonomy is more appropriate for representing the topic hierarchy of the target corpus with high coverage and low redundancy. To identify hierarchical topic clusters of terms, they mainly performed clustering on a low-dimensional text embedding space where textual semantic information is effectively encoded. However, their output topic taxonomy seems plausible by itself but often fails to match with the complete taxonomy designed by a human curator, because they rely on only the text corpus in an unsupervised manner. To be specific, their quality (e.g., coverage and accuracy) highly depends on the number of sub-topic clusters (i.e., child nodes), which has to be manually controlled by a user. In addition, it is sensitive to the topic imbalance in the document collection, which makes it difficult to find out minor topics. In the absence of any information about the topic hierarchy, the unsupervised methods intrinsically become vulnerable to these problems. On the other hand, for some other text mining tasks or NLP applications, several recent studies have tried to take advantage of auxiliary information about the latent topic structure [ 13, 21, 23 – 26 , 34 ]. Most of them focus on utilizing a hierarchy of topic surface names as additional supervision, because it can be easily given as a user’s interests or prior knowledge. Specifically, they retrieve the top-𝐾 relevant terms to each topic [ 21 , 26 ] or train a hierarchical text classifier using unlabeled documents and the topic names [ 24 , 34 ]. Despite their effectiveness, their major limitation is that they are only able to consider the known topics included in the given topic hierarchy. That is, the coverage of the obtained results is strictly limited to the given topics. Since it is very challenging for a user to be aware of a full topic structure, a naive solution to incorporate a user-provided hierarchy of topic names into the topic taxonomy is likely to only partially cover the text corpus. To tackle this limitation, we introduce a new problem setting, named topic taxonomy completion, to construct a complete topic taxonomy by making use of additional topic information assumed to be partial or incomplete. Formally, given a text corpus and its partial hierarchy of topic names, this task aims to identify the term clusters for each topic, while discovering the novel topics that do not exist in the given hierarchy but exist in the corpus. Figure 1 illustrates a toy example of our task, where the novel topics (e.g, arts and hockey) are correctly detected and placed in the right position within the taxonomy. This task can be practically applied not only for the case that a user’s incomplete knowledge is available, but also for incremental management of the topic taxonomy. In case that the document collection is constantly growing, and so are their topics, the out-dated topic taxonomy of the previous snapshot can serve as the partial hierarchy to capture emerging topics. The technical challenges of this task can be summarized as follows. First, novel topics should be identified by considering the hierarchical semantic relationship among the topics. In Figure 1, the topic hockey is not novel in terms of the root node, because it obviously belongs to its known sub-topic sports. However, hockey should be detected as a novel sub-topic of sports as it does not belong to any of the known sport sub-categories (i.e., soccer and baseball). Second, the granularity of novel sub-topics and that of known sub-topics need to be kept similar with each other, to achieve the consistency of semantic specificity among sibling nodes. In Figure 1, the root node should insert a single novel sub-topic arts, rather than two novel sub-topics music and dance, based on the semantic specificity of its known sub-topics (i.e., politics and sports). In this work, we propose TaxoCom, a hierarchical topic discovery framework to complete the topic taxonomy by recursively identifying novel sub-topic clusters of terms. For each topic node, TaxoCom performs (i) text embedding and (ii) text clustering, to assign the terms into one of either the existing child nodes (i.e., known sub-topics) or newly-created child nodes (i.e., novel subtopics). It first optimizes locally discriminative embedding which enforces the discrimination among the known sub-topics [ 21 , 26 ] by using the given topic surface names; this helps to make a clear distinction between known and novel sub-topic clusters as well. Then, it performs novelty adaptive clustering which separately finds the clusters on novel-topic terms and known-topic terms, respectively. In particular, TaxoCom selectively assigns the terms into the child nodes, referred to as anchor terms, while filtering out general terms based on their semantic relevance and representativeness. Extensive experiments on real-world datasets demonstrate that TaxoCom successfully completes a topic taxonomy with missing (i.e., novel) topic nodes correctly inserted. Our human evaluation quantitatively validates the superiority of topic taxonomies generated by TaxoCom, in terms of the topic coverage as well as semantic coherence among the topic terms. Furthermore, TaxoCom achieves the best performance among all baseline methods for a downstream task, which trains a weakly supervised text classifier by using the topic taxonomy instead of document-level labels.	TaxoCom: Topic Taxonomy Completion with Hierarchical Discovery of Novel Topic Clusters	https://arxiv.org/pdf/2001.09522	Taxonomies consist of machine-interpretable semantics and provide valuable knowledge for many web applications. For example, online retailers (e.g., Amazon and eBay) use taxonomies for product recommendation, and web search engines (e.g., Google and Bing) leverage taxonomies to enhance query understanding. Enormous efforts have been made on constructing taxonomies either manually or semi-automatically. However, with the fast-growing volume of web content, existing taxonomies will become outdated and fail to capture emerging knowledge. Therefore, in many applications, dynamic expansions of an existing taxonomy are in great demand. In this paper, we study how to expand an existing taxonomy by adding a set of new concepts. We propose a novel self-supervised framework, named TaxoExpan, which automatically generates a set of ⟨query concept, anchor concept⟩ pairs from the existing taxonomy as training data. Using such self-supervision data, TaxoExpan learns a model to predict whether a query concept is the direct hyponym of an anchor concept. We develop two innovative techniques in TaxoExpan: (1) a position-enhanced graph neural network that encodes the local structure of an anchor concept in the existing taxonomy, and (2) a noise-robust training objective that enables the learned model to be insensitive to the label noise in the self-supervision data. Extensive experiments on three large-scale datasets from different domains demonstrate both the effectiveness and the efficiency of TaxoExpan for taxonomy expansion.	Taxonomies have been fundamental to organizing knowledge for centuries [ 45 ]. In today’s Web, taxonomies provide valuable knowledge to support many applications such as query understanding [ 17 ], content browsing [54], personalized recommendation [ 18, 63 ], and web search [ 29 , 53 ]. For example, many online retailers (e.g., eBay and Amazon) organize products into categories of different granularities, so that customers can easily search and navigate this category taxonomy to find the items they want to purchase. In addition, web search engines (e.g., Google and Bing) leverage a taxonomy to better understand user queries and improve the search quality. Existing taxonomies are mostly constructed by human experts or in a crowdsourcing manner. Such manual curations are timeconsuming, labor-intensive, and rarely complete. To reduce the human efforts, many automatic taxonomy construction methods [ 31, 41 , 60 ] are proposed. They first identify “is-A” relations (e.g., “iPad” is an “Electronics”) using textual patterns [16, 38] or distributional similarities [ 3, 43 ], and then organize extracted concept pairs into a directed acyclic graph (DAG) as the output taxonomy [10, 14 , 24]. As the web contents and human knowledge are constantly growing, people need to expand an existing taxonomy to include new emerging concepts. Most of previous methods, however, construct a taxonomy entirely from scratch and thus when we add new concepts, we have to re-run the entire taxonomy construction process. Although being intuitive, this approach has several limitations. First, many taxonomies have a top-level design provided by domain experts and such design shall be preserved. Second, a newly constructed taxonomy may not be consistent with the old one, which can lead to instabilities of its dependent downstream applications. Finally, as targeting the scenario of building taxonomy from scratch, most previous methods are unsupervised and cannot leverage signals from the existing taxonomy to construct a new one. In this paper, we study the taxonomy expansion task: given an existing taxonomy and a set of new emerging concepts, we aim to automatically expand the taxonomy to incorporate these new concepts (without changing the existing relations in the given taxonomy).1 Figure 1 shows an example where a taxonomy in computer science domain is expanded to include new subfields (e.g., “Quantum Computing”) and new techniques (e.g., “Meta Learning” and “UDA”). Some previous studies [21 , 22 , 39 ] attempt this task by using an additional set of labeled concepts with their true insertion positions in the existing taxonomy. However, such labeled data are usually small and thus forbid us from learning a more powerful model that captures the subsumption semantics in the existing taxonomy. We propose a novel framework named TaxoExpan to tackle the lack-of-supervision challenge. TaxoExpan formulates a taxonomy as a directed acyclic graph (DAG), automatically generates pseudotraining data from the existing taxonomy, and uses them to learn a matching model for expanding a given taxonomy. Specifically, we view each concept in the existing taxonomy as a query and one of its parent concepts as an anchor. This gives us a set of positive ⟨query concept, anchor concept⟩ pairs. Then, we generate negative pairs by sampling those concepts that are neither the descendants nor the direct parents of the query concept in the existing taxonomy. In Figure 1, for example, the ⟨“GPU ”, “Integrated Circuit”⟩ is a positive pair and ⟨“GPU ”, “Label Propagation”⟩ is a negative pair. We refer to these training pairs as self-supervision data, because they are procedurally generated from the existing taxonomy and no human curation is involved. To make the best use of above self-supervision data, we develop two novel techniques in TaxoExpan. The first one is a positionenhanced graph neural network (GNN) which encodes the local structure of an anchor concept using its ego network (egonet) in the existing taxonomy. If we view this anchor concept as the “parent” of the query concept, this ego network includes the potential “siblings” and “grand parents” of the query concept. We apply graph neural networks (GNNs) to model this ego network. However, regular GNNs fail to distinguish nodes with different relative positions to the query (i.e., some nodes are grand parents of the query while the others are siblings of the query). To address this limitation, we present a simple but effective enhancement to inject such position information into GNNs using position embedding. We show that such embedding can be easily integrated with existing GNN architectures (e.g., GCN [ 23 ] and GAT [ 50 ]) and significantly boosts the prediction performance. The second technique is a new noise-robust training scheme based on the InfoNCE loss [47 ]. Instead of predicting whether each individual ⟨query concept, anchor concept⟩ pair is positive or not, we first group all pairs sharing the same query concept into a single training instance and learn a model to select the positive pair among other negative ones from the group. We show that such training scheme is robust to the label noise and leads to performance gains. We test the effectiveness of TaxoExpan framework on three realworld taxonomies from different domains. Our results show that TaxoExpan can generate high-quality concept taxonomies in scientific domains and achieves state-of-the-art performance on the WordNet taxonomy expansion challenge [22]. Contributions. To summarize, our major contributions include: (1) a self-supervised framework that automatically expands existing taxonomies without manually labeled data; (2) an effective method for enhancing graph neural network by incorporating hierarchical positional information; (3) a new training objective that enables the learned model to be robust to label noises in self-supervision data; and (4) extensive experiments that verify both the effectiveness and the efficiency of TaxoExpan framework on three real-world large-scale taxonomies from different domains. The rest of the paper is organized as follows. Section 2 discusses the related work. Section 3 formalizes our problem. Then, we present our TaxoExpan framework in Section 4 and conduct experiments in Section 5. Finally, we conclude this paper in Section 6.	TaxoExpan: Self-supervised Taxonomy Expansion with Position-Enhanced Graph Neural Network	Taxonomy completion versus taxonomy expansion in weakly supervised settings
https://arxiv.org/pdf/2004.12832	Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.	Over the past few years, the Information Retrieval (IR) community has witnessed the introduction of a host of neural ranking models, including DRMM [7 ], KNRM [4, 36], and Duet [20 , 22 ]. In contrast to prior learning-to-rank methods that rely on hand-craed features, these models employ embedding-based representations of queries and documents and directly model local interactions (i.e., fine-granular relationships) between their contents. Among them, a recent approach has emerged that fine-tunes deep pre-trained language models (LMs) like ELMo [ 29 ] and BERT [5] for estimating relevance. By computing deeply-contextualized semantic representations of query–document pairs, these LMs help bridge the pervasive vocabulary mismatch [21, 42 ] between documents and queries [ 30 ]. Indeed, in the span of just a few months, a number of ranking models based on BERT have achieved state-of-the-art results on various retrieval benchmarks [ 3, 18 , 25 , 39 ] and have been proprietarily adapted for deployment by Google1 and Bing2. However, the remarkable gains delivered by these LMs come at a steep increase in computational cost. Hofst  ̈aer et al. [ 9] and MacAvaney et al. [ 18 ] observe that BERT-based models in the literature are 100-1000× more computationally expensive than prior models—some of which are arguably not inexpensive to begin with [ 13 ]. is quality–cost tradeoff is summarized by Figure 1, which compares two BERT-based rankers [25 , 27 ] against a representative set of ranking models. e figure uses MS MARCO Ranking [ 24 ], a recent collection of 9M passages and 1M queries from Bing’s logs. It reports retrieval effectiveness (MRR@10) on the official validation set as well as average query latency (log-scale) using a high-end server that dedicates one Tesla V100 GPU per query for neural re-rankers. Following the re-ranking setup of MS MARCO, ColBERT (re-rank), the Neural Matching Models, and the Deep LMs re-rank the MS MARCO’s official top-1000 documents per query. Other methods, including ColBERT (full retrieval), directly retrieve the top-1000 results from the entire collection. As the figure shows, BERT considerably improves search precision, raising MRR@10 by almost 7% against the best previous methods; simultaneously, it increases latency by up to tens of thousands of milliseconds even with a high-end GPU. is poses a challenging tradeoff since raising query response times by as lile as 100ms is known to impact user experience and even measurably diminish revenue [ 17 ]. To tackle this problem, recent work has started exploring using Natural Language Understanding (NLU) techniques to augment traditional retrieval models like BM25 [32 ]. For example, Nogueira et al. [ 26, 28] expand documents with NLU-generated queries before indexing with BM25 scores and Dai & Callan [2] replace BM25’s term frequency with NLU-estimated term importance. Despite successfully reducing latency, these approaches generally reduce precision substantially relative to BERT. To reconcile efficiency and contextualization in IR, we propose ColBERT, a ranking model based on contextualized late interaction over BERT. As the name suggests, ColBERT proposes a novel late interaction paradigm for estimating relevance between a query q and a document d. Under late interaction, q and d are separately encoded into two sets of contextual embeddings, and relevance is evaluated using cheap and pruning-friendly computations between both sets—that is, fast computations that enable ranking without exhaustively evaluating every possible candidate. Figure 2 contrasts our proposed late interaction approach with existing neural matching paradigms. On the le, Figure 2 (a) illustrates representation-focused rankers, which independently compute an embedding for q and another for d and estimate relevance as a single similarity score between two vectors [ 12 , 41 ]. Moving to the right, Figure 2 (b) visualizes typical interaction-focused rankers. Instead of summarizing q and d into individual embeddings, these rankers model word- and phrase-level relationships across q and d and match them using a deep neural network (e.g., with CNNs/MLPs [ 22 ] or kernels [ 36 ]). In the simplest case, they feed the neural network an interaction matrix that reflects the similiarity between every pair of words across q and d. Further right, Figure 2 (c) illustrates a more powerful interaction-based paradigm, which models the interactions between words within as well as across q and d at the same time, as in BERT’s transformer architecture [25]. ese increasingly expressive architectures are in tension. While interaction-based models (i.e., Figure 2 (b) and (c)) tend to be superior for IR tasks [8 , 21 ], a representation-focused model—by isolating the computations among q and d—makes it possible to precompute document representations offline [ 41 ], greatly reducing the computational load per query. In this work, we observe that the fine-grained matching of interaction-based models and the precomputation of document representations of representation-based models can be combined by retaining yet judiciously delaying the query–document interaction. Figure 2 (d) illustrates an architecture that precisely does so. As illustrated, every query embedding interacts with all document embeddings via a MaxSim operator, which computes maximum similarity (e.g., cosine similarity), and the scalar outputs of these operators are summed across query terms. is paradigm allows ColBERT to exploit deep LM-based representations while shiing the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., [1, 15]) to retrieve the top-k results directly from a large document collection, substantially improving recall over models that only re-rank the output of term-based retrieval. As Figure 1 illustrates, ColBERT can serve queries in tens or few hundreds of milliseconds. For instance, when used for reranking as in “ColBERT (re-rank)”, it delivers over 170× speedup (and requires 14,000× fewer FLOPs) relative to existing BERT-based models, while being more effective than every non-BERT baseline (§4.2 & 4.3). ColBERT’s indexing—the only time it needs to feed documents through BERT—is also practical: it can index the MS MARCO collection of 9M passages in about 3 hours using a single server with four GPUs (§4.5), retaining its effectiveness with a space footprint of as lile as few tens of GiBs. Our extensive ablation study (§4.4) shows that late interaction, its implementation via MaxSim operations, and crucial design choices within our BERTbased encoders are all essential to ColBERT’s effectiveness. Our main contributions are as follows. (1) We propose late interaction (§3.1) as a paradigm for efficient and effective neural ranking. (2) We present ColBERT (§3.2 & 3.3), a highly-effective model that employs novel BERT-based query and document encoders within the late interaction paradigm.(3) We show how to leverage ColBERT both for re-ranking on top of a term-based retrieval model (§3.5) and for searching a full collection using vector similarity indexes (§3.6). (4) We evaluate ColBERT on MS MARCO and TREC CAR, two recent passage search collections.	ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT	https://arxiv.org/pdf/1810.04805	We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).	Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016). There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017). Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a “masked language model” (MLM) pre-training objective, inspired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a “next sentence prediction” task that jointly pretrains text-pair representations. The contributions of our paper are as follows: • We demonstrate the importance of bidirectional pre-training for language representations. Unlike Radford et al. (2018), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. • We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. • BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at https://github.com/ google-research/bert.	BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding	Continual pretraining of Bert for retrieval tasks
https://arxiv.org/pdf/2103.00113	Anomaly detection on attributed networks attracts considerable research interests due to wide applications of attributed networks in modeling a wide range of complex systems. Recently, the deep learning-based anomaly detection methods have shown promising results over shallow approaches, especially on networks with high-dimensional attributes and complex structures. However, existing approaches, which employ graph autoencoder as their backbone, do not fully exploit the rich information of the network, resulting in suboptimal performance. Furthermore, these methods do not directly target anomaly detection in their learning objective and fail to scale to large networks due to the full graph training mechanism. To overcome these limitations, in this paper, we present a novel contrastive self-supervised learning framework for anomaly detection on attributed networks. Our framework fully exploits the local information from network data by sampling a novel type of contrastive instance pair, which can capture the relationship between each node and its neighboring substructure in an unsupervised way. Meanwhile, a well-designed graph neural network-based contrastive learning model is proposed to learn informative embedding from high-dimensional attributes and local structure and measure the agreement of each instance pairs with its outputted scores. The multi-round predicted scores by the contrastive learning model are further used to evaluate the abnormality of each node with statistical estimation. In this way, the learning model is trained by a specific anomaly detection-aware target. Furthermore, since the input of the graph neural network module is batches of instance pairs instead of the full network, our framework can adapt to large networks flexibly. Experimental results show that our proposed framework outperforms the state-of-the-art baseline methods on all seven benchmark datasets.	Attributed networks (a.k.a. attributed graphs), where nodes with attributes indicate real-world entities and links indicate the relationship between entities, are ubiquitous in various scenarios, including finance (trading networks) [1], social media (social networks) [2], [3], and e-commerce (itemuser networks) [4], [5]. To utilize attributed network data to solve practical problems, a wide variety of graph analysis tasks have attracted significant research interests in recent years, such as node classification [6], [7], graph classification [8], [9], and link prediction [10], [11]. Among these tasks, anomaly detection task on attributed networks is a vital research problem. Aiming to detect the instances that significantly deviate from the majority of instances [12] (in attributed networks, the data instances are nodes generally), anomaly detection has significant implications in many security-related applications, e.g., fraud detection and social spam detection [13]. However, detecting anomalies effectively on attributed networks is not trivial due to the diversity of anomalies and the lack of supervision. Since attributed networks have both attribute information as well as structural information, they usually contain different types of anomalies. Figure 1 provides an example to illustrate two basic types of anomalies: structural anomaly and contextual anomaly. The attribute information of the structural anomalies is often normal, while they have several abnormal links to other nodes. The contextual anomalies, differently, have natural neighboring structures but their attributes are corrupted (noisy or entirely different from all neighbors). Such diversity makes it difficult to apply anomaly detection methods for attribute-only data (e.g., OC-SVM [14]) or plain networks (e.g., LOF [15]) to attributed networks directly. Therefore, an efficient anomaly detection approach should consider multiple patterns of anomalies. Moreover, resulting from the prohibitive cost for accessing ground-truth labels of anomalies, anomaly detection on attributed networks is predominately carried out in an unsupervised manner [13], [16]. That is to say, the algorithm has to conclude the normal pattern of data from the corrupted networks without supervision. Hence, a key is to fully and reasonably exploit existing information from attributed network data. Recently, various methods have been proposed to deal with the anomaly detection task for attributed networks. The shallow methods, including AMEN [16], Radar [17] and ANOMALOUS [18], leverage shallow learning mechanisms (e.g. ego-network analysis, residual analysis or CUR decomposition) to detect anomalies. Unfortunately, these models cannot fully address the computational challenge on attributed networks and fail to capture the complex interactions between different information modalities due to limitations of shallow mechanisms, especially when the feature is high-dimensional [13]. With the rocketing growth of deep learning for anomaly detection [12], [19], [20], [21], researchers also present deep neural networks-based methods to solve the anomaly detection problem on attributed networks. DOMINANT [13] is one of the representative methods. It constructs a graph autoencoder to reconstruct the attribute and structure information simultaneously, and the abnormality is evaluated by reconstruction error. SpecAE [22] also leverages graph autoencoder to extract low-dimensional embedding, and carries out detection via density estimation. Although existing deep learning-based methods [13], [22] have achieved considerable performance for anomaly detection on graphs, they still have several shortcomings, largely attributed to the autoencoder backbone in their architectures. First, autoencoders aim to learn the latent representation by reconstructing the original data instead of detecting the anomaly itself. Although the anomaly scores can be computed according to reconstruction errors [13], this kind of methods can only achieve suboptimal performance due to the fact that they do not target directly the anomaly detection objective. Second, autoencoder-based methods may not able to fully exploit the rich information of the attributed graph for effective graph representation learning. Specifically, autoencoders simply rebuild the original data and they do not have any refinement for data. However, recent works [23], [24], [25] have shown that more useful information can be mined in an unsupervised way if we design certain pretext tasks carefully based on augmented data. Third, graph autoencoder is the bottleneck to carry out anomaly detection on largescale networks. Generally, the graph convolution operation in graph autoencoder needs to input and reconstruct the full networked data, which is unfeasible due to the explosive memory requirements when the network is large. As an alternative unsupervised learning technique, selfsupervised contrastive learning is a promising solution to address the aforementioned limitations. By learning to contrast the elaborate instance pairs, the model can acquire informative knowledge without manual labels. Contrastive self-supervised learning has nice properties for anomaly detection task. First, contrastive learning mainly studies the matching of pairs of instances, which offers helpful information for anomaly detection. For the normal instance in graphs, there is a potential matching pattern between each node and its neighbors, e.g., the homophily hypothesis. The anomalies, on the opposite, often present when there is an inconsistency/mismatch between attributes and structure, which violates the original matching pattern of networks. Moreover, different types of anomalies have different manners of mismatching: in Figure 1, the structural anomaly has individual abnormal links with uncorrelated nodes, which is partial inconsistency; the contextual anomaly, differently, has mismatched attributes with all neighbors. Contrastive learning, naturally, is capable to learn the matching patterns and capture various mismatching patterns via its intrinsic discriminative mechanism. Second, contrastive learning models provide a specific predicted score to measure the agreement between the elements in each instance pair, and the scale is highly related to the abnormality of instance. Since anomaly detection methods usually output a list of scores or a ranking to represent the abnormality of each node, the predicted scores of contrastive learning model can be utilized for anomaly detection directly. In this way, we can train the model via an objective that is highly relevant to anomaly detection. In this paper, we propose a novel Contrastive self-supervised Learning framework for Anomaly detection on attributed networks (CoLA for abbreviation). By sampling the welldesigned instance pairs from the full network and using them to train the contrastive learning model, the information of network is exploited better. Concretely, our framework focuses on modeling the relationship between each node and its partial neighboring substructure, which can expose the various type of anomalies within networks. Meanwhile, our CoLA framework is trained with a direct target to assist the anomaly detection task. We set the learning objective of our model to discriminate the agreement between the elements within the instance pairs, and the results can be further used to evaluate the abnormality of nodes. Besides, by splitting the network into separated lightweight instance pairs, our anomaly detection framework is compatible with large-scale networks. Specifically, our framework does not need to run graph convolution on full networks, so it successfully avoids the memory explosion problem. To summarize, the main contributions are as follows: We propose a contrastive self-supervised learning framework, CoLA, for the anomaly detection problem on attributed networks. To the best of our knowledge, this is the first contrastive self-supervised learning-based method for graph anomaly detection.IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 3 We present a novel type of contrastive instance pair, “target node v.s. local subgraph”, for attributed networks to adapt to the anomaly detection task, which efficiently captures the local information of a node and its neighboring substructure. We design a contrastive learning model to learn the representative information from the node-subgraph instance pairs and provide discriminative scores for abnormality ranking. The proposed learning model is friendly to largescale networked data. We conduct extensive experiments on various datasets to demonstrate the effectiveness of CoLA and its superiority compared with a range of baseline methods. The rest of this paper is organized as follows. In Section II, we first review the related works. Then, the preliminary definitions and notations are introduced in Section III. Section IV illustrates the overall pipeline and the components of our framework in detail. After that, we analyze the experimental results in Section V and then conclude our work in section VI.	Anomaly Detection on Attributed Networks via Contrastive Self-Supervised Learning	https://arxiv.org/pdf/2310.14525	Graph contrastive learning (GCL) has emerged as a representative graph self-supervised method, achieving significant success. The currently prevalent optimization objective for GCL is InfoNCE. Typically, it employs augmentation techniques to obtain two views, where a node in one view acts as the anchor, the corresponding node in the other view serves as the positive sample, and all other nodes are regarded as negative samples. The goal is to minimize the distance between the anchor node and positive samples and maximize the distance to negative samples. However, due to the lack of label information during training, InfoNCE inevitably treats samples from the same class as negative samples, leading to the issue of false negative samples. This can impair the learned node representations and subsequently hinder performance in downstream tasks. While numerous methods have been proposed to mitigate the impact of false negatives, they still face various challenges. For instance, while increasing the number of negative samples can dilute the impact of false negatives, it concurrently increases computational burden. Thus, we propose GraphRank, a simple yet efficient graph contrastive learning method that addresses the problem of false negative samples by redefining the concept of negative samples to a certain extent, thereby avoiding the issue of false negative samples. The effectiveness of GraphRank is empirically validated through experiments on the node, edge, and graph level tasks.	Graph Neural Networks (GNNs) have become the standard approach for handling graph data, given their ability to leverage the underlying structure and features of graphs for effective analysis. Albeit the immense success achieved by supervised or semisupervised GNNs [ 16 , 31 ] across numerous application domains, their effectiveness is tied to the availability of labeled data for learning robust and impactful node representations. However, obtaining labeled data in real-world scenarios is a costly and timeconsuming endeavor, often constraining the availability of such data in many applications. Consequently, to mitigate this reliance on label data, graph self-supervised learning is attracting increasing attention, with graph contrastive learning emerging as the predominant method. Graph Contrastive Learning (GCL) typically starts with generating several views of a given graph through augmentation techniques. From these different views, one view serves as the anchor, with corresponding nodes in other views as positive examples and all other nodes as negative samples. The goal of GCL is then to bring the positive samples closer to the anchor in the representation space while pushing the negative samples further apart. Among various GCL approaches, InfoNCE [ 25 , 45, 46 ] has been recognized as the most commonly used optimization goal. It upholds the principle of minimizing distances between positive pairs and maximizing those between negative pairs, leveraging the contrastive nature of learning based on their representations. GRACE [ 45] relies on hybrid feature augmentations including node feature masking and edge dropping. Based on this data augmentation strategy, GCA [46 ] further introduces an adaptive augmentation for graph-structured data and make a competitive performance. GraphCL [ 39 ] further extends to graph-level representation to pull two views closer. However, InfoNCE experiences the issue of false negative samples [5, 23 ]. Its optimization objective is to minimize the distance with the positive samples and maximize the distance with the negative samples. Given that InfoNCE treats all nodes except the anchor node as negative samples, it unavoidably treats nodes of the same class as the anchor node as negative samples, which are referred to as false negative samples. Such false negative samples can impair the learned node representation and hinder downstream tasks. As shown in Figure 1, GRACE, a representative graph contrastive learning method using InfoNCE loss, is used to validate the issue of false negative samples across three academic citation network datasets. During the training of the GRACE method, we artificially removed the false negatives, which led to substantial performance improvements across all three datasets. However, in practical scenarios, there is a lack of data label information at the time of training, preventing manual removal of false negative samples based on label information. Therefore, in order to alleviate the issue of false negative samples and improve the performance of graph contrastive learning methods, many works have been explored, which are mainly in three ways. Firstly, increasing the number of negative samples helps dilute the impact of false negative samples. In cases where the quantities of various types of nodes are relatively balanced, the proportion of nodes in the same class is less, and increasing the number of negative samples can alleviate the issue of false negative samples. However, an increase in the number of negative samples bears computational and storage burdens, and as shown in Figure 1, GRACE, even when using all available negative samples, still experiences a notable false negative sample issue. Secondly, some works have proposed mechanisms for screening negative samples to attempt to remove false negative samples, thus improving the quality of negative samples. AUGCL [4] establishes a discriminative model based on collective affinity information to assess the uncertainty of negative samples, thereby facilitating the filtration of negative samples. Additionally, [ 23 ] designes a mechanism based on node similarity to sample high-quality positive and negative samples. Although a negative sample screening mechanism can effectively reduce the sampling of false negative samples, it necessitates the design of a complex and intricate screening mechanism to ensure the selection of high-quality negative samples. This, in turn, would introduce additional computational overhead. Lastly, some works have decided to forego the use of negative samples by employing contrastive learning methods that do not use negative samples, thereby avoiding the issue of false negative samples altogether. BGRL [ 30 ] is a contrastive learning method that does not require negative samples. It obtains two views through augmentation techniques; one view is used for learning the online representation, and the other view is used for learning the target representation. Updates are conducted by maximizing the similarity between these two views. However, its success relies on a relatively complex training strategy, specifically requiring a dual-encoder scheme with momentum update and exponential moving average to stabilize the training process. In light of the shortcomings of these graph contrastive methods above, we propose a new framework for graph self-supervised learning called GraphRank. The GraphRank framework involves generating two augmented graph views by applying random masks to nodes and edges. Subsequently,we utilize a GNN as the encoder and employ rank loss as the objective function for training. Specifically, we select a node 𝑣𝑖 as the target node in view 1, the node 𝑣+ 𝑖 corresponding to it in view 2 as a positive sample, and then randomly pick a node 𝑣 𝑗 from view 2 as a negative sample. The representations of these nodes are derived by the encoder, and then the similarity between the target node and the positive and negative samples are calculated accordingly. By employing rank loss as the objective function, our aim is to ensure that the similarity between the target node and the positive samples is greater than the similarity between the target node and the negative samples. GraphRank can effectively address the problems mentioned above. Firstly, a simple random mask approach is applied to GraphRank to obtain augmented graph data, which does not require sophisticatedly designed graph augmentation techniques to obtain highquality augmented graph data, nor does it require a complicated training strategy to stabilize the training. Secondly, we use rank loss as the objective function. Similar to the contrastive loss, e.g. InfoNCE, rank loss also endeavors to maximize the agreement between the target node and the positive sample. Different from contrastive loss, the purpose of rank loss is to make the similarity between the target node and the positive samples greater than the similarity between the target node and the negative samples, rather than separating the target node from the negative samples as much as possible, as in InfoNCE. Therefore, the rank loss would not separate the negative samples as far apart as possible, even if the negative samples selected were false negative samples. Finally, the calculation of rank loss involves only one positive and one negative sample resulting in a smaller computational overload compared to contrastive losses like InfoNCE. As a result, rank loss exhibits better scalability, making it more feasible for large-scale applications compared to typical contrastive losses.	Graph Ranking Contrastive Learning: A Extremely Simple yet Efficient Method	contrastive learning on graphs
https://arxiv.org/pdf/2010.03768	Given a simple request like Put a washed apple in the kitchen fridge, humans can reason in purely abstract terms by imagining action sequences and scoring their likelihood of success, prototypicality, and efficiency, all without moving a muscle. Once we see the kitchen in question, we can update our abstract plans to fit the scene. Embodied agents require the same abilities, but existing work does not yet provide the infrastructure necessary for both reasoning abstractly and executing concretely. We address this limitation by introducing ALFWorld, a simulator that enables agents to learn abstract, text based policies in TextWorld (Côté et al., 2018) and then execute goals from the ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment. ALFWorld enables the creation of a new BUTLER agent whose abstract knowledge, learned in TextWorld, corresponds directly to concrete, visually grounded actions. In turn, as we demonstrate empirically, this fosters better agent generalization than training only in the visually grounded environment. BUTLER's simple, modular design factors the problem to allow researchers to focus on models for improving every piece of the pipeline (language understanding, planning, navigation, and visual scene understanding).	Consider helping a friend prepare dinner in an unfamiliar house: when your friend asks you to clean and slice an apple for an appetizer, how would you approach the task? Intuitively, one could reason abstractly: (1) find an apple (2) wash the apple in the sink (3) put the clean apple on the cutting board (4) find a knife (5) use the knife to slice the apple (6) put the slices in a bowl. Even in an unfamiliar setting, abstract reasoning can help accomplish the goal by leveraging semantic priors. Priors like locations of objects – apples are commonly found in the kitchen along with implements for cleaning and slicing, object affordances – a sink is useful for washing an apple unlike a refrigerator, pre-conditions – better to wash an apple before slicing it, rather than the converse. We hypothesize that, learning to solve tasks using abstract language, unconstrained by the particulars of the physical world, enables agents to complete embodied tasks in novel environments by leveraging the kinds of semantic priors that are exposed by abstraction and interaction. To test this hypothesis, we have created the novel ALFWorld framework, the first interactive, parallel environment that aligns text descriptions and commands with physically embodied robotic simulation. We build ALFWorld by extending two prior works: TextWorld (Côté et al., 2018) - an engine for interactive text-based games, and ALFRED (Shridhar et al., 2020) - a large scale dataset for visionlanguage instruction following in embodied environments. ALFWorld provides two views of the same underlying world and two modes by which to interact with it: TextWorld, an abstract, text-based environment, generates textual observations of the world and responds to high-level text actions; ALFRED, the embodied simulator, renders the world in high-dimensional images and responds to low-level physical actions as from a robot (Figure 1).1 Unlike prior work on instruction following (MacMahon et al., 2006; Anderson et al., 2018a), which typically uses a static corpus of cross-modal expert demonstrations, we argue that aligned parallel environments like ALFWorld offer a distinct advantage: they allow agents to explore, interact, and learn in the abstract environment of language before encountering the complexities of the embodied environment. While fields such as robotic control use simulators like MuJoCo (Todorov et al., 2012) to provide infinite data through interaction, there has been no analogous mechanism – short of hiring a human around the clock – for providing linguistic feedback and annotations to an embodied agent. TextWorld addresses this discrepancy by providing programmatic and aligned linguistic signals during agent exploration. This facilitates the first work, to our knowledge, in which an embodied agent learns the meaning of complex multi-step policies, expressed in language, directly through interaction. Empowered by the ALFWorld framework, we introduce BUTLER (Building Understanding in Textworld via Language for Embodied Reasoning), an agent that first learns to perform abstract tasks in TextWorld using Imitation Learning (IL) and then transfers the learned policies to embodied tasks in ALFRED. When operating in the embodied world, BUTLER leverages the abstract understanding gained from TextWorld to generate text-based actions; these serve as high-level subgoals that facilitate physical action generation by a low-level controller. Broadly, we find that BUTLER is capable of generalizing in a zero-shot manner from TextWorld to unseen embodied tasks and settings. Our results show that training first in the abstract text-based environment is not only 7× faster, but also yields better performance than training from scratch in the embodied world. These results lend credibility to the hypothesis that solving abstract language-based tasks can help build priors that enable agents to generalize to unfamiliar embodied environments. Our contributions are as follows: § 2 ALFWorld environment: The first parallel interactive text-based and embodied environment. § 3 BUTLER architecture: An agent that learns high-level policies in language that transfer to low-level embodied executions, and whose modular components can be independently upgraded. § 4 Generalization: We demonstrate empirically that BUTLER, trained in the abstract text domain, generalizes better to unseen embodied settin	ALFWorld: Aligning Text and Embodied Environments for Interactive Learning	https://arxiv.org/pdf/2010.03768	Given a simple request like Put a washed apple in the kitchen fridge, humans can reason in purely abstract terms by imagining action sequences and scoring their likelihood of success, prototypicality, and efficiency, all without moving a muscle. Once we see the kitchen in question, we can update our abstract plans to fit the scene. Embodied agents require the same abilities, but existing work does not yet provide the infrastructure necessary for both reasoning abstractly and executing concretely. We address this limitation by introducing ALFWorld, a simulator that enables agents to learn abstract, text based policies in TextWorld (Côté et al., 2018) and then execute goals from the ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment. ALFWorld enables the creation of a new BUTLER agent whose abstract knowledge, learned in TextWorld, corresponds directly to concrete, visually grounded actions. In turn, as we demonstrate empirically, this fosters better agent generalization than training only in the visually grounded environment. BUTLER's simple, modular design factors the problem to allow researchers to focus on models for improving every piece of the pipeline (language understanding, planning, navigation, and visual scene understanding).	Consider helping a friend prepare dinner in an unfamiliar house: when your friend asks you to clean and slice an apple for an appetizer, how would you approach the task? Intuitively, one could reason abstractly: (1) find an apple (2) wash the apple in the sink (3) put the clean apple on the cutting board (4) find a knife (5) use the knife to slice the apple (6) put the slices in a bowl. Even in an unfamiliar setting, abstract reasoning can help accomplish the goal by leveraging semantic priors. Priors like locations of objects – apples are commonly found in the kitchen along with implements for cleaning and slicing, object affordances – a sink is useful for washing an apple unlike a refrigerator, pre-conditions – better to wash an apple before slicing it, rather than the converse. We hypothesize that, learning to solve tasks using abstract language, unconstrained by the particulars of the physical world, enables agents to complete embodied tasks in novel environments by leveraging the kinds of semantic priors that are exposed by abstraction and interaction. To test this hypothesis, we have created the novel ALFWorld framework, the first interactive, parallel environment that aligns text descriptions and commands with physically embodied robotic simulation. We build ALFWorld by extending two prior works: TextWorld (Côté et al., 2018) - an engine for interactive text-based games, and ALFRED (Shridhar et al., 2020) - a large scale dataset for visionlanguage instruction following in embodied environments. ALFWorld provides two views of the same underlying world and two modes by which to interact with it: TextWorld, an abstract, text-based environment, generates textual observations of the world and responds to high-level text actions; ALFRED, the embodied simulator, renders the world in high-dimensional images and responds to low-level physical actions as from a robot (Figure 1).1 Unlike prior work on instruction following (MacMahon et al., 2006; Anderson et al., 2018a), which typically uses a static corpus of cross-modal expert demonstrations, we argue that aligned parallel environments like ALFWorld offer a distinct advantage: they allow agents to explore, interact, and learn in the abstract environment of language before encountering the complexities of the embodied environment. While fields such as robotic control use simulators like MuJoCo (Todorov et al., 2012) to provide infinite data through interaction, there has been no analogous mechanism – short of hiring a human around the clock – for providing linguistic feedback and annotations to an embodied agent. TextWorld addresses this discrepancy by providing programmatic and aligned linguistic signals during agent exploration. This facilitates the first work, to our knowledge, in which an embodied agent learns the meaning of complex multi-step policies, expressed in language, directly through interaction. Empowered by the ALFWorld framework, we introduce BUTLER (Building Understanding in Textworld via Language for Embodied Reasoning), an agent that first learns to perform abstract tasks in TextWorld using Imitation Learning (IL) and then transfers the learned policies to embodied tasks in ALFRED. When operating in the embodied world, BUTLER leverages the abstract understanding gained from TextWorld to generate text-based actions; these serve as high-level subgoals that facilitate physical action generation by a low-level controller. Broadly, we find that BUTLER is capable of generalizing in a zero-shot manner from TextWorld to unseen embodied tasks and settings. Our results show that training first in the abstract text-based environment is not only 7× faster, but also yields better performance than training from scratch in the embodied world. These results lend credibility to the hypothesis that solving abstract language-based tasks can help build priors that enable agents to generalize to unfamiliar embodied environments. Our contributions are as follows: § 2 ALFWorld environment: The first parallel interactive text-based and embodied environment. § 3 BUTLER architecture: An agent that learns high-level policies in language that transfer to low-level embodied executions, and whose modular components can be independently upgraded. § 4 Generalization: We demonstrate empirically that BUTLER, trained in the abstract text domain, generalizes better to unseen embodied settin	Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling	multi-model embodied agents + environments
